<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>常见的消息队列有哪些</title>
    <link href="/posts/3315347902/"/>
    <url>/posts/3315347902/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  参考：</p><p>  <a href="https://javaguide.cn/high-performance/message-queue/message-queue.html#%E5%B8%B8%E8%A7%81%E7%9A%84%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E6%9C%89%E5%93%AA%E4%BA%9B">常见的消息队列有哪些？ - JavaGuide（√）</a></p></blockquote><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>Kafka 是 LinkedIn 开源的一个分布式流式处理平台，已经成为 Apache 顶级项目，早期被用来用于处理海量的日志，后面才慢慢发展成了一款功能全面的高性能消息队列。</p><p>Kafka 是一个分布式系统，由通过高性能 TCP 网络协议进行通信的服务器和客户端组成，可以部署在在本地和云环境中的裸机硬件、虚拟机和容器上。</p><hr><p>流式处理平台具有三个关键功能：</p><p>1.消息队列：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。</p><p>2.容错的持久方式存储记录消息流： Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。</p><p>3.流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。</p><hr><p>在 Kafka 2.8 之前，Kafka 最被大家诟病的就是其重度依赖于 Zookeeper 做元数据管理和集群的高可用。在 Kafka 2.8 之后，引入了基于 Raft 协议的 KRaft 模式，不再依赖 Zookeeper，大大简化了 Kafka 的架构，让你可以以一种轻量级的方式来使用 Kafka。</p><p>不过，要提示一下：如果要使用 KRaft 模式的话，建议选择较高版本的 Kafka，因为这个功能还在持续完善优化中。Kafka 3.3.1 版本是第一个将 KRaft（Kafka Raft）共识协议标记为生产就绪的版本。</p><hr><p>Kafka 官网：<a href="http://kafka.apache.org/">http://kafka.apache.org/</a></p><p>Kafka 更新记录（可以直观看到项目是否还在维护）：<a href="https://kafka.apache.org/downloads">https://kafka.apache.org/downloads</a></p><h2 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h2><p>RocketMQ 是阿里开源的一款云原生“消息、事件、流”实时数据处理平台，借鉴了 Kafka，已经成为 Apache 顶级项目。</p><p>Apache RocketMQ 自诞生以来，因其架构简单、业务功能丰富、具备极强可扩展性等特点被众多企业开发者以及云厂商广泛采用。历经十余年的大规模场景打磨，RocketMQ 已经成为业内共识的金融级可靠业务消息首选方案，被广泛应用于互联网、大数据、移动互联网、物联网等领域的业务场景。</p><hr><p>RocketMQ 的核心特性（摘自 RocketMQ 官网）：</p><ul><li>云原生：生与云，长与云，无限弹性扩缩，K8s 友好</li><li>高吞吐：万亿级吞吐保证，同时满足微服务与大数据场景。</li><li>流处理：提供轻量、高扩展、高性能和丰富功能的流计算引擎。</li><li>金融级：金融级的稳定性，广泛用于交易核心链路。</li><li>架构极简：零外部依赖，Shared-nothing 架构。</li><li>生态友好：无缝对接微服务、实时计算、数据湖等周边生态。</li></ul><hr><p>RocketMQ 官网：<a href="https://rocketmq.apache.org/">https://rocketmq.apache.org/</a> （文档很详细，推荐阅读）</p><p>RocketMQ 更新记录（可以直观看到项目是否还在维护）：<a href="https://github.com/apache/rocketmq/releases">https://github.com/apache/rocketmq/releases</a></p><h2 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h2><p>RabbitMQ 是采用 Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议）的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。</p><hr><p>RabbitMQ 发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ 的具体特点可以概括为以下几点：</p><ul><li><strong>可靠性：</strong> RabbitMQ 使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。</li><li><strong>灵活的路由：</strong> 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们讲 RabbitMQ 核心概念的时候详细介绍到。</li><li><strong>扩展性：</strong> 多个 RabbitMQ 节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。</li><li><strong>高可用性：</strong> 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。</li><li><strong>支持多种协议：</strong> RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT 等多种消息中间件协议。</li><li><strong>多语言客户端：</strong> RabbitMQ 几乎支持所有常用语言，比如 Java、Python、Ruby、PHP、C#、JavaScript 等。</li><li><strong>易用的管理界面：</strong> RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装 RabbitMQ 的时候会介绍到，安装好 RabbitMQ 就自带管理界面。</li><li><strong>插件机制：</strong> RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似 Dubbo 的 SPI 机制</li></ul><hr><p>RabbitMQ 官网：<a href="https://www.rabbitmq.com/">https://www.rabbitmq.com/</a> 。</p><p>RabbitMQ 更新记录（可以直观看到项目是否还在维护）：<a href="https://www.rabbitmq.com/news.html">https://www.rabbitmq.com/news.html</a></p><h2 id="Pulsar"><a href="#Pulsar" class="headerlink" title="Pulsar"></a>Pulsar</h2><p>Pulsar 是下一代云原生分布式消息流平台，最初由 Yahoo 开发 ，已经成为 Apache 顶级项目。</p><p>Pulsar 集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性，被看作是云原生时代实时消息流传输、存储和计算最佳解决方案。</p><hr><p>Pulsar 的关键特性如下（摘自官网）：</p><ul><li>是下一代云原生分布式消息流平台。</li><li>Pulsar 的单个实例原生支持多个集群，可跨机房在集群间无缝地完成消息复制。</li><li>极低的发布延迟和端到端延迟。</li><li>可无缝扩展到超过一百万个 topic。</li><li>简单的客户端 API，支持 Java、Go、Python 和 C++。</li><li>主题的多种订阅模式（独占、共享和故障转移）。</li><li>通过 Apache BookKeeper 提供的持久化消息存储机制保证消息传递 。</li><li>由轻量级的 serverless 计算框架 Pulsar Functions 实现流原生的数据处理。</li><li>基于 Pulsar Functions 的 serverless connector 框架 Pulsar IO 使得数据更易移入、移出 Apache Pulsar。</li><li>分层式存储可在数据陈旧时，将数据从热存储卸载到冷&#x2F;长期存储（如 S3、GCS）中。</li></ul><hr><p>Pulsar 官网：<a href="https://pulsar.apache.org/">https://pulsar.apache.org/</a></p><p>Pulsar 更新记录（可以直观看到项目是否还在维护）：<a href="https://github.com/apache/pulsar/releases">https://github.com/apache/pulsar/releases</a></p><h2 id="ActiveMQ"><a href="#ActiveMQ" class="headerlink" title="ActiveMQ"></a>ActiveMQ</h2><p>目前已经被淘汰，不推荐使用，不建议学习。</p>]]></content>
    
    
    <categories>
      
      <category>软件架构</category>
      
      <category>消息队列</category>
      
      <category>笔记</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>消息队列</tag>
      
      <tag>架构</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OS中的零拷贝</title>
    <link href="/posts/441628142/"/>
    <url>/posts/441628142/</url>
    
    <content type="html"><![CDATA[<p>&#x3D;&#x3D;传统的I&#x2F;O的方式：&#x3D;&#x3D;</p><p>数据的读取和写入需要从内核空间和用户空间之间来回的复制，而内核空间的数据则是通过操作系统的I&#x2F;O从磁盘读取。这个过程需要发生多次的上下文切换和数据拷贝。<img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211449759.png" alt="image-20221121144602045" style="zoom:50%;" /> </p><hr><p>&#x3D;&#x3D;传统I&#x2F;O方式的弊端：&#x3D;&#x3D;</p><p>发生多次的上下文切换和内存拷贝，导致IO性能低。</p><p>4次上下文切换和4次数据拷贝</p><hr><p>&#x3D;&#x3D;解决方案：零拷贝技术&#x3D;&#x3D;</p><p>零拷贝主要有两种实现方案，分别是：</p><ul><li>mmap + write</li><li>sendfile</li></ul><hr><p>&#x3D;&#x3D;零拷贝的方式1：mmap + write：&#x3D;&#x3D;</p><p>mmap()系统调用函数，会直接将内核空间中数据的映射到用户空间，二者通过相同的内存地址访问同一份数据，因此也就不需要数据拷贝。（类似于共享内存）</p><p>4次上下文切换和3次数据拷贝</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211455511.png" alt="image-20221121145454211" style="zoom:50%;" /> <hr><p>&#x3D;&#x3D;零拷贝的方式2：sendfile：&#x3D;&#x3D;</p><p>Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用，为 sendfile() ，这个系统调用可以直接将从硬盘 IO 进来的数据在内核空间直接发送到 socket 缓冲区，省去了先拷贝到用户空间这一步。</p><p>因此这一个命令替代了原来的 read() 和 write() 这两个命令。</p><p>因此就只有 2 次上下⽂切换，和 3 次数据拷⻉。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211502097.png" alt="image-20221121150146299" style="zoom:50%;" /> <hr><p>&#x3D;&#x3D;该技术的应用：&#x3D;&#x3D;</p><p>很多开源项目如Kafka、RocketMQ都采用了零拷贝技术来提升IO效率。</p>]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
      <category>操作系统</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>操作系统基础知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>OS中的阻塞与非阻塞IO、同步与异步IO</title>
    <link href="/posts/2169254623/"/>
    <url>/posts/2169254623/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  前言</p></blockquote><p>IO 指的是用户态的进程，需要操作系统帮助其进行数据的读入和写出。</p><p>以 IN 为例，可以分为两个步骤：从磁盘、网络等地方将数据放到内核空间（准备内核数据）、数据从内核空间拷贝到用户空间（内核空间的数据拷贝到用户空间）。</p><hr><blockquote><p>  阻塞 I&#x2F;O</p></blockquote><p>阻塞 I&#x2F;O 阻塞等待的是<code>两个过程</code>：准备内核数据 和 内核数据拷贝到用户空间。</p><p>用户进程需要数据的时候，调用 read() 函数，然后用户进程阻塞，直到数据拷贝到用户空间。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211631552.png" alt="image-20221121152445935" style="zoom:50%;" /> <hr><blockquote><p>  非阻塞 I&#x2F;O</p></blockquote><p>非阻塞 I&#x2F;O 阻塞<code>只等待一个过程</code>：<del>准备内核数据</del> 和 内核数据拷贝到用户空间。</p><p>当发起非阻塞的 IO 之后，如果发现需要的数据在内核中未准备好，那么直接返回，不等待。然后通过不断轮询内核，来查看有没有准备好数据。如果准备好了数据，那么用户进程再阻塞，等待数据从内核空间拷贝到用户空间这一过程。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211631553.png" alt="image-20221121153225613" style="zoom:50%;" /> <hr><blockquote><p>  基于非阻塞的 I&#x2F;O 多路复用</p></blockquote><p>非阻塞 I&#x2F;O 存在问题：当数据未准备好的时候，用户进程需要不断地去轮询内核，查看数据准备好了没有。这个不断轮询的过程，导致用户进程没办法做其它的事情。因此引入了 I&#x2F;O 多路复用技术。</p><p>I&#x2F;O 多路复用技术：内核数据为装备好的时候，用户直接返回，不阻塞，且不轮询。当内核中的数据准备好之后，以事件通知应⽤程序过来拿数据。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211631554.png" alt="image-20221121161111685" style="zoom:50%;" /> <hr><blockquote><p>  同步 I&#x2F;O</p></blockquote><p>⽆论是阻塞 I&#x2F;O、还是⾮阻塞 I&#x2F;O、非阻塞I&#x2F;O多路复用，都是同步调⽤。</p><p>因为它们在read调⽤时，第二个过程-内核将数据从内核空间拷⻉到应⽤程序空间，这个过程都是需要等待的，也就是说这个过程是同步的。</p><p>如果内核实现的拷⻉效率不⾼，read调⽤就会在这个同步过程中等待⽐较⻓的时间。</p><hr><blockquote><p>  异步 I&#x2F;O</p></blockquote><p>真正的异步 I&#x2F;O 是<code>内核数据准备好</code>和<code>数据从内核态拷⻉到⽤户态</code>这两个过程都不⽤等待。</p><p>发起 aio_read 之后，就⽴即返回，内核⾃动将数据从内核空间拷⻉到应⽤程序空间，这个拷⻉过程同样是异步的，内核⾃动完成的，和前⾯的同步操作不⼀样，应⽤程序并不需要主动发起拷⻉动作。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211211631555.png" alt="image-20221121162857355" style="zoom:50%;" /> ]]></content>
    
    
    <categories>
      
      <category>计算机基础</category>
      
      <category>操作系统</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>操作系统基础知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>6 - 循环神经网络</title>
    <link href="/posts/2701885837/"/>
    <url>/posts/2701885837/</url>
    
    <content type="html"><![CDATA[<h2 id="√-6-0-循环神经网络"><a href="#√-6-0-循环神经网络" class="headerlink" title="[√] 6.0 - 循环神经网络"></a>[√] 6.0 - 循环神经网络</h2><hr><blockquote><p>alec：</p><p>循环神经网络的变种版本：GRU和LSTM</p><p>前馈神经网络和卷积神经网络，信息是单向传播的</p><p>循环神经网络中，信息是带有环路的</p></blockquote><h2 id="√-6-1-给神经网络增加记忆能力"><a href="#√-6-1-给神经网络增加记忆能力" class="headerlink" title="[√] 6.1 - 给神经网络增加记忆能力"></a>[√] 6.1 - 给神经网络增加记忆能力</h2><hr><h4 id="√-前馈网络"><a href="#√-前馈网络" class="headerlink" title="[√] 前馈网络"></a>[√] 前馈网络</h4><hr><blockquote><p>alec：</p><p>前馈神经网络总体上是无法处理可变大小的输入的（卷积神经网络中也有全连接层）</p><p>序列数据通常来讲是变长的，因为无法通过前馈神经网路来处理</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220175742582.png" alt="image-20221220174728377"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220175302204.png" alt="image-20221220174924747"></p><blockquote><p>需要想办法给网络增加记忆能力，记住之前时刻的信息</p></blockquote><h4 id="√-有限状态自动机"><a href="#√-有限状态自动机" class="headerlink" title="[√] 有限状态自动机"></a>[√] 有限状态自动机</h4><hr><blockquote><p>alec：</p><ul><li>FNN指的是前馈神经网络</li><li>有的时候，输出不是只根据输入确定的，也取决于之前的输出，即当前的状态。</li></ul></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220175207273.png" alt="image-20221220175207273"></p><h4 id="√-图灵机"><a href="#√-图灵机" class="headerlink" title="[√] 图灵机"></a>[√] 图灵机</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220180401950.png" alt="image-20221220175302204"></p><h4 id="√-可计算问题"><a href="#√-可计算问题" class="headerlink" title="[√] 可计算问题"></a>[√] 可计算问题</h4><hr><blockquote><p>alec：</p><p>我们需要一种函数，这个函数不光需要输入，还需要记忆，然后才能根据输入和记忆计算当前时刻的输出</p><p>y &#x3D; f(x, 记忆)</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220181742185.png" alt="image-20221220175742582"></p><h4 id="√-如何给网络增加记忆能力"><a href="#√-如何给网络增加记忆能力" class="headerlink" title="[√] 如何给网络增加记忆能力"></a>[√] 如何给网络增加记忆能力</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220181410786.png" alt="image-20221220180048453"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220182130253.png" alt="image-20221220180307934"></p><h4 id="√-非线性自回归模型"><a href="#√-非线性自回归模型" class="headerlink" title="[√] 非线性自回归模型"></a>[√] 非线性自回归模型</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221153036798.png" alt="image-20221220180401950"></p><h2 id="√-6-2-循环神经网络"><a href="#√-6-2-循环神经网络" class="headerlink" title="[√] 6.2 - 循环神经网络"></a>[√] 6.2 - 循环神经网络</h2><hr><h4 id="√-循环神经网络"><a href="#√-循环神经网络" class="headerlink" title="[√] 循环神经网络"></a>[√] 循环神经网络</h4><hr><blockquote><p>alec：</p><p>当前的状态ht，通过延时器之后，变成ht-1，用作本次的记忆给下一次用</p><p>在我们的生物神经网络中，是有大量的循环边存在</p><p>循环神经网络有记忆能力</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221154130351.png" alt="image-20221220181119060"></p><h4 id="√-按时间展开"><a href="#√-按时间展开" class="headerlink" title="[√] 按时间展开"></a>[√] 按时间展开</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221153449474.png" alt="image-20221220181410786"></p><p>可以看出，从时间维度上，是一个非常深的网络，但是在非时间维度上（即竖着看）又是一个非常浅的网络</p><h4 id="√-简单循环网络（SRN）"><a href="#√-简单循环网络（SRN）" class="headerlink" title="[√] 简单循环网络（SRN）"></a>[√] 简单循环网络（SRN）</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221153643289.png" alt="image-20221220181522228"></p><h4 id="√-图灵完备"><a href="#√-图灵完备" class="headerlink" title="[√] 图灵完备"></a>[√] 图灵完备</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221153828413.png" alt="image-20221220181559597"></p><blockquote><p>alec：</p><p>我们认为，如果FNN能够模拟任何的函数的话，那么RNN就可以模拟任何的程序</p></blockquote><h4 id="√-循环神经网络-1"><a href="#√-循环神经网络-1" class="headerlink" title="[√] 循环神经网络"></a>[√] 循环神经网络</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221154712704.png" alt="image-20221220181742185"></p><blockquote><p>alec：</p><p>非常有名的联想记忆模型是hopfield</p></blockquote><h2 id="√-6-3-应用到机器学习"><a href="#√-6-3-应用到机器学习" class="headerlink" title="[√] 6.3 - 应用到机器学习"></a>[√] 6.3 - 应用到机器学习</h2><hr><h4 id="√-应用到机器学习"><a href="#√-应用到机器学习" class="headerlink" title="[√] 应用到机器学习"></a>[√] 应用到机器学习</h4><hr><p>主要有三类：</p><ul><li>序列到类别</li><li>同步的序列到序列模式</li><li>异步的序列到序列模式</li></ul><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221160422091.png" alt="image-20221220182130253"></p><h4 id="√-序列到类别"><a href="#√-序列到类别" class="headerlink" title="[√] 序列到类别"></a>[√] 序列到类别</h4><hr><h6 id="√-情感分类"><a href="#√-情感分类" class="headerlink" title="[√] 情感分类"></a>[√] 情感分类</h6><hr><blockquote><p>alec：</p><p>一个额外的东西：</p><ul><li>在用RNN做文本处理的时候，第一步会做一件事就是将文字映射到一个向量上面去，也叫wordembedding。这个操作可以通过查表操作完成。</li><li>这个操作简单，通过一个查表操作就可以了</li></ul></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221152830572.png" alt="image-20221221152830572"></p><h4 id="√-同步的序列到序列模式"><a href="#√-同步的序列到序列模式" class="headerlink" title="[√] 同步的序列到序列模式"></a>[√] 同步的序列到序列模式</h4><hr><blockquote><p>alec：</p><p>输入是一个序列，输出也是一个序列。且输入和输出和一一对应的。</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221170922033.png" alt="image-20221221153036798"></p><h6 id="√-举例：中文分词"><a href="#√-举例：中文分词" class="headerlink" title="[√] 举例：中文分词"></a>[√] 举例：中文分词</h6><hr><blockquote><p>alec：</p><p>中文分词问题存在歧义，因此不好分。因此通过RNN，通过序列标注进行学习。</p><p>s表示单个成词，b表示begin，e表示end</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221173048360.png" alt="image-20221221153449474"></p><h6 id="√-举例：信息抽取"><a href="#√-举例：信息抽取" class="headerlink" title="[√] 举例：信息抽取"></a>[√] 举例：信息抽取</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221160639622.png" alt="image-20221221153643289"></p><h6 id="√-语音识别"><a href="#√-语音识别" class="headerlink" title="[√] 语音识别"></a>[√] 语音识别</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221172453726.png" alt="image-20221221153828413"></p><h4 id="√-异步的序列到序列模式"><a href="#√-异步的序列到序列模式" class="headerlink" title="[√] 异步的序列到序列模式"></a>[√] 异步的序列到序列模式</h4><hr><p>是一个自回归模型，输入为编码，输出为解码。</p><p>decoder用了RNN、自回归</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221172341853.png" alt="image-20221221154130351"></p><h6 id="√-机器翻译"><a href="#√-机器翻译" class="headerlink" title="[√] 机器翻译"></a>[√] 机器翻译</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221154220846.png" alt="image-20221221154220846"></p><h2 id="√-6-4-参数问题与长程依赖问题"><a href="#√-6-4-参数问题与长程依赖问题" class="headerlink" title="[√] 6.4 - 参数问题与长程依赖问题"></a>[√] 6.4 - 参数问题与长程依赖问题</h2><hr><h4 id="√-参数学习"><a href="#√-参数学习" class="headerlink" title="[√] 参数学习"></a>[√] 参数学习</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221172838469.png" alt="image-20221221154712704"></p><blockquote><p>alec：</p><p>通过状态单元h，计算得到预测标签y，然后真实标签和预测标签计算损失。其中状态单元是由当前的输入和之前的状态得到的。</p></blockquote><h4 id="√-计算梯度"><a href="#√-计算梯度" class="headerlink" title="[√] 计算梯度"></a>[√] 计算梯度</h4><hr><blockquote><p>alec：</p><ul><li>简单循环神经网络，SRN</li><li>H<del>t-1</del>、X<del>t</del>、b通过线性计算得到Z<del>t</del>,Z<del>t</del>通过非线性函数，得到H<del>t</del></li><li>类比得到，Zt看做净活性值、Ht看做激活值。</li></ul></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221174157091.png" alt="image-20221221160422091"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221181115478.png" alt="image-20221221160639622"></p><h4 id="√-随时间的反向传播算法"><a href="#√-随时间的反向传播算法" class="headerlink" title="[√] 随时间的反向传播算法"></a>[√] 随时间的反向传播算法</h4><hr><h6 id="√-梯度-和-长程依赖问题"><a href="#√-梯度-和-长程依赖问题" class="headerlink" title="[√] 梯度 和 长程依赖问题"></a>[√] 梯度 和 长程依赖问题</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221195556037.png" alt="image-20221221170447313"></p><p>当 γ 大于1，t-k非常大的时候，梯度就是趋向于非常大，就会产生梯度爆炸问题。</p><p>当 γ 小于1，t-k非常大的时候，梯度就趋向于0，就会产生梯度消失问题。</p><p>因此SRN网络，当网络的深度很深的时候，就不好训练了。</p><p>虽然长周期之内存在依赖关系，但是由于长周期会有类似于梯度消失或者梯度爆炸问题，因此事实上只能学习到短周期的依赖关系，学不到长周期的依赖关系。</p><p>因为上图中损失函数对参数U的导数是一个对多个时刻的梯度求和，因此虽然单个存在梯度消失问题，但是总体上不会存在梯度消失问题，因为长距离内会梯度消失，但是短距离内不会梯度消失。这种现象就会导致参数只能学习到短周期内的，无法学习到长周期内的。</p><blockquote><p>alec：</p><p>问题思考：如何不发生梯度消失后者梯度爆炸呢？</p><p>答：令梯度公式中的γ等于1，就不会产生梯度问题了。</p></blockquote><h2 id="√-6-5-如何解决长程依赖问题？"><a href="#√-6-5-如何解决长程依赖问题？" class="headerlink" title="[√] 6.5 - 如何解决长程依赖问题？"></a>[√] 6.5 - 如何解决长程依赖问题？</h2><hr><h4 id="√-长程依赖问题"><a href="#√-长程依赖问题" class="headerlink" title="[√] 长程依赖问题"></a>[√] 长程依赖问题</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221180537962.png" alt="image-20221221170922033"></p><p>梯度爆炸问题，相对来说比较容易解决。</p><h6 id="√-改进方法"><a href="#√-改进方法" class="headerlink" title="[√] 改进方法"></a>[√] 改进方法</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221174404863.png" alt="image-20221221172341853"></p><p>方法1：循环边改为线性依赖关系</p><ul><li>这种方式解决了长程依赖问题，但是模型能力就变弱了</li></ul><p>方法2：进一步改进，增加非线性</p><ul><li>后面一项保证了非线性关系，同时前面一项保证了倒数为有1不会发生梯度消失等问题</li><li>这个式子的问题在于，非常容易饱和，饱和之后，非线性函数的值都是一样的，因此会导致信息的差别越来越小</li><li>解决办法是想办法主动的去丢弃一些东西，即接下来要讲的基于门控的方法</li></ul><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221195234976.png" alt="image-20221221172453726"></p><p>这种结构和残差网络非常像。因此解决梯度问题，无论是在前馈神经网络还是循环神经网络，原理都是非常类似的。</p><h2 id="√-6-6-GRU-和-LSTM"><a href="#√-6-6-GRU-和-LSTM" class="headerlink" title="[√] 6.6 - GRU 和 LSTM"></a>[√] 6.6 - GRU 和 LSTM</h2><hr><p>两个可以有效缓解长程依赖问题的模型：GRU 和 LSTM</p><h4 id="√-门控机制"><a href="#√-门控机制" class="headerlink" title="[√] 门控机制"></a>[√] 门控机制</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201527024.png" alt="image-20221221172838469"></p><h4 id="√-门控循环单元，GRU"><a href="#√-门控循环单元，GRU" class="headerlink" title="[√] 门控循环单元，GRU"></a>[√] 门控循环单元，GRU</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201435117.png" alt="image-20221221173048360"></p><p>如果后面这个非线性部分一直大于0的话，那么这个整体就会越来越大，然后通过非线性函数之后，可能就会走到梯度的饱和问题，最终不利于参数的学习。</p><p>因此引入门控机制，让这个ht不要太大。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201805291.png" alt="image-20221221174157091"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201713509.png" alt="image-20221221174404863"></p><h4 id="√-长短期记忆神经网络（LSTM）"><a href="#√-长短期记忆神经网络（LSTM）" class="headerlink" title="[√] 长短期记忆神经网络（LSTM）"></a>[√] 长短期记忆神经网络（LSTM）</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201839963.png" alt="image-20221221180537962"></p><p>这个网络引入了内部记忆单元 <code>c</code> ，用<code>c</code>来传递线性依赖关系。</p><p>通过线性+非线性计算得到当前时刻暂时的记忆单元c_t’，然后还有上一个时刻的记忆单元c_t-1，然后通过两个门控遗忘门f_t和输入门i_t控制这两个记忆单元的大小。</p><p>将被门控后的这两个记忆单元相加，得到当前时刻的记忆单元c_t。</p><p>然后将c_t通过非线性单元，并乘上一个输出门o_t，得到了第t个时刻我们要的最终的状态h_t。</p><p>遗忘门用于控制上一个时刻的记忆单元的传达阀门</p><p>输入门用于控制当前输入等计算得到的记忆单元的传递阀门</p><p>输出门用于控制最终的输出的非线性的传递阀门</p><h4 id="√-LSTM的各种变体"><a href="#√-LSTM的各种变体" class="headerlink" title="[√] LSTM的各种变体"></a>[√] LSTM的各种变体</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201603620.png" alt="image-20221221181115478"></p><p>最初提出的时候，是没有遗忘门，但是发现这样效果不太好，所以就将遗忘门加上了。</p><h2 id="√-6-7-深层循环神经网络"><a href="#√-6-7-深层循环神经网络" class="headerlink" title="[√] 6.7 - 深层循环神经网络"></a>[√] 6.7 - 深层循环神经网络</h2><hr><h4 id="√-堆叠RNN"><a href="#√-堆叠RNN" class="headerlink" title="[√] 堆叠RNN"></a>[√] 堆叠RNN</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221202243627.png" alt="image-20221221195234976"></p><p>堆叠的神经网络，一种使得RNN变深的网络。</p><h4 id="√-双向RNN"><a href="#√-双向RNN" class="headerlink" title="[√] 双向RNN"></a>[√] 双向RNN</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221202656869.png" alt="image-20221221195556037"></p><p>双向循环神经网络已经成为语音识别、机器翻译、文本分类等的标配模型</p><h4 id="√-RNN小结"><a href="#√-RNN小结" class="headerlink" title="[√] RNN小结"></a>[√] RNN小结</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221202817530.png" alt="image-20221221200736142"></p><h2 id="√-6-8-RNN应用"><a href="#√-6-8-RNN应用" class="headerlink" title="[√] 6.8 - RNN应用"></a>[√] 6.8 - RNN应用</h2><hr><h4 id="√-语言模型"><a href="#√-语言模型" class="headerlink" title="[√] 语言模型"></a>[√] 语言模型</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221202753102.png" alt="image-20221221201219407"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221203146076.png" alt="image-20221221201435117"></p><h4 id="√-生成Linux内核代码"><a href="#√-生成Linux内核代码" class="headerlink" title="[√] 生成Linux内核代码"></a>[√] 生成Linux内核代码</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221203729804.png" alt="image-20221221201527024"></p><h4 id="√-作词机"><a href="#√-作词机" class="headerlink" title="[√] 作词机"></a>[√] 作词机</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221203316448.png" alt="image-20221221201603620"></p><h4 id="√-作诗"><a href="#√-作诗" class="headerlink" title="[√] 作诗"></a>[√] 作诗</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221203429179.png" alt="image-20221221201713509"></p><h4 id="√-传统统计机器翻译"><a href="#√-传统统计机器翻译" class="headerlink" title="[√] 传统统计机器翻译"></a>[√] 传统统计机器翻译</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220174924747.png" alt="image-20221221201805291"></p><h4 id="√-基于序列到序列的机器翻译"><a href="#√-基于序列到序列的机器翻译" class="headerlink" title="[√] 基于序列到序列的机器翻译"></a>[√] 基于序列到序列的机器翻译</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221203210595.png" alt="image-20221221201839963"></p><h4 id="√-看图说话"><a href="#√-看图说话" class="headerlink" title="[√] 看图说话"></a>[√] 看图说话</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221200736142.png" alt="image-20221221201921874"></p><p>编码的时候使用CNN，解码的时候使用RNN</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220180307934.png" alt="image-20221221201954659"></p><h4 id="√-写字"><a href="#√-写字" class="headerlink" title="[√] 写字"></a>[√] 写字</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220174728377.png" alt="image-20221221202207358"></p><h4 id="√-对话系统"><a href="#√-对话系统" class="headerlink" title="[√] 对话系统"></a>[√] 对话系统</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220181559597.png" alt="image-20221221202243627"></p><h2 id="√-6-9-扩展到图结构"><a href="#√-6-9-扩展到图结构" class="headerlink" title="[√] 6.9 - 扩展到图结构"></a>[√] 6.9 - 扩展到图结构</h2><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220181119060.png" alt="image-20221221202656869"></p><h4 id="√-树结构"><a href="#√-树结构" class="headerlink" title="[√] 树结构"></a>[√] 树结构</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220181522228.png" alt="image-20221221202753102"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221170447313.png" alt="image-20221221202817530"></p><h4 id="√-递归循环网络"><a href="#√-递归循环网络" class="headerlink" title="[√] 递归循环网络"></a>[√] 递归循环网络</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201219407.png" alt="image-20221221203146076"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221202207358.png" alt="image-20221221203210595"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201921874.png" alt="image-20221221203316448"></p><h4 id="√-图网络"><a href="#√-图网络" class="headerlink" title="[√] 图网络"></a>[√] 图网络</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220180048453.png" alt="image-20221221203429179"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221201954659.png" alt="image-20221221203729804"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5 - 卷积神经网络</title>
    <link href="/posts/2118395793/"/>
    <url>/posts/2118395793/</url>
    
    <content type="html"><![CDATA[<h1 id="√-5-课节5：卷积神经网络"><a href="#√-5-课节5：卷积神经网络" class="headerlink" title="[√] 5 - 课节5：卷积神经网络"></a>[√] 5 - 课节5：卷积神经网络</h1><hr><h2 id="√-5-0-卷积神经网络概述"><a href="#√-5-0-卷积神经网络概述" class="headerlink" title="[√] 5.0 - 卷积神经网络概述"></a>[√] 5.0 - 卷积神经网络概述</h2><hr><p>卷积神经网络的信息也是单向传递的</p><hr><h4 id="√-全连接前馈神经网络"><a href="#√-全连接前馈神经网络" class="headerlink" title="[√] 全连接前馈神经网络"></a>[√] 全连接前馈神经网络</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212125963.png" alt="image-20221218204030255"></p><blockquote><p>alec：</p><p>全连接很难提取图像上局部不变的特性</p><p>图像上的一些特征是平移、旋转、缩放等不变的</p></blockquote><hr><h4 id="√-卷积神经网络"><a href="#√-卷积神经网络" class="headerlink" title="[√] 卷积神经网络"></a>[√] 卷积神经网络</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126522.png" alt="image-20221218204239863"></p><hr><h4 id="√-本章内容"><a href="#√-本章内容" class="headerlink" title="[√] 本章内容"></a>[√] 本章内容</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126523.png" alt="image-20221218204323550"></p><hr><h2 id="√-5-1-卷积"><a href="#√-5-1-卷积" class="headerlink" title="[√] 5.1 - 卷积"></a>[√] 5.1 - 卷积</h2><hr><hr><h4 id="——-gt-√-卷积"><a href="#——-gt-√-卷积" class="headerlink" title="——&gt;[√] 卷积"></a>——&gt;[√] 卷积</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126524.png" alt="image-20221218205145546"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126525.png" alt="image-20221218205436373"></p><blockquote><p>alec：</p><p>卷积输出的长度为：N-K+1，N为数据长度、K为滤波器长度、</p></blockquote><hr><h4 id="——-gt-√-卷积的作用"><a href="#——-gt-√-卷积的作用" class="headerlink" title="——&gt;[√] 卷积的作用"></a>——&gt;[√] 卷积的作用</h4><hr><h6 id="——-gt-√-近似微分"><a href="#——-gt-√-近似微分" class="headerlink" title="——&gt;[√] 近似微分"></a>——&gt;[√] 近似微分</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126526.png" alt="image-20221218211717113"></p><hr><h6 id="——-gt-√-低通滤波-x2F-高通滤波"><a href="#——-gt-√-低通滤波-x2F-高通滤波" class="headerlink" title="——&gt;[√] 低通滤波&#x2F;高通滤波"></a>——&gt;[√] 低通滤波&#x2F;高通滤波</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126527.png" alt="image-20221218212013581"></p><hr><h4 id="——-gt-√-卷积扩展"><a href="#——-gt-√-卷积扩展" class="headerlink" title="——&gt;[√] 卷积扩展"></a>——&gt;[√] 卷积扩展</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126529.png" alt="image-20221218212500156"></p><blockquote><p>alec：</p><p>当想要卷积后尺寸不变的话，那么padding的长度为（k-1）&#x2F; 2，其中k为卷积核的长度</p><p>卷积输出的长度为：</p><p>L &#x3D; （M+2P-K）&#x2F; 2 + 1</p></blockquote><hr><h4 id="——-gt-√-卷积类型"><a href="#——-gt-√-卷积类型" class="headerlink" title="——&gt;[√] 卷积类型"></a>——&gt;[√] 卷积类型</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126530.png" alt="image-20221218212956189"></p><hr><h4 id="——-gt-√-二维卷积"><a href="#——-gt-√-二维卷积" class="headerlink" title="——&gt;[√] 二维卷积"></a>——&gt;[√] 二维卷积</h4><hr><blockquote><p>alec：</p><p>将M-K+1记为1+M-K简单一些</p></blockquote><hr><h4 id="——-gt-√-卷积作为特征提取器"><a href="#——-gt-√-卷积作为特征提取器" class="headerlink" title="——&gt;[√] 卷积作为特征提取器"></a>——&gt;[√] 卷积作为特征提取器</h4><hr><blockquote><p>alec：</p><p>高斯滤波可以去噪，卷积核用周围点的信息来平均当前点的信息，使得图像更加的光滑</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126531.png" alt="image-20221218213756588"></p><p>提取高频信息</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126532.png" alt="image-20221218213808571"></p><p>针对性的提取有方向的边缘</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126533.png" alt="image-20221218213827166"></p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126534.png" alt="image-20221218213908134"></p><hr><h2 id="√-5-2-卷积神经网络"><a href="#√-5-2-卷积神经网络" class="headerlink" title="[√] 5.2 - 卷积神经网络"></a>[√] 5.2 - 卷积神经网络</h2><hr><h4 id="√-卷积神经网络-1"><a href="#√-卷积神经网络-1" class="headerlink" title="[√] 卷积神经网络"></a>[√] 卷积神经网络</h4><hr><blockquote><p>alec：</p><ul><li>全连接中，下一层的一个神经元信息是收集的前一层的所有神经元的信息</li><li>卷积中，下一层的一个神经炎信息是收集的在前一层，卷积核当前所在位置对应的神经元信息的信息。即只收集了局部信息。</li></ul><hr><ul><li>卷积特性：在不同位置的上的参数都是相等的，因此参数量大大减小，且共享权重</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126535.png" alt="image-20221218214556669"></p><hr><h4 id="√-互相关"><a href="#√-互相关" class="headerlink" title="[√] 互相关"></a>[√] 互相关</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126536.png" alt="image-20221218214801016"></p><hr><h4 id="√-多个卷积核"><a href="#√-多个卷积核" class="headerlink" title="[√] 多个卷积核"></a>[√] 多个卷积核</h4><hr><blockquote><p>alec：</p><p>单个卷积的参数量非常少，因此很自然的能力就会下降</p><p>因此通过在一层放多个卷积核，提取不同的特征，来提高网络的能力</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126537.png" alt="image-20221218215125765"></p><hr><h4 id="√-卷积层的映射关系"><a href="#√-卷积层的映射关系" class="headerlink" title="[√] 卷积层的映射关系"></a>[√] 卷积层的映射关系</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126538.png" alt="image-20221218215458748"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126539.png" alt="image-20221218215545076"></p><blockquote><p>alec:</p><p>三个通道对应三组卷积核，每组卷积核中的卷积核不一定是相等的</p></blockquote><hr><h4 id="√-卷积层"><a href="#√-卷积层" class="headerlink" title="[√] 卷积层"></a>[√] 卷积层</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126540.png" alt="image-20221218215750018"></p><hr><h4 id="√-汇聚层"><a href="#√-汇聚层" class="headerlink" title="[√] 汇聚层"></a>[√] 汇聚层</h4><hr><blockquote><p>alec：</p><ul><li>卷积层只是减少了连接的个数，但是神经元的个数并没有显著减少，下一层的神经元的个数是1+（M+2P-K）&#x2F; S</li><li>引入汇聚层减少神经元的个数</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126541.png" alt="image-20221218220234115"></p><hr><h4 id="√-卷积网络结构"><a href="#√-卷积网络结构" class="headerlink" title="[√] 卷积网络结构"></a>[√] 卷积网络结构</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126542.png" alt="image-20221218220427623"></p><hr><h4 id="√-表示学习"><a href="#√-表示学习" class="headerlink" title="[√] 表示学习"></a>[√] 表示学习</h4><hr><blockquote><p>alec：</p><p>卷积和表示学习是非常像的，用来学习特征。然后通过线性的分类器全连接网络进行特征的分类。</p><p>卷积的深层的神经元，视野更宽。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126543.png" alt="image-20221218220659104"></p><hr><h2 id="√-5-3-其它卷积种类"><a href="#√-5-3-其它卷积种类" class="headerlink" title="[√] 5.3 - 其它卷积种类"></a>[√] 5.3 - 其它卷积种类</h2><hr><h4 id="√-空洞卷积"><a href="#√-空洞卷积" class="headerlink" title="[√] 空洞卷积"></a>[√] 空洞卷积</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126544.png" alt="image-20221218221051392"></p><blockquote><p>alec：</p><p>空洞卷积的作用是增加感受野</p></blockquote><hr><h4 id="√-微步卷积-x2F-转置卷积"><a href="#√-微步卷积-x2F-转置卷积" class="headerlink" title="[√] 微步卷积&#x2F;转置卷积"></a>[√] 微步卷积&#x2F;转置卷积</h4><hr><blockquote><p>alec：</p><p>正常思路，随着卷积的进行，feature map会越来越小；当S&gt;&#x3D;1的是时候，输出会变小。因此当S≤1，输出就会变大。办法是对输入插0值，然后再卷积。</p><p>当想要输出比输入更大的时候，按照相反的思路来就可以，比如可以给输入进行补零，放大输入，然后再卷积，这样就能得到大的输出</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126545.png" alt="image-20221218221734175"></p><hr><h2 id="√-5-4-典型的卷积网络"><a href="#√-5-4-典型的卷积网络" class="headerlink" title="[√] 5.4 - 典型的卷积网络"></a>[√] 5.4 - 典型的卷积网络</h2><hr><h4 id="√-LeNet-5"><a href="#√-LeNet-5" class="headerlink" title="[√] LeNet-5"></a>[√] LeNet-5</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126546.png" alt="image-20221218222249336"></p><hr><h4 id="√-大规模视觉识别挑战"><a href="#√-大规模视觉识别挑战" class="headerlink" title="[√] 大规模视觉识别挑战"></a>[√] 大规模视觉识别挑战</h4><hr><blockquote><p>alec：</p><p>2015年何凯明提出的resnet在这个上面的准确率降到了3.几%，这个准确率已经超过了人的准确率。因此后面这个分类比赛就停办了。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126547.png" alt="image-20221218222512024"></p><hr><h4 id="√-AlexNet"><a href="#√-AlexNet" class="headerlink" title="[√] AlexNet"></a>[√] AlexNet</h4><hr><blockquote><p>alec：</p><p>卷积的起点就是AlexNet，Alex就是作者的名字</p><p>使用Dropout来防止过拟合</p><p>AlexNet是一个1000个分类的分类网络</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126548.png" alt="image-20221218223103069"></p><hr><h4 id="√-CNN可视化：滤波器"><a href="#√-CNN可视化：滤波器" class="headerlink" title="[√] CNN可视化：滤波器"></a>[√] CNN可视化：滤波器</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126549.png" alt="image-20221218223227492"></p><hr><h4 id="√-Inception网络"><a href="#√-Inception网络" class="headerlink" title="[√] Inception网络"></a>[√] Inception网络</h4><hr><blockquote><p>alec：</p><p>Googlenet是属于inception网络的第一版</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126550.png" alt="image-20221218223408237"></p><hr><h4 id="√-Inception模块V1"><a href="#√-Inception模块V1" class="headerlink" title="[√] Inception模块V1"></a>[√] Inception模块V1</h4><hr><blockquote><p>alec：</p><ul><li>在卷积中，卷积核选择多大的尺寸是一个非常难的问题。因此在Inception中，在同一层卷积中，选择多个不同大小的卷积核放在同一层，这种模块成为Inception模块。</li><li>同一层中不同尺寸的卷积核，卷积（等宽卷积）和最大汇聚后的特征图都是等宽的，因此在同一层之后，可以将特征图汇聚堆叠到一起，然后传给下一层。</li><li>穷举各种大小的卷积核，极大的提高了特征的丰富程度。因此网络的能力会变得更强。</li></ul><hr><ul><li>1×1卷积其实就是在深度（通道数）这个方向对元素做了加权组合。看成是不同通道上的特征融合。1×1卷积不改变大小，因此不需要padding</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126551.png" alt="image-20221218224424215"></p><hr><h4 id="√-Inception模块V3"><a href="#√-Inception模块V3" class="headerlink" title="[√] Inception模块V3"></a>[√] Inception模块V3</h4><hr><blockquote><p>alec：</p><p>3×3的感受野小于5×5的感受野，但是3×3串联3×3，那么感受野就变大了。因此通过串联小卷积核替代大卷积核，这样可以减少参数量。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126552.png" alt="image-20221219113922887"></p><hr><h4 id="√-残差网络"><a href="#√-残差网络" class="headerlink" title="[√] 残差网络"></a>[√] 残差网络</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126553.png" alt="image-20221219114121825"></p><blockquote><p>alec：</p><p>当f（x）是一个恒等函数的时候，反而用卷积神经网络模拟非线性的函数很难逼近这个函数。因此通过残差网络直连边的方式，能够优化这个问题。</p><p>h（x）&#x3D; x + f(x), x是线性部分，f（x）是非线性部分</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126554.png" alt="image-20221219114721505"></p><hr><h4 id="√-残差单元"><a href="#√-残差单元" class="headerlink" title="[√] 残差单元"></a>[√] 残差单元</h4><hr><blockquote><p>alec：</p><ul><li>残差单元，等宽卷积边和直连边相加之后，再激活</li><li>一个block中的卷积怎么搭配，是一个非常灵活的事情</li></ul><hr><p>为什么残差网络能够深度非常深？</p><ul><li>深层网络存在的一个问题是层数太深、梯度消失问题。残差网络的导数为（x+f（x））‘ &#x3D; 1 + f‘(x)</li><li>因为这个1的存在，所以梯度不会变的很小，所以能够缓解梯度消失问题</li><li><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126555.png" alt="image-20221219115216001"></li><li>因此，现在对于任意一个比较深的网络，即使不是残差网络，这种残差直连边的连接方式，已经成为了一种必不可少的技术</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126556.png" alt="image-20221219115337588"></p><hr><h2 id="√-5-5-卷积网络的应用"><a href="#√-5-5-卷积网络的应用" class="headerlink" title="[√] 5.5 - 卷积网络的应用"></a>[√] 5.5 - 卷积网络的应用</h2><hr><h4 id="√-AlphaGo"><a href="#√-AlphaGo" class="headerlink" title="[√] AlphaGo"></a>[√] AlphaGo</h4><hr><blockquote><p>alec：</p><p>强化学习中决策网络，下棋相当于在19×19的棋盘中，确定棋盘中下棋的位置，相当于一个输入是一张图像，输出是一个19×19的分类问题。</p><p>强化学习中的价值网络，用来判断走每一步对于后面的平均收益是多少。</p><p>这两种网络都是通过卷积网络来实现的。</p><hr><p>等宽卷积：</p><ul><li>填充 P &#x3D; (K - 1)&#x2F;2</li></ul><hr><p>等宽卷积的目的是为了适应残差网络中残差边和直连边的相加</p><p>残差网络的目的是为了优化梯度消失问题</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126557.png" alt="image-20221219123326383"></p><hr><h4 id="√-目标检测（Object-Detection）"><a href="#√-目标检测（Object-Detection）" class="headerlink" title="[√] 目标检测（Object Detection）"></a>[√] 目标检测（Object Detection）</h4><hr><blockquote><p>alec：</p><p>目标检测中有自己专门的卷积网络：RCN、区域卷积网络</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126558.png" alt="image-20221219123635743"></p><hr><h4 id="√-Mask-RCNN"><a href="#√-Mask-RCNN" class="headerlink" title="[√] Mask RCNN"></a>[√] Mask RCNN</h4><hr><blockquote><p>alec：</p><p>更细粒度的，像素级的图像分割，将轮廓找出来。</p><p>思想类似于讲图像中的某个区域拿出来，然后做像素级别的分类，从而找到目标轮廓。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126559.png" alt="image-20221219123918838"></p><hr><h4 id="√-OCR"><a href="#√-OCR" class="headerlink" title="[√] OCR"></a>[√] OCR</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126560.png" alt="image-20221219124041049"></p><hr><h2 id="√-5-6-卷积网络应用到文本数据"><a href="#√-5-6-卷积网络应用到文本数据" class="headerlink" title="[√] 5.6 - 卷积网络应用到文本数据"></a>[√] 5.6 - 卷积网络应用到文本数据</h2><hr><h4 id="√-Ngram特征与卷积"><a href="#√-Ngram特征与卷积" class="headerlink" title="[√] Ngram特征与卷积"></a>[√] Ngram特征与卷积</h4><hr><blockquote><p>alec：</p><p>卷积是从信号序列提取特征，文本本身就是信号序列，因此使用卷积提取文本信息是自然的</p><p>单个词语的提取，会丢失数据的顺序（unigrams）</p><p>因此可以两个、三个词语的提取（bigrams、trigrams）</p></blockquote><hr><h4 id="√-文本序列的卷积"><a href="#√-文本序列的卷积" class="headerlink" title="[√] 文本序列的卷积"></a>[√] 文本序列的卷积</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126561.png" alt="image-20221219124952755"></p><hr><h4 id="√-基于卷积模型的句子表示"><a href="#√-基于卷积模型的句子表示" class="headerlink" title="[√] 基于卷积模型的句子表示"></a>[√] 基于卷积模型的句子表示</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126562.png" alt="image-20221219125156850"></p><hr><h4 id="√-文本序列的卷积模型"><a href="#√-文本序列的卷积模型" class="headerlink" title="[√] 文本序列的卷积模型"></a>[√] 文本序列的卷积模型</h4><hr><blockquote><p>alec：</p><p>卷积层是指的使用卷积核卷积前一层数据之后，得到的新的特征图，这些特征图是通过卷积得到的，所以叫卷积层；这一层是卷积的结果，而不是说这一层是进行卷积的过程。</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212126563.png" alt="image-20221219125434730"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4 - 前馈神经网络</title>
    <link href="/posts/2713595903/"/>
    <url>/posts/2713595903/</url>
    
    <content type="html"><![CDATA[<h2 id="√-4-课节4：前馈神经网络"><a href="#√-4-课节4：前馈神经网络" class="headerlink" title="[√] 4 - 课节4：前馈神经网络"></a>[√] 4 - 课节4：前馈神经网络</h2><h3 id="√-4-0-前馈神经网络概述"><a href="#√-4-0-前馈神经网络概述" class="headerlink" title="[√] 4.0 - 前馈神经网络概述"></a>[√] 4.0 - 前馈神经网络概述</h3><p>本节讲非线性的分类器，这个分类器主要就是神经网络。今天讲的第一种神经网络是前馈神经网络。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122404.png" alt="image-20221215181003273"></p><hr><h3 id="√-4-1-神经元"><a href="#√-4-1-神经元" class="headerlink" title="[√] 4.1 - 神经元"></a>[√] 4.1 - 神经元</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122405.png" alt="image-20221215181534638"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122406.png" alt="image-20221215181917766"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122407.png" alt="image-20221215194456601"></p><p>偏置b的作用是：调节阈值，即达到什么程度就兴奋。</p><p>这个人工神经元可以看做是一个简单的线性模型。</p><p>这个神经元可以看做两部分，前半部分看做是收集信息，后半部分看做是一个非线性函数，用来将收集的信息映射到一个激活的状态上。</p><p>不同类型的神经元主要的区别其实就是在于激活函数怎么设计。</p><p>通常来讲a的取值范围是一个比z的取值范围更小的区域。</p><blockquote><p>三种常用的激活函数</p></blockquote><ol><li>s型函数（sigmoid function）：比如sigmoid激活函数、logistic函数、tanh函数等</li><li>斜坡函数（ramp function）：ReLU、leaky ReLU、ELU等</li><li>复合函数：既带有s型函数的性质、又带有斜坡函数的性质</li></ol><blockquote><p>S型函数</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122408.png" alt="image-20221215195938582"></p><p>比如logistic函数，值在0-1之间，模拟神经元的两种状态</p><p>tanh函数和logistic函数能够相互转换，因此这两种函数的能力基本上是等价的</p><p>tanh函数的能力比logistic函数的能力要好一些</p><p>logistic函数因为输出恒大于0，因此这个函数的输出作为输入的时候，会偏，因此对优化的性能不是很好。</p><p>比如在优化的时候，四个象限，logistic函数这种恒正的函数，只能在1、3象限优化，不能在第4象限优化，只能走之字形，因此效率会低。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122409.png" alt="image-20221215195717382"></p><p>&#x3D;&#x3D;解决这种非零中心化的方法&#x3D;&#x3D;</p><ol><li>归一化到零中心化</li><li>在函数的外面加一个可学习的参数 偏置b，缓解非零中心化带来的问题</li></ol><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122411.png" alt="image-20221215195914705"></p><blockquote><p>斜坡函数</p></blockquote><p>代表函数是ReLU函数，也叫修正的线性单元</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122412.png" alt="image-20221215200026130"></p><p>这种函数非常简单，目前的神经网络中大量的使用这种激活函数，一般激活函数首选ReLU</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122413.png" alt="image-20221215200944694"></p><p>&#x3D;&#x3D;ReLU函数的性质&#x3D;&#x3D;</p><ol><li>计算简单</li><li>生物学上的合理性，生物学家发现神经元兴奋的时候能够非常兴奋</li><li>优化性质非常好，因此右边的导数是1，因此在优化的时候，不会太小，也不会太大，从而更加有效的来学习参数</li></ol><p>&#x3D;&#x3D;ReLU存在的问题&#x3D;&#x3D;</p><p>死亡ReLU问题，因为左边为0，会导致梯度消失，神经元无法更新权重参数了。</p><p>&#x3D;&#x3D;ReLU问题解决&#x3D;&#x3D;</p><ol><li>通过归一化的方式来缓解死亡ReLU问题，使得数据的分散不要太集中。</li><li>初始化参数的时候，避免全部都初始化为负值</li><li>使用leaky ReLU，左边不要让其等于0，而是给一个很小的梯度</li><li>同时也可以将γ这个参数变成可学习的，即带参数的ReLU</li></ol><p>&#x3D;&#x3D;ReLU函数的非零中心化问题&#x3D;&#x3D;</p><blockquote><p>复合激活函数</p></blockquote><p>&#x3D;&#x3D;swish函数&#x3D;&#x3D;</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122414.png" alt="image-20221215213216501"></p><p>自门控函数，自己控制自己，<code>xσ(βx)</code></p><p>这种函数能够通过变换β的值，实现在线性函数和ReLU函数之间变换，是一种非常灵活的函数</p><p>&#x3D;&#x3D;高斯误差线性单元，GELU&#x3D;&#x3D;</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122415.png" alt="image-20221215213639624"></p><p>p(X&lt;x)的形状和s型函数是类似的，因此这个GELU函数的形状和swish函数的形状基本是一样的</p><p>目前在比较新的模型中基本都是用GELU作为激活函数，这种函数优化的性质相对来说要好一些</p><blockquote><p>常见的激活函数和导数的总结</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122416.png" alt="image-20221215213848098"></p><hr><h3 id="√-4-2-神经网络"><a href="#√-4-2-神经网络" class="headerlink" title="[√] 4.2 - 神经网络"></a>[√] 4.2 - 神经网络</h3><p>logistic激活函数的输出在0-1之间，如果只是希望非线性就够了，那么就优先采用ReLU函数。</p><blockquote><p>logistic函数</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122417.png" alt="image-20221215214229668"></p><blockquote><p>ReLU函数</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122418.png" alt="image-20221215214249490"></p><blockquote><p>tanh函数</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122419.png" alt="image-20221215214312194"></p><p>不是所有的网络都是通过梯度下降的方式来更新参数，比如hopfield网络</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122420.png" alt="image-20221215214518942"></p><blockquote><p>本课程会讲的3种网络</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122421.png" alt="image-20221215214610261"></p><p>记忆网络，有循环边，因此就会有状态的概念，即历史状态是什么</p><p>图网络一般是一组神经元，因此用方形的来表示。</p><p>这三种网络是分开讲的，但是在实际应用中，通常是不同的网络相互组合来用的。</p><p>神经网络主要是连接主义模型，区别于符号主义模型。符号主义中，知识或者信息是用符号来定义的，连接主义中是信息是存在连接上的</p><p>连接主义的模型是分布式并行处理网络，这种网络主要就是神经网络。</p><p>连接主义的三点：</p><ul><li>由网络来共同表示信息，而不是像符号主义一样一个符号就表示一个信息</li><li>知识是定义在单元之间的连接上的，单元之间连接强度的改变可以来学习新的知识</li></ul><p>神经网络就是一个典型的连接主义模型</p><hr><h3 id="√-4-3-前馈神经网络"><a href="#√-4-3-前馈神经网络" class="headerlink" title="[√] 4.3 - 前馈神经网络"></a>[√] 4.3 - 前馈神经网络</h3><blockquote><p>网络结构</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122422.png" alt="image-20221215220757911"></p><p>相邻层之间的神经元是全部两两连接（全连接）</p><p>单向传递</p><p>层内无连接</p><p>定义一个网络的层数的时候，不算输入层，因此上面的网络一共有3层</p><blockquote><p>前馈网络</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122423.png" alt="image-20221215221101546"></p><blockquote><p>信息传递过程</p></blockquote><p>第l层的传递过程：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122424.png" alt="image-20221215221524092"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122425.png" alt="image-20221215224933824"></p><hr><h6 id="gt-通用近似定理"><a href="#gt-通用近似定理" class="headerlink" title="-&gt; 通用近似定理"></a>-&gt; 通用近似定理</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122426.png" alt="image-20221215225311894"></p><hr><h6 id="gt-应用到机器学习"><a href="#gt-应用到机器学习" class="headerlink" title="-&gt; 应用到机器学习"></a>-&gt; 应用到机器学习</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122427.png" alt="image-20221215225634120"></p><hr><h6 id="gt-深层前馈神经网络"><a href="#gt-深层前馈神经网络" class="headerlink" title="-&gt; 深层前馈神经网络"></a>-&gt; 深层前馈神经网络</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122428.png" alt="image-20221215225737438"></p><p>前馈神经网络进行分类任务</p><hr><h6 id="gt-参数学习"><a href="#gt-参数学习" class="headerlink" title="-&gt; 参数学习"></a>-&gt; 参数学习</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122429.png" alt="image-20221215230300503"></p><p>神经网络用作多分类任务，相当于最后一层设计C个神经元。</p><p>softmax这个激活函数和其它的激活函数的区别是，这个函数的输出内容不光和前一层的内容相关，还和同一层的其它神经元的内容相关</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122430.png" alt="image-20221215230415463"></p><hr><h6 id="gt-如何计算梯度"><a href="#gt-如何计算梯度" class="headerlink" title="-&gt; 如何计算梯度"></a>-&gt; 如何计算梯度</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122431.png" alt="image-20221215231932781"></p><hr><h3 id="√-4-4-反向传播算法"><a href="#√-4-4-反向传播算法" class="headerlink" title="[√] 4.4 - 反向传播算法"></a>[√] 4.4 - 反向传播算法</h3><hr><h6 id="gt-矩阵微积分"><a href="#gt-矩阵微积分" class="headerlink" title="-&gt; 矩阵微积分"></a>-&gt; 矩阵微积分</h6><hr><p>分母布局就是使用列向量表示</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122432.png" alt="image-20221215232449651"></p><hr><h6 id="gt-链式法则"><a href="#gt-链式法则" class="headerlink" title="-&gt; 链式法则"></a>-&gt; 链式法则</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122433.png" alt="image-20221215232719986"></p><hr><h6 id="gt-计算梯度"><a href="#gt-计算梯度" class="headerlink" title="-&gt; 计算梯度"></a>-&gt; 计算梯度</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122434.png" alt="image-20221215233211582"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122435.png" alt="image-20221215233508059"></p><p>上面的1、3项已经有了，核心是计算第二项。第二项定义为第L层的误差项。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122436.png" alt="image-20221215233946968"></p><p>通过上面的推导能够看出，第L层的导数，能够通过第L+1层的导数以及第L+1的权重等推导出，因此这里得到反向传播的链式法则。</p><p>即前面的层能够通过后面的层推出，因此从最后一层开始，能够逐层的推导出前面层的导数（梯度）。然后通过比如梯度下降算法优化权重参数，就能够进行模型训练。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122437.png" alt="image-20221215234352611"></p><p>由上图看出，损失函数对第L层的w和b的导数，能够通过第L+1层的导数和第L-1层的激活层求出。</p><hr><h6 id="gt-使用反向传播算法的随机梯度下降训练过程"><a href="#gt-使用反向传播算法的随机梯度下降训练过程" class="headerlink" title="-&gt; 使用反向传播算法的随机梯度下降训练过程"></a>-&gt; 使用反向传播算法的随机梯度下降训练过程</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122438.png" alt="image-20221215234604104"></p><p>先前馈计算每一层的Z_l和A_l，其中A_l最后在梯度的反向传播计算的时候是需要用到的。</p><p>然后反向传播计算每一层参数的导数</p><p>最后用梯度下降算法更新参数</p><p>直到在验证集V上的错误率不再下降则停止训练</p><p>最后得到w和b</p><hr><h3 id="√-4-5-计算图与自动微分"><a href="#√-4-5-计算图与自动微分" class="headerlink" title="[√] 4.5 - 计算图与自动微分"></a>[√] 4.5 - 计算图与自动微分</h3><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122439.png" alt="image-20221217102433490"></p><hr><h6 id="gt-计算图与自动微分"><a href="#gt-计算图与自动微分" class="headerlink" title="-&gt; 计算图与自动微分"></a>-&gt; 计算图与自动微分</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122440.png" alt="image-20221217103114468"></p><hr><h6 id="gt-计算图"><a href="#gt-计算图" class="headerlink" title="-&gt; 计算图"></a>-&gt; 计算图</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122441.png" alt="image-20221217103303107"></p><p>通过上面的这种链式的计算方法，框架能够自动计算梯度，因此就不用人工计算，非常方便。</p><hr><h6 id="gt-自动微分"><a href="#gt-自动微分" class="headerlink" title="-&gt; 自动微分"></a>-&gt; 自动微分</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122442.png" alt="image-20221217103834792"></p><p>前向模式和反向模式的不同在于，前向模式由于是从前往后计算梯度链式法则中的每项，因此在计算的过程中需要保留中间项，如果链式非常长的话，那么需要保留的中间项非常多；因此一般使用反向模式。</p><hr><h6 id="gt-PyTorch例子"><a href="#gt-PyTorch例子" class="headerlink" title="-&gt; PyTorch例子"></a>-&gt; PyTorch例子</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122443.png" alt="image-20221217103921199"></p><hr><h6 id="gt-反向传播算法（自动微分的反向模式）"><a href="#gt-反向传播算法（自动微分的反向模式）" class="headerlink" title="-&gt; 反向传播算法（自动微分的反向模式）"></a>-&gt; 反向传播算法（自动微分的反向模式）</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122444.png" alt="image-20221217104135791"></p><hr><h6 id="gt-静态计算图和动态计算图"><a href="#gt-静态计算图和动态计算图" class="headerlink" title="-&gt; 静态计算图和动态计算图"></a>-&gt; 静态计算图和动态计算图</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122445.png" alt="image-20221217104542264"></p><p>静态计算图的计算效率比较高</p><p>动态计算图更加的灵活</p><hr><h6 id="gt-如何实现"><a href="#gt-如何实现" class="headerlink" title="-&gt; 如何实现"></a>-&gt; 如何实现</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122446.png" alt="image-20221217104633188"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122447.png" alt="image-20221217104843147"></p><p>keras在tensorflow的基础上，又进行了一次封装</p><hr><h6 id="gt-深度学习的三个步骤"><a href="#gt-深度学习的三个步骤" class="headerlink" title="-&gt; 深度学习的三个步骤"></a>-&gt; 深度学习的三个步骤</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122448.png" alt="image-20221217104950753"></p><hr><h3 id="√-4-6-优化问题"><a href="#√-4-6-优化问题" class="headerlink" title="[√] 4.6 - 优化问题"></a>[√] 4.6 - 优化问题</h3><hr><hr><h6 id="gt-神经网络优化问题之非凸优化问题"><a href="#gt-神经网络优化问题之非凸优化问题" class="headerlink" title="-&gt; 神经网络优化问题之非凸优化问题"></a>-&gt; 神经网络优化问题之非凸优化问题</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122449.png" alt="image-20221217105432953"></p><p>非凸函数，优化困难，比如存在局部最优问题，局部最小值如何再找到全局最小值是非常困难的。</p><p>另外，在高维中存在鞍点问题，使用梯度下降方法到了鞍点就走不动了</p><hr><h6 id="gt-神经网络优化问题之梯度消失问题"><a href="#gt-神经网络优化问题之梯度消失问题" class="headerlink" title="-&gt; 神经网络优化问题之梯度消失问题"></a>-&gt; 神经网络优化问题之梯度消失问题</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122450.png" alt="image-20221217105910303"></p><p>当梯度的链式很长，每个因子都在0-1之间，那么最后梯度整体就非常小，非常接近于0，这就是梯度消失问题，会导致更新很慢、很难学。</p><p>因此让激活函数最后在1左右是最好的，不能太小，也不能太大。这也是为什么激活函数推荐使用ReLU函数。因为ReLU函数在正的范围梯度是1.</p><hr><h6 id="gt-优化问题"><a href="#gt-优化问题" class="headerlink" title="-&gt; 优化问题"></a>-&gt; 优化问题</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122451.png" alt="image-20221217110122249"></p><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3 - 线性分类</title>
    <link href="/posts/292348030/"/>
    <url>/posts/292348030/</url>
    
    <content type="html"><![CDATA[<h2 id="3-课节3-线性分类"><a href="#3-课节3-线性分类" class="headerlink" title="3 - 课节3: 线性分类"></a>3 - 课节3: 线性分类</h2><h3 id="3-0-线性模型概述"><a href="#3-0-线性模型概述" class="headerlink" title="3.0 - 线性模型概述"></a>3.0 - 线性模型概述</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119145.png" alt="image-20221208180014930"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119318.png" alt="image-20221208180106694"></p><hr><h3 id="3-1-分类问题示例"><a href="#3-1-分类问题示例" class="headerlink" title="3.1 - 分类问题示例"></a>3.1 - 分类问题示例</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119319.png" alt="image-20221208180322450"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119320.png" alt="image-20221208180504177"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119321.png" alt="image-20221208180801488"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119322.png" alt="image-20221208181224143"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119323.png" alt="image-20221208181333389"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119324.png" alt="image-20221208181420194"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119325.png" alt="image-20221208181501060"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119326.png" alt="image-20221208182020884"></p><p>这种方式的缺点在于将文本的语序信息丢掉了</p><hr><h3 id="3-2-线性分类模型"><a href="#3-2-线性分类模型" class="headerlink" title="3.2 - 线性分类模型"></a>3.2 - 线性分类模型</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119327.png" alt="image-20221208212705641"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119328.png" alt="image-20221208212946373"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119329.png" alt="image-20221208213134894"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119330.png" alt="image-20221208214229031"></p><p>分类问题因为y不可导，因此要寻找更优的损失函数</p><h3 id="3-3-交叉熵与对数似然"><a href="#3-3-交叉熵与对数似然" class="headerlink" title="3.3 - 交叉熵与对数似然"></a>3.3 - 交叉熵与对数似然</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119331.png" alt="image-20221208214634724"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119332.png" alt="image-20221208215313851"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119333.png" alt="image-20221208215907378"></p><blockquote><p>交叉熵</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119334.png" alt="image-20221208220146206"></p><p>交叉熵可以用来衡量两个分布的差异。如果两个分布越近，那么蕴含的信息越少，交叉熵越小。</p><blockquote><p>KL散度</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119335.png" alt="image-20221208220427328"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119336.png" alt="image-20221208221107423"></p><h3 id="3-4-具体的线性分类模型：Logistic回归"><a href="#3-4-具体的线性分类模型：Logistic回归" class="headerlink" title="3.4 - 具体的线性分类模型：Logistic回归"></a>3.4 - 具体的线性分类模型：Logistic回归</h3><p>逻辑判断函数是不可导的，因此不能通过优化损失来学习。因此就需要一个可导的损失函数来优化。</p><p>因此需要将分类问题转换为概率的估计问题。通过交叉熵来建立损失函数。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119337.png" alt="image-20221208222452651"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119338.png" alt="image-20221208222919588"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119339.png" alt="image-20221208223403392"></p><p>σ(x)通常被指代为logistic函数。</p><p>通过这种方式，将实数域的值映射到（0,1）之间，转化为0-1之间的概率分布问题。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119340.png" alt="image-20221208223833903"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119341.png" alt="image-20221208224347080"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119342.png" alt="image-20221208224619407"></p><h3 id="3-5-Softmax回归"><a href="#3-5-Softmax回归" class="headerlink" title="3.5 - Softmax回归"></a>3.5 - Softmax回归</h3><p>logistic回归是用于二分类问题的，其在多分类问题下的扩展形式是softmax回归。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119343.png" alt="image-20221208225612942"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119344.png" alt="image-20221208225923034"></p><p>softmax函数，将预测的内容转化为总和为1的概率。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119345.png" alt="image-20221208230459772"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119346.png" alt="image-20221208230719167"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119347.png" alt="image-20221208230914312"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119348.png" alt="image-20221208231205625"></p><h3 id="3-6-感知器"><a href="#3-6-感知器" class="headerlink" title="3.6 - 感知器"></a>3.6 - 感知器</h3><p>感知器目前是一个简单的线性分类器</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119349.png" alt="image-20221209135048024"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119350.png" alt="image-20221209135431524"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119351.png" alt="image-20221209135755122"></p><p>感知器这种学习方式类似于现有的随机梯度下降算法，即每次选择一组数据进行训练更新。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119352.png" alt="image-20221209140126702"></p><p>对于logistic回归来说，参数w的更新，要看犯错的程度，如果输出和标签之前的差异越小，那么参数w的更新越小。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119353.png" alt="image-20221209140257280"></p><p>而感知器是不参考犯错的程度的，只要犯错就按照特定的方式更新。</p><p>但是感知器在正确分类的时候是不更新的，这一点是比较好的。</p><blockquote><p>感知器的更新过程</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119354.png" alt="image-20221209140850180"></p><blockquote><p>感知器很好的性能：收敛性</p></blockquote><p>如果数据集是线性可分的话，那么模型一定会在有限的更新次数内找到使得数据分开的权重</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119355.png" alt="image-20221209141216585"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119356.png" alt="image-20221209142003870"></p><hr><h3 id="3-7-支持向量机"><a href="#3-7-支持向量机" class="headerlink" title="3.7 - 支持向量机"></a>3.7 - 支持向量机</h3><p>感知器存在的问题：分界面有可能找到多个，能不能找到一个最好的分界面？</p><p>理想的分界面是距离所有的数据有比较远，这样直观上感觉健壮性会更好。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119357.png" alt="image-20221209142417975"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119358.png" alt="image-20221209142511906"></p><p>支持向量机的优化标准就是选择间隔最大的分界线。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119359.png" alt="image-20221209143003355"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119360.png" alt="image-20221209143355250"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119361.png" alt="image-20221209143527567"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119362.png" alt="image-20221209151217297"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119363.png" alt="image-20221209151930652"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119364.png" alt="image-20221209152107398"></p><h3 id="3-8-线性分类模型小结"><a href="#3-8-线性分类模型小结" class="headerlink" title="3.8 - 线性分类模型小结"></a>3.8 - 线性分类模型小结</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119365.png" alt="image-20221209154000340"></p><p>yf(x;w)为正的话，表示分类正确，为负数表示分类错误，且该数越大则表示分类越正确。</p><p>&#x3D;&#x3D;平方损失分析：&#x3D;&#x3D;</p><p>当yf(x;w)大于1的时候，随着增大应该，损失应该减小，但是在图中看出损失却随着yf(x;w)的增加而增大，因此平方损失是不适合做分类任务的。</p><p>&#x3D;&#x3D;logistic回归的损失函数：交叉熵损失函数&#x3D;&#x3D;</p><p>在图中可以看出，随着yf(x;w）的增加，损失是下降的。这样是合理的。</p><p>虽然是合理的，但是在分类正确的情况下可以看出交叉熵损失仍然是有惩罚的，因此虽然合理，但是对于分类任务来讲，分类正确仍然惩罚是没有必要的。这个损失函数依然有改进的空间。</p><p>&#x3D;&#x3D;感知器的损失函数：&#x3D;&#x3D;</p><p>感知器的损失可以看出是标准的为分类而设计的，如果小于0则有损失，如果大于0分类正确则没有损失。</p><p>&#x3D;&#x3D;软间隔的支持向量机的损失函数：&#x3D;&#x3D;</p><p>在距离边界比较近的地方，依然是有惩罚的。</p><p>软间隔的SVM的loss在直觉上会带来更好的效果。</p><blockquote><p>线性分类模型小结</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119366.png" alt="image-20221209154049507"></p><blockquote><p>线性分类器无法解决非线性问题</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119367.png" alt="image-20221209154332443"></p><blockquote><p>使用“基函数”的广义线性模型解决非线性问题的分类问题</p></blockquote><p>使用基函数，将分布映射到另一个可分的空间就可以进行分类了</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119368.png" alt="image-20221209154631205"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212119369.png" alt="image-20221209154734740"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2 - 机器学习</title>
    <link href="/posts/3917031307/"/>
    <url>/posts/3917031307/</url>
    
    <content type="html"><![CDATA[<h2 id="2-课节2-机器学习"><a href="#2-课节2-机器学习" class="headerlink" title="2 - 课节2: 机器学习"></a>2 - 课节2: 机器学习</h2><h3 id="2-0-机器学习概述"><a href="#2-0-机器学习概述" class="headerlink" title="2.0 - 机器学习概述"></a>2.0 - 机器学习概述</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110258.png" alt="image-20221208121015008"></p><hr><h3 id="2-1-关于概率的一些基本概念"><a href="#2-1-关于概率的一些基本概念" class="headerlink" title="2.1 - 关于概率的一些基本概念"></a>2.1 - 关于概率的一些基本概念</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110259.png" alt="image-20221208121409099"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110260.png" alt="image-20221208121549907"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110261.png" alt="image-20221208122203885"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110262.png" alt="image-20221208122405057"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110263.png" alt="image-20221208122650924"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110264.png" alt="image-20221208122850740"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110265.png" alt="image-20221208123102667"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110266.png" alt="image-20221208123337529"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110267.png" alt="image-20221208124108800"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110268.png" alt="image-20221208124334869"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110269.png" alt="image-20221208124534561"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110270.png" alt="image-20221208124711955"></p><hr><h3 id="2-2-机器学习定义"><a href="#2-2-机器学习定义" class="headerlink" title="2.2 - 机器学习定义"></a>2.2 - 机器学习定义</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110271.png" alt="image-20221208124908228"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110272.png" alt="image-20221208124955058"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110273.png" alt="image-20221208125741307"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110274.png" alt="image-20221208125823265"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110275.png" alt="image-20221208131914841"></p><hr><h3 id="2-3-机器学习类型"><a href="#2-3-机器学习类型" class="headerlink" title="2.3 - 机器学习类型"></a>2.3 - 机器学习类型</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110277.png" alt="image-20221208132044174"></p><p>输出是连续的，这类问题称为回归问题。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110278.png" alt="image-20221208132134381"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110279.png" alt="image-20221208132220372"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110280.png" alt="image-20221208132239293"></p><p>最终的结果是离散的，为分类问题</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110281.png" alt="image-20221208132334658"></p><p>检测框内有没有人脸，二分类问题</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110282.png" alt="image-20221208132400834"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110283.png" alt="image-20221208132512553"></p><p>聚类问题是无监督学习问题</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110284.png" alt="image-20221208132742760"></p><p>强化学习：尝试各种可能性，不断试错，看哪种可能性对最终的结果帮助最大，然后做出选择。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110285.png" alt="image-20221208132859419"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110286.png" alt="image-20221208133001707"></p><p>半监督学习：有一部分的数据是有标注的，一部分是没有标注的</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110287.png" alt="image-20221208133147350"></p><p>类型不同，因此学习的决策函数是有差异的、学习效果的衡量标准是不同的</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110288.png" alt="image-20221208133312051"></p><h3 id="2-4-机器学习要素"><a href="#2-4-机器学习要素" class="headerlink" title="2.4 - 机器学习要素"></a>2.4 - 机器学习要素</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110289.png" alt="image-20221208133548479"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110290.png" alt="image-20221208133940863"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110291.png" alt="image-20221208134135495"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110292.png" alt="image-20221208134444527"></p><p>因为x、y之间的分布是未知的，因此期望风险是无法计算的。因此就通过大数定律来进行近似。</p><p>根据大数定律，当n趋向于无穷大的时候，经验风险逼近于期望风险。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110293.png" alt="image-20221208134705077"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110294.png" alt="image-20221208134913400"></p><p>&#x3D;&#x3D;最优化问题的解决方法1：&#x3D;&#x3D;让极值点的导数等于0，找到极值点</p><p>凸优化问题，只有一个最优点，因此容易找到。</p><p>非凸优化问题，找到最优点是比较困难的。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110295.png" alt="image-20221208135946972"></p><p>&#x3D;&#x3D;最优化问题的解决方法2:&#x3D;&#x3D;更具有一般性的解决方法，梯度下降法</p><p>梯度下降算法是一个迭代的方法，给任意一个起始点，计算这个点的梯度，沿着这个梯度的反方向走，那么这个损失值一把来说会是下降的。</p><p>如果走的步长不是太大，那么一定会收敛到一个极值点。</p><p>在梯度下降算法中，有一个参数 α 用来表示每次走的步长。在机器学习中，也叫学习率。</p><p>α 这是一个非常重要的超参数。对于损失函数中的参数，是可学习的。对于 α 是无法学习的，需要人为的去选择，所以叫超参数。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110296.png" alt="image-20221208140032340"></p><p>学习率太大的话，就来回震荡，永远不收敛；学习率太小的话，就学的很慢，或者陷入局部最优。</p><p>自适应的学习率是动态变化的，比较理想。</p><p>&#x3D;&#x3D;梯度下降法的变种：随机梯度下降算法&#x3D;&#x3D;</p><p>不需要在每个样本上采集梯度，而是随机的选择一个样本采集梯度，更新参数就可以了。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110297.png" alt="image-20221208140318590"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110298.png" alt="image-20221208142358803"></p><p>随机梯度下降算法的缺点是无法充分利用计算机的并行能力，因此一个折中的方法是<code>小批量随机梯度下降法</code>。</p><p>批量k的选择一般是将显卡的内存用满就可以了，最大程度的发挥计算机的并行计算能力。</p><p>目前大部分的机器学习算法中，通常是使用mini-batch</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110299.png" alt="image-20221208142706602"></p><p>问题1：</p><p>在随机梯度下降法中，为了避免某些样本始终采集不到，因此每次训练完一轮后，对所有的样本再次打乱随机排序，这样就避免了一些样本始终采集不到的问题。</p><p>问题2：什么时候判断SGD已经学习好了</p><p>通常停止迭代的标准是：设定另外一个验证集V，在验证集上的错误率不再下降，那么认为这个训练集已经收敛了</p><hr><h3 id="2-5-泛化与正则化"><a href="#2-5-泛化与正则化" class="headerlink" title="2.5 - 泛化与正则化"></a>2.5 - 泛化与正则化</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110300.png" alt="image-20221208143608123"></p><p>↑ ：机器学习问题不是绝对等价于优化问题。在训练数据少和噪声的情况下最优的优化会导致过拟合。</p><p>欠拟合难问题：可能是由于模型能力不够，比如本来问题是非线性的，但是我们使用线性的模型去拟合，就会导致欠拟合问题。</p><p>机器学习问题关注的不是在训练集上的错误率，而是在整个期望上错误率，即期望风险。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110301.png" alt="image-20221208143719873"></p><p>期望风险大，经验风险很小，这时就发生了过拟合。</p><p>期望风险小，经验风险很大，这时就发生了欠拟合。</p><p>机器学习的真正目标是期望期望风险和经验风险都低。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110302.png" alt="image-20221208143908028"></p><p>优化的时候，可能导致过拟合，即训练出来的模型复杂度很高。</p><p>通过一些手段，比如正则化，期望模型不要那么拟合，从而降低模型的复杂度。</p><p>正则化是降低泛化误差的一个有效手段。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110303.png" alt="image-20221208144334518"></p><p>验证集和训练集是独立的，都是独立同分布采样的</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110304.png" alt="image-20221208144527387"></p><p>提前停止是目前在机器学习上，配合SGD用的最多的一种正则化方法</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110305.png" alt="image-20221208144615740"></p><hr><h3 id="2-6-线性回归"><a href="#2-6-线性回归" class="headerlink" title="2.6 - 线性回归"></a>2.6 - 线性回归</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110306.png" alt="image-20221208144941227"></p><p>输入是一个低维的向量（R的右上角有D），输出是一个标量（R的右上角为1次方）。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110307.png" alt="image-20221208145333165"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110308.png" alt="image-20221208145745904"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110309.png" alt="image-20221208145818597"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110310.png" alt="image-20221208150122329"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110311.png" alt="image-20221208151142870"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110312.png" alt="image-20221208151813415"></p><p>为了防止经验风险输入x之间因为存在特征的冗余导致最终的解不稳定的问题，因此引入结构风险，后半部分就是正则化项。</p><p>λ 是正则化系数，是人为设置的一个超参数，λ 越大对 w 的限制越大。</p><hr><h3 id="2-7-多项式回归"><a href="#2-7-多项式回归" class="headerlink" title="2.7 - 多项式回归"></a>2.7 - 多项式回归</h3><p>多项式回归是线性回归的非线性形式。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110313.png" alt="image-20221208163135143"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110314.png" alt="image-20221208163334412"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110315.png" alt="image-20221208163518725"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110316.png" alt="image-20221208163739041"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110317.png" alt="image-20221208163905737"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110318.png" alt="image-20221208164022635"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110319.png" alt="image-20221208164219781"></p><p>防止模型过拟合的方法，除了增加正则化之外，最简单的办法就是增加训练样本的数量。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110320.png" alt="image-20221208164329463"></p><p>当样本数量N非常大的时候，经验风险就趋向于期望风险。</p><hr><h3 id="2-8-线性回归的概率视角"><a href="#2-8-线性回归的概率视角" class="headerlink" title="2.8 - 线性回归的概率视角"></a>2.8 - 线性回归的概率视角</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110321.png" alt="image-20221208165239356"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110322.png" alt="image-20221208165424225"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110323.png" alt="image-20221208165539745"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110324.png" alt="image-20221208165856833"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110325.png" alt="image-20221208170022474"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110326.png" alt="image-20221208170256637"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110327.png" alt="image-20221208170714384"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110328.png" alt="image-20221208170752130"></p><p>最大后验估计和结构风险最小化是非常相似的</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110329.png" alt="image-20221208171046111"></p><hr><h3 id="2-9-模型选择与”偏差-方差”分解"><a href="#2-9-模型选择与”偏差-方差”分解" class="headerlink" title="2.9 - 模型选择与”偏差-方差”分解"></a>2.9 - 模型选择与”偏差-方差”分解</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110330.png" alt="image-20221208171250746"></p><p>如何选择模型：引入验证集帮助选择模型，选择在验证集上错误最小的模型</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110331.png" alt="image-20221208171530737"></p><p>&#x3D;&#x3D;使用验证集存在的问题：&#x3D;&#x3D;</p><p>由于本来数据就少，还要拿出一部分作为验证集，就会导致训练数据更加的稀疏。</p><p>解决方法是<code>交叉验证</code>的方法。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110332.png" alt="image-20221208171732136"></p><p>除了在验证集的指导下选择模型，还可以在一些准则的指导下选择模型。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110333.png" alt="image-20221208173151845"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110334.png" alt="image-20221208174034082"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110335.png" alt="image-20221208174418466"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110336.png" alt="image-20221208174456015"></p><p>1）偏差：在不同的数据集学习到的模型的平均值和最优模型之间的差</p><p>2）方差：在不同的数据集学习到的模型之间的差值</p><p>3）机器学习模型无法避免的错误</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110337.png" alt="image-20221208174639636"></p><p>低方差、低偏差：理想、最优</p><p>低方差、高偏差：通常是模型能力不够，欠拟合</p><p>低偏差、高方差：模型能力是可以的，但是能力过高、过拟合</p><p>高偏差、高方差：尽可能的避免</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212110338.png" alt="image-20221208175418842"></p><p>通常来讲，随着模型复杂度的增加，模型的偏差是在不断地减小、方差是在不断的增加。即完成了任务、但是开始躁动、不稳定了。</p><p>最优的模型不一定是处于偏差线和方差线之间的交点上。</p><p>随着模型能力的提高，开始过拟合，即低偏差、高方差。</p><p>解决<code>低偏差、高方差</code>问题的一个手段是集成模型。即将在不同的数据集上训练出来的模型做一个平均。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1 - 课节1 绪论</title>
    <link href="/posts/2249908027/"/>
    <url>/posts/2249908027/</url>
    
    <content type="html"><![CDATA[<h2 id="1-课节1-绪论"><a href="#1-课节1-绪论" class="headerlink" title="1 - 课节1: 绪论"></a>1 - 课节1: 绪论</h2><h3 id="1-0-绪论"><a href="#1-0-绪论" class="headerlink" title="1.0 - 绪论"></a>1.0 - 绪论</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107424.png" alt="image-20221207170827279"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107686.png" alt="image-20221207171145135"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107687.png" alt="image-20221207171847495"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107688.png" alt="image-20221207171854477"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107689.png" alt="image-20221207171903791"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107690.png" alt="image-20221207171958377"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107691.png" alt="image-20221207172206926"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107692.png" alt="image-20221207172316695"></p><hr><h3 id="1-1-人工智能"><a href="#1-1-人工智能" class="headerlink" title="1.1 - 人工智能"></a>1.1 - 人工智能</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107693.png" alt="image-20221207173118410"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107694.png" alt="image-20221207173533968"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107695.png" alt="image-20221207174108012"></p><hr><h3 id="1-2-如何开发人工智能系统"><a href="#1-2-如何开发人工智能系统" class="headerlink" title="1.2 - 如何开发人工智能系统"></a>1.2 - 如何开发人工智能系统</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107696.png" alt="image-20221207174718263"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107697.png" alt="image-20221207204413823"></p><hr><h3 id="1-3-表示学习"><a href="#1-3-表示学习" class="headerlink" title="1.3 - 表示学习"></a>1.3 - 表示学习</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107698.png" alt="image-20221207204933064"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107699.png" alt="image-20221207205211147"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107700.png" alt="image-20221207205339195"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107701.png" alt="image-20221207205748579"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107702.png" alt="image-20221207210030232"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107703.png" alt="image-20221207210139541"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107704.png" alt="image-20221207210231829"><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107705.png" alt="image-20221207210232055"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107706.png" alt="image-20221207210342202"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107707.png" alt="image-20221207210452441"></p><ul><li>特征提取含有人工的过程，不能保证学习到的特征一定能帮助于分类</li><li>表示学习是将输入和输出直接串联到一起，希望学到的这种表示(特征)是对后面的这种分类是直接有帮助的，希望这种表示能够蕴含高层的语义特征。表示学习的难点在于没有明确的目标。它所有的信息都间接的来自于后面的分类器的效果。因此这种学习要和整个模型的预测效果一起学习，所谓的从输入到输出的端到端学习。</li></ul><hr><h3 id="1-4-深度学习"><a href="#1-4-深度学习" class="headerlink" title="1.4 - 深度学习"></a>1.4 - 深度学习</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107708.png" alt="image-20221207215041274"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107709.png" alt="image-20221207215210308"></p><ul><li>深度学习和浅层学习的区别在于，不需要人为的去进行特征的提取，而是由计算机来自动的学习提取特征。浅层的学习的时候，比如芒果分类，需要人为的寻找出比如颜色、大小、品种、价格等特征，然后作为输入特征x，以及标签甜度y，来进行浅层的预测学习映射函数。而到了深度学习的这里，不需要人为的去设计特征，而是由计算机自动的去学习特征。深度学习 &#x3D; 表示学习(特征学习)+浅层学习(预测、决策学习)。</li></ul><p>深度学习和浅层学习相比，难点、也就是和浅层学习相比不同点是什么，核心是贡献度分配问题。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107710.png" alt="image-20221207215751482"></p><ul><li>即到底是哪个模块对最终的预测结果影响最大、即贡献最大。这就是贡献度分配问题。</li><li>无法直接推导出是哪个模块贡献度最大，就比如下棋，无法知道哪一步棋对最终的结果影响最大，只能是得到最后的结果之后，一步步的往前推演，才能知道哪一步对最终的胜局帮助最大。</li><li>在理想情况下可以通过强化学习来解决贡献度分配问题，但是在一般的学习中无法解决贡献度分配问题。一般情况下解决贡献度分配问题的很好的模型就是神经网络。</li></ul><p>&#x3D;&#x3D;端到端：&#x3D;&#x3D;</p><p>端到端学习就是在整个过程中，中间是没有任何干预的。</p><p>&#x3D;&#x3D;浅层学习：&#x3D;&#x3D;</p><p>y &#x3D; f(x)，即输入特征x和结果y，计算机自动来学习分类器映射函数f。在深度学习中，可能x这个特征也不是直接拿到的，这个也是需要计算机来自动的学习。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107711.png" alt="image-20221207224308969"></p><hr><h3 id="1-5-神经网络"><a href="#1-5-神经网络" class="headerlink" title="1.5 - 神经网络"></a>1.5 - 神经网络</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107712.png" alt="image-20221207224430875"></p><p>单个神经元，通过树突接受来自其它神经元的刺激，当这种刺激积累到一定的程度，就会兴奋，如果没有达到这个阈值的话，就是抑制状态，即不产生信号。当产生兴奋的时候，通过轴突将信号传递给其它的神经元。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107713.png" alt="image-20221207224729031"></p><p>&#x3D;&#x3D;如何模拟人工神经元&#x3D;&#x3D;</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107714.png" alt="image-20221207224901433"></p><p>x代表来自其它神经元的信息，即当前神经元接收的信号</p><p>w用来模拟不同神经元之间的连接强度</p><p>激活函数表示为阈值函数，也可以是上面的这种阈值函数。如果上面的接收的信号汇总起来越小，那么值越低、就不兴奋；汇总的信息量越大，那么会兴奋。模拟人类的神经元的兴奋与抑制。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107715.png" alt="image-20221207225326740"></p><p>不同的神经网络的区别主要在于上面的三个方面的区别。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107716.png" alt="image-20221207225434013"></p><p>前馈网络：信息是单向传递的</p><p>记忆网络：是有反馈边的</p><p>图网络：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107717.png" alt="image-20221207225840718"></p><p>每一层都可以看做是特征的抽取器，整个的前馈网络的学习可以看成是端到端的学习。前面的可以看成是表示学习，后面的是浅层学习。</p><p>&#x3D;&#x3D;神经网络是如何解决贡献度分配问题的额呢？&#x3D;&#x3D;</p><p>核心是神经网络是连续可导的。</p><p>如何看x对y的影响是多大呢，可以在某个位置对x做一个扰动，看y的变动有多少，这样就能知道当前x的贡献度。从而知道当前x的贡献度。</p><p>如果当前的参数比较重要的话，那么对输入做一个扰动，那么输出的变化应该就很大，意味着贡献度很大。因此这样就能确定在每一个模块中的贡献度问题。当结果不好的时候就知道应该调哪些参数，因此神经网络比较完美的解决了贡献度分配问题。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107718.png" alt="image-20221207230411453"></p><p>神经网络给深度学习提供了一种很好的解决贡献度问题的方法。</p><hr><h3 id="1-6-神经网络发展史"><a href="#1-6-神经网络发展史" class="headerlink" title="1.6 - 神经网络发展史"></a>1.6 - 神经网络发展史</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107719.png" alt="image-20221207230704947"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107720.png" alt="image-20221207230853446"></p><p>如何优化神经网络：反向传播算法</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107721.png" alt="image-20221207230956143"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107722.png" alt="image-20221207231118538"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212107723.png" alt="image-20221207231238666"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>视频学习</category>
      
      <category>神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）</category>
      
      <category>笔记</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6 - 循环神经网络 - 视频</title>
    <link href="/posts/1557301731/"/>
    <url>/posts/1557301731/</url>
    
    <content type="html"><![CDATA[<h2 id="√-6-1-循环神经网络及应用模式"><a href="#√-6-1-循环神经网络及应用模式" class="headerlink" title="[√] 6.1 - 循环神经网络及应用模式"></a>[√] 6.1 - 循环神经网络及应用模式</h2><hr><h4 id="√-循环神经网络和前馈神经网络"><a href="#√-循环神经网络和前馈神经网络" class="headerlink" title="[√] 循环神经网络和前馈神经网络"></a>[√] 循环神经网络和前馈神经网络</h4><hr><blockquote><p>alec：</p><ul><li>NLP:one-hot向量维度1w+、embedding维度128+</li><li>↑：将单词转成在计算机中的表示形式：编码为one-hot向量或者embedding编码</li><li>循环神经网络的显著特点：<ul><li>时间步上的连接</li><li>权重共享</li></ul></li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700369.png" alt="image-20221221220504768"></p><h4 id="√-循环神经网络-RNN"><a href="#√-循环神经网络-RNN" class="headerlink" title="[√] 循环神经网络(RNN)"></a>[√] 循环神经网络(RNN)</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700370.png" alt="image-20221221220631399"></p><h4 id="√-RNN常见的应用模式"><a href="#√-RNN常见的应用模式" class="headerlink" title="[√] RNN常见的应用模式"></a>[√] RNN常见的应用模式</h4><hr><h6 id="√-序列到类别"><a href="#√-序列到类别" class="headerlink" title="[√] 序列到类别"></a>[√] 序列到类别</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700371.png" alt="image-20221221220803433"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700372.png" alt="image-20221221220835629"></p><h6 id="√-同步的序列到序列"><a href="#√-同步的序列到序列" class="headerlink" title="[√] 同步的序列到序列"></a>[√] 同步的序列到序列</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700373.png" alt="image-20221221220921762"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700374.png" alt="image-20221221220936757"></p><h6 id="√-异步的序列到序列模式"><a href="#√-异步的序列到序列模式" class="headerlink" title="[√] 异步的序列到序列模式"></a>[√] 异步的序列到序列模式</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700375.png" alt="image-20221221221029056"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700376.png" alt="image-20221221221043080"></p><h2 id="√-6-2-SRN的记忆能力实验"><a href="#√-6-2-SRN的记忆能力实验" class="headerlink" title="[√] 6.2 - SRN的记忆能力实验"></a>[√] 6.2 - SRN的记忆能力实验</h2><hr><h4 id="√-RNN与SRN"><a href="#√-RNN与SRN" class="headerlink" title="[√] RNN与SRN"></a>[√] RNN与SRN</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700377.png" alt="image-20221221221321729"></p><h4 id="√-机器学习实践五要素"><a href="#√-机器学习实践五要素" class="headerlink" title="[√] 机器学习实践五要素"></a>[√] 机器学习实践五要素</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700378.png" alt="image-20221221221507667"></p><h4 id="√-SRN记忆能力实验-模型构建"><a href="#√-SRN记忆能力实验-模型构建" class="headerlink" title="[√] SRN记忆能力实验-模型构建"></a>[√] SRN记忆能力实验-模型构建</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700379.png" alt="image-20221222130706448"></p><blockquote><p>alec：</p><ul><li>嵌入层的作用，是将输入的单词转换为一个向量，方便进行表示和学习</li><li>输入的shape为B×L×M，其中B是批量大小，L是时间维度的长度，M是单词转换为向量之后一个单词的通道数维度</li></ul></blockquote><h6 id="√-通过查表和one-hot两种方式，将数字映射为向量"><a href="#√-通过查表和one-hot两种方式，将数字映射为向量" class="headerlink" title="[√] 通过查表和one-hot两种方式，将数字映射为向量"></a>[√] 通过查表和one-hot两种方式，将数字映射为向量</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700380.png" alt="image-20221222131025931"></p><p>嵌入矩阵W的维度为（嵌入向量的数量A，嵌入向量的维度B），然后查表，输出一个维度为B的转换后的向量。</p><h6 id="√-SRN层"><a href="#√-SRN层" class="headerlink" title="[√] SRN层"></a>[√] SRN层</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700381.png" alt="image-20221222131209016"></p><p>嵌入层将每个输入的单词，转换为一个M维的表示向量</p><p>输入向量的通道数维度为M，状态向量的通道数维度为D</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700382.png" alt="image-20221222131744903"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700383.png" alt="image-20221222131626260"></p><p>此处初始化H_0</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700384.png" alt="image-20221222131700205"></p><p>不同的时间步，一步一步的往前传递</p><h6 id="√-手动推导SRN前向计算"><a href="#√-手动推导SRN前向计算" class="headerlink" title="[√] 手动推导SRN前向计算"></a>[√] 手动推导SRN前向计算</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700385.png" alt="image-20221222131952437"></p><blockquote><p>alec：</p><ul><li>可以看出，对于SRN，不同的时间步，参数W和U，是一样的。这个就是权重共享。</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700386.png" alt="image-20221222132032699"></p><h6 id="√-自定义SRN和Paddle-SRN速度对比"><a href="#√-自定义SRN和Paddle-SRN速度对比" class="headerlink" title="[√] 自定义SRN和Paddle SRN速度对比"></a>[√] 自定义SRN和Paddle SRN速度对比</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700387.png" alt="image-20221222132528171"></p><h6 id="√-线性层"><a href="#√-线性层" class="headerlink" title="[√] 线性层"></a>[√] 线性层</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700388.png" alt="image-20221222132616133"></p><p>线性层直接使用全连接实现。</p><h6 id="√-模型汇总"><a href="#√-模型汇总" class="headerlink" title="[√] 模型汇总"></a>[√] 模型汇总</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700389.png" alt="image-20221222133030416"></p><p>SRN的构成：嵌入层 + SRN层 + 线性层</p><p>其中线性层用于将最终的状态转为预测的类别</p><p>本实验案例中，前两个数字都是0-9，因此最终的结果再0-18中，所以最终线性层输出中，一共预测19个分类。</p><h6 id="√-多组训练"><a href="#√-多组训练" class="headerlink" title="[√] 多组训练"></a>[√] 多组训练</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700390.png" alt="image-20221222133227173"></p><p>模型中的shape，长度指的是时间维度的长度</p><h6 id="√-SRN在不同长度数据集训练损失的变化"><a href="#√-SRN在不同长度数据集训练损失的变化" class="headerlink" title="[√] SRN在不同长度数据集训练损失的变化"></a>[√] SRN在不同长度数据集训练损失的变化</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700391.png" alt="image-20221222133415317"></p><p>可以看出，长度越长，效果越差。网络的记忆能力越差，越来越记不住了。</p><h6 id="√-利用测试集对指定长度的模型评价"><a href="#√-利用测试集对指定长度的模型评价" class="headerlink" title="[√] 利用测试集对指定长度的模型评价"></a>[√] 利用测试集对指定长度的模型评价</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700392.png" alt="image-20221222133557984"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700393.png" alt="image-20221222133615995"></p><h2 id="√-6-3-SRN的记忆能力实验代码演示"><a href="#√-6-3-SRN的记忆能力实验代码演示" class="headerlink" title="[√] 6.3 - SRN的记忆能力实验代码演示"></a>[√] 6.3 - SRN的记忆能力实验代码演示</h2><hr><blockquote><p>alec:</p><ul><li>SRN层将最后时刻的隐状态作为整个序列的表示</li><li>SRN中，嵌入层和SRN都是可以学习的</li></ul></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700394.png" alt="image-20221222134930771"></p><p>嵌入矩阵的维度，是10xM，意思是这个嵌入矩阵可以转换10个数字为M维的向量，10的意思是用于查表。通过第一个维度的索引来查表。</p><h2 id="√-6-4-SRN的梯度爆炸实验"><a href="#√-6-4-SRN的梯度爆炸实验" class="headerlink" title="[√] 6.4 - SRN的梯度爆炸实验"></a>[√] 6.4 - SRN的梯度爆炸实验</h2><hr><blockquote><p>alec:</p><p>模型优化目标通常为损失函数和正则化项的加权组合</p></blockquote><h4 id="√-SRN梯度爆炸实验-实验说明"><a href="#√-SRN梯度爆炸实验-实验说明" class="headerlink" title="[√] SRN梯度爆炸实验-实验说明"></a>[√] SRN梯度爆炸实验-实验说明</h4><hr><h6 id="√-SRN的主要问题"><a href="#√-SRN的主要问题" class="headerlink" title="[√] SRN的主要问题"></a>[√] SRN的主要问题</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700395.png" alt="image-20221222142341122"></p><blockquote><p>alec：</p><p>梯度爆炸相对来说容易解决，通过梯度截断的方式来解决</p><p>梯度消失问题，通过改变模型，比如通过LSTM模型来解决梯度消失问题</p></blockquote><h4 id="√-SRN梯度爆炸实验-复现梯度爆炸实验"><a href="#√-SRN梯度爆炸实验-复现梯度爆炸实验" class="headerlink" title="[√] SRN梯度爆炸实验-复现梯度爆炸实验"></a>[√] SRN梯度爆炸实验-复现梯度爆炸实验</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700396.png" alt="image-20221222142737167"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700397.png" alt="image-20221222142751623"></p><h4 id="√-SRN梯度爆炸实验-解决梯度爆炸问题"><a href="#√-SRN梯度爆炸实验-解决梯度爆炸问题" class="headerlink" title="[√] SRN梯度爆炸实验-解决梯度爆炸问题"></a>[√] SRN梯度爆炸实验-解决梯度爆炸问题</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700398.png" alt="image-20221222142907961"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700399.png" alt="image-20221222142934352"></p><h2 id="√-6-5-LSTM的记忆能力实验"><a href="#√-6-5-LSTM的记忆能力实验" class="headerlink" title="[√] 6.5 - LSTM的记忆能力实验"></a>[√] 6.5 - LSTM的记忆能力实验</h2><hr><h4 id="√-机器学习实践五要素-1"><a href="#√-机器学习实践五要素-1" class="headerlink" title="[√] 机器学习实践五要素"></a>[√] 机器学习实践五要素</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700400.png" alt="image-20221222143136583"></p><h4 id="√-LSTM记忆能力实验-实验说明"><a href="#√-LSTM记忆能力实验-实验说明" class="headerlink" title="[√] LSTM记忆能力实验-实验说明"></a>[√] LSTM记忆能力实验-实验说明</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700401.png" alt="image-20221222143212076"></p><h4 id="√-LSTM记忆能力实验-模型介绍"><a href="#√-LSTM记忆能力实验-模型介绍" class="headerlink" title="[√] LSTM记忆能力实验-模型介绍"></a>[√] LSTM记忆能力实验-模型介绍</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700402.png" alt="image-20221222144628564"></p><blockquote><p>alec：</p><p>LSTM引入了内部状态C和输出状态H，同时引入了比较复杂的门控机制。</p><p>引入内部状态C是为了做序列上的记忆。</p><p>这样的好处是，一个状态用来做记忆，一个状态用来做输出。</p><p>一共有三个门控，分别是遗忘门、输入门、输出门</p><p>其中遗忘门是控制器前一个状态的C_t-1</p><p>输入门是用来控制新的tanh(x_t, h_t-1)</p><p>输出门空来控制c_t-1, h_t-1, x_t的混合计算之后的非线性输出的大小</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700403.png" alt="image-20221222144727692"></p><h4 id="√-LSTM记忆能力实验-模型实现"><a href="#√-LSTM记忆能力实验-模型实现" class="headerlink" title="[√] LSTM记忆能力实验-模型实现"></a>[√] LSTM记忆能力实验-模型实现</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700404.png" alt="image-20221222145003973"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700405.png" alt="image-20221222145013174"></p><blockquote><p>alec：</p><p>其中初始状态，可以是外面传进来，也可以是默认的初始化</p></blockquote><h4 id="√-LSTM记忆能力实验-模型构建"><a href="#√-LSTM记忆能力实验-模型构建" class="headerlink" title="[√] LSTM记忆能力实验-模型构建"></a>[√] LSTM记忆能力实验-模型构建</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700406.png" alt="image-20221222145334715"></p><p>其中，权重W、U、B在不同的时间步是权重共享的</p><h4 id="√-LSTM记忆能力实验-模型评价"><a href="#√-LSTM记忆能力实验-模型评价" class="headerlink" title="[√] LSTM记忆能力实验-模型评价"></a>[√] LSTM记忆能力实验-模型评价</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700407.png" alt="image-20221222150122638"></p><h4 id="√-LSTM记忆能力实验-可视化"><a href="#√-LSTM记忆能力实验-可视化" class="headerlink" title="[√] LSTM记忆能力实验-可视化"></a>[√] LSTM记忆能力实验-可视化</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700408.png" alt="image-20221222150146927"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700409.png" alt="image-20221222150244590"></p><h2 id="√-6-6-LSTM的记忆能力实验代码演示"><a href="#√-6-6-LSTM的记忆能力实验代码演示" class="headerlink" title="[√] 6.6 - LSTM的记忆能力实验代码演示"></a>[√] 6.6 - LSTM的记忆能力实验代码演示</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700410.png" alt="image-20221222153312595"></p><blockquote><p>alec：</p><ul><li>三个门：<ul><li>遗忘门、输入门、输出门</li></ul></li><li>三个状态<ul><li>输出状态、内部状态（记忆 ）、候选状态</li></ul></li><li>遗忘门控制t-1的内部状态、输入门控制候选状态、输出门控制当前的记忆状态</li></ul><hr><ul><li>alec: 每个时刻对应单元的内部记忆状态C_t和输出状态h_t其实是有关系，C是未激活和门控的内容，H是C经过激活和门控后的内容。将C作为记忆直接传给下一个单元用作信息参考，同时将C激活和门控之后，传给下一个单元。</li><li>个人理解：将未激活的C直接往后传，其实类似于残差。为了防止梯度消失问题，并且提供给深层更多的信息参考，因此通过这种类似于残差的直连边的方式给后面的层提供更多的信息。</li></ul></blockquote><h2 id="√-6-7-双向LSTM完成文本分类任务"><a href="#√-6-7-双向LSTM完成文本分类任务" class="headerlink" title="[√] 6.7 - 双向LSTM完成文本分类任务"></a>[√] 6.7 - 双向LSTM完成文本分类任务</h2><hr><h4 id="√-机器学习实践五要素-2"><a href="#√-机器学习实践五要素-2" class="headerlink" title="[√] 机器学习实践五要素"></a>[√] 机器学习实践五要素</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700411.png" alt="image-20221222154558242"></p><h4 id="√-数据处理"><a href="#√-数据处理" class="headerlink" title="[√] 数据处理"></a>[√] 数据处理</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700412.png" alt="image-20221222154712062"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700413.png" alt="image-20221222154743282"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700414.png" alt="image-20221222154849096"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700415.png" alt="image-20221222154953796"></p><p>其中，词典中[PAD]和[UNK]分别是填充和unknown的意思，长度对齐的时候，会有[PAD]，遇到词典中没有的、不认识的词，则表示为[UNK]</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700416.png" alt="image-20221222155229688"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700417.png" alt="image-20221222155341618"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700418.png" alt="image-20221222155422716"></p><h4 id="√-封装DataLoader"><a href="#√-封装DataLoader" class="headerlink" title="[√] 封装DataLoader"></a>[√] 封装DataLoader</h4><hr><h6 id="√-长度截断"><a href="#√-长度截断" class="headerlink" title="[√] 长度截断"></a>[√] 长度截断</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700419.png" alt="image-20221222155732166"></p><h6 id="√-长度补齐"><a href="#√-长度补齐" class="headerlink" title="[√] 长度补齐"></a>[√] 长度补齐</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700420.png" alt="image-20221222155803243"></p><h4 id="√-模型构建"><a href="#√-模型构建" class="headerlink" title="[√] 模型构建"></a>[√] 模型构建</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700421.png" alt="image-20221222160205188"></p><blockquote><p>alec：</p><p>双向LSTM有一个汇聚层，将所有的时刻输出的隐状态向量汇聚（均值池化），然后传入到输出层进行分类</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700422.png" alt="image-20221222160342601"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700423.png" alt="image-20221222160533286"></p><h6 id="√-举例理解"><a href="#√-举例理解" class="headerlink" title="[√] 举例理解"></a>[√] 举例理解</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700424.png" alt="image-20221222160731752"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700425.png" alt="image-20221222160804024"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700426.png" alt="image-20221222160833149"></p><h4 id="√-模型训练"><a href="#√-模型训练" class="headerlink" title="[√] 模型训练"></a>[√] 模型训练</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700427.png" alt="image-20221222160905994"></p><h6 id="√-训练结果"><a href="#√-训练结果" class="headerlink" title="[√] 训练结果"></a>[√] 训练结果</h6><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700428.png" alt="image-20221222160926108"></p><h4 id="√-模型评价"><a href="#√-模型评价" class="headerlink" title="[√] 模型评价"></a>[√] 模型评价</h4><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212221700430.png" alt="image-20221222161018092"></p><h2 id="√-6-8-自定义和飞桨LSTM对比"><a href="#√-6-8-自定义和飞桨LSTM对比" class="headerlink" title="[√] 6.8 - 自定义和飞桨LSTM对比"></a>[√] 6.8 - 自定义和飞桨LSTM对比</h2><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6 - 循环神经网络 - 实践：基于双向LSTM模型完成文本分类任务</title>
    <link href="/posts/1748706165/"/>
    <url>/posts/1748706165/</url>
    
    <content type="html"><![CDATA[<h1 id="√-6-4-实践：基于双向LSTM模型完成文本分类任务"><a href="#√-6-4-实践：基于双向LSTM模型完成文本分类任务" class="headerlink" title="[√] 6.4 实践：基于双向LSTM模型完成文本分类任务"></a>[√] 6.4 实践：基于双向LSTM模型完成文本分类任务</h1><p>电影评论可以蕴含丰富的情感：比如喜欢、讨厌、等等．情感分析（Sentiment Analysis）是为一个文本分类问题，即使用判定给定的一段文本信息表达的情感属于积极情绪，还是消极情绪．</p><p>本实践使用 IMDB 电影评论数据集，使用双向 LSTM 对电影评论进行情感分析．</p><h2 id="√-6-4-1-数据处理"><a href="#√-6-4-1-数据处理" class="headerlink" title="[√] 6.4.1 数据处理"></a>[√] 6.4.1 数据处理</h2><hr><p><a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data">IMDB电影评论数据集</a>是一份关于电影评论的经典二分类数据集．IMDB 按照评分的高低筛选出了积极评论和消极评论，如果评分 $\ge 7$，则认为是积极评论；如果评分 $\le4$，则认为是消极评论．数据集包含训练集和测试集数据，数量各为 25000 条，每条数据都是一段用户关于某个电影的真实评价，以及观众对这个电影的情感倾向，其目录结构如下所示</p><pre><code class="hljs">  ├── train/      ├── neg                 # 消极数据        ├── pos                 # 积极数据      ├── unsup               # 无标签数据  ├── test/      ├── neg                 # 消极数据      ├── pos                 # 积极数据</code></pre><p>在test&#x2F;neg目录中任选一条电影评论数据，内容如下：</p><blockquote><p>“Cover Girl” is a lacklustre WWII musical with absolutely nothing memorable about it, save for its signature song, “Long Ago and Far Away.” </p></blockquote><p>LSTM 模型不能直接处理文本数据，需要先将文本中单词转为向量表示，称为词向量（Word Embedding）．为了提高转换效率，通常会事先把文本的每个单词转换为数字 ID，再使用第节中介绍的方法进行向量转换．因此，需要准备一个词典（Vocabulary），将文本中的每个单词转换为它在词典中的序号 ID．同时还要设置一个特殊的词 [UNK]，表示未知词．在处理文本时，如果碰到不在词表的词，一律按 [UNK] 处理．</p><blockquote><p>alec:</p><ul><li>LSTM 模型不能直接处理文本数据，需要先将文本中单词转为向量表示，称为词向量（Word Embedding）．</li><li>为了提高转换效率，通常会事先把文本的每个单词转换为数字 ID</li><li>需要准备一个词典（Vocabulary），将文本中的每个单词转换为它在词典中的序号 ID．同时还要设置一个特殊的词 [UNK]，表示未知词．在处理文本时，如果碰到不在词表的词，一律按 [UNK] 处理．</li></ul></blockquote><h4 id="√-6-4-1-1-数据加载"><a href="#√-6-4-1-1-数据加载" class="headerlink" title="[√] 6.4.1.1 数据加载"></a>[√] 6.4.1.1 数据加载</h4><hr><p>原始训练集和测试集数据分别25000条，本节将原始的测试集平均分为两份，分别作为验证集和测试集，存放于<code>./dataset</code>目录下。使用如下代码便可以将数据加载至内存：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># 加载数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_imdb_data</span>(<span class="hljs-params">path</span>):<br>    <span class="hljs-keyword">assert</span> os.path.exists(path) <br>    trainset, devset, testset = [], [], []<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(path, <span class="hljs-string">&quot;train.txt&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> fr:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr:<br>            sentence_label, sentence = line.strip().lower().split(<span class="hljs-string">&quot;\t&quot;</span>, maxsplit=<span class="hljs-number">1</span>)<br>            trainset.append((sentence, sentence_label))<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(path, <span class="hljs-string">&quot;dev.txt&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> fr:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr:<br>            sentence_label, sentence = line.strip().lower().split(<span class="hljs-string">&quot;\t&quot;</span>, maxsplit=<span class="hljs-number">1</span>)<br>            devset.append((sentence, sentence_label))<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(path, <span class="hljs-string">&quot;test.txt&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> fr:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fr:<br>            sentence_label, sentence = line.strip().lower().split(<span class="hljs-string">&quot;\t&quot;</span>, maxsplit=<span class="hljs-number">1</span>)<br>            testset.append((sentence, sentence_label))<br><br>    <span class="hljs-keyword">return</span> trainset, devset, testset<br><br><span class="hljs-comment"># 加载IMDB数据集</span><br>train_data, dev_data, test_data = load_imdb_data(<span class="hljs-string">&quot;./dataset/&quot;</span>) <br><span class="hljs-comment"># 打印一下加载后的数据样式</span><br><span class="hljs-built_in">print</span>(train_data[<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">(<span class="hljs-string">&quot;the premise of an african-american female scrooge in the modern, struggling city was inspired, but nothing else in this film is. here, ms. scrooge is a miserly banker who takes advantage of the employees and customers in the largely poor and black neighborhood it inhabits. there is no doubt about the good intentions of the people involved. part of the problem is that story&#x27;s roots don&#x27;t translate well into the urban setting of this film, and the script fails to make the update work. also, the constant message about sharing and giving is repeated so endlessly, the audience becomes tired of it well before the movie reaches its familiar end. this is a message film that doesn&#x27;t know when to quit. in the title role, the talented cicely tyson gives an overly uptight performance, and at times lines are difficult to understand. the charles dickens novel has been adapted so many times, it&#x27;s a struggle to adapt it in a way that makes it fresh and relevant, in spite of its very relevant message.&quot;</span>, <span class="hljs-string">&#x27;0&#x27;</span>)<br></code></pre></td></tr></table></figure><p>从输出结果看，加载后的每条样本包含两部分内容：文本串和标签。</p><h4 id="√-6-4-1-2-构造Dataset类"><a href="#√-6-4-1-2-构造Dataset类" class="headerlink" title="[√] 6.4.1.2 构造Dataset类"></a>[√] 6.4.1.2 构造Dataset类</h4><hr><p>首先，我们构造IMDBDataset类用于数据管理，它继承自paddle.io.DataSet类。</p><p>由于这里的输入是文本序列，需要先将其中的每个词转换为该词在词表中的序号 ID，然后根据词表ID查询这些词对应的词向量，该过程同第同6.1节中将数字向量化的操作，在获得词向量后会将其输入至模型进行后续计算。可以使用IMDBDataset类中的words_to_id方法实现这个功能。 具体而言，利用词表word2id_dict将序列中的每个词映射为对应的数字编号，便于进一步转为为词向量。当序列中的词没有包含在词表时，默认会将该词用[UNK]代替。words_to_id方法利用一个如图6.14所示的哈希表来进行转换。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212231613472.png" alt="image-20221222171316126"></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> paddle.io <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> utils.data <span class="hljs-keyword">import</span> load_vocab<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IMDBDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, examples, word2id_dict</span>):<br>        <span class="hljs-built_in">super</span>(IMDBDataset, self).__init__()<br>        <span class="hljs-comment"># 词典，用于将单词转为字典索引的数字</span><br>        self.word2id_dict =  word2id_dict<br>        <span class="hljs-comment"># 加载后的数据集</span><br>        self.examples = self.words_to_id(examples)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">words_to_id</span>(<span class="hljs-params">self, examples</span>):<br>        tmp_examples = []<br>        <span class="hljs-keyword">for</span> idx, example <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples):<br>            seq, label = example<br>            <span class="hljs-comment"># 将单词映射为字典索引的ID， 对于词典中没有的单词用[UNK]对应的ID进行替代</span><br>            seq = [self.word2id_dict.get(word, self.word2id_dict[<span class="hljs-string">&#x27;[UNK]&#x27;</span>]) <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> seq.split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(label)<br>            tmp_examples.append([seq, label])<br>        <span class="hljs-keyword">return</span> tmp_examples<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        seq, label = self.examples[idx]<br>        <span class="hljs-keyword">return</span> seq, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.examples)<br>    <br><span class="hljs-comment"># 加载词表</span><br>word2id_dict= load_vocab(<span class="hljs-string">&quot;./dataset/vocab.txt&quot;</span>) <br><br><span class="hljs-comment"># 实例化Dataset</span><br>train_set = IMDBDataset(train_data, word2id_dict)<br>dev_set = IMDBDataset(dev_data, word2id_dict)<br>test_set = IMDBDataset(test_data, word2id_dict)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;训练集样本数：&#x27;</span>, <span class="hljs-built_in">len</span>(train_set))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;样本示例：&#x27;</span>, train_set[<span class="hljs-number">4</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">训练集样本数： <span class="hljs-number">25000</span><br>样本示例： ([<span class="hljs-number">2</span>, <span class="hljs-number">976</span>, <span class="hljs-number">5</span>, <span class="hljs-number">32</span>, <span class="hljs-number">6860</span>, <span class="hljs-number">618</span>, <span class="hljs-number">7673</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">13073</span>, <span class="hljs-number">2525</span>, <span class="hljs-number">724</span>, <span class="hljs-number">14</span>, <span class="hljs-number">22837</span>, <span class="hljs-number">18</span>, <span class="hljs-number">164</span>, <span class="hljs-number">416</span>, <span class="hljs-number">8</span>, <span class="hljs-number">10</span>, <span class="hljs-number">24</span>, <span class="hljs-number">701</span>, <span class="hljs-number">611</span>, <span class="hljs-number">1743</span>, <span class="hljs-number">7673</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">56391</span>, <span class="hljs-number">21652</span>, <span class="hljs-number">36</span>, <span class="hljs-number">271</span>, <span class="hljs-number">3495</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">11373</span>, <span class="hljs-number">4</span>, <span class="hljs-number">13244</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2157</span>, <span class="hljs-number">350</span>, <span class="hljs-number">4</span>, <span class="hljs-number">328</span>, <span class="hljs-number">4118</span>, <span class="hljs-number">12</span>, <span class="hljs-number">48810</span>, <span class="hljs-number">52</span>, <span class="hljs-number">7</span>, <span class="hljs-number">60</span>, <span class="hljs-number">860</span>, <span class="hljs-number">43</span>, <span class="hljs-number">2</span>, <span class="hljs-number">56</span>, <span class="hljs-number">4393</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">89</span>, <span class="hljs-number">4152</span>, <span class="hljs-number">182</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">461</span>, <span class="hljs-number">7</span>, <span class="hljs-number">11</span>, <span class="hljs-number">7321</span>, <span class="hljs-number">7730</span>, <span class="hljs-number">86</span>, <span class="hljs-number">7931</span>, <span class="hljs-number">107</span>, <span class="hljs-number">72</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2830</span>, <span class="hljs-number">1165</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">151</span>, <span class="hljs-number">4</span>, <span class="hljs-number">2</span>, <span class="hljs-number">272</span>, <span class="hljs-number">1003</span>, <span class="hljs-number">6</span>, <span class="hljs-number">91</span>, <span class="hljs-number">2</span>, <span class="hljs-number">10491</span>, <span class="hljs-number">912</span>, <span class="hljs-number">826</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1750</span>, <span class="hljs-number">889</span>, <span class="hljs-number">43</span>, <span class="hljs-number">6723</span>, <span class="hljs-number">4</span>, <span class="hljs-number">647</span>, <span class="hljs-number">7</span>, <span class="hljs-number">2535</span>, <span class="hljs-number">38</span>, <span class="hljs-number">39222</span>, <span class="hljs-number">2</span>, <span class="hljs-number">357</span>, <span class="hljs-number">398</span>, <span class="hljs-number">1505</span>, <span class="hljs-number">5</span>, <span class="hljs-number">12</span>, <span class="hljs-number">107</span>, <span class="hljs-number">179</span>, <span class="hljs-number">2</span>, <span class="hljs-number">20</span>, <span class="hljs-number">4279</span>, <span class="hljs-number">83</span>, <span class="hljs-number">1163</span>, <span class="hljs-number">692</span>, <span class="hljs-number">10</span>, <span class="hljs-number">7</span>, <span class="hljs-number">3</span>, <span class="hljs-number">889</span>, <span class="hljs-number">24</span>, <span class="hljs-number">11</span>, <span class="hljs-number">141</span>, <span class="hljs-number">118</span>, <span class="hljs-number">50</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28642</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2</span>, <span class="hljs-number">490</span>, <span class="hljs-number">1469</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1039</span>, <span class="hljs-number">98975</span>, <span class="hljs-number">24541</span>, <span class="hljs-number">344</span>, <span class="hljs-number">32</span>, <span class="hljs-number">2074</span>, <span class="hljs-number">11852</span>, <span class="hljs-number">1683</span>, <span class="hljs-number">4</span>, <span class="hljs-number">29</span>, <span class="hljs-number">286</span>, <span class="hljs-number">478</span>, <span class="hljs-number">22</span>, <span class="hljs-number">823</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5222</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1490</span>, <span class="hljs-number">6893</span>, <span class="hljs-number">883</span>, <span class="hljs-number">41</span>, <span class="hljs-number">71</span>, <span class="hljs-number">3254</span>, <span class="hljs-number">38</span>, <span class="hljs-number">100</span>, <span class="hljs-number">1021</span>, <span class="hljs-number">44</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1700</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8768</span>, <span class="hljs-number">12</span>, <span class="hljs-number">8</span>, <span class="hljs-number">3</span>, <span class="hljs-number">108</span>, <span class="hljs-number">11</span>, <span class="hljs-number">146</span>, <span class="hljs-number">12</span>, <span class="hljs-number">1761</span>, <span class="hljs-number">4</span>, <span class="hljs-number">92295</span>, <span class="hljs-number">8</span>, <span class="hljs-number">2641</span>, <span class="hljs-number">5</span>, <span class="hljs-number">83</span>, <span class="hljs-number">49</span>, <span class="hljs-number">3866</span>, <span class="hljs-number">5352</span>], <span class="hljs-number">0</span>)<br></code></pre></td></tr></table></figure><blockquote><p>alec：</p><ul><li>神经网络模型通常需要同一批处理的数据的序列长度是相同的</li><li>RNN可以处理变长的数据，因此注意这里不是把所有的数据都处理成同一长度，而是将同一批的数据处理成相同的长度</li><li>回调函数的意思是，写好了等着别的地方调用的工具函数</li></ul></blockquote><h4 id="√-6-4-1-3-封装DataLoader"><a href="#√-6-4-1-3-封装DataLoader" class="headerlink" title="[√] 6.4.1.3 封装DataLoader"></a>[√] 6.4.1.3 封装DataLoader</h4><hr><p>在构建 Dataset 类之后，我们构造对应的 DataLoader，用于批次数据的迭代．和前几章的 DataLoader 不同，这里的 DataLoader 需要引入下面两个功能：</p><ol><li>长度限制：需要将序列的长度控制在一定的范围内，避免部分数据过长影响整体训练效果</li><li>长度补齐：神经网络模型通常需要同一批处理的数据的序列长度是相同的，然而在分批时通常会将不同长度序列放在同一批，因此需要对序列进行补齐处理．</li></ol><p>对于长度限制，我们使用max_seq_len参数对于过长的文本进行截断．<br>对于长度补齐，我们先统计该批数据中序列的最大长度，并将短的序列填充一些没有特殊意义的占位符 [PAD]，将长度补齐到该批次的最大长度，这样便能使得同一批次的数据变得规整．比如给定两个句子：</p><ul><li>句子1: This movie was craptacular.</li><li>句子2: I got stuck in traffic on the way to the theater.</li></ul><p>将上面的两个句子补齐，变为：</p><ul><li>句子1: This movie was craptacular [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]</li><li>句子2: I got stuck in traffic on the way to the theater</li></ul><p>具体来讲，本节定义了一个collate_fn函数来做数据的截断和填充. 该函数可以作为回调函数传入 DataLoader，DataLoader 在返回一批数据之前，调用该函数去处理数据，并返回处理后的序列数据和对应标签。</p><p>另外，使用[PAD]占位符对短序列填充后，再进行文本分类任务时，默认无须使用[PAD]位置，因此需要使用变量seq_lens来表示序列中非[PAD]位置的真实长度。seq_lens可以在collate_fn函数处理批次数据时进行获取并返回。需要注意的是，由于RunnerV3类默认按照输入数据和标签两类信息获取数据，因此需要将序列数据和序列长度组成元组作为输入数据进行返回，以方便RunnerV3解析数据。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> functools <span class="hljs-keyword">import</span> partial<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">batch_data, pad_val=<span class="hljs-number">0</span>, max_seq_len=<span class="hljs-number">256</span></span>):<br>    seqs, seq_lens, labels = [], [], []<br>    max_len = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> batch_data:<br>        seq, label = example<br>        <span class="hljs-comment"># 对数据序列进行截断</span><br>        seq = seq[:max_seq_len]<br>        <span class="hljs-comment"># 对数据截断并保存于seqs中</span><br>        seqs.append(seq)<br>        seq_lens.append(<span class="hljs-built_in">len</span>(seq))<br>        labels.append(label)<br>        <span class="hljs-comment"># 保存序列最大长度</span><br>        max_len = <span class="hljs-built_in">max</span>(max_len, <span class="hljs-built_in">len</span>(seq))<br>    <span class="hljs-comment"># 对数据序列进行填充至最大长度</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(seqs)):<br>        seqs[i] = seqs[i] + [pad_val] * (max_len - <span class="hljs-built_in">len</span>(seqs[i]))<br><br>    <span class="hljs-keyword">return</span> (paddle.to_tensor(seqs), paddle.to_tensor(seq_lens)), paddle.to_tensor(labels)<br><br>    <br></code></pre></td></tr></table></figure><p>下面我们自定义一批数据来测试一下collate_fn函数的功能，这里假定一下max_seq_len为5，然后定义序列长度分别为6和3的两条数据，传入collate_fn函数中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">max_seq_len = <span class="hljs-number">5</span><br>batch_data = [[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>], <span class="hljs-number">1</span>], [[<span class="hljs-number">2</span>,<span class="hljs-number">4</span>,<span class="hljs-number">6</span>], <span class="hljs-number">0</span>]]<br>(seqs, seq_lens), labels = collate_fn(batch_data, pad_val=word2id_dict[<span class="hljs-string">&quot;[PAD]&quot;</span>], max_seq_len=max_seq_len)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;seqs: &quot;</span>, seqs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;seq_lens: &quot;</span>, seq_lens)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;labels: &quot;</span>, labels)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">seqs:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=int64, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>        [<span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>seq_lens:  Tensor(shape=[<span class="hljs-number">2</span>], dtype=int64, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br>labels:  Tensor(shape=[<span class="hljs-number">2</span>], dtype=int64, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><p>可以看到，原始序列中长度为6的序列被截断为5，同时原始序列中长度为3的序列被填充到5，同时返回了非<code>[PAD]</code>的序列长度。</p><p>接下来，我们将collate_fn作为回调函数传入DataLoader中， 其在返回一批数据时，可以通过collate_fn函数处理该批次的数据。 这里需要注意的是，这里通过partial函数对collate_fn函数中的关键词参数进行设置，并返回一个新的函数对象作为collate_fn。 </p><p>在使用DataLoader按批次迭代数据时，最后一批的数据样本数量可能不够设定的batch_size，可以通过参数drop_last来判断是否丢弃最后一个batch的数据。</p><blockquote><p>alec:</p><ul><li>在使用DataLoader按批次迭代数据时，最后一批的数据样本数量可能不够设定的batch_size，可以通过参数drop_last来判断是否丢弃最后一个batch的数据。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">max_seq_len = <span class="hljs-number">256</span><br>batch_size = <span class="hljs-number">128</span><br>collate_fn = partial(collate_fn, pad_val=word2id_dict[<span class="hljs-string">&quot;[PAD]&quot;</span>], max_seq_len=max_seq_len)<br>train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>, drop_last=<span class="hljs-literal">False</span>, collate_fn=collate_fn)<br>dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>, drop_last=<span class="hljs-literal">False</span>, collate_fn=collate_fn)<br>test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>, drop_last=<span class="hljs-literal">False</span>, collate_fn=collate_fn)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">运行时长: <span class="hljs-number">5</span>毫秒<br>结束时间: <span class="hljs-number">2022</span>-<span class="hljs-number">12</span>-<span class="hljs-number">23</span> <span class="hljs-number">11</span>:<span class="hljs-number">10</span>:<span class="hljs-number">28</span><br></code></pre></td></tr></table></figure><h2 id="√-6-4-2-模型构建"><a href="#√-6-4-2-模型构建" class="headerlink" title="[√] 6.4.2 模型构建"></a>[√] 6.4.2 模型构建</h2><hr><p>本实践的整个模型结构如图6.15所示．</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/265d4edfc903476a8fd2eca56f027070686265c4cc8d4738aa6e684ecd613d23" width=50%/></center><center>图6.15 基于双向LSTM的文本分类模型结构</center><p>由如下几部分组成：<br>（1）嵌入层：将输入的数字序列进行向量化，即将每个数字映射为向量。这里直接使用飞桨API：paddle.nn.Embedding来完成。</p><blockquote><p>class paddle.nn.Embedding(num_embeddings, embedding_dim, padding_idx&#x3D;None, sparse&#x3D;False, weight_attr&#x3D;None, name&#x3D;None)</p></blockquote><p>该API有两个重要的参数：num_embeddings表示需要用到的Embedding的数量。embedding_dim表示嵌入向量的维度。<br>paddle.nn.Embedding会根据[num_embeddings, embedding_dim]自动构造一个二维嵌入矩阵。参数padding_idx是指用来补齐序列的占位符[PAD]对应的词表ID，那么在训练过程中遇到此ID时，其参数及对应的梯度将会以0进行填充。在实现中为了简单起见，我们通常会将[PAD]放在词表中的第一位，即对应的ID为0。</p><blockquote><p>alec收获&#x2F;总结：</p><ul><li>辨析：嵌入层，是将输入的数字序列进行向量化。注意嵌入层这里，不是指的将单词转为数字，将单词转为数字这个步骤在嵌入层之前的数据处理阶段已经完成了。嵌入层，是将单个数字，通过一定的方法，转为M维的向量，方便神经网络运算。</li></ul></blockquote><p>（2）双向LSTM层：接收向量序列，分别用前向和反向更新循环单元。这里我们直接使用飞桨API：paddle.nn.LSTM来完成。只需要在定义LSTM时设置参数direction为bidirectional，便可以直接使用双向LSTM。</p><blockquote><p>思考: 在实现双向LSTM时，因为需要进行序列补齐，在计算反向LSTM时，占位符[PAD]是否会对LSTM参数梯度的更新有影响。如果有的话，如何消除影响？<br>注：在调用paddle.nn.LSTM实现双向LSTM时，可以传入该批次数据的真实长度，paddle.nn.LSTM会根据真实序列长度处理数据，对占位符[PAD]进行掩蔽，[PAD]位置将返回零向量。</p></blockquote><p>（3）聚合层：将双向LSTM层所有位置上的隐状态进行平均，作为整个句子的表示。</p><p>（4）输出层：输出层，输出分类的几率。这里可以直接调用paddle.nn.Linear来完成。 </p><blockquote><p><strong>动手练习6.5</strong>：改进第6.3.1.1节中的LSTM算子，使其可以支持一个批次中包含不同长度的序列样本。</p></blockquote><p>上面模型中的嵌入层、双向LSTM层和线性层都可以直接调用飞桨API来实现，这里我们只需要实现汇聚层算子。需要注意的是，虽然飞桨内置LSTM在传入批次数据的真实长度后，会对[PAD]位置返回零向量，但考虑到汇聚层与处理序列数据的模型进行解耦，因此在本节汇聚层的实现中，会对[PAD]位置进行掩码。</p><h4 id="√-汇聚层算子"><a href="#√-汇聚层算子" class="headerlink" title="[√] 汇聚层算子"></a>[√] 汇聚层算子</h4><hr><p>汇聚层算子将双向LSTM层所有位置上的隐状态进行平均，作为整个句子的表示。这里我们实现了AveragePooling算子进行隐状态的汇聚，首先利用序列长度向量生成掩码（Mask）矩阵，用于对文本序列中[PAD]位置的向量进行掩蔽，然后将该序列的向量进行相加后取均值。代码实现如下：</p><p>将上面各个模块汇总到一起，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AveragePooling</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AveragePooling, self).__init__()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sequence_output, sequence_length</span>):<br>        sequence_length = paddle.cast(sequence_length.unsqueeze(-<span class="hljs-number">1</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 根据sequence_length生成mask矩阵，用于对Padding位置的信息进行mask</span><br>        max_len = sequence_output.shape[<span class="hljs-number">1</span>]<br>        mask = paddle.arange(max_len) &lt; sequence_length<br>        mask = paddle.cast(mask, dtype=<span class="hljs-string">&quot;float32&quot;</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 对序列中paddling部分进行mask</span><br>        sequence_output = paddle.multiply(sequence_output, mask)<br>        <span class="hljs-comment"># 对序列中的向量取均值</span><br>        batch_mean_hidden = paddle.divide(paddle.<span class="hljs-built_in">sum</span>(sequence_output, axis=<span class="hljs-number">1</span>), sequence_length)<br>        <span class="hljs-keyword">return</span> batch_mean_hidden<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">运行时长: <span class="hljs-number">6</span>毫秒<br>结束时间: <span class="hljs-number">2022</span>-<span class="hljs-number">12</span>-<span class="hljs-number">23</span> <span class="hljs-number">11</span>:<span class="hljs-number">41</span>:02<br></code></pre></td></tr></table></figure><h4 id="√-模型汇总"><a href="#√-模型汇总" class="headerlink" title="[√] 模型汇总"></a>[√] 模型汇总</h4><hr><p>将上面的算子汇总，组合为最终的分类模型。代码实现如下：</p><blockquote><p>alec收获&#x2F;总结：</p><ul><li>num_embeddings是指的词典的大小</li><li>num_classes是指的分类的数量</li><li>嵌入层的作用是，将输入的每个时间点上的一个数字，转为一个向量，这个向量可以更加充分的表达这个单词的信息。总体的过程是，输入一个单词，通过词典将单词转为一个数字，然后再嵌入层将这个数字转为一个向量。</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_BiLSTM_FC</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, input_size, hidden_size, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(Model_BiLSTM_FC, self).__init__()<br>        <span class="hljs-comment"># 词典大小</span><br>        self.num_embeddings = num_embeddings<br>        <span class="hljs-comment"># 单词向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># LSTM隐藏单元数量</span><br>        self.hidden_size = hidden_size<br>        <span class="hljs-comment"># 情感分类类别数量</span><br>        self.num_classes = num_classes<br>        <span class="hljs-comment"># 实例化嵌入层</span><br>        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 实例化LSTM层</span><br>        self.lstm_layer = nn.LSTM(input_size, hidden_size, direction=<span class="hljs-string">&quot;bidirectional&quot;</span>)<br>        <span class="hljs-comment"># 实例化聚合层</span><br>        self.average_layer = AveragePooling()<br>        <span class="hljs-comment"># 实例化输出层</span><br>        self.output_layer = nn.Linear(hidden_size * <span class="hljs-number">2</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 对模型输入拆分为序列数据和mask</span><br>        input_ids, sequence_length = inputs<br>        <span class="hljs-comment"># 获取词向量</span><br>        <span class="hljs-built_in">print</span>(input_ids)<br>        inputs_emb = self.embedding_layer(input_ids)<br>        <span class="hljs-built_in">print</span>(inputs_emb)<br>        <span class="hljs-comment"># 使用lstm处理数据</span><br>        sequence_output, _ = self.lstm_layer(inputs_emb, sequence_length=sequence_length)<br>        <span class="hljs-comment"># 使用聚合层聚合sequence_output</span><br>        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)<br>        <span class="hljs-comment"># 输出文本分类logits</span><br>        logits = self.output_layer(batch_mean_hidden)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">运行时长: <span class="hljs-number">7</span>毫秒<br>结束时间: <span class="hljs-number">2022</span>-<span class="hljs-number">12</span>-<span class="hljs-number">23</span> <span class="hljs-number">11</span>:<span class="hljs-number">45</span>:<span class="hljs-number">23</span><br></code></pre></td></tr></table></figure><figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs dns"><span class="hljs-number">5098</span><br>[-<span class="hljs-number">0.00338363</span>, -<span class="hljs-number">0.00304994</span>, -<span class="hljs-number">0.00280701</span>, ..., -<span class="hljs-number">0.00184787</span>,<br>           <span class="hljs-number">0.00203779</span>,  <span class="hljs-number">0.00440233</span>],<br></code></pre></td></tr></table></figure><p>如上所示，嵌入层，将单词对应的5098这个数字，转为了一个长度input_size为256的向量，以更加充分的表达信息。</p><h2 id="√-6-4-3-模型训练"><a href="#√-6-4-3-模型训练" class="headerlink" title="[√] 6.4.3 模型训练"></a>[√] 6.4.3 模型训练</h2><hr><p>本节将基于RunnerV3进行训练，首先指定模型训练的超参，然后设定模型、优化器、损失函数和评估指标，其中损失函数使用<code>paddle.nn.CrossEntropyLoss</code>，该损失函数内部会对预测结果使用<code>softmax</code>进行计算，数字预测模型输出层的输出<code>logits</code>不需要使用softmax进行归一化，定义完Runner的相关组件后，便可以进行模型训练。代码实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> Accuracy, RunnerV3<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 指定训练轮次</span><br>num_epochs = <span class="hljs-number">3</span><br><span class="hljs-comment"># 指定学习率</span><br>learning_rate = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 指定embedding的数量为词表长度</span><br>num_embeddings = <span class="hljs-built_in">len</span>(word2id_dict)<br><span class="hljs-comment"># embedding向量的维度</span><br>input_size = <span class="hljs-number">256</span><br><span class="hljs-comment"># LSTM网络隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">256</span><br><br><span class="hljs-comment"># 实例化模型</span><br>model = Model_BiLSTM_FC(num_embeddings, input_size, hidden_size)<br><span class="hljs-comment"># 指定优化器</span><br>optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=<span class="hljs-number">0.9</span>, beta2=<span class="hljs-number">0.999</span>, parameters= model.parameters()) <br><span class="hljs-comment"># 指定损失函数</span><br>loss_fn = paddle.nn.CrossEntropyLoss() <br><span class="hljs-comment"># 指定评估指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 模型训练</span><br>start_time = time.time()<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">10</span>, log_steps=<span class="hljs-number">10</span>, save_path=<span class="hljs-string">&quot;./checkpoints/best.pdparams&quot;</span>)<br>end_time = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;time: &quot;</span>, (end_time-start_time))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.69256</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">10</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.68554</span><br>[Evaluate]  dev score: <span class="hljs-number">0.50232</span>, dev loss: <span class="hljs-number">0.68516</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.50232</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">20</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.64730</span><br>[Evaluate]  dev score: <span class="hljs-number">0.63776</span>, dev loss: <span class="hljs-number">0.63676</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.50232</span> --&gt; <span class="hljs-number">0.63776</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">30</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.51626</span><br>[Evaluate]  dev score: <span class="hljs-number">0.71568</span>, dev loss: <span class="hljs-number">0.55013</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.63776</span> --&gt; <span class="hljs-number">0.71568</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">40</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.47814</span><br>[Evaluate]  dev score: <span class="hljs-number">0.78512</span>, dev loss: <span class="hljs-number">0.50590</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.71568</span> --&gt; <span class="hljs-number">0.78512</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">50</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.42821</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82264</span>, dev loss: <span class="hljs-number">0.42005</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.78512</span> --&gt; <span class="hljs-number">0.82264</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">60</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.31650</span><br>[Evaluate]  dev score: <span class="hljs-number">0.77160</span>, dev loss: <span class="hljs-number">0.45519</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">70</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.40439</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83688</span>, dev loss: <span class="hljs-number">0.38745</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.82264</span> --&gt; <span class="hljs-number">0.83688</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">80</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.29193</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83008</span>, dev loss: <span class="hljs-number">0.37972</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">90</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.30325</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83360</span>, dev loss: <span class="hljs-number">0.37426</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">100</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.37036</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85728</span>, dev loss: <span class="hljs-number">0.34226</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.83688</span> --&gt; <span class="hljs-number">0.85728</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">110</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.31189</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85928</span>, dev loss: <span class="hljs-number">0.33819</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.85728</span> --&gt; <span class="hljs-number">0.85928</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">120</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.29616</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85496</span>, dev loss: <span class="hljs-number">0.34090</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">130</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.24705</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84976</span>, dev loss: <span class="hljs-number">0.34640</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">140</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.25672</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83880</span>, dev loss: <span class="hljs-number">0.36268</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">150</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.35837</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83592</span>, dev loss: <span class="hljs-number">0.36889</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">160</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.26096</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85928</span>, dev loss: <span class="hljs-number">0.33418</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">170</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.28331</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86072</span>, dev loss: <span class="hljs-number">0.32538</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.85928</span> --&gt; <span class="hljs-number">0.86072</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">180</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.25884</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86416</span>, dev loss: <span class="hljs-number">0.32144</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.86072</span> --&gt; <span class="hljs-number">0.86416</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">190</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.21110</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86000</span>, dev loss: <span class="hljs-number">0.32458</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">200</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.12052</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86160</span>, dev loss: <span class="hljs-number">0.32210</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">210</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.12474</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86072</span>, dev loss: <span class="hljs-number">0.42033</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">220</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.08961</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85456</span>, dev loss: <span class="hljs-number">0.37783</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">230</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.07447</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85096</span>, dev loss: <span class="hljs-number">0.40512</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">240</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.09026</span><br>[Evaluate]  dev score: <span class="hljs-number">0.85448</span>, dev loss: <span class="hljs-number">0.39271</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">250</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.02511</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84408</span>, dev loss: <span class="hljs-number">0.41523</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">260</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.16499</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83024</span>, dev loss: <span class="hljs-number">0.44774</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">270</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.08498</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84400</span>, dev loss: <span class="hljs-number">0.41809</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">280</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.08351</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84584</span>, dev loss: <span class="hljs-number">0.39286</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">290</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06237</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84432</span>, dev loss: <span class="hljs-number">0.39758</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">300</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06990</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83792</span>, dev loss: <span class="hljs-number">0.44654</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">310</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06123</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84536</span>, dev loss: <span class="hljs-number">0.40967</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">320</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.08561</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84440</span>, dev loss: <span class="hljs-number">0.44798</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">330</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06285</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84432</span>, dev loss: <span class="hljs-number">0.40742</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">340</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.07999</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84232</span>, dev loss: <span class="hljs-number">0.42844</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">350</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.08248</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84016</span>, dev loss: <span class="hljs-number">0.44600</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">360</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06058</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84216</span>, dev loss: <span class="hljs-number">0.44513</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">370</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.06609</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83880</span>, dev loss: <span class="hljs-number">0.44186</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">380</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.09265</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82920</span>, dev loss: <span class="hljs-number">0.42367</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">390</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.10980</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80704</span>, dev loss: <span class="hljs-number">0.52548</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">400</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00454</span><br>[Evaluate]  dev score: <span class="hljs-number">0.81176</span>, dev loss: <span class="hljs-number">0.64645</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">410</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00482</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82864</span>, dev loss: <span class="hljs-number">0.77510</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">420</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00236</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84096</span>, dev loss: <span class="hljs-number">0.51046</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">430</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.01271</span><br>[Evaluate]  dev score: <span class="hljs-number">0.81872</span>, dev loss: <span class="hljs-number">0.63595</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">440</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00441</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84000</span>, dev loss: <span class="hljs-number">0.57609</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">450</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00362</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83816</span>, dev loss: <span class="hljs-number">0.61525</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">460</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00602</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83072</span>, dev loss: <span class="hljs-number">0.75598</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">470</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.05197</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82520</span>, dev loss: <span class="hljs-number">0.79471</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">480</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00367</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83832</span>, dev loss: <span class="hljs-number">0.57327</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">490</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00216</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83336</span>, dev loss: <span class="hljs-number">0.69600</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">500</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.02041</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83928</span>, dev loss: <span class="hljs-number">0.69365</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">510</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00702</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82800</span>, dev loss: <span class="hljs-number">0.71270</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">520</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.02936</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83392</span>, dev loss: <span class="hljs-number">0.48017</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">530</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00411</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83472</span>, dev loss: <span class="hljs-number">0.52984</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">540</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00985</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82872</span>, dev loss: <span class="hljs-number">0.79777</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">550</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00389</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83616</span>, dev loss: <span class="hljs-number">0.56220</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">560</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00744</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83544</span>, dev loss: <span class="hljs-number">0.54016</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">570</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.00548</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83784</span>, dev loss: <span class="hljs-number">0.59388</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">3</span>, step: <span class="hljs-number">580</span>/<span class="hljs-number">588</span>, loss: <span class="hljs-number">0.01626</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83088</span>, dev loss: <span class="hljs-number">0.64671</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83280</span>, dev loss: <span class="hljs-number">0.64175</span><br>[Train] Training done!<br>time:  <span class="hljs-number">180.83241629600525</span><br></code></pre></td></tr></table></figure><p>绘制训练过程中在训练集和验证集上的损失图像和在验证集上的准确率图像：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> plot_training_loss_acc<br><br><span class="hljs-comment"># 图像名字</span><br>fig_name = <span class="hljs-string">&quot;./images/6.16.pdf&quot;</span><br><span class="hljs-comment"># sample_step: 训练损失的采样step，即每隔多少个点选择1个点绘制</span><br><span class="hljs-comment"># loss_legend_loc: loss 图像的图例放置位置</span><br><span class="hljs-comment"># acc_legend_loc： acc 图像的图例放置位置</span><br>plot_training_loss_acc(runner, fig_name, fig_size=(<span class="hljs-number">16</span>,<span class="hljs-number">6</span>), sample_step=<span class="hljs-number">10</span>, loss_legend_loc=<span class="hljs-string">&quot;lower left&quot;</span>, acc_legend_loc=<span class="hljs-string">&quot;lower right&quot;</span>)<br></code></pre></td></tr></table></figure><p>图6.16 展示了文本分类模型在训练过程中的损失曲线和在验证集上的准确率曲线，其中在损失图像中，实线表示训练集上的损失变化，虚线表示验证集上的损失变化. 可以看到，随着训练过程的进行，训练集的损失不断下降， 验证集上的损失在大概200步后开始上升，这是因为在训练过程中发生了过拟合，可以选择保存在训练过程中在验证集上效果最好的模型来解决这个问题. 从准确率曲线上可以看到，首先在验证集上的准确率大幅度上升，然后大概200步后准确率不再上升，并且由于过拟合的因素，在验证集上的准确率稍微降低。</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212231613473.png" alt="image-20221223115743322"></p><h2 id="√-6-4-4-模型评价"><a href="#√-6-4-4-模型评价" class="headerlink" title="[√] 6.4.4 模型评价"></a>[√] 6.4.4 模型评价</h2><hr><p>加载训练过程中效果最好的模型，然后使用测试集进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model_path = <span class="hljs-string">&quot;./checkpoints/best.pdparams&quot;</span><br>runner.load_model(model_path)<br>accuracy, _ =  runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate on test set, Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Evaluate</span> <span class="hljs-literal">on</span> test set, Accuracy: <span class="hljs-number">0</span>.<span class="hljs-number">86064</span><br></code></pre></td></tr></table></figure><h2 id="√-6-4-5-模型预测"><a href="#√-6-4-5-模型预测" class="headerlink" title="[√] 6.4.5 模型预测"></a>[√] 6.4.5 模型预测</h2><hr><p>给定任意的一句话，使用训练好的模型进行预测，判断这句话中所蕴含的情感极性。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">id2label=&#123;<span class="hljs-number">0</span>:<span class="hljs-string">&quot;消极情绪&quot;</span>, <span class="hljs-number">1</span>:<span class="hljs-string">&quot;积极情绪&quot;</span>&#125;<br>text = <span class="hljs-string">&quot;this movie is so great. I watched it three times already&quot;</span><br><span class="hljs-comment"># 处理单条文本</span><br>sentence = text.split(<span class="hljs-string">&quot; &quot;</span>)<br><span class="hljs-built_in">print</span>(sentence)<br>words = [word2id_dict[word] <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> word2id_dict <span class="hljs-keyword">else</span> word2id_dict[<span class="hljs-string">&#x27;[UNK]&#x27;</span>] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence] <br><span class="hljs-built_in">print</span>(words)<br>words = words[:max_seq_len]<br>sequence_length = paddle.to_tensor([<span class="hljs-built_in">len</span>(words)], dtype=<span class="hljs-string">&quot;int64&quot;</span>)<br><span class="hljs-built_in">print</span>(sequence_length)<br>words = paddle.to_tensor(words, dtype=<span class="hljs-string">&quot;int64&quot;</span>).unsqueeze(<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(words)<br><span class="hljs-comment"># 使用模型进行预测</span><br>logits = runner.predict((words, sequence_length))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;logits:&#x27;</span>,logits)<br>max_label_id = paddle.argmax(logits, axis=-<span class="hljs-number">1</span>).numpy()[<span class="hljs-number">0</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;max_label_id:&#x27;</span>,max_label_id)<br>pred_label = id2label[max_label_id]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Label: &quot;</span>, pred_label)<br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">[<span class="hljs-string">&#x27;this&#x27;</span>, <span class="hljs-string">&#x27;movie&#x27;</span>, <span class="hljs-string">&#x27;is&#x27;</span>, <span class="hljs-string">&#x27;so&#x27;</span>, <span class="hljs-string">&#x27;great.&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&#x27;watched&#x27;</span>, <span class="hljs-string">&#x27;it&#x27;</span>, <span class="hljs-string">&#x27;three&#x27;</span>, <span class="hljs-string">&#x27;times&#x27;</span>, <span class="hljs-string">&#x27;already&#x27;</span>]<br>[<span class="hljs-number">10</span>, <span class="hljs-number">20</span>, <span class="hljs-number">7</span>, <span class="hljs-number">38</span>, <span class="hljs-number">1208</span>, <span class="hljs-number">1</span>, <span class="hljs-number">273</span>, <span class="hljs-number">12</span>, <span class="hljs-number">284</span>, <span class="hljs-number">286</span>, <span class="hljs-number">445</span>]<br>Tensor(shape=[<span class="hljs-number">1</span>], dtype=int64, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">11</span>])<br>Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">11</span>], dtype=int64, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">10</span> , <span class="hljs-number">20</span> , <span class="hljs-number">7</span>  , <span class="hljs-number">38</span> , <span class="hljs-number">1208</span>, <span class="hljs-number">1</span>  , <span class="hljs-number">273</span>, <span class="hljs-number">12</span> , <span class="hljs-number">284</span>, <span class="hljs-number">286</span>, <span class="hljs-number">445</span>]])<br>logits: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">True</span>,<br>       [[-<span class="hljs-number">1.59379756</span>,  <span class="hljs-number">1.63701093</span>]])<br>max_label_id: <span class="hljs-number">1</span><br>Label:  积极情绪<br></code></pre></td></tr></table></figure><h1 id="√-6-5-拓展实验"><a href="#√-6-5-拓展实验" class="headerlink" title="[√] 6.5 拓展实验"></a>[√] 6.5 拓展实验</h1><hr><h2 id="√-6-6-1-使用Paddle内置的单向LSTM进行文本分类实验"><a href="#√-6-6-1-使用Paddle内置的单向LSTM进行文本分类实验" class="headerlink" title="[√] 6.6.1 使用Paddle内置的单向LSTM进行文本分类实验"></a>[√] 6.6.1 使用Paddle内置的单向LSTM进行文本分类实验</h2><hr><p>首先，修改模型定义，将<code>nn.LSTM</code>中的<code>direction</code>设置为<code>forward</code>以使用单向LSTM模型，同时设置线性层的shape为<code>[hidden_size， num_classes]</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AveragePooling</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AveragePooling, self).__init__()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sequence_output, sequence_length</span>):<br>        sequence_length = paddle.cast(sequence_length.unsqueeze(-<span class="hljs-number">1</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 根据sequence_length生成mask矩阵，用于对Padding位置的信息进行mask</span><br>        max_len = sequence_output.shape[<span class="hljs-number">1</span>]<br>        mask = paddle.arange(max_len) &lt; sequence_length<br>        mask = paddle.cast(mask, dtype=<span class="hljs-string">&quot;float32&quot;</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 对序列中paddling部分进行mask</span><br>        sequence_output = paddle.multiply(sequence_output, mask)<br>        <span class="hljs-comment"># 对序列中的向量取均值</span><br>        batch_mean_hidden = paddle.divide(paddle.<span class="hljs-built_in">sum</span>(sequence_output, axis=<span class="hljs-number">1</span>), sequence_length)<br>        <span class="hljs-keyword">return</span> batch_mean_hidden<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_BiLSTM_FC</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, input_size, hidden_size, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(Model_BiLSTM_FC, self).__init__()<br>        <span class="hljs-comment"># 词典大小</span><br>        self.num_embeddings = num_embeddings<br>        <span class="hljs-comment"># 单词向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># LSTM隐藏单元数量</span><br>        self.hidden_size = hidden_size<br>        <span class="hljs-comment"># 情感分类类别数量</span><br>        self.num_classes = num_classes<br>        <span class="hljs-comment"># 实例化嵌入层</span><br>        <span class="hljs-comment"># 嵌入层将单个的单词映射成长度为input_size的向量</span><br>        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 实例化LSTM层</span><br>        self.lstm_layer = nn.LSTM(input_size, hidden_size, direction=<span class="hljs-string">&quot;forward&quot;</span>)<br>        <span class="hljs-comment"># 实例化聚合层</span><br>        self.average_layer = AveragePooling()<br>        <span class="hljs-comment"># 实例化输出层</span><br>        self.output_layer = nn.Linear(hidden_size, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 对模型输入拆分为序列数据和mask</span><br>        input_ids, sequence_length = inputs<br>        <span class="hljs-comment"># 获取词向量</span><br>        inputs_emb = self.embedding_layer(input_ids)<br>        <span class="hljs-comment"># 使用lstm处理数据</span><br>        sequence_output, _ = self.lstm_layer(inputs_emb, sequence_length=sequence_length)<br>        <span class="hljs-comment"># 使用聚合层聚合sequence_output</span><br>        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)<br>        <span class="hljs-comment"># 输出文本分类logits</span><br>        logits = self.output_layer(batch_mean_hidden)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><p>接下来，基于Paddle的单向模型开始进行训练，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> Accuracy, RunnerV3<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 指定训练轮次</span><br>num_epochs = <span class="hljs-number">3</span><br><span class="hljs-comment"># 指定学习率</span><br>learning_rate = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 指定embedding的数量为词表长度</span><br>num_embeddings = <span class="hljs-built_in">len</span>(word2id_dict)<br><span class="hljs-comment"># embedding向量的维度</span><br>input_size = <span class="hljs-number">256</span><br><span class="hljs-comment"># LSTM网络隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">256</span><br><br><span class="hljs-comment"># 实例化模型</span><br>model = Model_BiLSTM_FC(num_embeddings, input_size, hidden_size)<br><span class="hljs-comment"># 指定优化器</span><br>optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=<span class="hljs-number">0.9</span>, beta2=<span class="hljs-number">0.999</span>, parameters= model.parameters()) <br><span class="hljs-comment"># 指定损失函数</span><br>loss_fn = paddle.nn.CrossEntropyLoss() <br><span class="hljs-comment"># 指定评估指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 模型训练</span><br>start_time = time.time()<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">10</span>, log_steps=<span class="hljs-number">10</span>, save_path=<span class="hljs-string">&quot;./checkpoints/best_forward.pdparams&quot;</span>)<br>end_time = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;time: &quot;</span>, (end_time-start_time))<br></code></pre></td></tr></table></figure><p>基于Paddle的单向LSTM进行模型评价，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model_path = <span class="hljs-string">&quot;./checkpoints/best_forward.pdparams&quot;</span><br>runner.load_model(model_path)<br>accuracy, _ =  runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate on test set, Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="√-6-6-2-使用Paddle内置的单向LSTM进行文本分类实验"><a href="#√-6-6-2-使用Paddle内置的单向LSTM进行文本分类实验" class="headerlink" title="[√] 6.6.2 使用Paddle内置的单向LSTM进行文本分类实验"></a>[√] 6.6.2 使用Paddle内置的单向LSTM进行文本分类实验</h2><hr><p>由于之前实现的LSTM默认只返回最后时刻的隐状态，然而本实验中需要用到所有时刻的隐状态向量，因此需要对自己实现的LSTM进行修改，使其返回序列向量，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment"># 声明LSTM和相关参数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTM</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, Wi_attr=<span class="hljs-literal">None</span>, Wf_attr=<span class="hljs-literal">None</span>, Wo_attr=<span class="hljs-literal">None</span>, Wc_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 Ui_attr=<span class="hljs-literal">None</span>, Uf_attr=<span class="hljs-literal">None</span>, Uo_attr=<span class="hljs-literal">None</span>, Uc_attr=<span class="hljs-literal">None</span>, bi_attr=<span class="hljs-literal">None</span>, bf_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 bo_attr=<span class="hljs-literal">None</span>, bc_attr=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(LSTM, self).__init__()<br>        self.input_size = input_size<br>        self.hidden_size = hidden_size<br><br>        <span class="hljs-comment"># 初始化模型参数</span><br>        self.W_i = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wi_attr)<br>        self.W_f = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wf_attr)<br>        self.W_o = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wo_attr)<br>        self.W_c = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wc_attr)<br>        self.U_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Ui_attr)<br>        self.U_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uf_attr)<br>        self.U_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uo_attr)<br>        self.U_c = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uc_attr)<br>        self.b_i = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bi_attr)<br>        self.b_f = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bf_attr)<br>        self.b_o = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bo_attr)<br>        self.b_c = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bc_attr)<br>    <br>    <span class="hljs-comment"># 初始化状态向量和隐状态向量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, batch_size</span>):<br>        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        hidden_state.stop_gradient = <span class="hljs-literal">False</span><br>        cell_state .stop_gradient = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> hidden_state, cell_state<br><br>    <span class="hljs-comment"># 定义前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, states=<span class="hljs-literal">None</span>, sequence_length=<span class="hljs-literal">None</span></span>):<br>        batch_size, seq_len, input_size = inputs.shape  <span class="hljs-comment"># inputs batch_size x seq_len x input_size</span><br><br>        <span class="hljs-keyword">if</span> states <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            states = self.init_state(batch_size)<br>        hidden_state, cell_state = states<br><br>        outputs = []<br>        <span class="hljs-comment"># 执行LSTM计算，包括：隐藏门、输入门、遗忘门、候选状态向量、状态向量和隐状态向量</span><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>            input_step = inputs[:, step, :]<br>            I_gate = F.sigmoid(paddle.matmul(input_step, self.W_i) + paddle.matmul(hidden_state, self.U_i) + self.b_i)<br>            F_gate = F.sigmoid(paddle.matmul(input_step, self.W_f) + paddle.matmul(hidden_state, self.U_f) + self.b_f)<br>            O_gate = F.sigmoid(paddle.matmul(input_step, self.W_o) + paddle.matmul(hidden_state, self.U_o) + self.b_o)<br>            C_tilde = F.tanh(paddle.matmul(input_step, self.W_c) + paddle.matmul(hidden_state, self.U_c) + self.b_c)<br>            cell_state = F_gate * cell_state + I_gate * C_tilde<br>            hidden_state = O_gate * F.tanh(cell_state)<br>            <br>            outputs.append(hidden_state.unsqueeze(axis=<span class="hljs-number">1</span>))<br><br>        outputs = paddle.concat(outputs, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><p>接下来，修改Model_BiLSTM_FC模型，将<code>nn.LSTM</code>换为自己实现的LSTM模型，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AveragePooling</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(AveragePooling, self).__init__()<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sequence_output, sequence_length</span>):<br>        sequence_length = paddle.cast(sequence_length.unsqueeze(-<span class="hljs-number">1</span>), dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-comment"># 根据sequence_length生成mask矩阵，用于对Padding位置的信息进行mask</span><br>        max_len = sequence_output.shape[<span class="hljs-number">1</span>]<br>        mask = paddle.arange(max_len) &lt; sequence_length<br>        mask = paddle.cast(mask, dtype=<span class="hljs-string">&quot;float32&quot;</span>).unsqueeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 对序列中paddling部分进行mask</span><br>        sequence_output = paddle.multiply(sequence_output, mask)<br>        <span class="hljs-comment"># 对序列中的向量取均值</span><br>        batch_mean_hidden = paddle.divide(paddle.<span class="hljs-built_in">sum</span>(sequence_output, axis=<span class="hljs-number">1</span>), sequence_length)<br>        <span class="hljs-keyword">return</span> batch_mean_hidden<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_BiLSTM_FC</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, input_size, hidden_size, num_classes=<span class="hljs-number">2</span></span>):<br>        <span class="hljs-built_in">super</span>(Model_BiLSTM_FC, self).__init__()<br>        <span class="hljs-comment"># 词典大小</span><br>        self.num_embeddings = num_embeddings<br>        <span class="hljs-comment"># 单词向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># LSTM隐藏单元数量</span><br>        self.hidden_size = hidden_size<br>        <span class="hljs-comment"># 情感分类类别数量</span><br>        self.num_classes = num_classes<br>        <span class="hljs-comment"># 实例化嵌入层</span><br>        self.embedding_layer = nn.Embedding(num_embeddings, input_size, padding_idx=<span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 实例化LSTM层</span><br>        self.lstm_layer = LSTM(input_size, hidden_size)<br>        <span class="hljs-comment"># 实例化聚合层</span><br>        self.average_layer = AveragePooling()<br>        <span class="hljs-comment"># 实例化输出层</span><br>        self.output_layer = nn.Linear(hidden_size, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 对模型输入拆分为序列数据和mask</span><br>        input_ids, sequence_length = inputs<br>        <span class="hljs-comment"># 获取词向量</span><br>        inputs_emb = self.embedding_layer(input_ids)<br>        <span class="hljs-comment"># 使用lstm处理数据</span><br>        sequence_output = self.lstm_layer(inputs_emb)<br>        <span class="hljs-comment"># 使用聚合层聚合sequence_output</span><br>        batch_mean_hidden = self.average_layer(sequence_output, sequence_length)<br>        <span class="hljs-comment"># 输出文本分类logits</span><br>        logits = self.output_layer(batch_mean_hidden)<br>        <span class="hljs-keyword">return</span> logits<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> Accuracy, RunnerV3<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 指定训练轮次</span><br>num_epochs = <span class="hljs-number">3</span><br><span class="hljs-comment"># 指定学习率</span><br>learning_rate = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 指定embedding的数量为词表长度</span><br>num_embeddings = <span class="hljs-built_in">len</span>(word2id_dict)<br><span class="hljs-comment"># embedding向量的维度</span><br>input_size = <span class="hljs-number">256</span><br><span class="hljs-comment"># LSTM网络隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">256</span><br><br><span class="hljs-comment"># 实例化模型</span><br>model = Model_BiLSTM_FC(num_embeddings, input_size, hidden_size)<br><span class="hljs-comment"># 指定优化器</span><br>optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=<span class="hljs-number">0.9</span>, beta2=<span class="hljs-number">0.999</span>, parameters= model.parameters()) <br><span class="hljs-comment"># 指定损失函数</span><br>loss_fn = paddle.nn.CrossEntropyLoss() <br><span class="hljs-comment"># 指定评估指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 模型训练</span><br>start_time = time.time()<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">10</span>, log_steps=<span class="hljs-number">10</span>, save_path=<span class="hljs-string">&quot;./checkpoints/best_self_forward.pdparams&quot;</span>)<br>end_time = time.time()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;time: &quot;</span>, (end_time-start_time))<br></code></pre></td></tr></table></figure><p>基于Paddle的单向LSTM进行模型评价，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">model_path = <span class="hljs-string">&quot;./checkpoints/best_self_forward.pdparams&quot;</span><br>runner.load_model(model_path)<br>accuracy, _ =  runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate on test set, Accuracy: <span class="hljs-subst">&#123;accuracy:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h1 id="√-6-6-小结"><a href="#√-6-6-小结" class="headerlink" title="[√] 6.6 小结"></a>[√] 6.6 小结</h1><hr><p>本章通过实践来加深对循环神经网络的基本概念、网络结构和长程依赖问题问题的理解．我们构建一个数字求和任务，并动手实现了 SRN 和 LSTM 模型，对比它们在数字求和任务上的记忆能力．在实践部分，我们利用双向 LSTM 模型来进行文本分类任务：IMDB 电影评论情感分析，并了解如何通过嵌入层将文本数据转换为向量表示.</p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第6章 - 循环神经网络 - 书籍</title>
    <link href="/posts/1133786115/"/>
    <url>/posts/1133786115/</url>
    
    <content type="html"><![CDATA[<blockquote><p>alec：</p><ul><li>在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构</li><li>循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．</li></ul></blockquote><p>循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构．目前，循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．</p><p>本章内容基于《神经网络与深度学习》第6章：循环神经网络的相关内容进行设计。在阅读本章之前，建议先了解如图6.1所示的关键知识点，以便更好地理解和掌握相应的理论和实践知识。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220155549904.png" alt="image-20221220110618357"></p><p>本章内容主要包含两部分：</p><ul><li>模型解读：介绍经典循环神经网络原理，为了更好地理解长程依赖问题，我们设计一个简单的数字求和任务来验证简单循环网络的记忆能力。长程依赖问题具体可分为梯度爆炸和梯度消失两种情况。对于梯度爆炸，我们复现简单循环网络的梯度爆炸现象并尝试解决。对于梯度消失，一种有效的方式是改进模型，我们也动手实现一个长短期记忆网络，并观察是否可以缓解长程依赖问题。</li><li>案例实践：基于双向长短期记忆网络实现文本分类任务．并了解如何进行补齐序列数据，如何将文本数据转为向量表示，如何对补齐位置进行掩蔽等实践知识。</li></ul><blockquote><p>alec：</p><ul><li>循环神经网络非常擅于处理序列数据，通过使用带自反馈的神经元，能够处理任意长度的序列数据</li><li>循环神经网络从左到右扫描该序列，并不断调用一个相同的组合函数$f(\cdot)$来处理时序信息．这个函数也称为循环神经网络单元（RNN Cell）</li><li>在每个时刻$t$，循环神经网络接受输入信息$\boldsymbol{x}<em>t \in \mathbb{R}^{M}$，并与前一时刻的隐状态$\boldsymbol{h}</em>{t-1} \in \mathbb{R}^D$一起进行计算，输出一个新的当前时刻的隐状态$\boldsymbol{h}_t$</li><li>循环神经网络的参数可以通过梯度下降法来学习</li><li>循环神经网络被认为是图灵完备的，一个完全连接的循环神经网络可以近似解决所有的可计算问题</li><li>长程依赖问题：虽然理论上循环神经网络可以建立长时间间隔的状态之间的依赖关系，但是由于具体的实现方式和参数学习方式会导致梯度爆炸或梯度消失问题，实际上，通常循环神经网络只能学习到短期的依赖关系，很难建模这种长距离的依赖关系，称为长程依赖问题（Long-Term Dependencies Problem）</li></ul></blockquote><blockquote><p>alec：</p><p>输入序列$[\boldsymbol{x}_0, \boldsymbol{x}_1, \boldsymbol{x}_2, …]$，其中x0-xn指的是从时间0，到时刻n，每个时刻对应的输入数据。</p><p>每个时刻t，xt是一个M维的向量，和前一时刻的D维状态一起运算，得到当前时刻对应的新的D维的状态</p></blockquote><p>循环神经网络非常擅于处理序列数据，通过使用带自反馈的神经元，能够处理任意长度的序列数据．给定输入序列$[\boldsymbol{x}_0, \boldsymbol{x}_1, \boldsymbol{x}_2, …]$，循环神经网络从左到右扫描该序列，并不断调用一个相同的组合函数$f(\cdot)$来处理时序信息．这个函数也称为循环神经网络单元（RNN Cell）. 在每个时刻$t$，循环神经网络接受输入信息$\boldsymbol{x}<em>t \in \mathbb{R}^{M}$，并与前一时刻的隐状态$\boldsymbol{h}</em>{t-1} \in \mathbb{R}^D$一起进行计算，输出一个新的当前时刻的隐状态$\boldsymbol{h}_t$.<br>$$<br>\boldsymbol{h}<em>t &#x3D; f(\boldsymbol{h}</em>{t-1}, \boldsymbol{x}_t),<br>$$</p><p>其中$\boldsymbol{h}_{0} &#x3D; 0$，$f(\cdot)$是一个非线性函数. </p><p>循环神经网络的参数可以通过梯度下降法来学习。和前馈神经网络类似，我们可以使用随时间反向传播（BackPropagation Through Time，BPTT）算法高效地手工计算梯度，也可以使用自动微分的方法，通过计算图自动计算梯度。</p><p>循环神经网络被认为是图灵完备的，一个完全连接的循环神经网络可以近似解决所有的可计算问题。然而，虽然理论上循环神经网络可以建立长时间间隔的状态之间的依赖关系，但是由于具体的实现方式和参数学习方式会导致梯度爆炸或梯度消失问题，实际上，通常循环神经网络只能学习到短期的依赖关系，很难建模这种长距离的依赖关系，称为长程依赖问题（Long-Term Dependencies Problem）。</p><hr><h2 id="√-6-1-循环神经网络的记忆能力实验"><a href="#√-6-1-循环神经网络的记忆能力实验" class="headerlink" title="[√] 6.1 - 循环神经网络的记忆能力实验"></a>[√] 6.1 - 循环神经网络的记忆能力实验</h2><hr><p>循环神经网络的一种简单实现是简单循环网络（Simple Recurrent Network，SRN）．</p><p>令向量$\boldsymbol{x}_t \in \mathbb{R}^M$表示在时刻$t$时网络的输入，$\boldsymbol{h_t} \in \mathbb{R}^D$ 表示隐藏层状态（即隐藏层神经元活性值），则$\boldsymbol{h}_t$不仅和当前时刻的输入$\boldsymbol{x}<em>t$相关，也和上一个时刻的隐藏层状态$\boldsymbol{h}</em>{t-1}$相关. 简单循环网络在时刻$t$的更新公式为</p><p>$$<br>\boldsymbol{h}_t &#x3D; f(\boldsymbol{W}\boldsymbol{x}<em>t + \boldsymbol{U}\boldsymbol{h}</em>{t-1} + b),<br>$$</p><p>其中$\boldsymbol{h}_{t}$为隐状态向量，$\boldsymbol{U} \in \mathbb{R}^{D\times D}$为<strong>状态-状态</strong>权重矩阵，$\boldsymbol{W} \in \mathbb{R}^{D\times M}$为<strong>状态-输入</strong>权重矩阵，$\boldsymbol{b}\in \mathbb{R}^{D}$为偏置向量。</p><blockquote><p>alec：</p><p>参数U用于上一时刻D维的状态映射到当前时刻新的D维的状态</p><p>参数W用于将M维的输入向量映射为新的D维的状态</p><p>比如下图中的X1，M&#x3D;3，时刻1输入了一个3维的向量，然后和H0一起计算，得到新的D&#x3D;3维的状态，然后循环计算</p></blockquote><p><strong>图6.2</strong> 展示了一个按时间展开的循环神经网络。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/a813c79080c84187ace2267f0c40352c61f69c8f5c7a4fa3a1f57eb24ed9fa27" width=50%></center><center>图6.2 循环神经网络结构</center></br><p>简单循环网络在参数学习时存在长程依赖问题，很难建模长时间间隔（Long Range）的状态之间的依赖关系。为了测试简单循环网络的记忆能力，本节构建一个数字求和任务进行实验。</p><p>数字求和任务的输入是一串数字，前两个位置的数字为0-9，其余数字随机生成（主要为0），预测目标是输入序列中前两个数字的加和。图6.3展示了长度为10的数字序列．</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/40d2e25dfa5a44a386c6f98ff93e2c2a2a44bab0c1764c0ab22aec45c18e61f5" width=50%></center><center>图6.3 数字求和任务示例</center></br><p>如果序列长度越长，准确率越高，则说明网络的记忆能力越好．因此，我们可以构建不同长度的数据集，通过验证简单循环网络在不同长度的数据集上的表现，从而测试简单循环网络的长程依赖能力.</p><h4 id="√-6-1-1-数据集构建"><a href="#√-6-1-1-数据集构建" class="headerlink" title="[√] 6.1.1 数据集构建"></a>[√] 6.1.1 数据集构建</h4><hr><p>我们首先构建不同长度的数字预测数据集DigitSum.</p><h6 id="√-6-1-1-1-数据集的构建函数"><a href="#√-6-1-1-1-数据集的构建函数" class="headerlink" title="[√] 6.1.1.1 数据集的构建函数"></a>[√] 6.1.1.1 数据集的构建函数</h6><hr><p>由于在本任务中，输入序列的前两位数字为 0 − 9，其组合数是固定的，所以可以穷举所有的前两位数字组合，并在后面默认用0填充到固定长度. 但考虑到数据的多样性，这里对生成的数字序列中的零位置进行随机采样，并将其随机替换成0-9的数字以增加样本的数量．</p><p>我们可以通过设置k的数值来指定一条样本随机生成的数字序列数量.当生成某个指定长度的数据集时，会同时生成训练集、验证集和测试集。当k&#x3D;3时，生成训练集。当k&#x3D;1时，生成验证集和测试集. 代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 固定随机种子</span><br>random.seed(<span class="hljs-number">0</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_data</span>(<span class="hljs-params">length, k, save_path</span>):<br>    <span class="hljs-keyword">if</span> length &lt; <span class="hljs-number">3</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The length of data should be greater than 2.&quot;</span>)<br>    <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;k should be greater than 0.&quot;</span>)<br>    <span class="hljs-comment"># 生成100条长度为length的数字序列，除前两个字符外，序列其余数字暂用0填充</span><br>    base_examples = []<br>    <span class="hljs-keyword">for</span> n1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">for</span> n2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):<br>            seq = [n1, n2] + [<span class="hljs-number">0</span>] * (length - <span class="hljs-number">2</span>)<br>            label = n1 + n2<br>            base_examples.append((seq, label))<br>    <br>    examples = []<br>    <span class="hljs-comment"># 数据增强：对base_examples中的每条数据，默认生成k条数据，放入examples</span><br>    <span class="hljs-comment"># 对于每个数据，生成k条带干扰数值的数据</span><br>    <span class="hljs-keyword">for</span> base_example <span class="hljs-keyword">in</span> base_examples:<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            <span class="hljs-comment"># 随机生成替换的元素位置和元素</span><br>            idx = np.random.randint(<span class="hljs-number">2</span>, length)<br>            val = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>)<br>            <span class="hljs-comment"># 对序列中的对应零元素进行替换</span><br>            seq = base_example[<span class="hljs-number">0</span>].copy()<br>            label = base_example[<span class="hljs-number">1</span>]<br>            seq[idx] = val<br>            examples.append((seq, label))<br><br>    <span class="hljs-comment"># 保存增强后的数据</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples:<br>            <span class="hljs-comment"># 将数据转为字符串类型，方便保存</span><br>            seq = [<span class="hljs-built_in">str</span>(e) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> example[<span class="hljs-number">0</span>]]<br>            label = <span class="hljs-built_in">str</span>(example[<span class="hljs-number">1</span>])<br>            line = <span class="hljs-string">&quot; &quot;</span>.join(seq) + <span class="hljs-string">&quot;\t&quot;</span> + label + <span class="hljs-string">&quot;\n&quot;</span><br>            f.write(line)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;generate data to: <span class="hljs-subst">&#123;save_path&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 定义生成的数字序列长度</span><br>lengths = [<span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    <span class="hljs-comment"># 生成长度为length的训练数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/train.txt&quot;</span><br>    k = <span class="hljs-number">3</span><br>    generate_data(length, k, save_path)<br>    <span class="hljs-comment"># 生成长度为length的验证数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/dev.txt&quot;</span><br>    k = <span class="hljs-number">1</span><br>    generate_data(length, k, save_path)<br>    <span class="hljs-comment"># 生成长度为length的测试数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/test.txt&quot;</span><br>    k = <span class="hljs-number">1</span><br>    generate_data(length, k, save_path)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">generate data to: ./datasets/<span class="hljs-number">5</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">5</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">5</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/test.txt.<br></code></pre></td></tr></table></figure><h6 id="√-6-1-1-2-加载数据并进行数据划分"><a href="#√-6-1-1-2-加载数据并进行数据划分" class="headerlink" title="[√] 6.1.1.2 加载数据并进行数据划分"></a>[√] 6.1.1.2 加载数据并进行数据划分</h6><hr><p>为方便使用，本实验提前生成了长度分别为5、10、 15、20、25、30和35的7份数据，存放于“.&#x2F;datasets”目录下，读者可以直接加载使用。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># 加载数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">data_path</span>):<br>    <span class="hljs-comment"># 加载训练集</span><br>    train_examples = []<br>    train_path = os.path.join(data_path, <span class="hljs-string">&quot;train.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(train_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            train_examples.append((seq, label))<br><br>    <span class="hljs-comment"># 加载验证集</span><br>    dev_examples = []<br>    dev_path = os.path.join(data_path, <span class="hljs-string">&quot;dev.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(dev_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            dev_examples.append((seq, label))<br><br>    <span class="hljs-comment"># 加载测试集</span><br>    test_examples = []<br>    test_path = os.path.join(data_path, <span class="hljs-string">&quot;test.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(test_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            test_examples.append((seq, label))<br><br>    <span class="hljs-keyword">return</span> train_examples, dev_examples, test_examples<br><br><span class="hljs-comment"># 设定加载的数据集的长度</span><br>length = <span class="hljs-number">5</span><br><span class="hljs-comment"># 该长度的数据集的存放目录</span><br>data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br><span class="hljs-comment"># 加载该数据集</span><br>train_examples, dev_examples, test_examples = load_data(data_path)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dev example:&quot;</span>, dev_examples[:<span class="hljs-number">4</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练集数量：&quot;</span>, <span class="hljs-built_in">len</span>(train_examples))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;验证集数量：&quot;</span>, <span class="hljs-built_in">len</span>(dev_examples))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试集数量：&quot;</span>, <span class="hljs-built_in">len</span>(test_examples))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dev example: [([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], <span class="hljs-number">0</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>], <span class="hljs-number">1</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>], <span class="hljs-number">2</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>], <span class="hljs-number">3</span>)]<br>训练集数量： <span class="hljs-number">300</span><br>验证集数量： <span class="hljs-number">100</span><br>测试集数量： <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure><h6 id="√-6-1-1-3-构造Dataset类"><a href="#√-6-1-1-3-构造Dataset类" class="headerlink" title="[√] 6.1.1.3 构造Dataset类"></a>[√] 6.1.1.3 构造Dataset类</h6><hr><p>为了方便使用梯度下降法进行优化，我们构造了DigitSum数据集的Dataset类，函数<code>__getitem__</code>负责根据索引读取数据，并将数据转换为张量。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.io <span class="hljs-keyword">import</span> Dataset<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DigitSumDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br>    <br>    <span class="hljs-comment"># 获取指定索引的数据并稍作处理</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        example = self.data[idx]<br>        seq = paddle.to_tensor(example[<span class="hljs-number">0</span>], dtype=<span class="hljs-string">&quot;int64&quot;</span>)<br>        label = paddle.to_tensor(example[<span class="hljs-number">1</span>], dtype=<span class="hljs-string">&quot;int64&quot;</span>)<br>        <span class="hljs-keyword">return</span> seq, label<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br>        <br></code></pre></td></tr></table></figure><h4 id="√-6-1-2-模型构建"><a href="#√-6-1-2-模型构建" class="headerlink" title="[√] 6.1.2 模型构建"></a>[√] 6.1.2 模型构建</h4><hr><p>使用SRN模型进行数字加和任务的模型结构为如图6.4所示.</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/4bdc5e83a4e24feba0c3300535454b5a2030e48a992b4c3184f86c032484a928" width=50%></center><center>图6.4 基于SRN模型的数字预测</center><p>整个模型由以下几个部分组成：  </p><p>（1） 嵌入层：将输入的数字序列进行向量化，即将每个数字映射为向量；  </p><p>（2） SRN 层：接收向量序列，更新循环单元，将最后时刻的隐状态作为整个序列的表示；  </p><p>（3） 输出层：一个线性层，输出分类的结果.  </p><blockquote><p>alec：</p><p>网络具有记忆能力，即最后时刻的输出，等于刚开始的时候输入的两个元素的和</p></blockquote><h6 id="√-6-1-2-1-嵌入层"><a href="#√-6-1-2-1-嵌入层" class="headerlink" title="[√] 6.1.2.1 嵌入层"></a>[√] 6.1.2.1 嵌入层</h6><hr><p>本任务输入的样本是数字序列，为了更好地表示数字，需要将数字映射为一个嵌入（Embedding）向量。嵌入向量中的每个维度均能用来刻画该数字本身的某种特性。由于向量能够表达该数字更多的信息，利用向量进行数字求和任务，可以使得模型具有更强的拟合能力。</p><p>首先，我们构建一个嵌入矩阵（Embedding Matrix）$\boldsymbol{E}\in \mathbb{R}^{10\times M}$，其中第$i$行对应数字$i$的嵌入向量，每个嵌入向量的维度是$M$。如图6.5所示。<br>给定一个组数字序列$\boldsymbol{S} \in \mathbb{R}^{B\times L}$，其中$B$为批大小，$L$为序列长度，可以通过查表将其映射为嵌入表示$\boldsymbol{X}\in \mathbb{R}^{B\times L \times M}$。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/22d9a00278914221a4074618567af6492323e5b2ed0c47849c8669540ece7dfb" width=50%></center><center>图6.5 嵌入矩阵</center><blockquote><p>alec：</p><p>如上图所示举例，一个词向量的长度是5，一个批次的数据包含3个向量。</p></blockquote><blockquote><p><strong>提醒</strong>：为了和代码的实现保持一致性，这里使用形状为$(样本数量\times 序列长度\times 特征维度)$的张量来表示一组样本。</p></blockquote><p>或者也可以将每个数字表示为10维的one-hot向量，使用矩阵运算得到嵌入表示：</p><p>$$<br>\boldsymbol{X} &#x3D; \boldsymbol{S}^{‘} \boldsymbol{E}，<br>$$</p><p>其中$\boldsymbol{S}’ \in \mathbb{R}^{B\times L\times 10}$是序列$\boldsymbol{S}$对应的one-hot表示。</p><blockquote><p><strong>思考</strong>：如果不使用嵌入层，直接将数字作为SRN层输入有什么问题？</p></blockquote><p>基于索引方式的嵌入层的实现如下：</p><p>嵌入层对输入序列中的每个元素设置一个权重参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-comment"># 嵌入层的实现代码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embedding</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, embedding_dim, para_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.XavierUniform(<span class="hljs-params"></span>)</span>)</span>):<br>        <span class="hljs-built_in">super</span>(Embedding, self).__init__()<br>        <span class="hljs-comment"># 定义嵌入矩阵</span><br>        self.W = paddle.create_parameter(shape=[num_embeddings, embedding_dim], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=para_attr)<br>    <br>    <span class="hljs-comment"># alec：得到随机初始化的参数矩阵，并返回第inputs行的参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 根据索引获取对应词向量</span><br>        embs = self.W[inputs]<br>        <span class="hljs-keyword">return</span> embs<br><br><span class="hljs-comment"># 得到10行5列的嵌入矩阵，并返回第0、1、2、3行的参数</span><br>emb_layer = Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>)<br>inputs = paddle.to_tensor([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>emb_layer(inputs)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[-<span class="hljs-number">0.42956319</span>,  <span class="hljs-number">0.24104618</span>, -<span class="hljs-number">0.37770554</span>,  <span class="hljs-number">0.20341983</span>, -<span class="hljs-number">0.22462121</span>],<br>        [ <span class="hljs-number">0.12806311</span>,  <span class="hljs-number">0.11553246</span>, -<span class="hljs-number">0.12461890</span>,  <span class="hljs-number">0.43855545</span>, -<span class="hljs-number">0.04116940</span>],<br>        [-<span class="hljs-number">0.11202095</span>,  <span class="hljs-number">0.13205586</span>,  <span class="hljs-number">0.58078343</span>, -<span class="hljs-number">0.49379382</span>,  <span class="hljs-number">0.06259152</span>],<br>        [-<span class="hljs-number">0.51902902</span>,  <span class="hljs-number">0.04430389</span>, -<span class="hljs-number">0.37035075</span>, -<span class="hljs-number">0.21242915</span>,  <span class="hljs-number">0.46721438</span>]])<br></code></pre></td></tr></table></figure><h6 id="√-6-1-2-2-SRN层"><a href="#√-6-1-2-2-SRN层" class="headerlink" title="[√] 6.1.2.2 SRN层"></a>[√] 6.1.2.2 SRN层</h6><hr><blockquote><p>alec：</p><p>SRN的非线性激活函数是使用的tanh函数</p></blockquote><p>数字序列$\boldsymbol{S} \in \mathbb{R}^{B\times L}$经过嵌入层映射后，转换为$\boldsymbol{X}\in \mathbb{R}^{B\times L\times M}$，其中$B$为批大小，$L$为序列长度，$M$为嵌入维度。</p><p>在时刻$t$，SRN将当前的输入$\boldsymbol{X}<em>t \in \mathbb{R}^{B \times M}$与隐状态$\boldsymbol{H}</em>{t-1}  \in \mathbb{R}^{B \times D}$进行线性变换和组合，并通过一个非线性激活函数$f(\cdot)$得到新的隐状态，SRN的状态更新函数为:</p><p>$$<br>\boldsymbol{H}_t &#x3D; \text{Tanh}(\boldsymbol{X}<em>t\boldsymbol{W} + \boldsymbol{H}</em>{t-1}\boldsymbol{U} + \boldsymbol{b}),<br>$$</p><p>其中$\boldsymbol{W} \in \mathbb{R}^{M \times D}, \boldsymbol{U} \in \mathbb{R}^{D \times D}, \boldsymbol{b} \in \mathbb{R}^{1 \times D}$是可学习参数，$D$表示隐状态向量的维度。</p><p>简单循环网络的代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># SRN模型</span><br><span class="hljs-comment"># 隐状态是指的中间状态的维度？</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SRN</span>(nn.Layer):<br>    <span class="hljs-comment"># input_size是输入数据的通道维度，hidden_size是中间状态的通道维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size,  hidden_size, W_attr=<span class="hljs-literal">None</span>, U_attr=<span class="hljs-literal">None</span>, b_attr=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(SRN, self).__init__()<br>        <span class="hljs-comment"># 嵌入向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># 隐状态的维度</span><br>        self.hidden_size = hidden_size<br>        <span class="hljs-comment"># 定义模型参数W，其shape为 input_size x hidden_size</span><br>        self.W = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=W_attr)<br>        <span class="hljs-comment"># 定义模型参数U，其shape为hidden_size x hidden_size</span><br>        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>,attr=U_attr)<br>        <span class="hljs-comment"># 定义模型参数b，其shape为 1 x hidden_size</span><br>        self.b = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=b_attr)<br><br>    <span class="hljs-comment"># 初始化向量</span><br>    <span class="hljs-comment"># 向量有batch_size个，每个长度为hidden_size</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, batch_size</span>):<br>        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">return</span> hidden_state<br><br>    <span class="hljs-comment"># 定义前向计算</span><br>    <span class="hljs-comment"># input_size指的是输入数据的维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, hidden_state=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># inputs: 输入数据, 其shape为batch_size x seq_len x input_size</span><br>        batch_size, seq_len, input_size = inputs.shape<br><br>        <span class="hljs-comment"># 初始化起始状态的隐向量, 其shape为 batch_size x hidden_size</span><br>        <span class="hljs-comment"># 隐状态的向量用于接收中间的状态</span><br>        <span class="hljs-keyword">if</span> hidden_state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            hidden_state = self.init_state(batch_size)<br><br>        <span class="hljs-comment"># 循环执行RNN计算</span><br>        <span class="hljs-comment"># 序列中的每个元素，一步步的来计算</span><br>        <span class="hljs-comment"># 循环累计隐状态</span><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>            <span class="hljs-comment"># batch_size是输入数据的批数量，input_size是输入数据的维度</span><br>            <span class="hljs-comment"># 获取当前时刻的输入数据step_input, 其shape为 batch_size x input_size</span><br>            step_input = inputs[:, step, :]<br>            <span class="hljs-comment"># 获取当前时刻的隐状态向量hidden_state, 其shape为 batch_size x hidden_size</span><br>            <span class="hljs-comment"># 对于batch_size x seq_len x input_size的输入数据，其中seq_len是指的时间长度，因此对于当前时刻，</span><br>            <span class="hljs-comment"># 维度就只有对于batch_size × input_size，然后将当前时刻的输入数据转为了batch_size × hidden_state</span><br>            <span class="hljs-comment"># 权重w用于和当前时刻的输入向量相乘</span><br>            <span class="hljs-comment"># 权重U用于和当前的隐状态相乘</span><br>            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)<br>        <span class="hljs-keyword">return</span> hidden_state<br></code></pre></td></tr></table></figure><blockquote><p>提醒： 这里只保留了简单循环网络的最后一个时刻的输出向量。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 初始化参数并运行</span><br><span class="hljs-comment"># 输入的数据通道维度为2，隐状态的维度为2</span><br>W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>]]))<br>U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.0</span>]]))<br>b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br><br>srn = SRN(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)<br><span class="hljs-comment"># 输入数据的维度为batch_size x seq_len x input_size，1批1个、序列长度为2，通道维度为2</span><br>inputs = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]]], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>hidden_state = srn(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hidden_state&quot;</span>, hidden_state)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_state Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[<span class="hljs-number">0.31773996</span>, <span class="hljs-number">0.47749740</span>]])<br></code></pre></td></tr></table></figure><p>飞桨框架已经内置了SRN的API <code>paddle.nn.SimpleRNN</code>，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：</p><p>$$<br>\boldsymbol{H}_t &#x3D; \text{Tanh}(\boldsymbol{W}\boldsymbol{X}_t + \boldsymbol{b}<em>x +  \boldsymbol{U}\boldsymbol{H}</em>{t-1} + \boldsymbol{b}_h),<br>$$</p><p>其中$\boldsymbol{W} \in \mathbb{R}^{M \times D}, \boldsymbol{U} \in \mathbb{R}^{D \times D}, \boldsymbol{b}_x \in \mathbb{R}^{1 \times D}, \boldsymbol{b}_h \in \mathbb{R}^{1 \times D}$是可学习参数，$M$表示嵌入向量的维度，$D$表示隐状态向量的维度。</p><p>另外，内置SRN API在执行完前向计算后，会返回两个参数：序列向量和最后时刻的隐状态向量。在飞桨实现时，考虑到了双向和多层SRN的因素，返回的向量附带了这些信息。</p><p>其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的隐状态向量shape为[num_layers * num_directions, batch_size, hidden_size]。</p><p>这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size、</span><br><span class="hljs-comment"># 每批8个数据，时间长度是20，每个时刻的特征维度是32</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br><span class="hljs-comment"># 隐层特征数量也是32个</span><br>hidden_size = <span class="hljs-number">32</span><br>paddle_srn = nn.SimpleRNN(input_size, hidden_size)<br>self_srn = SRN(input_size, hidden_size)<br><br>self_hidden_state = self_srn(inputs)<br>paddle_outputs, paddle_hidden_state = paddle_srn(inputs)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self_srn hidden_state: &quot;</span>, self_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_srn outpus:&quot;</span>, paddle_outputs.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_srn hidden_state:&quot;</span>, paddle_hidden_state.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self_srn hidden_state:  [<span class="hljs-number">8</span>, <span class="hljs-number">32</span>] <span class="hljs-comment"># 当前时刻的状态，每批8个，隐藏状态特征数是32</span><br>paddle_srn outpus: [<span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span>]<br>paddle_srn hidden_state: [<span class="hljs-number">1</span>, <span class="hljs-number">8</span>, <span class="hljs-number">32</span>]<br></code></pre></td></tr></table></figure><p>可以看到，自己实现的SRN由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化SRN时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].</p><p>接下来，我们可以将自己实现的SRN与Paddle内置的SRN在输出值的精度上进行对比，这里首先根据Paddle内置的SRN实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_x$设置为0），然后提取该模型对应的参数，使用该参数去初始化自己实现的SRN，从而保证两者在参数初始化时是一致的。</p><p>在进行实验时，首先定义输入数据<code>inputs</code>，然后将该数据分别传入Paddle内置的SRN与自己实现的SRN模型中，最后通过对比两者的隐状态输出向量。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br><span class="hljs-comment"># 每批2个数据，每个时刻的每个数据维度是10维，时间长度为5，隐层特征的数据维度是10维</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>bx_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([hidden_size, ])))<br>paddle_srn = nn.SimpleRNN(input_size, hidden_size, bias_ih_attr=bx_attr)<br><br><span class="hljs-comment"># 获取paddle_srn中的参数，并设置相应的paramAttr,用于初始化SRN</span><br>W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_ih_l0.T))<br>U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_hh_l0.T))<br>b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.bias_hh_l0))<br>self_srn = SRN(input_size, hidden_size, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)<br><br><span class="hljs-comment"># 进行前向计算，获取隐状态向量，并打印展示</span><br>self_hidden_state = self_srn(inputs)<br>paddle_outputs, paddle_hidden_state = paddle_srn(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle SRN:\n&quot;</span>, paddle_hidden_state.numpy().squeeze(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self SRN:\n&quot;</span>, self_hidden_state.numpy())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle SRN:<br> [[ <span class="hljs-number">0.3246606</span>  -<span class="hljs-number">0.05465741</span> -<span class="hljs-number">0.3090897</span>  -<span class="hljs-number">0.51604617</span> -<span class="hljs-number">0.11149617</span>  <span class="hljs-number">0.4267313</span><br>   <span class="hljs-number">0.47200006</span> -<span class="hljs-number">0.06585315</span>  <span class="hljs-number">0.85319966</span>  <span class="hljs-number">0.18898569</span>]<br> [-<span class="hljs-number">0.4299355</span>  -<span class="hljs-number">0.6067489</span>  -<span class="hljs-number">0.59150505</span>  <span class="hljs-number">0.30245274</span> -<span class="hljs-number">0.03939498</span>  <span class="hljs-number">0.61462754</span><br>   <span class="hljs-number">0.4030218</span>   <span class="hljs-number">0.49883503</span>  <span class="hljs-number">0.02484456</span> -<span class="hljs-number">0.38516262</span>]]<br>self SRN:<br> [[ <span class="hljs-number">0.32466057</span> -<span class="hljs-number">0.05465744</span> -<span class="hljs-number">0.3090897</span>  -<span class="hljs-number">0.51604617</span> -<span class="hljs-number">0.11149605</span>  <span class="hljs-number">0.4267313</span><br>   <span class="hljs-number">0.47200006</span> -<span class="hljs-number">0.06585318</span>  <span class="hljs-number">0.85319966</span>  <span class="hljs-number">0.18898569</span>]<br> [-<span class="hljs-number">0.42993543</span> -<span class="hljs-number">0.6067488</span>  -<span class="hljs-number">0.59150493</span>  <span class="hljs-number">0.3024528</span>  -<span class="hljs-number">0.03939501</span>  <span class="hljs-number">0.61462754</span><br>   <span class="hljs-number">0.40302184</span>  <span class="hljs-number">0.49883503</span>  <span class="hljs-number">0.02484456</span> -<span class="hljs-number">0.38516262</span>]]<br></code></pre></td></tr></table></figure><p>可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 实例化模型</span><br>self_srn = SRN(input_size, hidden_size)<br>paddle_srn = nn.SimpleRNN(input_size, hidden_size)<br><br><span class="hljs-comment"># 计算自己实现的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    out = self_srn(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;self_srn speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br><span class="hljs-comment"># 计算Paddle内置的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    out = paddle_srn(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;paddle_srn speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">self_srn</span> speed: <span class="hljs-number">0</span>.<span class="hljs-number">0010721974902682834</span> s<br><span class="hljs-attribute">paddle_srn</span> speed: <span class="hljs-number">0</span>.<span class="hljs-number">0004733721415201823</span> s<br></code></pre></td></tr></table></figure><p>可以看到，由于Paddle内部相关算子由C++实现，Paddle框架实现的SRN的运行效率显著高于自己实现的SRN效率。</p><h6 id="√-6-1-2-3-线性层"><a href="#√-6-1-2-3-线性层" class="headerlink" title="[√] 6.1.2.3 线性层"></a>[√] 6.1.2.3 线性层</h6><hr><p>线性层会将最后一个时刻的隐状态向量$\boldsymbol{H}_L \in \mathbb{R}^{B \times D}$进行线性变换，输出分类的对数几率（Logits）为：<br>$$<br>\boldsymbol{Y} &#x3D; \boldsymbol{H}_L \boldsymbol{W}_o + \boldsymbol{b}_o，<br>$$</p><p>其中$\boldsymbol{W}_o \in \mathbb{R}^{D \times 19}$，$\boldsymbol{b}_o \in \mathbb{R}^{19}$为可学习的权重矩阵和偏置。</p><blockquote><p>提醒：在分类问题的实践中，我们通常只需要模型输出分类的对数几率（Logits），而不用输出每个类的概率。这需要损失函数可以直接接收对数几率来损失计算。</p></blockquote><p>线性层直接使用<code>paddle.nn.Linear</code>算子。</p><h6 id="√-6-1-2-4-模型汇总"><a href="#√-6-1-2-4-模型汇总" class="headerlink" title="[√] 6.1.2.4 模型汇总"></a>[√] 6.1.2.4 模型汇总</h6><hr><p>在定义了每一层的算子之后，我们定义一个数字求和模型Model_RNN4SeqClass，该模型会将嵌入层、SRN层和线性层进行组合，以实现数字求和的功能.</p><p>具体来讲，Model_RNN4SeqClass会接收一个SRN层实例，用于处理数字序列数据，同时在<code>__init__</code>函数中定义一个<code>Embedding</code>嵌入层，其会将输入的数字作为索引，输出对应的向量，最后会使用<code>paddle.nn.Linear</code>定义一个线性层。</p><blockquote><p>提醒：为了方便进行对比实验，我们将SRN层的实例化放在\code{Model_RNN4SeqClass}类外面。通常情况下，模型内部算子的实例化是放在模型里面。</p></blockquote><p>在<code>forward</code>函数中，调用上文实现的嵌入层、SRN层和线性层处理数字序列，同时返回最后一个位置的隐状态向量。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 基于RNN实现数字预测的模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_RNN4SeqClass</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, num_digits, input_size, hidden_size, num_classes</span>):<br>        <span class="hljs-built_in">super</span>(Model_RNN4SeqClass, self).__init__()<br>        <span class="hljs-comment"># 传入实例化的RNN层，例如SRN</span><br>        self.rnn_model = model<br>        <span class="hljs-comment"># 词典大小</span><br>        self.num_digits = num_digits<br>        <span class="hljs-comment"># 嵌入向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># 定义Embedding层</span><br>        self.embedding = Embedding(num_digits, input_size)<br>        <span class="hljs-comment"># 定义线性层</span><br>        self.linear = nn.Linear(hidden_size, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 将数字序列映射为相应向量</span><br>        inputs_emb = self.embedding(inputs)<br>        <span class="hljs-comment"># 调用RNN模型</span><br>        hidden_state = self.rnn_model(inputs_emb)<br>        <span class="hljs-comment"># 使用最后一个时刻的状态进行数字预测</span><br>        logits = self.linear(hidden_state)<br>        <span class="hljs-keyword">return</span> logits<br><br><span class="hljs-comment"># 实例化一个input_size为4， hidden_size为5的SRN</span><br>srn = SRN(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-comment"># 基于srn实例化一个数字预测模型实例</span><br>model = Model_RNN4SeqClass(srn, <span class="hljs-number">10</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">19</span>)<br><span class="hljs-comment"># 生成一个shape为 2 x 3 的批次数据</span><br>inputs = paddle.to_tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-comment"># 进行模型前向预测</span><br>logits = model(inputs)<br><span class="hljs-built_in">print</span>(logits)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">19</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[ <span class="hljs-number">0.36087763</span>, -<span class="hljs-number">0.03377634</span>, -<span class="hljs-number">0.04800312</span>,  <span class="hljs-number">0.49252868</span>, -<span class="hljs-number">0.45962709</span>,<br>         -<span class="hljs-number">0.44703209</span>,  <span class="hljs-number">0.64295375</span>,  <span class="hljs-number">0.53624588</span>, -<span class="hljs-number">0.19376591</span>,  <span class="hljs-number">0.11085325</span>,<br>         -<span class="hljs-number">0.31243768</span>,  <span class="hljs-number">0.29747075</span>, -<span class="hljs-number">0.31725749</span>, -<span class="hljs-number">0.41438878</span>,  <span class="hljs-number">0.00990404</span>,<br>          <span class="hljs-number">0.45916951</span>, -<span class="hljs-number">0.31540897</span>,  <span class="hljs-number">0.57389849</span>, -<span class="hljs-number">0.03416194</span>],<br>        [ <span class="hljs-number">0.01424524</span>, -<span class="hljs-number">0.12685573</span>, -<span class="hljs-number">0.07969519</span>,  <span class="hljs-number">0.56528699</span>, -<span class="hljs-number">0.65557188</span>,<br>         -<span class="hljs-number">0.57581109</span>,  <span class="hljs-number">0.84303617</span>,  <span class="hljs-number">0.07659776</span>,  <span class="hljs-number">0.01592400</span>, -<span class="hljs-number">0.38144892</span>,<br>         -<span class="hljs-number">0.24371660</span>,  <span class="hljs-number">0.38759732</span>, -<span class="hljs-number">0.46055052</span>, -<span class="hljs-number">0.87889659</span>, -<span class="hljs-number">0.16003403</span>,<br>          <span class="hljs-number">0.67612255</span>, -<span class="hljs-number">0.36139122</span>,  <span class="hljs-number">0.40609291</span>, -<span class="hljs-number">0.26660436</span>]])<br></code></pre></td></tr></table></figure><h4 id="√-6-1-3-模型训练"><a href="#√-6-1-3-模型训练" class="headerlink" title="[√] 6.1.3 模型训练"></a>[√] 6.1.3 模型训练</h4><hr><h6 id="√-6-1-3-1-训练指定长度的数字预测模型"><a href="#√-6-1-3-1-训练指定长度的数字预测模型" class="headerlink" title="[√] 6.1.3.1 训练指定长度的数字预测模型"></a>[√] 6.1.3.1 训练指定长度的数字预测模型</h6><hr><p>基于RunnerV3类进行训练，只需要指定length便可以加载相应的数据。设置超参数，使用Adam优化器，学习率为 0.001，实例化模型，使用第4.5.4节定义的Accuracy计算准确率。使用Runner进行训练，训练回合数设为500。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> Accuracy, RunnerV3<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">500</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">8</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><span class="hljs-comment"># 通过指定length进行不同长度数据的实验</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">length</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training SRN with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    <span class="hljs-comment"># 固定随机种子</span><br>    np.random.seed(<span class="hljs-number">0</span>)<br>    random.seed(<span class="hljs-number">0</span>)<br>    paddle.seed(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)<br>    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br>    <span class="hljs-comment"># 实例化模型</span><br>    base_model = SRN(input_size, hidden_size)<br>    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br>    <span class="hljs-comment"># 指定优化器</span><br>    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters()) <br>    <span class="hljs-comment"># 定义评价指标</span><br>    metric = Accuracy()<br>    <span class="hljs-comment"># 定义损失函数</span><br>    loss_fn = nn.CrossEntropyLoss()<br><br>    <span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>    runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br>    <span class="hljs-comment"># 进行模型训练</span><br>    model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_srn_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">100</span>, save_path=model_save_path)<br><br>    <span class="hljs-keyword">return</span> runner<br><br></code></pre></td></tr></table></figure><h6 id="√-6-1-3-2-多组训练"><a href="#√-6-1-3-2-多组训练" class="headerlink" title="[√] 6.1.3.2 多组训练"></a>[√] 6.1.3.2 多组训练</h6><hr><p>接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的<code>runner</code>保存至<code>runners</code>字典中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">srn_runners = &#123;&#125;<br><br>lengths = [<span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = train(length)<br>    srn_runners[length] = runner<br><br></code></pre></td></tr></table></figure><h6 id="√-6-1-3-3-损失曲线展示"><a href="#√-6-1-3-3-损失曲线展示" class="headerlink" title="[√] 6.1.3.3 损失曲线展示"></a>[√] 6.1.3.3 损失曲线展示</h6><hr><p>定义<code>plot_training_loss</code>函数，分别画出各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，实现代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_training_loss</span>(<span class="hljs-params">runner, fig_name, sample_step</span>):<br><br>    plt.figure()<br>    train_items = runner.train_step_losses[::sample_step]<br>    train_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    train_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    plt.plot(train_steps, train_losses, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    <br>    dev_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>    dev_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>    plt.plot(dev_steps, dev_losses, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br><br>    <span class="hljs-comment">#绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>    plt.savefig(fig_name)<br>    plt.show()<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 画出训练过程中的损失图</span><br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = srn_runners[length]<br>    fig_name = <span class="hljs-string">f&quot;./images/6.6_<span class="hljs-subst">&#123;length&#125;</span>.pdf&quot;</span><br>    plot_training_loss(runner, fig_name, sample_step=<span class="hljs-number">100</span>)<br><br></code></pre></td></tr></table></figure><p>图6.6展示了在6个数据集上的损失变化情况，数据集的长度分别为10、15、20、25、30和35. 从输出结果看，随着数据序列长度的增加，虽然训练集损失逐渐逼近于0，但是验证集损失整体趋向越来越大，这表明当序列变长时，SRN模型保持序列长期依赖能力在逐渐变弱，越来越无法学习到有用的知识.</p><blockquote><p>alec：</p><p>随着序列的变长，SRN模型越来越不能学习到有用的知识。</p></blockquote><h4 id="√-6-1-4-模型评价"><a href="#√-6-1-4-模型评价" class="headerlink" title="[√] 6.1.4 模型评价"></a>[√] 6.1.4 模型评价</h4><hr><p>在模型评价时，加载不同长度的效果最好的模型，然后使用测试集对该模型进行评价，观察模型在测试集上预测的准确度. 同时记录一下不同长度模型在训练过程中，在验证集上最好的效果。代码实现如下。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">srn_dev_scores = []<br>srn_test_scores = []<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    runner = srn_runners[length]<br>    <span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>    model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_srn_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.load_model(model_path)<br>    <br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    test_set = DigitSumDataset(test_examples)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br><br>    <span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>    score, _ = runner.evaluate(test_loader)<br>    srn_test_scores.append(score)<br>    srn_dev_scores.append(<span class="hljs-built_in">max</span>(runner.dev_scores))<br><br><span class="hljs-keyword">for</span> length, dev_score, test_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(lengths, srn_dev_scores, srn_test_scores):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, dev_score: <span class="hljs-subst">&#123;dev_score&#125;</span>, test_score: <span class="hljs-subst">&#123;test_score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>接下来，将SRN在不同长度的验证集和测试集数据上的表现，绘制成图片进行观察。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.plot(lengths, srn_dev_scores, <span class="hljs-string">&#x27;-o&#x27;</span>, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>,  label=<span class="hljs-string">&quot;Dev Accuracy&quot;</span>)<br>plt.plot(lengths, srn_test_scores,<span class="hljs-string">&#x27;-o&#x27;</span>, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, label=<span class="hljs-string">&quot;Test Accuracy&quot;</span>)<br><br><span class="hljs-comment">#绘制坐标轴和图例</span><br>plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>fig_name = <span class="hljs-string">&quot;./images/6.7.pdf&quot;</span><br>plt.savefig(fig_name)<br>plt.show()<br></code></pre></td></tr></table></figure><p>图6.7 展示了SRN模型在不同长度数据训练出来的最好模型在验证集和测试集上的表现。可以看到，随着序列长度的增加，验证集和测试集的准确度整体趋势是降低的，这同样说明SRN模型保持长期依赖的能力在不断降低.</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220110618357.png" alt="image-20221220155549904"></p><h2 id="√-6-2-梯度爆炸实验"><a href="#√-6-2-梯度爆炸实验" class="headerlink" title="[√] 6.2 梯度爆炸实验"></a>[√] 6.2 梯度爆炸实验</h2><hr><p>造成简单循环网络较难建模长程依赖问题的原因有两个：梯度爆炸和梯度消失。一般来讲，循环网络的梯度爆炸问题比较容易解决，一般通过权重衰减或梯度截断可以较好地来避免；对于梯度消失问题，更加有效的方式是改变模型，比如通过长短期记忆网络LSTM来进行缓解。</p><p>本节将首先进行复现简单循环网络中的梯度爆炸问题，然后尝试使用梯度截断的方式进行解决。这里采用长度为20的数据集进行实验，训练过程中将进行输出$W$,$U$,$b$的梯度向量的范数，以此来衡量梯度的变化情况。</p><blockquote><p>alec：</p><ul><li>梯度爆炸问题解决：<ul><li>通过权重衰减或者梯度截断解决</li></ul></li><li>梯度消失问题解决：<ul><li>有效的方式是改变模型，比如使用LSTM网络</li></ul></li></ul></blockquote><h4 id="√-6-2-1-梯度打印函数"><a href="#√-6-2-1-梯度打印函数" class="headerlink" title="[√] 6.2.1 梯度打印函数"></a>[√] 6.2.1 梯度打印函数</h4><hr><p>使用<code>custom_print_log</code>实现了在训练过程中打印梯度的功能，<code>custom_print_log</code>需要接收runner的实例，并通过<code>model.named_parameters()</code>获取该模型中的参数名和参数值. 这里我们分别定义<code>W_list</code>, <code>U_list</code>和<code>b_list</code>，用于分别存储训练过程中参数$W, U 和 b$的梯度范数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">W_list = []<br>U_list = []<br>b_list = []<br><span class="hljs-comment"># 计算梯度范数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_print_log</span>(<span class="hljs-params">runner</span>):<br>    model = runner.model<br>    W_grad_l2, U_grad_l2, b_grad_l2 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters(): <br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.W&quot;</span>:  <br>            W_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.U&quot;</span>: <br>            U_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.b&quot;</span>: <br>            b_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Training] W_grad_l2: <span class="hljs-subst">&#123;W_grad_l2:<span class="hljs-number">.5</span>f&#125;</span>, U_grad_l2: <span class="hljs-subst">&#123;U_grad_l2:<span class="hljs-number">.5</span>f&#125;</span>, b_grad_l2: <span class="hljs-subst">&#123;b_grad_l2:<span class="hljs-number">.5</span>f&#125;</span> &quot;</span>) <br>    W_list.append(W_grad_l2)<br>    U_list.append(U_grad_l2)<br>    b_list.append(b_grad_l2)<br><br>    <br></code></pre></td></tr></table></figure><h4 id="√-6-2-2-复现梯度爆炸现象"><a href="#√-6-2-2-复现梯度爆炸现象" class="headerlink" title="[√] 6.2.2 复现梯度爆炸现象"></a>[√] 6.2.2 复现梯度爆炸现象</h4><hr><p>为了更好地复现梯度爆炸问题，使用SGD优化器将批大小和学习率调大，学习率为0.2，同时在计算交叉熵损失时，将reduction设置为sum，表示将损失进行累加。 代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">50</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.2</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><br><span class="hljs-comment"># 可以设置不同的length进行不同长度数据的预测实验</span><br>length = <span class="hljs-number">20</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training SRN with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 加载长度为length的数据</span><br>data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>train_examples, dev_examples, test_examples = load_data(data_path)<br>train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples),DigitSumDataset(test_examples)<br>train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br><span class="hljs-comment"># 实例化模型</span><br>base_model = SRN(input_size, hidden_size)<br>model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br><span class="hljs-comment"># 指定优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters()) <br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&quot;sum&quot;</span>)<br><br><span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br><span class="hljs-comment"># 进行模型训练</span><br>model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">1</span>, <br>             save_path=model_save_path, custom_print_log=custom_print_log)<br><br><br><br></code></pre></td></tr></table></figure><p>接下来，可以获取训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_grad</span>(<span class="hljs-params">W_list, U_list, b_list, save_path, keep_steps=<span class="hljs-number">40</span></span>):<br><br>    <span class="hljs-comment"># 开始绘制图片</span><br>    plt.figure()<br>    <span class="hljs-comment"># 默认保留前40步的结果</span><br>    steps = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(keep_steps))<br>    plt.plot(steps, W_list[:keep_steps], <span class="hljs-string">&quot;r-&quot;</span>, color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;W_grad_l2&quot;</span>)<br>    plt.plot(steps, U_list[:keep_steps], <span class="hljs-string">&quot;-.&quot;</span>, color=<span class="hljs-string">&quot;#E20079&quot;</span>, label=<span class="hljs-string">&quot;U_grad_l2&quot;</span>)<br>    plt.plot(steps, b_list[:keep_steps], <span class="hljs-string">&quot;--&quot;</span>, color=<span class="hljs-string">&quot;#3D3D3F&quot;</span>, label=<span class="hljs-string">&quot;b_grad_l2&quot;</span>)<br>    <br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;L2 Norm&quot;</span>)<br>    plt.legend(loc=<span class="hljs-string">&quot;upper right&quot;</span>)<br>    plt.savefig(save_path)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;image has been saved to: &quot;</span>, save_path)<br><br>save_path =  <span class="hljs-string">f&quot;./images/6.8.pdf&quot;</span><br>plot_grad(W_list, U_list, b_list, save_path)<br></code></pre></td></tr></table></figure><p>图6.8 展示了在训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，可以看到经过学习率等方式的调整，梯度范数急剧变大，而后梯度范数几乎为0. 这是因为$\text{Tanh}$为$\text{Sigmoid}$型函数，其饱和区的导数接近于0，由于梯度的急剧变化，参数数值变的较大或较小，容易落入梯度饱和区，导致梯度为0，模型很难继续训练.</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/8942af416fdc461cb0115956598b32a4d83f147aee0d4ec8a9ebff43e65e85bf" width=50%></center><center>图6.8 梯度变化图</center><p>接下来，使用该模型在测试集上进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.load_model(model_path)<br><br><span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>score, _ = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, Score: <span class="hljs-subst">&#123;score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><h4 id="√-6-2-3-使用梯度截断解决梯度爆炸问题"><a href="#√-6-2-3-使用梯度截断解决梯度爆炸问题" class="headerlink" title="[√] 6.2.3 使用梯度截断解决梯度爆炸问题"></a>[√] 6.2.3 使用梯度截断解决梯度爆炸问题</h4><hr><p>梯度截断是一种可以有效解决梯度爆炸问题的启发式方法，当梯度的模大于一定阈值时，就将它截断成为一个较小的数。一般有两种截断方式：按值截断和按模截断．本实验使用按模截断的方式解决梯度爆炸问题。</p><p>按模截断是按照梯度向量$\boldsymbol{g}$的模进行截断，保证梯度向量的模值不大于阈值$b$，裁剪后的梯度为:</p><p>$$<br>\boldsymbol{g} &#x3D; \left{\begin{matrix} \boldsymbol{g},  &amp;  ||\boldsymbol{g}||\leq b \ \frac{b}{||\boldsymbol{g}||} * \boldsymbol{g},   &amp;  ||\boldsymbol{g}||\gt b \end{matrix} \right..<br>$$</p><p>当梯度向量$\boldsymbol{g}$的模不大于阈值$b$时，$\boldsymbol{g}$数值不变，否则对$\boldsymbol{g}$进行数值缩放。</p><blockquote><p>在飞桨中，可以使用<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/ClipGradByNorm_cn.html#clipgradbynorm">paddle.nn.ClipGradByNorm</a>进行按模截断. 在代码实现时，将ClipGradByNorm传入优化器，优化器在反向迭代过程中，每次梯度更新时默认可以对所有梯度裁剪。</p></blockquote><p>在引入梯度截断之后，将重新观察模型的训练情况。这里我们重新实例化一下：模型和优化器，然后组装runner，进行训练。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 清空梯度列表</span><br>W_list.clear()<br>U_list.clear()<br>b_list.clear()<br><span class="hljs-comment"># 实例化模型</span><br>base_model = SRN(input_size, hidden_size)<br>model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br><br><span class="hljs-comment"># 定义clip，并实例化优化器</span><br><span class="hljs-comment"># alec：按模进行梯度截断，将梯度截断的工具传给优化器使用，在优化的过程中发现梯度超过了阈值就进行梯度截断</span><br>clip = nn.ClipGradByNorm(clip_norm=<span class="hljs-number">5.0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(clip))<br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters(), grad_clip=clip)<br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&quot;sum&quot;</span>)<br><br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br><span class="hljs-comment"># 训练模型</span><br>model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_fix_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">1</span>, save_path=model_save_path, custom_print_log=custom_print_log)<br><br></code></pre></td></tr></table></figure><p>在引入梯度截断后，获取训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">save_path =  <span class="hljs-string">f&quot;./images/6.9.pdf&quot;</span><br>plot_grad(W_list, U_list, b_list, save_path, keep_steps=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure><p><strong>图6.9</strong> 展示了引入按模截断的策略之后，模型训练时参数梯度的变化情况。可以看到，随着迭代步骤的进行，梯度始终保持在一个有值的状态，表明按模截断能够很好地解决梯度爆炸的问题.</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/c80fb991231c417dad312c6c1396fdbd0db944566c2c46f29c4772387bcee278" width=40%></center><center>图6.9 增加梯度截断策略后，SRN参数梯度L2范数变化趋势</center><blockquote><p>alec：</p><p>使用了梯度截断策略之后，模型的梯度没有爆炸</p></blockquote><p>接下来，使用梯度截断策略的模型在测试集上进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_fix_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.load_model(model_path)<br><br><span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>score, _ = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, Score: <span class="hljs-subst">&#123;score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure><p>由于为复现梯度爆炸现象，改变了学习率，优化器等，因此准确率相对比较低。但由于采用梯度截断策略后，在后续训练过程中，模型参数能够被更新优化，因此准确率有一定的提升。</p><h2 id="6-3-LSTM的记忆能力实验"><a href="#6-3-LSTM的记忆能力实验" class="headerlink" title="[] 6.3 LSTM的记忆能力实验"></a>[] 6.3 LSTM的记忆能力实验</h2><hr><p>长短期记忆网络（Long Short-Term Memory Network，LSTM）是一种可以有效缓解长程依赖问题的循环神经网络．LSTM 的特点是引入了一个新的内部状态（Internal State）$c \in \mathbb{R}^D$ 和门控机制（Gating Mechanism）．不同时刻的内部状态以近似线性的方式进行传递，从而缓解梯度消失或梯度爆炸问题．同时门控机制进行信息筛选，可以有效地增加记忆能力．例如，输入门可以让网络忽略无关紧要的输入信息，遗忘门可以使得网络保留有用的历史信息．在上一节的数字求和任务中，如果模型能够记住前两个非零数字，同时忽略掉一些不重要的干扰信息，那么即时序列很长，模型也有效地进行预测.</p><p>LSTM 模型在第 $t$ 步时，循环单元的内部结构如图6.10所示．</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/6ddebc52f6494af49d1c77c1548ec4086c9a466869fd4389b3a02df3333e271a" width=700></center><br><center>图6.10 LSTM网络的循环单元结构</center></br><blockquote><p><strong>提醒</strong>：为了和代码的实现保存一致性，这里使用形状为 (样本数量 × 序列长度 × 特征维度) 的张量来表示一组样本.</p></blockquote><p>假设一组输入序列为$\boldsymbol{X}\in \mathbb{R}^{B\times L\times M}$，其中$B$为批大小，$L$为序列长度，$M$为输入特征维度，LSTM从从左到右依次扫描序列，并通过循环单元计算更新每一时刻的状态内部状态$\boldsymbol{C}<em>{t}  \in \mathbb{R}^{B \times D}$和输出状态$\boldsymbol{H}</em>{t}  \in \mathbb{R}^{B \times D}$。</p><p>具体计算分为三步：</p><p><strong>（1）计算三个“门”</strong></p><p>在时刻$t$，LSTM的循环单元将当前时刻的输入$\boldsymbol{X}<em>t \in \mathbb{R}^{B \times M}$与上一时刻的输出状态$\boldsymbol{H}</em>{t-1}  \in \mathbb{R}^{B \times D}$，计算一组输入门$\boldsymbol{I}_t$、遗忘门$\boldsymbol{F}_t$和输出门$\boldsymbol{O}_t$，其计算公式为</p><p>$$<br>\boldsymbol{I}_{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>i+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_i+\boldsymbol{b}<em>i) \in \mathbb{R}^{B \times D},\<br>\boldsymbol{F}</em>{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>f+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_f+\boldsymbol{b}<em>f) \in \mathbb{R}^{B \times D},\<br>\boldsymbol{O}</em>{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>o+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_o+\boldsymbol{b}_o) \in \mathbb{R}^{B \times D},<br>$$</p><p>其中$\boldsymbol{W}<em>* \in \mathbb{R}^{M \times D},\boldsymbol{U}</em>* \in \mathbb{R}^{D \times D},\boldsymbol{b}_* \in \mathbb{R}^{D}$为可学习的参数，$\sigma$表示Logistic函数，将“门”的取值控制在$(0,1)$区间。这里的“门”都是$B$个样本组成的矩阵，每一行为一个样本的“门”向量。</p><p><strong>（2）计算内部状态</strong></p><p>首先计算候选内部状态：</p><p>$$<br>\tilde{\boldsymbol{C}}_{t}&#x3D;\tanh(\boldsymbol{X}_t\boldsymbol{W}<em>c+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_c+\boldsymbol{b}_c) \in \mathbb{R}^{B \times D},<br>$$</p><p>其中$\boldsymbol{W}_c \in \mathbb{R}^{M \times D}, \boldsymbol{U}_c \in \mathbb{R}^{D \times D},\boldsymbol{b}_c \in \mathbb{R}^{D}$为可学习的参数。</p><p>使用遗忘门和输入门，计算时刻$t$的内部状态：<br>$$<br>\boldsymbol{C}<em>{t} &#x3D; \boldsymbol{F}<em>t \odot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}</em>{t} \odot \boldsymbol{\tilde{C}}_{t},<br>$$<br>其中$\odot$为逐元素积。</p><p><strong>3）计算输出状态</strong><br>当前LSTM单元状态（候选状态）的计算公式为:<br>LSTM单元状态向量$\boldsymbol{C}<em>{t}$和$\boldsymbol{H}<em>t$的计算公式为<br>$$<br>\boldsymbol{C}</em>{t} &#x3D; \boldsymbol F_t \odot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}<em>{t} \odot \boldsymbol{\tilde{C}}</em>{t}，\<br>\boldsymbol{H}<em>{t} &#x3D; \boldsymbol{O}</em>{t} \odot \text{tanh}(\boldsymbol{C}_{t}).<br>$$</p><p>LSTM循环单元结构的输入是$t-1$时刻内部状态向量$\boldsymbol{C}<em>{t-1} \in \mathbb{R}^{B \times D}$和隐状态向量$\boldsymbol{H}</em>{t-1} \in \mathbb{R}^{B \times D}$，输出是当前时刻$t$的状态向量$\boldsymbol{C}<em>{t} \in \mathbb{R}^{B \times D}$和隐状态向量$\boldsymbol{H}</em>{t} \in \mathbb{R}^{B \times D}$。通过LSTM循环单元，整个网络可以建立较长距离的时序依赖关系。</p><p>通过学习这些门的设置，LSTM可以选择性地忽略或者强化当前的记忆或是输入信息，帮助网络更好地学习长句子的语义信息。</p><p>在本节中，我们使用LSTM模型重新进行数字求和实验，验证LSTM模型的长程依赖能力。</p><h4 id="√-6-3-1-模型构建"><a href="#√-6-3-1-模型构建" class="headerlink" title="[√] 6.3.1 模型构建"></a>[√] 6.3.1 模型构建</h4><hr><p>在本实验中，我们将使用第6.1.2.4节中定义Model_RNN4SeqClass模型，并构建 LSTM 算子．只需要实例化 LSTM 算，并传入Model_RNN4SeqClass模型，就可以用 LSTM 进行数字求和实验。</p><h6 id="√-6-3-1-1-LSTM层"><a href="#√-6-3-1-1-LSTM层" class="headerlink" title="[√] 6.3.1.1 LSTM层"></a>[√] 6.3.1.1 LSTM层</h6><hr><p>LSTM层的代码与SRN层结构相似，只是在SRN层的基础上增加了内部状态、输入门、遗忘门和输出门的定义和计算。这里LSTM层的输出也依然为序列的最后一个位置的隐状态向量。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment"># 声明LSTM和相关参数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTM</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, Wi_attr=<span class="hljs-literal">None</span>, Wf_attr=<span class="hljs-literal">None</span>, Wo_attr=<span class="hljs-literal">None</span>, Wc_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 Ui_attr=<span class="hljs-literal">None</span>, Uf_attr=<span class="hljs-literal">None</span>, Uo_attr=<span class="hljs-literal">None</span>, Uc_attr=<span class="hljs-literal">None</span>, bi_attr=<span class="hljs-literal">None</span>, bf_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 bo_attr=<span class="hljs-literal">None</span>, bc_attr=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(LSTM, self).__init__()<br>        self.input_size = input_size<br>        self.hidden_size = hidden_size<br><br>        <span class="hljs-comment"># 初始化模型参数</span><br>        self.W_i = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wi_attr)<br>        self.W_f = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wf_attr)<br>        self.W_o = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wo_attr)<br>        self.W_c = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wc_attr)<br>        self.U_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Ui_attr)<br>        self.U_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uf_attr)<br>        self.U_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uo_attr)<br>        self.U_c = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uc_attr)<br>        self.b_i = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bi_attr)<br>        self.b_f = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bf_attr)<br>        self.b_o = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bo_attr)<br>        self.b_c = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bc_attr)<br>    <br>    <span class="hljs-comment"># 初始化状态向量和隐状态向量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, batch_size</span>):<br>        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">return</span> hidden_state, cell_state<br><br>    <span class="hljs-comment"># 定义前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, states=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># inputs: 输入数据，其shape为batch_size x seq_len x input_size</span><br>        batch_size, seq_len, input_size = inputs.shape <br>        <br>        <span class="hljs-comment"># 初始化起始的单元状态和隐状态向量，其shape为batch_size x hidden_size</span><br>        <span class="hljs-keyword">if</span> states <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            states = self.init_state(batch_size)<br>        hidden_state, cell_state = states<br><br>        <span class="hljs-comment"># 执行LSTM计算，包括：输入门、遗忘门和输出门、候选内部状态、内部状态和隐状态向量</span><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>            <span class="hljs-comment"># 获取当前时刻的输入数据step_input: 其shape为batch_size x input_size</span><br>            step_input = inputs[:, step, :]<br>            <span class="hljs-comment"># 计算输入门, 遗忘门和输出门, 其shape为：batch_size x hidden_size</span><br>            I_gate = F.sigmoid(paddle.matmul(step_input, self.W_i) + paddle.matmul(hidden_state, self.U_i) + self.b_i)<br>            F_gate = F.sigmoid(paddle.matmul(step_input, self.W_f) + paddle.matmul(hidden_state, self.U_f) + self.b_f)<br>            O_gate = F.sigmoid(paddle.matmul(step_input, self.W_o) + paddle.matmul(hidden_state, self.U_o) + self.b_o)<br>            <span class="hljs-comment"># 计算候选状态向量, 其shape为：batch_size x hidden_size</span><br>            C_tilde = F.tanh(paddle.matmul(step_input, self.W_c) + paddle.matmul(hidden_state, self.U_c) + self.b_c)<br>            <span class="hljs-comment"># 计算单元状态向量, 其shape为：batch_size x hidden_size</span><br>            <span class="hljs-comment"># alec：单元状态向量是由候选状态向量和上一次的单元状态向量得到的</span><br>            cell_state = F_gate * cell_state + I_gate * C_tilde<br>            <span class="hljs-comment"># 计算隐状态向量，其shape为：batch_size x hidden_size</span><br>            <span class="hljs-comment"># 隐状态向量是由候选状态向量非线性变换得到的</span><br>            hidden_state = O_gate * F.tanh(cell_state)<br><br>        <span class="hljs-keyword">return</span> hidden_state<br>        <br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br><br>lstm = LSTM(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,<br>                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,<br>                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)<br><br>inputs = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>hidden_state = lstm(inputs)<br><span class="hljs-built_in">print</span>(hidden_state)<br></code></pre></td></tr></table></figure><p>飞桨框架已经内置了LSTM的API <code>paddle.nn.LSTM</code>，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：。</p><p>$$<br>\boldsymbol{I}<em>{t}&#x3D;\sigma(\boldsymbol{W}</em>{ii}\boldsymbol{X}<em>t + \boldsymbol{b}</em>{ii} + \boldsymbol{U}<em>{hi}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}<em>{hi}) \<br>\boldsymbol{F}</em>{t}&#x3D;\sigma(\boldsymbol{W}<em>{if}\boldsymbol{X}<em>t + \boldsymbol{b}</em>{if}+ \boldsymbol{U}</em>{hf}\boldsymbol{H}<em>{t-1}+\boldsymbol{b}</em>{hf}) \<br>\boldsymbol{O}<em>{t}&#x3D;\sigma(\boldsymbol{W}</em>{io}\boldsymbol{X}<em>t+ \boldsymbol{b}</em>{io} +\boldsymbol{U}<em>{ho}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}_{ho}),<br>$$</p><p>$$<br>\tilde{\boldsymbol{C}}<em>{t}&#x3D;\tanh(\boldsymbol{W}</em>{ic}\boldsymbol{X}<em>t+\boldsymbol{b}</em>{ic}+\boldsymbol{U}<em>{hc}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}_{hc}) ,<br>$$</p><p>$$<br>\boldsymbol{C}<em>{t} &#x3D; \boldsymbol F_t \cdot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}<em>{t} \cdot \boldsymbol{\tilde{C}}</em>{t}，\<br>\boldsymbol{H}<em>{t} &#x3D; \boldsymbol{O}</em>{t} \cdot \text{tanh}(\boldsymbol{C}_{t}).<br>$$</p><p>其中$\boldsymbol{W}<em>* \in \mathbb{R}^{M \times D}, \boldsymbol{U}</em>* \in \mathbb{R}^{D \times D}, \boldsymbol{b}<em>{i*} \in \mathbb{R}^{1 \times D}, \boldsymbol{b}</em>{h*} \in \mathbb{R}^{1 \times D}$是可学习参数。</p><p>另外，在Paddle内置LSTM实现时，对于参数$\boldsymbol{W}<em>{ii}, \boldsymbol{W}</em>{if}, \boldsymbol{W}<em>{io}, \boldsymbol{W}</em>{ic}$ ，并不是分别申请这些矩阵，而是申请了一个大的矩阵$\boldsymbol{W}<em>{ih}$，将这个大的矩阵分割为4份，便可以得到$\boldsymbol{W}</em>{ii}, \boldsymbol{W}<em>{if},\boldsymbol{W}</em>{ic},\boldsymbol{W}<em>{io}$。 同理，将会得到$\boldsymbol{W}</em>{hh}$, $\boldsymbol{b}<em>{ih}$和$\boldsymbol{b}</em>{hh}$.</p><p>最后，Paddle内置LSTM API将会返回参数序列向量outputs和最后时刻的状态向量，其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的状态向量是个元组，其包含了两个向量，分别是隐状态向量和单元状态向量，其shape均为[num_layers * num_directions, batch_size, hidden_size]。</p><p>这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。</p><blockquote><p>alec：</p><p>候选状态向量是核心组成，遗忘门控×前一个单元状态向量 + 输入门控×候选状态向量 &#x3D; 单元状态向量</p><p>输出门控 × 激活函数（单元状态向量） &#x3D; 隐状态向量</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>hidden_size = <span class="hljs-number">32</span><br>paddle_lstm = nn.LSTM(input_size, hidden_size)<br>self_lstm = LSTM(input_size, hidden_size)<br><br>self_hidden_state = self_lstm(inputs)<br>paddle_outputs, (paddle_hidden_state, paddle_cell_state) = paddle_lstm(inputs)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self_lstm hidden_state: &quot;</span>, self_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm outpus:&quot;</span>, paddle_outputs.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm hidden_state:&quot;</span>, paddle_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm cell_state:&quot;</span>, paddle_cell_state.shape)<br><br></code></pre></td></tr></table></figure><p>可以看到，自己实现的LSTM由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化LSTM时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].</p><p>接下来，我们可以将自己实现的LSTM与Paddle内置的LSTM在输出值的精度上进行对比，这里首先根据Paddle内置的LSTM实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_{ih}$设置为0），然后提取该模型对应的参数，进行参数分割后，使用相应参数去初始化自己实现的LSTM，从而保证两者在参数初始化时是一致的。</p><p>在进行实验时，首先定义输入数据<code>inputs</code>，然后将该数据分别传入Paddle内置的LSTM与自己实现的LSTM模型中，最后通过对比两者的隐状态输出向量。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>bih_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([<span class="hljs-number">4</span>*hidden_size, ])))<br>paddle_lstm = nn.LSTM(input_size, hidden_size, bias_ih_attr=bih_attr)<br><br><span class="hljs-comment"># 获取paddle_lstm中的参数，并设置相应的paramAttr,用于初始化lstm</span><br><span class="hljs-built_in">print</span>(paddle_lstm.weight_ih_l0.T.shape)<br>chunked_W = paddle.split(paddle_lstm.weight_ih_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br>chunked_U = paddle.split(paddle_lstm.weight_hh_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br>chunked_b = paddle.split(paddle_lstm.bias_hh_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br><br>Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">0</span>]))<br>Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">1</span>]))<br>Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">2</span>]))<br>Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">3</span>]))<br>Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">0</span>]))<br>Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">1</span>]))<br>Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">2</span>]))<br>Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">3</span>]))<br>bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">0</span>]))<br>bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">1</span>]))<br>bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">2</span>]))<br>bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">3</span>]))<br>self_lstm = LSTM(input_size, hidden_size, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,<br>                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,<br>                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)<br><br><span class="hljs-comment"># 进行前向计算，获取隐状态向量，并打印展示</span><br>self_hidden_state = self_lstm(inputs)<br>paddle_outputs, (paddle_hidden_state, _) = paddle_lstm(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle SRN:\n&quot;</span>, paddle_hidden_state.numpy().squeeze(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self SRN:\n&quot;</span>, self_hidden_state.numpy())<br></code></pre></td></tr></table></figure><p>可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：</p><blockquote><p>alec：</p><p>paddle_lstm &#x3D; nn.LSTM(input_size, hidden_size)</p><p><code>nn.name()</code>的方式，也是在实例化一个类</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>hidden_size = <span class="hljs-number">32</span><br>self_lstm = LSTM(input_size, hidden_size)<br>paddle_lstm = nn.LSTM(input_size, hidden_size)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(paddle_lstm))<br><br><span class="hljs-comment"># 计算自己实现的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    hidden_state = self_lstm(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;self_lstm speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br><span class="hljs-comment"># 计算Paddle内置的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    outputs, (hidden_state, cell_state) = paddle_lstm(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;paddle_lstm speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self_lstm speed: <span class="hljs-number">0.02104825178782145</span> s<br>paddle_lstm speed: <span class="hljs-number">0.001053280300564236</span> s<br></code></pre></td></tr></table></figure><p>可以看到，由于Paddle框架的LSTM底层采用了C++实现并进行优化，Paddle框架内置的LSTM运行效率远远高于自己实现的LSTM。</p><h6 id="√-6-3-1-2-模型汇总"><a href="#√-6-3-1-2-模型汇总" class="headerlink" title="[√] 6.3.1.2 模型汇总"></a>[√] 6.3.1.2 模型汇总</h6><hr><p>在本节实验中，我们将使用6.1.2.4的Model_RNN4SeqClass作为预测模型，不同在于在实例化时将传入实例化的LSTM层。</p><blockquote><p>动手联系6.2 在我们手动实现的LSTM算子中，是逐步计算每个时刻的隐状态。请思考如何实现更加高效的LSTM算子。</p></blockquote><h4 id="√-6-3-2-模型训练"><a href="#√-6-3-2-模型训练" class="headerlink" title="[√] 6.3.2 模型训练"></a>[√] 6.3.2 模型训练</h4><hr><h6 id="√-6-3-2-1-训练指定长度的数字预测模型"><a href="#√-6-3-2-1-训练指定长度的数字预测模型" class="headerlink" title="[√] 6.3.2.1 训练指定长度的数字预测模型"></a>[√] 6.3.2.1 训练指定长度的数字预测模型</h6><hr><p>本节将基于RunnerV3类进行训练，首先定义模型训练的超参数，并保证和简单循环网络的超参数一致. 然后定义一个<code>train</code>函数，其可以通过指定长度的数据集，并进行训练. 在<code>train</code>函数中，首先加载长度为<code>length</code>的数据，然后实例化各项组件并创建对应的Runner，然后训练该Runner。同时在本节将使用4.5.4节定义的准确度（Accuracy）作为评估指标，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> RunnerV3<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">500</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">8</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><span class="hljs-comment"># 可以设置不同的length进行不同长度数据的预测实验</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">length</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training LSTM with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    np.random.seed(<span class="hljs-number">0</span>)<br>    random.seed(<span class="hljs-number">0</span>)<br>    paddle.seed(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)<br>    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br>    <span class="hljs-comment"># 实例化模型</span><br>    base_model = LSTM(input_size, hidden_size)<br>    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br>    <span class="hljs-comment"># 指定优化器</span><br>    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())<br>    <span class="hljs-comment"># 定义评价指标</span><br>    metric = Accuracy()<br>    <span class="hljs-comment"># 定义损失函数</span><br>    loss_fn = paddle.nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>    runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br>    <span class="hljs-comment"># 进行模型训练</span><br>    model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_lstm_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">100</span>, save_path=model_save_path)<br><br>    <span class="hljs-keyword">return</span> runner<br></code></pre></td></tr></table></figure><h6 id="√-6-3-2-2-多组训练"><a href="#√-6-3-2-2-多组训练" class="headerlink" title="[√] 6.3.2.2 多组训练"></a>[√] 6.3.2.2 多组训练</h6><hr><p>接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的<code>runner</code>保存至<code>runners</code>字典中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lstm_runners = &#123;&#125;<br><br>lengths = [<span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = train(length)<br>    lstm_runners[length] = runner<br></code></pre></td></tr></table></figure><p>[] 6.3.2.3 损失曲线展示</p><hr><p>分别画出基于LSTM的各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 画出训练过程中的损失图</span><br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = lstm_runners[length]<br>    fig_name = <span class="hljs-string">f&quot;./images/6.11_<span class="hljs-subst">&#123;length&#125;</span>.pdf&quot;</span><br>    plot_training_loss(runner, fig_name, sample_step=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure><p>图6.11展示了LSTM模型在不同长度数据集上进行训练后的损失变化，同SRN模型一样，随着序列长度的增加，训练集上的损失逐渐不稳定，验证集上的损失整体趋向于变大，这说明当序列长度增加时，保持长期依赖的能力同样在逐渐变弱. 同图6.5相比，LSTM模型在序列长度增加时，收敛情况比SRN模型更好。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/f319b75922c74c0c822cb0ab6383a22ca2a8d473ffa9472589e4db90cb70a062" width=100%></center><center>图6.11 LSTM在不同长度数据集训练损失变化图</center></br>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第5章 - 卷积神经网络 - 视频</title>
    <link href="/posts/1033337951/"/>
    <url>/posts/1033337951/</url>
    
    <content type="html"><![CDATA[<hr><h2 id="√-5-1-卷积"><a href="#√-5-1-卷积" class="headerlink" title="[√] 5.1 - 卷积"></a>[√] 5.1 - 卷积</h2><hr><h4 id="√-目录"><a href="#√-目录" class="headerlink" title="[√] 目录"></a>[√] 目录</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219143242822.png" alt="image-20221219130508758"> </p><hr><h4 id="√-卷积"><a href="#√-卷积" class="headerlink" title="[√] 卷积"></a>[√] 卷积</h4><hr><h6 id="√-用卷积替代全连接"><a href="#√-用卷积替代全连接" class="headerlink" title="[√] 用卷积替代全连接"></a>[√] 用卷积替代全连接</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219142645749.png" alt="image-20221219142645749"></p><hr><h6 id="√-卷积算子"><a href="#√-卷积算子" class="headerlink" title="[√] 卷积算子"></a>[√] 卷积算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219130508758.png" alt="image-20221219143242822"></p><hr><h6 id="√-二维卷积算子"><a href="#√-二维卷积算子" class="headerlink" title="[√] 二维卷积算子"></a>[√] 二维卷积算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219143422307.png" alt="image-20221219143422307"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219144446993.png" alt="image-20221219143509534"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219143509534.png" alt="image-20221219143721941"></p><hr><h6 id="√-二维卷积算子的参数量"><a href="#√-二维卷积算子的参数量" class="headerlink" title="[√] 二维卷积算子的参数量"></a>[√] 二维卷积算子的参数量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219144809626.png" alt="image-20221219144138294"></p><blockquote><p>alec：</p><p>L层得到30×30个神经元，</p><p>使用全连接的话则需要30×30×32×32≈900000&#x3D;90万个参数</p><p>使用卷积的话，则需要的参数量为9个（不同的神经元参数共享、单个神经元局部连接，通过这两个来节省参数）</p></blockquote><hr><h6 id="√-二维卷积算子的计算量"><a href="#√-二维卷积算子的计算量" class="headerlink" title="[√] 二维卷积算子的计算量"></a>[√] 二维卷积算子的计算量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219144634395.png" alt="image-20221219144446993"></p><hr><h6 id="√-感受野"><a href="#√-感受野" class="headerlink" title="[√] 感受野"></a>[√] 感受野</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219144943348.png" alt="image-20221219144634395"></p><hr><h6 id="√-步长"><a href="#√-步长" class="headerlink" title="[√] 步长"></a>[√] 步长</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219145246003.png" alt="image-20221219144809626"></p><hr><h6 id="√-零填充"><a href="#√-零填充" class="headerlink" title="[√] 零填充"></a>[√] 零填充</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219150120256.png" alt="image-20221219144943348"></p><hr><h4 id="√-卷积的变种"><a href="#√-卷积的变种" class="headerlink" title="[√] 卷积的变种"></a>[√] 卷积的变种</h4><hr><h6 id="√-常用的等宽卷积"><a href="#√-常用的等宽卷积" class="headerlink" title="[√] 常用的等宽卷积"></a>[√] 常用的等宽卷积</h6><hr><blockquote><p>alec：</p><p>等宽卷积：P &#x3D; （U - 1）&#x2F; 2</p><p>常用等宽卷积的有：resnet、vgg</p><p>等宽卷积举例：卷积核大小3×3，填充P &#x3D; （3-1）&#x2F; 2 &#x3D; 1</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219145759222.png" alt="image-20221219145246003"></p><hr><h6 id="√-带步长和零填充的二维卷积算子"><a href="#√-带步长和零填充的二维卷积算子" class="headerlink" title="[√] 带步长和零填充的二维卷积算子"></a>[√] 带步长和零填充的二维卷积算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219150239521.png" alt="image-20221219145412085"></p><hr><h4 id="√-使用卷积完成图像边缘检测任务"><a href="#√-使用卷积完成图像边缘检测任务" class="headerlink" title="[√] 使用卷积完成图像边缘检测任务"></a>[√] 使用卷积完成图像边缘检测任务</h4><hr><blockquote><p>alec：</p><p>拉布拉斯算子是一个二维微分算子，能够对边缘进行提取</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219151258835.png" alt="image-20221219145722897"></p><hr><h6 id="√-拉普拉斯算子"><a href="#√-拉普拉斯算子" class="headerlink" title="[√] 拉普拉斯算子"></a>[√] 拉普拉斯算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219150542776.png" alt="image-20221219145759222"></p><hr><h2 id="√-5-2-卷积神经网络的基础算子"><a href="#√-5-2-卷积神经网络的基础算子" class="headerlink" title="[√] 5.2 - 卷积神经网络的基础算子"></a>[√] 5.2 - 卷积神经网络的基础算子</h2><hr><h4 id="√-卷积神经网络的基础算子"><a href="#√-卷积神经网络的基础算子" class="headerlink" title="[√] 卷积神经网络的基础算子"></a>[√] 卷积神经网络的基础算子</h4><hr><h6 id="√-卷积神经网络"><a href="#√-卷积神经网络" class="headerlink" title="[√] 卷积神经网络"></a>[√] 卷积神经网络</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219150735517.png" alt="image-20221219150120256"></p><hr><h6 id="√-卷积层算子"><a href="#√-卷积层算子" class="headerlink" title="[√] 卷积层算子"></a>[√] 卷积层算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219145722897.png" alt="image-20221219150239521"></p><hr><h6 id="√-一张输出特征图的计算"><a href="#√-一张输出特征图的计算" class="headerlink" title="[√] 一张输出特征图的计算"></a>[√] 一张输出特征图的计算</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219161403997.png" alt="image-20221219150423360"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219161536118.png" alt="image-20221219150542776"></p><blockquote><p>alec：</p><p>输出通道数，等于卷积核的个数，也等于输出特征图的深度</p><p>输入特征图的深度一般定义为D，输出特征图的深度一般定义为P</p></blockquote><hr><h4 id="√-卷积层算子-1"><a href="#√-卷积层算子-1" class="headerlink" title="[√] 卷积层算子"></a>[√] 卷积层算子</h4><hr><h6 id="√-多张输出特征图的计算"><a href="#√-多张输出特征图的计算" class="headerlink" title="[√] 多张输出特征图的计算"></a>[√] 多张输出特征图的计算</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162856215.png" alt="image-20221219150735517"></p><blockquote><p>alec：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162715734.png" alt="image-20221219151258835"></p><p>上面这个一层的卷子算子，out_channels为3，表示本层防止了3组卷积核，in_channels为2，表示上一层数据有2层，因此本层的每组卷积核要设置2层，每个卷积核的长宽为kernel_size的平方</p></blockquote><hr><h6 id="√-多通道卷积层算子的参数量"><a href="#√-多通道卷积层算子的参数量" class="headerlink" title="[√] 多通道卷积层算子的参数量"></a>[√] 多通道卷积层算子的参数量</h6><hr><blockquote><p>alec：</p><p>参数量：P×D×U×V + P</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219150423360.png" alt="image-20221219161403997"></p><hr><h6 id="√-多通道卷积层算子计算量"><a href="#√-多通道卷积层算子计算量" class="headerlink" title="[√] 多通道卷积层算子计算量"></a>[√] 多通道卷积层算子计算量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162318771.png" alt="image-20221219161536118"></p><blockquote><p>alec：</p><p>一组卷积核对应一个偏置参数</p></blockquote><hr><h4 id="√-汇聚层算子"><a href="#√-汇聚层算子" class="headerlink" title="[√] 汇聚层算子"></a>[√] 汇聚层算子</h4><hr><h6 id="√-汇聚层算子-1"><a href="#√-汇聚层算子-1" class="headerlink" title="[√] 汇聚层算子"></a>[√] 汇聚层算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162922050.png" alt="image-20221219162318771"></p><blockquote><p>alec：</p><p>汇聚层的参数量都是0，没有需要学习的参数。</p><p>汇聚层的计算量，最大汇聚为0，平均汇聚为M‘×N’×P，即一个输出像素对应1计算量，因为输出有P层特征图，每个特征图为M’×N‘，因此平均汇聚的计算量为如上</p><p>汇聚层帮助减少计算量，但是不会帮助减少参数量</p></blockquote><hr><h2 id="√-5-3-基于LeNet实现手写数字识别"><a href="#√-5-3-基于LeNet实现手写数字识别" class="headerlink" title="[√] 5.3 - 基于LeNet实现手写数字识别"></a>[√] 5.3 - 基于LeNet实现手写数字识别</h2><hr><h4 id="√-机器学习时间5要素"><a href="#√-机器学习时间5要素" class="headerlink" title="[√] 机器学习时间5要素"></a>[√] 机器学习时间5要素</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164333869.png" alt="image-20221219162715734"></p><hr><h4 id="√-数据"><a href="#√-数据" class="headerlink" title="[√] 数据"></a>[√] 数据</h4><hr><h6 id="√-数据集介绍"><a href="#√-数据集介绍" class="headerlink" title="[√] 数据集介绍"></a>[√] 数据集介绍</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162822345.png" alt="image-20221219162741543"></p><hr><h6 id="√-数据集分布"><a href="#√-数据集分布" class="headerlink" title="[√] 数据集分布"></a>[√] 数据集分布</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219163725636.png" alt="image-20221219162822345"></p><hr><h6 id="√-数据形状"><a href="#√-数据形状" class="headerlink" title="[√] 数据形状"></a>[√] 数据形状</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219162741543.png" alt="image-20221219162856215"></p><hr><h6 id="√-数据可视化"><a href="#√-数据可视化" class="headerlink" title="[√] 数据可视化"></a>[√] 数据可视化</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219163205537.png" alt="image-20221219162922050"></p><hr><h6 id="√-数据预处理"><a href="#√-数据预处理" class="headerlink" title="[√] 数据预处理"></a>[√] 数据预处理</h6><hr><p>resize + normalize</p><hr><h6 id="√-数据集封装"><a href="#√-数据集封装" class="headerlink" title="[√] 数据集封装"></a>[√] 数据集封装</h6><hr><blockquote><p>alec：</p><p>数据集类继承Dataset类，类中有__getitem__方法和--len--方法，其中getitem方法，根据索引获取数据，然后对数据预处理，然后返回处理后的数据和标签</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164238798.png" alt="image-20221219163205537"></p><hr><h4 id="√-模型构建"><a href="#√-模型构建" class="headerlink" title="[√] 模型构建"></a>[√] 模型构建</h4><hr><h6 id="√-LeNet-5"><a href="#√-LeNet-5" class="headerlink" title="[√] LeNet-5"></a>[√] LeNet-5</h6><hr><blockquote><p>alec：</p><p>随着网络的加深，虽然特征图长度越来越小，甚至变成1×1的，但是因为高层的一个像素（神经元）对应的输入图像的感受野非常大，所以 即使一个像素，也包含了大的特征。同时随着深度加深，通常特征通道数即特征图像个数会增加。最后通过全连接层将这些多个高级特征全连接用于分类等任务。</p><p>一开始，输入图像的三个通道，只是单纯的三个通道，但是在卷积网络中，每一层的多个通道都是对应的多种特征图。、</p><hr><blockquote><p>全连接层对比卷积层的不足在于，全连接无法提取图像的局部不变性特征</p></blockquote><hr><p>卷积之后，先激活，然后再汇聚</p><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164621216.png" alt="image-20221219164333869"></p><p>卷积层进入全连接之前，需要做一个reshape操作，因为卷积得到的是[B,C,H,W]形状的特征图，全连接需要输入一维列向量，因此需要将[B,C,H,W]转为[B,C×H×W]。</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219172424150.png" alt="image-20221219163725636"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164529451.png" alt="image-20221219164238798"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219172519358.png" alt="image-20221219164430763"></p><hr><h6 id="√-LeNet-5的参数量"><a href="#√-LeNet-5的参数量" class="headerlink" title="[√] LeNet-5的参数量"></a>[√] LeNet-5的参数量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219173720075.png" alt="image-20221219164529451"></p><hr><h6 id="√-LeNet-5的计算量"><a href="#√-LeNet-5的计算量" class="headerlink" title="[√] LeNet-5的计算量"></a>[√] LeNet-5的计算量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219173345892.png" alt="image-20221219164621216"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219174919075.png" alt="image-20221219164634943"></p><blockquote><p>alec：</p><p>flops方法可以计算模型的计算量，就不用自己手动算了</p></blockquote><hr><h6 id="√-模型运算速度对比"><a href="#√-模型运算速度对比" class="headerlink" title="[√] 模型运算速度对比"></a>[√] 模型运算速度对比</h6><hr><h4 id="√-模型训练"><a href="#√-模型训练" class="headerlink" title="[√] 模型训练"></a>[√] 模型训练</h4><hr><h6 id="√-模型训练-1"><a href="#√-模型训练-1" class="headerlink" title="[√] 模型训练"></a>[√] 模型训练</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219172458036.png" alt="image-20221219172306611"></p><hr><h4 id="√-模型训练与评价"><a href="#√-模型训练与评价" class="headerlink" title="[√] 模型训练与评价"></a>[√] 模型训练与评价</h4><hr><h6 id="√-训练过程可视化-amp-amp-模型评价"><a href="#√-训练过程可视化-amp-amp-模型评价" class="headerlink" title="[√] 训练过程可视化 &amp;&amp; 模型评价"></a>[√] 训练过程可视化 &amp;&amp; 模型评价</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219175218043.png" alt="image-20221219172424150"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180020784.png" alt="image-20221219172458036"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180509425.png" alt="image-20221219172519358"></p><hr><h2 id="√-5-4-基于残差网络实现手写数字识别"><a href="#√-5-4-基于残差网络实现手写数字识别" class="headerlink" title="[√] 5.4 - 基于残差网络实现手写数字识别"></a>[√] 5.4 - 基于残差网络实现手写数字识别</h2><hr><h4 id="√-残差网络"><a href="#√-残差网络" class="headerlink" title="[√] 残差网络"></a>[√] 残差网络</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219173844842.png" alt="image-20221219173345892"></p><blockquote><p>alec：</p><p>残差网络中，将目标函数分为了两部分：恒等函数 + 残差函数</p><p>残差的思想在Gradient Boosting也有</p></blockquote><hr><h6 id="√-残差单元"><a href="#√-残差单元" class="headerlink" title="[√] 残差单元"></a>[√] 残差单元</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180824283.png" alt="image-20221219173720075"></p><blockquote><p>alec：</p><p>较深的网络中，残差单元的结构变成了中间细、两头粗的沙漏结构</p></blockquote><hr><h6 id="√-1×1卷积"><a href="#√-1×1卷积" class="headerlink" title="[√] 1×1卷积"></a>[√] 1×1卷积</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180556815.png" alt="image-20221219173844842"></p><hr><h6 id="√-ResNet18"><a href="#√-ResNet18" class="headerlink" title="[√] ResNet18"></a>[√] ResNet18</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219143721941.png" alt="image-20221219174743636"></p><blockquote><p>alec：</p><p>第一种残差单元：通道数不变</p><p>第二种残差单元：通道数翻倍（在直连边通过1×1卷积调整通道数）</p><p>即特征图的数量不变或者翻倍</p><hr><p>残差单元的结构：</p><p>卷积+BN+ReLU</p><p>卷积+BN</p><p>(残差边 + 直连边)-&gt;ReLU</p><hr><p>直连边 和 残差变 相加之后，再通过非线性激活函数</p><hr><p>BN，BatchNorm2D：</p><ul><li>根据当前批次数据按通道计算的均值和方差进行归一化</li></ul></blockquote><hr><h6 id="√-ResNet18各模块"><a href="#√-ResNet18各模块" class="headerlink" title="[√] ResNet18各模块"></a>[√] ResNet18各模块</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219145412085.png" alt="image-20221219174919075"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164430763.png" alt="image-20221219175122875"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219144138294.png" alt="image-20221219175218043"></p><hr><h6 id="√-ResNet18的参数量和计算量"><a href="#√-ResNet18的参数量和计算量" class="headerlink" title="[√] ResNet18的参数量和计算量"></a>[√] ResNet18的参数量和计算量</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219164634943.png" alt="image-20221219175400105"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219172306611.png" alt="image-20221219175423093"></p><hr><h4 id="√-模型训练-2"><a href="#√-模型训练-2" class="headerlink" title="[√] 模型训练"></a>[√] 模型训练</h4><hr><h6 id="√-没有残差连接的ResNet18"><a href="#√-没有残差连接的ResNet18" class="headerlink" title="[√] 没有残差连接的ResNet18"></a>[√] 没有残差连接的ResNet18</h6><hr><blockquote><p>alec：</p><p>self.short表示的是直连边</p><p>paddle.sumary()方法用来计算参数量</p></blockquote><hr><h4 id="√-模型评价"><a href="#√-模型评价" class="headerlink" title="[√] 模型评价"></a>[√] 模型评价</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219175400105.png" alt="image-20221219180020784"></p><hr><h2 id="√-5-5-ResNet18完成图像分类任务"><a href="#√-5-5-ResNet18完成图像分类任务" class="headerlink" title="[√] 5.5 - ResNet18完成图像分类任务"></a>[√] 5.5 - ResNet18完成图像分类任务</h2><hr><h4 id="√-数据处理"><a href="#√-数据处理" class="headerlink" title="[√] 数据处理"></a>[√] 数据处理</h4><hr><h6 id="√-数据集介绍-1"><a href="#√-数据集介绍-1" class="headerlink" title="[√] 数据集介绍"></a>[√] 数据集介绍</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219174743636.png" alt="image-20221219180356578"></p><hr><h6 id="√-数据读取"><a href="#√-数据读取" class="headerlink" title="[√] 数据读取"></a>[√] 数据读取</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180648994.png" alt="image-20221219180509425"></p><hr><h6 id="√-数据可视化-1"><a href="#√-数据可视化-1" class="headerlink" title="[√] 数据可视化"></a>[√] 数据可视化</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180902611.png" alt="image-20221219180556815"></p><hr><h6 id="√-构建Dataset类"><a href="#√-构建Dataset类" class="headerlink" title="[√] 构建Dataset类"></a>[√] 构建Dataset类</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219181000117.png" alt="image-20221219180648994"></p><hr><h4 id="√-模型构建与训练"><a href="#√-模型构建与训练" class="headerlink" title="[√] 模型构建与训练"></a>[√] 模型构建与训练</h4><hr><h6 id="√-模型构建-1"><a href="#√-模型构建-1" class="headerlink" title="[√] 模型构建"></a>[√] 模型构建</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219181022501.png" alt="image-20221219180824283"></p><hr><h6 id="√-模型训练-3"><a href="#√-模型训练-3" class="headerlink" title="[√] 模型训练"></a>[√] 模型训练</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219180356578.png" alt="image-20221219180902611"></p><hr><h4 id="√-训练结果"><a href="#√-训练结果" class="headerlink" title="[√] 训练结果"></a>[√] 训练结果</h4><hr><h6 id="√-结果可视化-amp-amp-评价-amp-amp-预测"><a href="#√-结果可视化-amp-amp-评价-amp-amp-预测" class="headerlink" title="[√] 结果可视化 &amp;&amp; 评价 &amp;&amp; 预测"></a>[√] 结果可视化 &amp;&amp; 评价 &amp;&amp; 预测</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219175423093.png" alt="image-20221219181000117"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221219175122875.png" alt="image-20221219181022501"></p><blockquote><p>alec：</p><p>weight_decay用来L2正则化</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第5章 - 卷积神经网络实践：ResNet18图像分类</title>
    <link href="/posts/3952900717/"/>
    <url>/posts/3952900717/</url>
    
    <content type="html"><![CDATA[<hr><h2 id="√-5-5-实践：基于ResNet18网络完成图像分类任务"><a href="#√-5-5-实践：基于ResNet18网络完成图像分类任务" class="headerlink" title="[√] 5.5 - 实践：基于ResNet18网络完成图像分类任务"></a>[√] 5.5 - 实践：基于ResNet18网络完成图像分类任务</h2><hr><blockquote><p>alec：</p><ul><li>很多任务可以看做是图像分类任务，比如人脸检测可以看做检测是否是人脸的二分类任务</li></ul></blockquote><p>本节使用的优化器是adam优化器。</p><p>在本实践中，我们实践一个更通用的图像分类任务。</p><p><strong>图像分类</strong>（Image Classification）是计算机视觉中的一个基础任务，将图像的语义将不同图像划分到不同类别。很多任务也可以转换为图像分类任务。比如人脸检测就是判断一个区域内是否有人脸，可以看作一个二分类的图像分类任务。</p><p>这里，我们使用的计算机视觉领域的经典数据集：CIFAR-10数据集，网络为ResNet18模型，损失函数为交叉熵损失，优化器为Adam优化器，评价指标为准确率。</p><p>Adam优化器的介绍参考《神经网络与深度学习》第7.2.4.3节。</p><hr><h4 id="√-5-5-1-数据处理"><a href="#√-5-5-1-数据处理" class="headerlink" title="[√] 5.5.1 - 数据处理"></a>[√] 5.5.1 - 数据处理</h4><hr><h6 id="√-5-5-1-1-数据集介绍"><a href="#√-5-5-1-1-数据集介绍" class="headerlink" title="[√] 5.5.1.1 - 数据集介绍"></a>[√] 5.5.1.1 - 数据集介绍</h6><hr><p>CIFAR-10数据集包含了10种不同的类别、共60,000张图像，其中每个类别的图像都是6000张，图像大小均为$32\times32$像素。CIFAR-10数据集的示例如 <strong>图5.15</strong> 所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218183851356.png" alt="image-20221218183028491"></p><p>将数据集文件进行解压：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 解压数据集</span><br><span class="hljs-comment"># 初次运行时将注释取消，以便解压文件</span><br><span class="hljs-comment"># 如果已经解压过，不需要运行此段代码，否则由于文件已经存在，解压时会报错</span><br>!mkdir /home/aistudio/datasets/<br>!tar -xvf /home/aistudio/data/data9154/cifar-<span class="hljs-number">10</span>-python.tar.gz -C /home/aistudio/datasets/<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">cifar-<span class="hljs-number">10</span>-batches-py/<br>cifar-<span class="hljs-number">10</span>-batches-py/data_batch_4<br>cifar-<span class="hljs-number">10</span>-batches-py/readme.html<br>cifar-<span class="hljs-number">10</span>-batches-py/test_batch<br>cifar-<span class="hljs-number">10</span>-batches-py/data_batch_3<br>cifar-<span class="hljs-number">10</span>-batches-py/batches.meta<br>cifar-<span class="hljs-number">10</span>-batches-py/data_batch_2<br>cifar-<span class="hljs-number">10</span>-batches-py/data_batch_5<br>cifar-<span class="hljs-number">10</span>-batches-py/data_batch_1<br></code></pre></td></tr></table></figure><hr><h6 id="√-5-5-1-2-数据读取"><a href="#√-5-5-1-2-数据读取" class="headerlink" title="[√] 5.5.1.2 - 数据读取"></a>[√] 5.5.1.2 - 数据读取</h6><hr><p>在本实验中，将原始训练集拆分成了train_set、dev_set两个部分，分别包括40 000条和10 000条样本。将data_batch_1到data_batch_4作为训练集，data_batch_5作为验证集，test_batch作为测试集。<br>最终的数据集构成为：</p><ul><li>训练集：40 000条样本。</li><li>验证集：10 000条样本。</li><li>测试集：10 000条样本。</li></ul><p>读取一个batch数据的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pickle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_cifar10_batch</span>(<span class="hljs-params">folder_path, batch_id=<span class="hljs-number">1</span>, mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>    <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>        file_path = os.path.join(folder_path, <span class="hljs-string">&#x27;test_batch&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        file_path = os.path.join(folder_path, <span class="hljs-string">&#x27;data_batch_&#x27;</span>+<span class="hljs-built_in">str</span>(batch_id))<br><br>    <span class="hljs-comment"># 加载数据集文件</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">&#x27;rb&#x27;</span>) <span class="hljs-keyword">as</span> batch_file:<br>        batch = pickle.load(batch_file, encoding = <span class="hljs-string">&#x27;latin1&#x27;</span>)<br><br>    imgs = batch[<span class="hljs-string">&#x27;data&#x27;</span>].reshape((<span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&#x27;data&#x27;</span>]),<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>)) / <span class="hljs-number">255.</span><span class="hljs-comment">#数据归一化</span><br>    labels = batch[<span class="hljs-string">&#x27;labels&#x27;</span>]<br><br>    <span class="hljs-keyword">return</span> np.array(imgs, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>), np.array(labels)<br><br>imgs_batch, labels_batch = load_cifar10_batch(folder_path=<span class="hljs-string">&#x27;datasets/cifar-10-batches-py&#x27;</span>, <br>                                                batch_id=<span class="hljs-number">1</span>, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br><br></code></pre></td></tr></table></figure><p>查看数据的维度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印一下每个batch中X和y的维度</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;batch of imgs shape: &quot;</span>,imgs_batch.shape, <span class="hljs-string">&quot;batch of labels shape: &quot;</span>, labels_batch.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">batch of imgs shape:  (<span class="hljs-number">10000</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>) batch of labels shape:  (<span class="hljs-number">10000</span>,)<br></code></pre></td></tr></table></figure><p>1万张图像，1万个分类标签，每张图像3个通道，像素为32*32</p><p>可视化观察其中的一张样本图像和对应的标签，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>image, label = imgs_batch[<span class="hljs-number">1</span>], labels_batch[<span class="hljs-number">1</span>]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The label in the picture is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label))<br>plt.figure(figsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>plt.imshow(image.transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>plt.savefig(<span class="hljs-string">&#x27;cnn-car.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218190218195.png" alt="image-20221218183851356"></p><p>类别9是卡车</p><hr><h6 id="√-5-5-1-3-构造Dataset类"><a href="#√-5-5-1-3-构造Dataset类" class="headerlink" title="[√] 5.5.1.3 - 构造Dataset类"></a>[√] 5.5.1.3 - 构造Dataset类</h6><hr><p>构造一个CIFAR10Dataset类，其将继承自<code>paddle.io.Dataset</code>类，可以逐个数据进行处理。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">from</span> paddle.vision.transforms <span class="hljs-keyword">import</span> Normalize<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CIFAR10Dataset</span>(io.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, folder_path=<span class="hljs-string">&#x27;/home/aistudio/cifar-10-batches-py&#x27;</span>, mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            <span class="hljs-comment"># 加载batch1-batch4作为训练集</span><br>            self.imgs, self.labels = load_cifar10_batch(folder_path=folder_path, batch_id=<span class="hljs-number">1</span>, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>):<br>                imgs_batch, labels_batch = load_cifar10_batch(folder_path=folder_path, batch_id=i, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>                self.imgs, self.labels = np.concatenate([self.imgs, imgs_batch]), np.concatenate([self.labels, labels_batch])<br>        <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;dev&#x27;</span>:<br>            <span class="hljs-comment"># 加载batch5作为验证集</span><br>            self.imgs, self.labels = load_cifar10_batch(folder_path=folder_path, batch_id=<span class="hljs-number">5</span>, mode=<span class="hljs-string">&#x27;dev&#x27;</span>)<br>        <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;test&#x27;</span>:<br>            <span class="hljs-comment"># 加载测试集</span><br>            self.imgs, self.labels = load_cifar10_batch(folder_path=folder_path, mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br>        <span class="hljs-comment"># 对每张图像进行normalize处理</span><br>        self.transform = Normalize(mean=[<span class="hljs-number">0.4914</span>, <span class="hljs-number">0.4822</span>, <span class="hljs-number">0.4465</span>], std=[<span class="hljs-number">0.2023</span>, <span class="hljs-number">0.1994</span>, <span class="hljs-number">0.2010</span>], data_format=<span class="hljs-string">&#x27;CHW&#x27;</span>)<br><br>    <span class="hljs-comment"># 先拿到数据x、y，然后对图像x进行处理，然后返回数据</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        img, label = self.imgs[idx], self.labels[idx]<br>        img = self.transform(img)<br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.imgs)<br><br>paddle.seed(<span class="hljs-number">100</span>)<br>train_dataset = CIFAR10Dataset(folder_path=<span class="hljs-string">&#x27;/home/aistudio/datasets/cifar-10-batches-py&#x27;</span>, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>dev_dataset = CIFAR10Dataset(folder_path=<span class="hljs-string">&#x27;/home/aistudio/datasets/cifar-10-batches-py&#x27;</span>, mode=<span class="hljs-string">&#x27;dev&#x27;</span>)<br>test_dataset = CIFAR10Dataset(folder_path=<span class="hljs-string">&#x27;/home/aistudio/datasets/cifar-10-batches-py&#x27;</span>, mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br></code></pre></td></tr></table></figure><hr><h4 id="√-5-5-2-模型构建"><a href="#√-5-5-2-模型构建" class="headerlink" title="[√] 5.5.2 - 模型构建"></a>[√] 5.5.2 - 模型构建</h4><hr><p>对于Reset18这种比较经典的图像分类网络，飞桨高层API中都为大家提供了实现好的版本，大家可以不再从头开始实现。这里首先使用飞桨高层API中的Resnet18进行图像分类实验。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.vision.models <span class="hljs-keyword">import</span> resnet18<br><br>resnet18_model = resnet18()<br></code></pre></td></tr></table></figure><hr><p>飞桨高层 API是对飞桨API的进一步封装与升级，提供了更加简洁易用的API，进一步提升了飞桨的易学易用性。其中，飞桨高层API封装了以下模块：</p><ol><li>Model类，支持仅用几行代码完成模型的训练；</li><li>图像预处理模块，包含数十种数据处理函数，基本涵盖了常用的数据处理、数据增强方法；</li><li>计算机视觉领域和自然语言处理领域的常用模型，包括但不限于mobilenet、resnet、yolov3、cyclegan、bert、transformer、seq2seq等等，同时发布了对应模型的预训练模型，可以直接使用这些模型或者在此基础上完成二次开发。</li></ol><p>飞桨高层 API主要包含在<code>paddle.vision</code>和<code>paddle.text</code>目录中。</p><hr><h4 id="√-5-5-3-模型训练"><a href="#√-5-5-3-模型训练" class="headerlink" title="[√] 5.5.3 - 模型训练"></a>[√] 5.5.3 - 模型训练</h4><hr><p>复用RunnerV3类，实例化RunnerV3类，并传入训练配置。 使用训练集和验证集进行模型训练，共训练30个epoch。 在实验中，保存准确率最高的模型作为最佳模型。代码实现如下：</p><blockquote><p>alec：</p><ul><li>什么是Adam优化器？</li><li>什么是L2正则化策略</li><li>什么是批量归一化？</li><li>评价指标函数中的<code>is_logist=True</code>参数是什么意思</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> opt<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> RunnerV3, metric<br><br><span class="hljs-comment"># 指定运行设备</span><br>use_gpu = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> paddle.get_device().startswith(<span class="hljs-string">&quot;gpu&quot;</span>) <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> use_gpu:<br>    paddle.set_device(<span class="hljs-string">&#x27;gpu:0&#x27;</span>)<br><span class="hljs-comment"># 学习率大小</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;------------------------------\n&#x27;</span>)<br><span class="hljs-built_in">print</span>(use_gpu)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;------------------------------\n&#x27;</span>)<br>lr = <span class="hljs-number">0.001</span>  <br><span class="hljs-comment"># 批次大小</span><br>batch_size = <span class="hljs-number">64</span> <br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size) <br><span class="hljs-comment"># 定义网络</span><br>model = resnet18_model<br><span class="hljs-comment"># 定义优化器，这里使用Adam优化器以及l2正则化策略，相关内容在7.3.3.2和7.6.2中会进行详细介绍</span><br><span class="hljs-comment"># 使用Adam优化器和L2正则化策略</span><br>optimizer = opt.Adam(learning_rate=lr, parameters=model.parameters(), weight_decay=<span class="hljs-number">0.005</span>)<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = F.cross_entropy<br><span class="hljs-comment"># 定义评价指标</span><br>metric = metric.Accuracy(is_logist=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 实例化RunnerV3</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">3000</span><br>eval_steps = <span class="hljs-number">3000</span><br>runner.train(train_loader, dev_loader, num_epochs=<span class="hljs-number">30</span>, log_steps=log_steps, <br>                eval_steps=eval_steps, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python">运行时长: <span class="hljs-number">7</span>分钟<span class="hljs-number">8</span>秒<span class="hljs-number">92</span>毫秒<br>结束时间: <span class="hljs-number">2022</span>-<span class="hljs-number">12</span>-<span class="hljs-number">18</span> <span class="hljs-number">18</span>:<span class="hljs-number">59</span>:<span class="hljs-number">13</span><br>------------------------------<br><br><span class="hljs-literal">True</span><br>------------------------------<br><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">7.30469</span><br>/opt/conda/envs/python35-paddle120-env/lib/python3<span class="hljs-number">.7</span>/site-packages/paddle/nn/layer/norm.py:<span class="hljs-number">653</span>: UserWarning: When training, we now always track <span class="hljs-keyword">global</span> mean <span class="hljs-keyword">and</span> variance.<br>  <span class="hljs-string">&quot;When training, we now always track global mean and variance.&quot;</span>)<br>[Train] epoch: <span class="hljs-number">4</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">3000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.92417</span><br>[Evaluate]  dev score: <span class="hljs-number">0.65710</span>, dev loss: <span class="hljs-number">0.99306</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.65710</span><br>[Train] epoch: <span class="hljs-number">9</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">6000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.59166</span><br>[Evaluate]  dev score: <span class="hljs-number">0.70810</span>, dev loss: <span class="hljs-number">0.85658</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.65710</span> --&gt; <span class="hljs-number">0.70810</span><br>[Train] epoch: <span class="hljs-number">14</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">9000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.75011</span><br>[Evaluate]  dev score: <span class="hljs-number">0.70640</span>, dev loss: <span class="hljs-number">0.87181</span><br>[Train] epoch: <span class="hljs-number">19</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">12000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.57231</span><br>[Evaluate]  dev score: <span class="hljs-number">0.72950</span>, dev loss: <span class="hljs-number">0.81165</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.70810</span> --&gt; <span class="hljs-number">0.72950</span><br>[Train] epoch: <span class="hljs-number">24</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">15000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.49599</span><br>[Evaluate]  dev score: <span class="hljs-number">0.73080</span>, dev loss: <span class="hljs-number">0.79610</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.72950</span> --&gt; <span class="hljs-number">0.73080</span><br>[Train] epoch: <span class="hljs-number">28</span>/<span class="hljs-number">30</span>, step: <span class="hljs-number">18000</span>/<span class="hljs-number">18750</span>, loss: <span class="hljs-number">0.60004</span><br>[Evaluate]  dev score: <span class="hljs-number">0.70690</span>, dev loss: <span class="hljs-number">0.88804</span><br>[Evaluate]  dev score: <span class="hljs-number">0.71990</span>, dev loss: <span class="hljs-number">0.84240</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure><p>可视化观察训练集与验证集的准确率及损失变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> plot_training_loss_acc<br><br>plot_training_loss_acc(runner, fig_name=<span class="hljs-string">&#x27;cnn-loss4.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218190000774.png" alt="image-20221218190000774"></p><p>在本实验中，使用了第7章中介绍的Adam优化器进行网络优化，如果使用SGD优化器，会造成过拟合的现象，在验证集上无法得到很好的收敛效果。可以尝试使用第7章中其他优化策略调整训练配置，达到更高的模型精度。</p><blockquote><p>alec：</p><p>此处的优化器使用了Adam优化器，没有使用SGD这个随机梯度下降优化器，因此使用SGD会造成过拟合的现象，过拟合会导致在验证集上无法收敛，效果差。过拟合即过度拟合了训练数据集，泛化性查。</p></blockquote><hr><h4 id="√-5-5-4-模型评价"><a href="#√-5-5-4-模型评价" class="headerlink" title="[√] 5.5.4 - 模型评价"></a>[√] 5.5.4 - 模型评价</h4><hr><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失情况。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] accuracy/loss: <span class="hljs-number">0.7257</span>/<span class="hljs-number">0.8251</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-5-5-5-模型预测"><a href="#√-5-5-5-模型预测" class="headerlink" title="[√] 5.5.5 - 模型预测"></a>[√] 5.5.5 - 模型预测</h4><hr><p>同样地，也可以使用保存好的模型，对测试集中的数据进行模型预测，观察模型效果，具体代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">id2label = &#123;<span class="hljs-number">0</span>:<span class="hljs-string">&#x27;airplane&#x27;</span>, <span class="hljs-number">1</span>:<span class="hljs-string">&#x27;automobile&#x27;</span>, <span class="hljs-number">2</span>:<span class="hljs-string">&#x27;bird&#x27;</span>, <span class="hljs-number">3</span>:<span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-number">4</span>:<span class="hljs-string">&#x27;deer&#x27;</span>, <span class="hljs-number">5</span>:<span class="hljs-string">&#x27;dog&#x27;</span>, <span class="hljs-number">6</span>:<span class="hljs-string">&#x27;frog&#x27;</span>, <span class="hljs-number">7</span>:<span class="hljs-string">&#x27;horse&#x27;</span>, <span class="hljs-number">8</span>:<span class="hljs-string">&#x27;ship&#x27;</span>, <span class="hljs-number">9</span>:<span class="hljs-string">&#x27;truck&#x27;</span>&#125;<br><span class="hljs-comment"># 获取测试集中的一个batch的数据</span><br>X, label_ids = <span class="hljs-built_in">next</span>(test_loader())<br>logits = runner.predict(X)<br><span class="hljs-comment"># 多分类，使用softmax计算预测概率</span><br>pred = F.softmax(logits)<br><span class="hljs-comment"># 获取概率最大的类别</span><br>pred_class_id = paddle.argmax(pred[<span class="hljs-number">2</span>]).numpy()<br>label_id = label_ids[<span class="hljs-number">2</span>][<span class="hljs-number">0</span>].numpy()<br>pred_class = id2label[pred_class_id[<span class="hljs-number">0</span>]]<br>label = id2label[label_id[<span class="hljs-number">0</span>]]<br><span class="hljs-comment"># 输出真实类别与预测类别</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label, pred_class))<br><span class="hljs-comment"># 可视化图片</span><br>plt.figure(figsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>imgs, labels = load_cifar10_batch(folder_path=<span class="hljs-string">&#x27;/home/aistudio/datasets/cifar-10-batches-py&#x27;</span>, mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br>plt.imshow(imgs[<span class="hljs-number">2</span>].transpose(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))<br>plt.savefig(<span class="hljs-string">&#x27;cnn-test-vis.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218183028491.png" alt="image-20221218190218195"></p><hr><h4 id="√-5-5-7-基于自定义的ResNet18网络进行图像分类实验"><a href="#√-5-5-7-基于自定义的ResNet18网络进行图像分类实验" class="headerlink" title="[√] 5.5.7 - 基于自定义的ResNet18网络进行图像分类实验"></a>[√] 5.5.7 - 基于自定义的ResNet18网络进行图像分类实验</h4><hr><p>这里使用自定义的 <code>Model_ResNet18</code> 模型进行图像分类实验，观察两者结果是否一致。</p><hr><h6 id="√-5-5-7-1-模型训练"><a href="#√-5-5-7-1-模型训练" class="headerlink" title="[√] 5.5.7.1 - 模型训练"></a>[√] 5.5.7.1 - 模型训练</h6><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> opt<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> RunnerV3, metric, op<br><br><span class="hljs-comment"># 指定运行设备</span><br>use_gpu = <span class="hljs-literal">True</span> <span class="hljs-keyword">if</span> paddle.get_device().startswith(<span class="hljs-string">&quot;gpu&quot;</span>) <span class="hljs-keyword">else</span> <span class="hljs-literal">False</span><br><span class="hljs-keyword">if</span> use_gpu:<br>    paddle.set_device(<span class="hljs-string">&#x27;gpu:0&#x27;</span>)<br><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.001</span>  <br><span class="hljs-comment"># 批次大小</span><br>batch_size = <span class="hljs-number">64</span> <br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size) <br><span class="hljs-comment"># 定义网络</span><br>model = op.Model_ResNet18(in_channels=<span class="hljs-number">3</span>, num_classes=<span class="hljs-number">10</span>, use_residual=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 定义优化器，这里使用Adam优化器以及l2正则化策略，相关内容在7.3.3.2和7.6.2中会进行详细介绍</span><br>optimizer = opt.Adam(learning_rate=lr, parameters=model.parameters(), weight_decay=<span class="hljs-number">0.005</span>)<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = F.cross_entropy<br><span class="hljs-comment"># 定义评价指标</span><br>metric = metric.Accuracy(is_logist=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 实例化RunnerV3</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">3000</span><br>eval_steps = <span class="hljs-number">3000</span><br>runner.train(train_loader, dev_loader, num_epochs=<span class="hljs-number">30</span>, log_steps=log_steps, <br>                eval_steps=eval_steps, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure><hr><h6 id="√-5-5-7-2-模型评价"><a href="#√-5-5-7-2-模型评价" class="headerlink" title="[√] 5.5.7.2 - 模型评价"></a>[√] 5.5.7.2 - 模型评价</h6><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><p>可以看到，使用自定义的Resnet18模型与高层API中的Resnet18模型训练效果可以基本一致。</p><hr><h2 id="√-5-6-实验拓展"><a href="#√-5-6-实验拓展" class="headerlink" title="[√] 5.6 - 实验拓展"></a>[√] 5.6 - 实验拓展</h2><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第5章 - 卷积神经网络 - 书籍</title>
    <link href="/posts/1254558254/"/>
    <url>/posts/1254558254/</url>
    
    <content type="html"><![CDATA[<h1 id="√-第5章-卷积神经网络-书籍"><a href="#√-第5章-卷积神经网络-书籍" class="headerlink" title="[√] 第5章 - 卷积神经网络 - 书籍"></a>[√] 第5章 - 卷积神经网络 - 书籍</h1><p>卷积神经网络（Convolutional Neural Network，CNN）是受生物学上<code>感受野机制的启发</code>而提出的。</p><p>目前的卷积神经网络一般是由<code>卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络</code>，有三个结构上的特性：局部连接、权重共享以及汇聚。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。</p><p>和前馈神经网络相比，卷积神经网络的<code>参数更少</code>。</p><p>卷积神经网络<code>主要应用在图像和视频分析的任务上</code>，其准确率一般也远远超出了其他的神经网络模型。近年来卷积神经网络也广泛地应用到自然语言处理、推荐系统等领域。</p><p>在学习本章内容前，建议您先阅读《神经网络与深度学习》第5章：卷积神经网络的相关内容，关键知识点如 图5.1 所示，以便更好的理解和掌握书中的理论知识在实践中的应用方法。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217201001298.png" alt="image-20221217201001298"></p><p>本实践基于 <strong>《神经网络与深度学习》第5章：卷积神经网络</strong> 相关内容进行设计，主要包含两部分：</p><ul><li><strong>模型解读</strong>：介绍卷积的原理、卷积神经网络的网络结构、残差连接的原理以及残差网络的网络结构，并使用简单卷积神经网络和残差网络，完成手写数字识别任务；</li><li><strong>案例与实践</strong>：基于残差网络ResNet18完成CIFAR-10图像分类任务。</li></ul><hr><h2 id="√-B-gt-5-1-卷积"><a href="#√-B-gt-5-1-卷积" class="headerlink" title="[√] B ~&gt; 5.1 - 卷积"></a>[√] B ~&gt; 5.1 - 卷积</h2><hr><blockquote><p>全连接网络会出现的问题：（alec）</p><ol><li><strong>模型参数过多，容易发生过拟合。</strong> 在全连接前馈网络中，隐藏层的每个神经元都要跟该层所有输入的神经元相连接。随着隐藏层神经元数量的增多，参数的规模也会急剧增加，导致整个神经网络的训练效率非常低，也很容易发生过拟合。</li><li><strong>难以提取图像中的局部不变性特征。</strong> 自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息。而全连接前馈网络很难提取这些局部不变性特征。</li></ol></blockquote><blockquote><p>卷积神经网络有三个结构上的特性：局部连接、权重共享和汇聚。这些特性使得卷积神经网络具有一定程度上的平移、缩放和旋转不变性。和前馈神经网络相比，卷积神经网络的参数也更少。因此，通常会使用卷积神经网络来处理图像信息。(alec)</p></blockquote><p><strong>卷积</strong>是分析数学中的一种重要运算，常用于信号处理或图像处理任务。本节以二维卷积为例来进行实践。</p><hr><h4 id="√-D-x3D-gt-5-1-1-二维卷积运算"><a href="#√-D-x3D-gt-5-1-1-二维卷积运算" class="headerlink" title="[√] D &#x3D;&gt; 5.1.1 - 二维卷积运算"></a>[√] D &#x3D;&gt; 5.1.1 - 二维卷积运算</h4><hr><p>在机器学习和图像处理领域，卷积的主要功能是在一个图像（或特征图）上滑动一个卷积核，通过卷积操作得到一组新的特征。</p><blockquote><p>alec：</p><p>通过卷积得到一组新的特征</p></blockquote><p>在计算卷积的过程中，需要进行卷积核的翻转，而这也会带来一些不必要的操作和开销。因此，在具体实现上，一般会以数学中的<strong>互相关</strong>（Cross-Correlatio）运算来代替卷积。 </p><blockquote><p>alec：</p><p>具体实现上，一般会以互相关操作来代替真正的卷积</p><p>卷积的主要作用是抽取特征，是否翻转并不影响特征抽取的能力</p><p>卷积核是可学习的参数</p></blockquote><p>在神经网络中，卷积运算的主要作用是抽取特征，卷积核是否进行翻转并不会影响其特征抽取的能力。特别是当卷积核是可学习的参数时，卷积和互相关在能力上是等价的。因此，很多时候，为方便起见，会直接用互相关来代替卷积。</p><hr><p><strong>说明：</strong></p><p>在本案例之后的描述中，除非特别声明，卷积一般指“互相关”。</p><hr><p>对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，它们的卷积为</p><p>$$y_{i,j}&#x3D;\sum_{u&#x3D;0}^{U-1} \sum_{v&#x3D;0}^{V-1} w_{uv}x_{i+u,j+v}。（5.1）$$</p><p><strong>图5.2</strong> 给出了卷积计算的示例。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217202933355.png" alt="image-20221217202933355"></p><p>经过卷积运算后，最终输出矩阵大小则为</p><p>$$M’ &#x3D; M - U + 1,（5.2）$$<br>$$N’ &#x3D; N - V + 1.（5.3）$$</p><blockquote><p>alec：</p><p>此处可以记忆帮助为，当卷积核大小为1x1的时候，输出的大小为M’ &#x3D; M &#x3D; M - 1 + 1，N’ &#x3D; N &#x3D; N - 1 + 1</p></blockquote><p>可以发现，使用卷积处理图像，会有以下两个特性：</p><ol><li>在卷积层(假设是第$l$层)中的每一个神经元都只和前一层(第$l-1$层)中某个局部窗口内的神经元相连，构成一个局部连接网络，这也就是卷积神经网络的<strong>局部连接</strong>特性。</li><li>由于卷积的主要功能是在一个图像（或特征图）上滑动一个卷积核，所以作为参数的卷积核$\mathbf W \in\Bbb{R}^{U\times V}$对于第$l$层的所有的神经元都是相同的，这也就是卷积神经网络的<strong>权重共享</strong>特性。</li></ol><blockquote><p>alec:</p><p>每一个神经元都只和前一层(第$l-1$层)中某个局部窗口内的神经元相连，构成一个局部连接网络</p></blockquote><hr><h4 id="√-D-x3D-gt-5-1-2-二维卷积算子"><a href="#√-D-x3D-gt-5-1-2-二维卷积算子" class="headerlink" title="[√] D &#x3D;&gt; 5.1.2 - 二维卷积算子"></a>[√] D &#x3D;&gt; 5.1.2 - 二维卷积算子</h4><hr><p>在本书后面的实现中，算子都继承<code>paddle.nn.Layer</code>，并使用支持反向传播的飞桨API进行实现，这样我们就可以不用手工写<code>backward()</code>的代码实现。</p><hr><p>根据公式（5.1），我们首先实现一个简单的二维卷积算子，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, </span><br><span class="hljs-params">                    weight_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.Assign(<span class="hljs-params">value=[[<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>],[<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>]]</span>)</span>)</span>):<br>        <span class="hljs-built_in">super</span>(Conv2D, self).__init__()<br>        <span class="hljs-comment"># 使用&#x27;paddle.create_parameter&#x27;创建卷积核</span><br>        <span class="hljs-comment"># 使用&#x27;paddle.ParamAttr&#x27;进行参数初始化</span><br>        self.weight = paddle.create_parameter(shape=[kernel_size,kernel_size],<br>                                                dtype=<span class="hljs-string">&#x27;float32&#x27;</span>,<br>                                                attr=weight_attr)<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - X：输入矩阵，shape=[B, M, N]，B为样本数量</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - output：输出矩阵</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        u, v = self.weight.shape<br>        output = paddle.zeros([X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>] - u + <span class="hljs-number">1</span>, X.shape[<span class="hljs-number">2</span>] - v + <span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output.shape[<span class="hljs-number">2</span>]):<br>                output[:, i, j] = paddle.<span class="hljs-built_in">sum</span>(X[:, i:i+u, j:j+v]*self.weight, axis=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>        <span class="hljs-keyword">return</span> output<br><br><span class="hljs-comment"># 随机构造一个二维输入矩阵</span><br>paddle.seed(<span class="hljs-number">100</span>)<br>inputs = paddle.to_tensor([[[<span class="hljs-number">1.</span>,<span class="hljs-number">2.</span>,<span class="hljs-number">3.</span>],[<span class="hljs-number">4.</span>,<span class="hljs-number">5.</span>,<span class="hljs-number">6.</span>],[<span class="hljs-number">7.</span>,<span class="hljs-number">8.</span>,<span class="hljs-number">9.</span>]]])<br><br>conv2d = Conv2D(kernel_size=<span class="hljs-number">2</span>)<br>outputs = conv2d(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input: &#123;&#125;, \noutput: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(inputs, outputs))<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span>: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],<br>         [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>],<br>         [<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">9.</span>]]]), <br>output: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">False</span>,<br>       [[[<span class="hljs-number">25.</span>, <span class="hljs-number">31.</span>],<br>         [<span class="hljs-number">43.</span>, <span class="hljs-number">49.</span>]]])<br></code></pre></td></tr></table></figure><hr><h4 id="√-D-x3D-gt-5-1-3-二维卷积的参数量和计算量"><a href="#√-D-x3D-gt-5-1-3-二维卷积的参数量和计算量" class="headerlink" title="[√] D &#x3D;&gt; 5.1.3 - 二维卷积的参数量和计算量"></a>[√] D &#x3D;&gt; 5.1.3 - 二维卷积的参数量和计算量</h4><hr><h6 id="√-F-gt-参数量"><a href="#√-F-gt-参数量" class="headerlink" title="[√] F -&gt; 参数量"></a>[√] F -&gt; 参数量</h6><hr><p>由于二维卷积的运算方式为在一个图像（或特征图）上滑动一个卷积核，通过卷积操作得到一组新的特征。所以参数量仅仅与卷积核的尺寸有关，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，卷积核的参数量为$U\times V$。</p><p>假设有一幅大小为$32\times 32$的图像，如果使用全连接前馈网络进行处理，即便第一个隐藏层神经元个数为1，此时该层的参数量也高达$1025$个，此时该层的计算过程如 <strong>图5.3</strong> 所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217204505790.png" alt="image-20221217204505790"></p><blockquote><p>alec：</p><p>使用全连接由前一层推得后一层的一个神经元，假设前一层的图像大小为32*32，而不算三个图层通道，也需要参数量为32 * 32 + 1 &#x3D; 1025个参数</p></blockquote><p>可以想像，随着隐藏层神经元数量的变多以及层数的加深，使用全连接前馈网络处理图像数据时，参数量会急剧增加。</p><p>如果使用卷积进行图像处理，当卷积核为$3\times 3$时，参数量仅为$9$，相较于全连接前馈网络，参数量少了非常多。</p><blockquote><p>alec：</p><p>卷积运算方式的形象化理解就是，首先其实二维和一维在本质上没区别，只不过是把一行改成一行一行的；其次，全连接的方式，就是相当远给上一层的每个特征点一个VIP的待遇，即32x32个像素点每个像素点都配一个权重参数，当使用卷积的时候，相当于一个固定大小的卷积核，反复的运用在一个32x32的图像上，反复利用，因此节省了参数。（为什么可以共享呢？因为图像具有平移的特征不变形等，因此可以使用共享卷积核的方式来提取特征。）</p></blockquote><hr><h6 id="√-F-gt-计算量"><a href="#√-F-gt-计算量" class="headerlink" title="[√] F -&gt; 计算量"></a>[√] F -&gt; 计算量</h6><hr><p>在卷积神经网络中运算时，通常会统计网络总的乘加运算次数作为计算量（FLOPs，floating point of operations），来衡量整个网络的运算速度。对于单个二维卷积，计算量的统计方式为：</p><blockquote><p>alec：</p><p>计算量（FLOPs，浮点运算次数）是用来衡量运算次数的，比如不同算力的显卡，FLOPs就不同。</p><p>对于输出得到的特征图，一个点是通过一次卷积运算得来的，假设卷积核大小是$U\times V$，则需要$U\times V$ 次</p><p>$M’\times N’$大小的特征图需要的运算次数为$$FLOPs&#x3D;M’\times N’\times U\times V。$$</p></blockquote><p>$$FLOPs&#x3D;M’\times N’\times U\times V。（5.4）$$</p><p>其中$M’\times N’$表示输出特征图的尺寸，即输出特征图上每个点都要与卷积核$\mathbf W \in\Bbb{R}^{U\times V}$进行$U\times V$次乘加运算。对于一幅大小为$32\times 32$的图像，使用$3\times 3$的卷积核进行运算可以得到以下的输出特征图尺寸：</p><p>$$M’ &#x3D; M - U + 1 &#x3D; 30$$<br>$$N’ &#x3D; N - V + 1 &#x3D; 30$$</p><p>此时，计算量为：</p><p>$$FLOPs&#x3D;M’\times N’\times U\times V&#x3D;30\times 30\times 3\times 3&#x3D;8100$$</p><hr><h4 id="√-D-x3D-gt-5-1-4-感受野"><a href="#√-D-x3D-gt-5-1-4-感受野" class="headerlink" title="[√] D &#x3D;&gt; 5.1.4 - 感受野"></a>[√] D &#x3D;&gt; 5.1.4 - 感受野</h4><hr><h6 id="√-F-gt-感受野定义"><a href="#√-F-gt-感受野定义" class="headerlink" title="[√] F -&gt; 感受野定义"></a>[√] F -&gt; 感受野定义</h6><hr><p>输出特征图上每个点的数值，是由输入图片上大小为$U\times V$的区域的元素与卷积核每个元素相乘再相加得到的，所以输入图像上$U\times V$区域内每个元素数值的改变，都会影响输出点的像素值。我们将这个区域叫做输出特征图上对应点的感受野。</p><p>感受野内每个元素数值的变动，都会影响输出点的数值变化。比如$3\times3$卷积对应的感受野大小就是$3\times3$，如 <strong>图5.4</strong> 所示。</p><blockquote><p>alec：</p><p>输出特征图上的一个点就代表一个神经元计算得来的。有多少个特征点，就相当于有多少个神经元。神经元从前面一层收集信息，然后当兴奋达到阈值的时候，触发冲动，通过激活函数，得到一个值往后传。</p><p>对于全连接来说，该输出值是通过前面一层的全部的像素点计算得来的，对于卷积来说，该特征点，只是由卷积核收集的卷积核当前位置对应的感受野内的像素点的信息计算得来的。</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217212520143.png" alt="image-20221217205612951"></p><p>而当通过两层$3\times3$的卷积之后，感受野的大小将会增加到$5\times5$，如 <strong>图5.5</strong> 所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217211039853.png" alt="image-20221217205629584"></p><p>因此，当增加卷积网络深度的同时，感受野将会增大，输出特征图中的一个像素点将会<code>包含更多的图像语义信息</code>。</p><blockquote><p>alec：</p><p>随着层数的加深，深层特征图上的一个像素点，层层递进之后，这个像素点能够感受到输入图片的一个很大的区域。即感受野大。</p><p>因此这个深层像素点含有的是更高维的特征，而浅层像素点由于感受野小，只能含有低维的特征。</p><p>感受野可以理解为每层特征上的一个点在输入层图像上能够感受多大范围的区域。</p><p>浅层的特征图上的像素点感受的小感受野，随着层层递进，深层的一个像素点能够感受一个大范围内的信息。</p></blockquote><hr><h4 id="√-D-x3D-gt-5-1-5-卷积的变种"><a href="#√-D-x3D-gt-5-1-5-卷积的变种" class="headerlink" title="[√] D &#x3D;&gt; 5.1.5 - 卷积的变种"></a>[√] D &#x3D;&gt; 5.1.5 - 卷积的变种</h4><hr><p>在卷积的标准定义基础上，还可以引入卷积核的滑动步长和零填充来增加卷积的多样性，从而更灵活地进行特征抽取。</p><hr><h6 id="√-F-gt-5-1-5-1-步长"><a href="#√-F-gt-5-1-5-1-步长" class="headerlink" title="[√] F -&gt; 5.1.5.1 - 步长"></a>[√] F -&gt; 5.1.5.1 - 步长</h6><hr><p>在卷积运算的过程中，有时会希望跳过一些位置来降低计算的开销，也可以把这一过程看作是对标准卷积运算输出的<strong>下采样</strong>。</p><p>在计算卷积时，可以在所有维度上每间隔$S$个元素计算一次，$S$称为卷积运算的<strong>步长</strong>（Stride），也就是卷积核在滑动时的间隔。</p><blockquote><p>alec：</p><p>我们在一幅图片缩小之后，仍然知道这个图片想要表达的信息，这种现象可以看出图像的像素信息具有冗余性。</p><p>因此为了减少计算的开销，可以在卷积计算的过程中，对于被卷积核计算的特征图像，可以跳着卷积，以降低计算开销。</p><p>在所有维度上每间隔$S$个元素计算一次，$S$称为卷积运算的<strong>步长</strong>（Stride），也就是卷积核在滑动时的间隔。</p><p>这种也可以看做是对标准卷积的一个下采样。</p><hr><p>在卷积的时候，通过步长来进行卷积是一种下采样操作；</p><p>然后通过池化来精炼信息，也可以看做是一种下采样操作；</p><p>下采样的目的都是精简、提取主要的信息，去除冗余性。</p></blockquote><p>此时，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，它们的卷积为</p><p>$$y_{i,j}&#x3D;\sum_{u&#x3D;0}^{U-1} \sum_{v&#x3D;0}^{V-1} w_{uv}x_{i\times S+u,j\times S+v}，（5.5）$$</p><p>在二维卷积运算中，当步长$S&#x3D;2$时，计算过程如 <strong>图5.6</strong> 所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217221144575.png" alt="image-20221217211039853"></p><blockquote><p>alec：</p><p>带步长的卷积计算公式，可以通过上面这个图像为8x8，步长为2，卷积核大小为2x2的来记忆。</p></blockquote><hr><h6 id="√-F-gt-5-1-5-2-零填充（Zero-Padding）"><a href="#√-F-gt-5-1-5-2-零填充（Zero-Padding）" class="headerlink" title="[√] F -&gt; 5.1.5.2 - 零填充（Zero Padding）"></a>[√] F -&gt; 5.1.5.2 - 零填充（Zero Padding）</h6><hr><p>在卷积运算中，还可以对输入用零进行填充使得其尺寸变大。根据卷积的定义，如果不进行填充，当卷积核尺寸大于1时，输出特征会缩减。对输入进行零填充则可以对卷积核的宽度和输出的大小进行独立的控制。</p><blockquote><p>alec：</p><p>通过零填充来控制输出特征图的大小。</p></blockquote><p>在二维卷积运算中，<strong>零填充</strong>（Zero Padding）是指在输入矩阵周围对称地补上$P$个$0$。</p><p><strong>图5.7</strong> 为使用零填充的示例。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217205612951.png" alt="image-20221217212520143"></p><p>对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个滤波器$\mathbf W \in\Bbb{R}^{U\times V}$，，步长为$S$，对输入矩阵进行零填充，那么最终输出矩阵大小则为</p><p>$$M’ &#x3D; \frac{M + 2P - U}{S} + 1,（5.6）$$<br>$$N’ &#x3D; \frac{N + 2P - V}{S} + 1.（5.7）$$</p><blockquote><p>alec：</p><p>输出矩阵的大小，</p><p>M’ &#x3D; （M + 2P - U） &#x2F; S + 1。M是原图宽，P是填充边宽，U是卷积核大小，S是步长，+1。</p></blockquote><p>引入步长和零填充后的卷积，参数量和计算量的统计方式与之前一致，参数量与卷积核的尺寸有关，为：$U\times V$，计算量与输出特征图和卷积核的尺寸有关，为：</p><p>$$FLOPs&#x3D;M’\times N’\times U\times V&#x3D;(\frac{M + 2P - U}{S} + 1)\times (\frac{N + 2P - V}{S} + 1)\times U\times V。（5.8）$$</p><p>一般常用的卷积有以下三类：</p><ol><li><strong>窄卷积</strong>：步长$S&#x3D;1$，两端不补零$P&#x3D;0$，卷积后输出尺寸为：</li></ol><p>$$M’ &#x3D; M - U + 1,（5.9）$$<br>$$N’ &#x3D; N - V + 1.（5.10）$$</p><ol start="2"><li><strong>宽卷积</strong>：步长$S&#x3D;1$，两端补零$P&#x3D;U-1&#x3D;V-1$，卷积后输出尺寸为：</li></ol><p>$$M’ &#x3D; M + U - 1,（5.11）$$<br>$$N’ &#x3D; N + V - 1.（5.12）$$</p><ol start="3"><li><strong>等宽卷积</strong>：步长$S&#x3D;1$，两端补零$P&#x3D;\frac{(U-1)}{2}&#x3D;\frac{(V-1)}{2}$，卷积后输出尺寸为：</li></ol><p>$$M’ &#x3D; M,（5.13）$$<br>$$N’ &#x3D; N.（5.14）$$</p><blockquote><p>alec：</p><p>在步长为1的时候，根据填充的长短，分为窄卷积（不填充）、等宽卷积、宽卷积。</p></blockquote><p>通常情况下，在层数较深的卷积神经网络，比如：VGG、ResNet中，会使用等宽卷积保证输出特征图的大小不会随着层数的变深而快速缩减。例如：当卷积核的大小为$3\times 3$时，会将步长设置为$S&#x3D;1$，两端补零$P&#x3D;1$，此时，卷积后的输出尺寸就可以保持不变。在本章后续的案例中，会使用ResNet进行实验。</p><blockquote><p>alec：</p><p>卷积神经网络的层数很深的时候，如果使用窄卷积，那么随着层数的加深，特征图会越来越小，变得很小。因此一般层数深的网络，会使用等宽卷积以保证特征图大小不变。等宽卷积，2P - U + 1 &#x3D; 0，即填充和损耗相抵消了。P &#x3D; （U-1）&#x2F;2。</p><p>比如常见的3x3的卷积核，等宽卷积，则填充为P &#x3D; (U-1)&#x2F;2 &#x3D; 1。</p></blockquote><blockquote><p>alec：</p><p>层数较深的卷积神经网络，比如VGG、ResNet</p></blockquote><hr><h4 id="√-D-x3D-gt-5-1-6-带步长和零填充的二维卷积算子"><a href="#√-D-x3D-gt-5-1-6-带步长和零填充的二维卷积算子" class="headerlink" title="[√] D &#x3D;&gt; 5.1.6 - 带步长和零填充的二维卷积算子"></a>[√] D &#x3D;&gt; 5.1.6 - 带步长和零填充的二维卷积算子</h4><hr><p>引入步长和零填充后，二维卷积算子代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, </span><br><span class="hljs-params">                    weight_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.Constant(<span class="hljs-params">value=<span class="hljs-number">1.0</span></span>)</span>)</span>):<br>        <span class="hljs-built_in">super</span>(Conv2D, self).__init__()<br>        self.weight = paddle.create_parameter(shape=[kernel_size,kernel_size], <br>                                                dtype=<span class="hljs-string">&#x27;float32&#x27;</span>, <br>                                                attr=weight_attr)<br>        <span class="hljs-comment"># 步长</span><br>        self.stride = stride<br>        <span class="hljs-comment"># 零填充</span><br>        self.padding = padding<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 零填充</span><br>        new_X = paddle.zeros([X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>*self.padding, X.shape[<span class="hljs-number">2</span>]+<span class="hljs-number">2</span>*self.padding])<br>        new_X[:, self.padding:X.shape[<span class="hljs-number">1</span>]+self.padding, self.padding:X.shape[<span class="hljs-number">2</span>]+self.padding] = X<br>        u, v = self.weight.shape<br>        <span class="hljs-comment"># 零填充之后，计算输出特征图的shape</span><br>        output_w = (new_X.shape[<span class="hljs-number">1</span>] - u) // self.stride + <span class="hljs-number">1</span><br>        output_h = (new_X.shape[<span class="hljs-number">2</span>] - v) // self.stride + <span class="hljs-number">1</span><br>        output = paddle.zeros([X.shape[<span class="hljs-number">0</span>], output_w, output_h])<br>        <span class="hljs-comment"># 遍历计算输出特征图</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, output.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, output.shape[<span class="hljs-number">2</span>]):<br>                output[:, i, j] = paddle.<span class="hljs-built_in">sum</span>(<br>                    new_X[:, self.stride*i:self.stride*i+u, self.stride*j:self.stride*j+v]*self.weight,<br>                    axis=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>        <span class="hljs-keyword">return</span> output<br><br>inputs = paddle.randn(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>])<br>conv2d_padding = Conv2D(kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>outputs = conv2d_padding(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;When kernel_size=3, padding=1 stride=1, input&#x27;s shape: &#123;&#125;, output&#x27;s shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(inputs.shape, outputs.shape))<br>conv2d_stride = Conv2D(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>outputs = conv2d_stride(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;When kernel_size=3, padding=1 stride=2, input&#x27;s shape: &#123;&#125;, output&#x27;s shape: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(inputs.shape, outputs.shape))<br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">When <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1 <span class="hljs-attribute">stride</span>=1, input<span class="hljs-string">&#x27;s shape: [2, 8, 8], output&#x27;</span>s shape: [2, 8, 8]<br>When <span class="hljs-attribute">kernel_size</span>=3, <span class="hljs-attribute">padding</span>=1 <span class="hljs-attribute">stride</span>=2, input<span class="hljs-string">&#x27;s shape: [2, 8, 8], output&#x27;</span>s shape: [2, 4, 4]<br></code></pre></td></tr></table></figure><p>从输出结果看出，使用$3\times3$大小卷积，<code>padding</code>为1，当<code>stride</code>&#x3D;1时，模型的输出特征图可以与输入特征图保持一致；当<code>stride</code>&#x3D;2时，输出特征图的宽和高都缩小一倍。</p><hr><h4 id="√-D-x3D-gt-5-1-7-使用卷积运算完成图像边缘检测任务"><a href="#√-D-x3D-gt-5-1-7-使用卷积运算完成图像边缘检测任务" class="headerlink" title="[√] D &#x3D;&gt; 5.1.7 - 使用卷积运算完成图像边缘检测任务"></a>[√] D &#x3D;&gt; 5.1.7 - 使用卷积运算完成图像边缘检测任务</h4><hr><blockquote><p>alec：</p><p>拉布拉斯算子常用语图像边缘检测提取</p></blockquote><p>在图像处理任务中，常用<strong>拉普拉斯算子</strong>对物体边缘进行提取，拉普拉斯算子为一个大小为$3 \times 3$的卷积核，中心元素值是$8$，其余元素值是$-1$。</p><p>考虑到边缘其实就是图像上像素值变化很大的点的集合，因此可以通过计算二阶微分得到，当二阶微分为0时，像素值的变化最大。此时，对$x$方向和$y$方向分别求取二阶导数：</p><p>$$\frac{\delta^2 I}{\delta x^2} &#x3D; I(i, j+1) - 2I(i,j) + I(i,j-1),（5.15）$$<br>$$\frac{\delta^2 I}{\delta y^2} &#x3D; I(i+1, j) - 2I(i,j) + I(i-1,j).（5.16）$$</p><p>完整的二阶微分公式为：</p><p>$$\nabla^2I &#x3D; \frac{\delta^2 I}{\delta x^2} + \frac{\delta^2 I}{\delta y^2} &#x3D;  - 4I(i,j) + I(i,j-1) + I(i, j+1) + I(i+1, j) + I(i-1,j),（5.17）$$</p><p>上述公式也被称为<strong>拉普拉斯算子</strong>，对应的二阶微分卷积核为：</p><p>$$\begin{bmatrix}<br>0      &amp; 1   &amp;  0    \<br>1      &amp; -4 &amp;  1 \<br>0      &amp;  1 &amp;  0 \<br>\end{bmatrix}$$</p><p>对上述算子全部求反也可以起到相同的作用，此时，该算子可以表示为：</p><p>$$\begin{bmatrix}<br>0      &amp; -1   &amp;  0    \<br>-1      &amp; 4 &amp;  -1 \<br>0      &amp;  -1 &amp;  0 \<br>\end{bmatrix}$$</p><p>也就是一个点的四邻域拉普拉斯的算子计算结果是自己像素值的四倍减去上下左右的像素的和，将这个算子旋转$45°$后与原算子相加，就变成八邻域的拉普拉斯算子，也就是一个像素自己值的八倍减去周围一圈八个像素值的和，做为拉普拉斯计算结果，此时，该算子可以表示为：</p><p>$$\begin{bmatrix}<br>-1      &amp; -1   &amp;  -1    \<br>-1      &amp; 8 &amp;  -1 \<br>-1      &amp;  -1 &amp;  -1 \<br>\end{bmatrix}$$</p><hr><p>下面我们利用上面定义的<code>Conv2D</code>算子，构造一个简单的拉普拉斯算子，并对一张输入的灰度图片进行边缘检测，提取出目标的外形轮廓。</p><blockquote><p>alec：</p><p>对拉普拉斯算子来说，如果覆盖的像素全部一样，那么最终的卷积结果就是0.检测不到东西。如果此处是高频信息，那么这个算子就能检测到东西。拉普拉斯算子卷积核所有的参数权重之和为0，因此可以检测出不均衡的高频信息。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 读取图片</span><br>img = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./number.jpg&#x27;</span>).resize((<span class="hljs-number">256</span>,<span class="hljs-number">256</span>))<br><br><span class="hljs-comment"># 设置卷积核参数</span><br>w = np.array([[-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>,<span class="hljs-number">8</span>,-<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>,-<span class="hljs-number">1</span>]], dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-comment"># 创建卷积算子，卷积核大小为3x3，并使用上面的设置好的数值作为卷积核权重的初始化参数</span><br>conv = Conv2D(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>, <br>                weight_attr=paddle.ParamAttr(initializer=nn.initializer.Assign(value=w)))<br><br><span class="hljs-comment"># 将读入的图片转化为float32类型的numpy.ndarray</span><br>inputs = np.array(img).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;bf to_tensor, inputs:&quot;</span>,inputs)<br><span class="hljs-comment"># 将图片转为Tensor</span><br>inputs = paddle.to_tensor(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;bf unsqueeze, inputs:&quot;</span>,inputs)<br>inputs = paddle.unsqueeze(inputs, axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;af unsqueeze, inputs:&quot;</span>,inputs)<br>outputs = conv(inputs)<br>outputs = outputs.numpy()<br><span class="hljs-comment"># 可视化结果</span><br>plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">4</span>))<br>f = plt.subplot(<span class="hljs-number">121</span>)<br>f.set_title(<span class="hljs-string">&#x27;input image&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br>plt.imshow(img)<br>f = plt.subplot(<span class="hljs-number">122</span>)<br>f.set_title(<span class="hljs-string">&#x27;output feature map&#x27;</span>, fontsize=<span class="hljs-number">15</span>)<br>plt.imshow(outputs.squeeze(), cmap=<span class="hljs-string">&#x27;gray&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;conv-vis.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217205629584.png" alt="image-20221217220900496"></p><p>从输出结果看，使用拉普拉斯算子，目标的边缘可以成功被检测出来。</p><hr><h2 id="√-B-gt-5-2-卷积神经网络的基础算子"><a href="#√-B-gt-5-2-卷积神经网络的基础算子" class="headerlink" title="[√] B ~&gt; 5.2 - 卷积神经网络的基础算子"></a>[√] B ~&gt; 5.2 - 卷积神经网络的基础算子</h2><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217220900496.png" alt="image-20221217221144575"></p><p>从上图可以看出，卷积网络是由多个基础的算子组合而成。下面我们先实现卷积网络的两个基础算子：卷积层算子和汇聚层算子。</p><hr><h4 id="√-D-x3D-gt-5-2-1-卷积算子"><a href="#√-D-x3D-gt-5-2-1-卷积算子" class="headerlink" title="[√] D &#x3D;&gt; 5.2.1 - 卷积算子"></a>[√] D &#x3D;&gt; 5.2.1 - 卷积算子</h4><hr><p>卷积层是指用卷积操作来实现神经网络中一层。为了提取不同种类的特征，通常会使用多个卷积核一起进行特征提取。</p><blockquote><p>alec：</p><p>为了提取不同种类的特征，通常会使用多个卷积核一起进行特征提取。</p></blockquote><hr><h6 id="√-F-gt-5-2-1-1-多通道卷积"><a href="#√-F-gt-5-2-1-1-多通道卷积" class="headerlink" title="[√] F -&gt; 5.2.1.1 多通道卷积"></a>[√] F -&gt; 5.2.1.1 多通道卷积</h6><hr><p>在前面介绍的二维卷积运算中，卷积的输入数据是二维矩阵。但实际应用中，一幅大小为$M\times N$的图片中的每个像素的特征表示不仅仅只有灰度值的标量，通常有多个特征，可以表示为$D$维的向量，比如RGB三个通道的特征向量。因此，图像上的卷积操作的输入数据通常是一个三维张量，分别对应了图片的高度$M$、宽度$N$和深度$D$，其中深度$D$通常也被称为<strong>输入通道数</strong>$D$。如果输入如果是灰度图像，则输入通道数为1；如果输入是彩色图像，分别有$R、G、B$三个通道，则输入通道数为3。</p><p>此外，由于具有单个核的卷积每次只能提取一种类型的特征，即输出一张大小为$U\times V$的<strong>特征图</strong>（Feature Map）。而在实际应用中，我们也希望每一个卷积层能够提取多种不同类型的特征，所以一个卷积层通常会组合多个不同的卷积核来提取特征，经过卷积运算后会输出多张特征图，不同的特征图对应不同类型的特征。输出特征图的个数通常将其称为<strong>输出通道数</strong>$P$。</p><blockquote><p>alec：</p><p>（1）输入图像的特征通道数，比如RGB图像的三层图像通道，称为<code>输入通道数D</code></p><p>（2）一个卷积核只能提取一种类型的特征信息，因此我们在一个卷积层中，组合<code>多个不同的卷积核</code>，这样可以提取多张特征图，在一层卷积层中，放置P个卷积核，能够提取P种特征，将这P个输出特征图的个数称为<code>输出通道数P</code></p><p>（3）举例：RGB图像，3层通道，3x128x128，使用16个大小为3x3的卷积核，进行等宽卷积，得到16x128x128，其中每张128x128都是一类特定于卷积核的特征。</p></blockquote><hr><p><strong>说明：</strong></p><p>《神经网络与深度学习》将Feature Map翻译为“特征映射”，这里翻译为“特征图”。</p><hr><p>假设一个卷积层的输入特征图$\mathbf X\in \mathbb{R}^{D\times M\times N}$，其中$(M,N)$为特征图的尺寸，$D$代表通道数；卷积核为$\mathbf W\in \mathbb{R}^{P\times D\times U\times V}$，其中$(U,V)$为卷积核的尺寸，$D$代表输入通道数，$P$代表输出通道数。</p><hr><p><strong>说明：</strong></p><p>在实践中，根据目前深度学习框架中张量的组织和运算性质，这里特征图的大小为$D\times M\times N$，和《神经网络与深度学习》中$M\times N \times D$的定义并不一致。<br>相应地，卷积核$W$的大小为$\mathbb{R}^{P\times D\times U\times V}$。</p><hr><h6 id="√-F-gt-一张输出特征图的计算"><a href="#√-F-gt-一张输出特征图的计算" class="headerlink" title="[√] F -&gt; 一张输出特征图的计算"></a>[√] F -&gt; 一张输出特征图的计算</h6><hr><p>对于$D$个输入通道，分别对每个通道的特征图$\mathbf X^d$设计一个二维卷积核$\mathbf W^{p,d}$，并与对应的输入特征图$\mathbf X^d$进行卷积运算，再将得到的$D$个结果进行加和，得到一张输出特征图$\mathbf Z^p$。计算方式如下：</p><p>$$<br>\mathbf Z^p &#x3D; \sum_{d&#x3D;1}^D \mathbf W^{p,d} \otimes \mathbf X^d + b^p，（5.18）<br>$$</p><p>$$<br>\mathbf Y^p &#x3D; f(\mathbf Z^p)。（5.19）<br>$$</p><p>其中$p$表示输出特征图的索引编号，$\mathbf W^{p,d} \in \mathbb{R}^{U\times V}$为二维卷积核，$b^p$为标量偏置，$f(·)$为非线性激活函数，一般用ReLU函数。</p><blockquote><p>说明：</p><p>在代码实现时，通常将非线性激活函数放在卷积层算子外部。</p></blockquote><p>公式（5.13）对应的可视化如<strong>图5.9</strong>所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217224049920.png" alt="image-20221217223939981"></p><hr><h6 id="√-F-gt-多张输出特征图的计算"><a href="#√-F-gt-多张输出特征图的计算" class="headerlink" title="[√] F -&gt; 多张输出特征图的计算"></a>[√] F -&gt; 多张输出特征图的计算</h6><hr><p>对于大小为$D\times M\times N$的输入特征图，每一个输出特征图都需要一组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算。使用$P$组卷积核分布进行卷积运算，得到$P$个输出特征图$\mathbf Y^1, \mathbf Y^2,\cdots,\mathbf Y^P$。然后将$P$个输出特征图进行拼接，获得大小为$P\times M’ \times N’$的多通道输出特征图。上面计算方式的可视化如下<strong>图5.10</strong>所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217223939981.png" alt="image-20221217224049920"></p><hr><h6 id="√-F-gt-5-2-1-2-多通道卷积层算子"><a href="#√-F-gt-5-2-1-2-多通道卷积层算子" class="headerlink" title="[√] F -&gt; 5.2.1.2 - 多通道卷积层算子"></a>[√] F -&gt; 5.2.1.2 - 多通道卷积层算子</h6><hr><p>根据上面的公式，多通道卷积卷积层的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Conv2D</span>(nn.Layer):<br>    <span class="hljs-comment"># in_channels 是上一层特征的通道数</span><br>    <span class="hljs-comment"># out_channels 是本层卷积层卷积之后得到的特征图的通道数（个数），即本层有out_channels组卷积核，得到out_channels个特征图</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, kernel_size, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">    <span class="hljs-comment"># 权重初始化1，偏置初始化为0</span></span><br><span class="hljs-params">                    weight_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.Constant(<span class="hljs-params">value=<span class="hljs-number">1.0</span></span>)</span>),</span><br><span class="hljs-params">                    bias_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.Constant(<span class="hljs-params">value=<span class="hljs-number">0.0</span></span>)</span>)</span>):<br>        <span class="hljs-built_in">super</span>(Conv2D, self).__init__()<br>        <span class="hljs-comment"># 创建卷积核</span><br>        <span class="hljs-comment"># 卷积核shape=[out_channels, in_channels, kernel_size,kernel_size]</span><br>        <span class="hljs-comment"># 有out_channels组卷积核，每组卷积核有in_channels个通道，每个卷积核大小为kernel_size,kernel_size</span><br>        self.weight = paddle.create_parameter(shape=[out_channels, in_channels, kernel_size,kernel_size],<br>                                                dtype=<span class="hljs-string">&#x27;float32&#x27;</span>,<br>                                                attr=weight_attr)<br>        <span class="hljs-comment"># 创建偏置</span><br>        self.bias = paddle.create_parameter(shape=[out_channels, <span class="hljs-number">1</span>],<br>                                                dtype=<span class="hljs-string">&#x27;float32&#x27;</span>,<br>                                                attr=bias_attr)<br>        self.stride = stride<br>        self.padding = padding<br>        <span class="hljs-comment"># 输入通道数 # 输入通道数为上一层特征图的通道数</span><br>        self.in_channels = in_channels<br>        <span class="hljs-comment"># 输出通道数 # 输入通道数为本层卷积核的个数</span><br>        self.out_channels = out_channels<br><br>    <span class="hljs-comment"># 基础卷积运算</span><br>    <span class="hljs-comment"># 单组卷积运算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">single_forward</span>(<span class="hljs-params">self, X, weight</span>):<br>        <span class="hljs-comment"># 零填充</span><br>        new_X = paddle.zeros([X.shape[<span class="hljs-number">0</span>], X.shape[<span class="hljs-number">1</span>]+<span class="hljs-number">2</span>*self.padding, X.shape[<span class="hljs-number">2</span>]+<span class="hljs-number">2</span>*self.padding])<br>        new_X[:, self.padding:X.shape[<span class="hljs-number">1</span>]+self.padding, self.padding:X.shape[<span class="hljs-number">2</span>]+self.padding] = X<br>        u, v = weight.shape<br>        output_w = (new_X.shape[<span class="hljs-number">1</span>] - u) // self.stride + <span class="hljs-number">1</span><br>        output_h = (new_X.shape[<span class="hljs-number">2</span>] - v) // self.stride + <span class="hljs-number">1</span><br>        output = paddle.zeros([X.shape[<span class="hljs-number">0</span>], output_w, output_h])<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, output.shape[<span class="hljs-number">1</span>]):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, output.shape[<span class="hljs-number">2</span>]):<br>                output[:, i, j] = paddle.<span class="hljs-built_in">sum</span>(<br>                    new_X[:, self.stride*i:self.stride*i+u, self.stride*j:self.stride*j+v]*weight, <br>                    axis=[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>])<br>        <span class="hljs-keyword">return</span> output<br><br>    <span class="hljs-comment">#out_channels组卷积核运算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - B张图，每张图D个通道，长宽为M、N</span><br><span class="hljs-string">            - inputs：输入矩阵，shape=[B, D, M, N]</span><br><span class="hljs-string">            - P组，d个通道，长宽为UV</span><br><span class="hljs-string">            - weights：P组二维卷积核，shape=[P, D, U, V]</span><br><span class="hljs-string">            - bias：P个偏置，shape=[P, 1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        feature_maps = []<br>        <span class="hljs-comment"># 进行多次多输入通道卷积运算</span><br>        p=<span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> w, b <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.weight, self.bias): <span class="hljs-comment"># P个(w,b),每次计算一个特征图Zp # 遍历P个卷积核，每个卷积核都对这些图片进行卷积特征提取</span><br>            multi_outs = []<br>            <span class="hljs-comment"># 循环计算每个输入特征图对应的卷积结果</span><br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.in_channels): <span class="hljs-comment"># 遍历B张图</span><br>                single = self.single_forward(inputs[:,i,:,:], w[i])<br>                multi_outs.append(single)<br>                <span class="hljs-comment"># print(&quot;Conv2D in_channels:&quot;,self.in_channels,&quot;i:&quot;,i,&quot;single:&quot;,single.shape)</span><br>            <span class="hljs-comment"># 将所有卷积结果相加</span><br>            feature_map = paddle.<span class="hljs-built_in">sum</span>(paddle.stack(multi_outs), axis=<span class="hljs-number">0</span>) + b <span class="hljs-comment">#Zp</span><br>            feature_maps.append(feature_map)<br>            <span class="hljs-comment"># print(&quot;Conv2D out_channels:&quot;,self.out_channels, &quot;p:&quot;,p,&quot;feature_map:&quot;,feature_map.shape)</span><br>            p+=<span class="hljs-number">1</span><br>        <span class="hljs-comment"># 将所有Zp进行堆叠</span><br>        out = paddle.stack(feature_maps, <span class="hljs-number">1</span>) <br>        <span class="hljs-keyword">return</span> out<br><br><span class="hljs-comment">#1张图，2个通道，长宽3x3</span><br>inputs = paddle.to_tensor([[[[<span class="hljs-number">0.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>], [<span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>], [<span class="hljs-number">6.0</span>, <span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>]],<br>               [[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>], [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>], [<span class="hljs-number">7.0</span>, <span class="hljs-number">8.0</span>, <span class="hljs-number">9.0</span>]]]])<br>conv2d = Conv2D(in_channels=<span class="hljs-number">2</span>, out_channels=<span class="hljs-number">3</span>, kernel_size=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;inputs shape:&quot;</span>,inputs.shape)<span class="hljs-comment">#1 * 2 * 3 * 3</span><br>outputs = conv2d(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Conv2D outputs shape:&quot;</span>,outputs.shape)<span class="hljs-comment"># 1 * 3 * 2 * 2</span><br><br><span class="hljs-comment"># 比较与paddle API运算结果</span><br>conv2d_paddle = nn.Conv2D(in_channels=<span class="hljs-number">2</span>, out_channels=<span class="hljs-number">3</span>, kernel_size=<span class="hljs-number">2</span>,<br>                            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">1.0</span>)),<br>                            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">0.0</span>)))<br>outputs_paddle = conv2d_paddle(inputs)<br><span class="hljs-comment"># 自定义算子运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Conv2D outputs:&#x27;</span>, outputs)<br><span class="hljs-comment"># paddle API运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;nn.Conv2D outputs:&#x27;</span>, outputs_paddle)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">inputs shape: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>]<br>Conv2D outputs shape: [<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>Conv2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">False</span>,<br>       [[[[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]],<br><br>         [[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]],<br><br>         [[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]]]])<br>nn.Conv2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">False</span>,<br>       [[[[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]],<br><br>         [[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]],<br><br>         [[<span class="hljs-number">20.</span>, <span class="hljs-number">28.</span>],<br>          [<span class="hljs-number">44.</span>, <span class="hljs-number">52.</span>]]]])<br></code></pre></td></tr></table></figure><hr><h6 id="√-F-gt-5-2-1-3-卷积算子的参数量和计算量"><a href="#√-F-gt-5-2-1-3-卷积算子的参数量和计算量" class="headerlink" title="[√] F -&gt; 5.2.1.3 - 卷积算子的参数量和计算量"></a>[√] F -&gt; 5.2.1.3 - 卷积算子的参数量和计算量</h6><hr><p><strong>参数量</strong></p><p>对于大小为$D\times M\times N$的输入特征图，使用$P$组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算，参数量计算方式为：<br>$$<br>parameters &#x3D; P \times D \times U \times V + P.（5.20）<br>$$</p><p>其中，最后的$P$代表偏置个数。例如：输入特征图大小为$3\times 32\times 32$，使用$6$组大小为$3\times 3\times 3$的卷积核进行卷积运算，参数量为：<br>$$<br>parameters &#x3D; 6 \times 3 \times 3 \times 3 + 6&#x3D; 168.<br>$$</p><p><strong>计算量</strong></p><p>对于大小为$D\times M\times N$的输入特征图，使用$P$组大小为$\mathbf W\in \mathbb{R}^{D\times U\times V}$的卷积核进行卷积运算，计算量计算方式为：</p><p>$$FLOPs&#x3D;M’\times N’\times P\times D\times U\times V + M’\times N’\times P。（5.21）$$</p><p>其中$M’\times N’\times P$代表加偏置的计算量，即输出特征图上每个点都要与$P$组卷积核$\mathbf W\in \mathbb{R}^{D\times U\times V}$进行$U\times V\times D$次乘法运算后再加上偏置。比如对于输入特征图大小为$3\times 32\times 32$，使用$6$组大小为$3\times 3\times 3$的卷积核进行卷积运算，计算量为：</p><p>$$FLOPs&#x3D;M’\times N’\times P\times D\times U\times V + M’\times N’\times P&#x3D; 30\times 30\times 3\times 3\times 6\times 3 + 30\times 30\times 6&#x3D; 151200$$</p><hr><h4 id="√-D-x3D-gt-5-2-2-汇聚层算子"><a href="#√-D-x3D-gt-5-2-2-汇聚层算子" class="headerlink" title="[√] D &#x3D;&gt; 5.2.2 - 汇聚层算子"></a>[√] D &#x3D;&gt; 5.2.2 - 汇聚层算子</h4><hr><blockquote><p>alec：</p><p><strong>汇聚层</strong>的作用是进行特征选择，降低特征数量，从而减少参数数量。</p><p>汇聚之后特征图会变小，提取出精简主要的信息。</p></blockquote><p><strong>汇聚层</strong>的作用是进行特征选择，降低特征数量，从而减少参数数量。由于汇聚之后特征图会变得更小，如果后面连接的是全连接层，可以有效地减小神经元的个数，节省存储空间并提高计算效率。</p><p>常用的汇聚方法有两种，分别是：平均汇聚和最大汇聚。（均匀池化和最大池化）</p><ul><li>平均汇聚：将输入特征图划分为$2\times2$大小的区域，对每个区域内的神经元活性值取平均值作为这个区域的表示；</li><li>最大汇聚：使用输入特征图的每个子区域内所有神经元的最大活性值作为这个区域的表示。</li></ul><p><strong>图5.11</strong> 给出了两种汇聚层的示例。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217225846462.png" alt="image-20221217225846462"></p><p>汇聚层输出的计算尺寸与卷积层一致，对于一个输入矩阵$\mathbf X\in\Bbb{R}^{M\times N}$和一个运算区域大小为$U\times V$的汇聚层，步长为$S$，对输入矩阵进行零填充，那么最终输出矩阵大小则为</p><p>$$M’ &#x3D; \frac{M + 2P - U}{S} + 1,（5.20）$$<br>$$N’ &#x3D; \frac{N + 2P - V}{S} + 1.（5.21）$$</p><blockquote><p>alec：</p><p>池化层和卷积层的输出矩阵大小的计算公式是一样的</p></blockquote><p>由于过大的采样区域会急剧减少神经元的数量，也会造成过多的信息丢失。目前，在卷积神经网络中比较典型的汇聚层是将每个输入特征图划分为$2\times2$大小的不重叠区域，然后使用最大汇聚的方式进行下采样。</p><blockquote><p>alec：</p><p>池化中经典的池化方式是使用2*2大小</p></blockquote><p>由于汇聚是使用某一位置的相邻输出的总体统计特征代替网络在该位置的输出，所以其好处是当输入数据做出少量平移时，经过汇聚运算后的大多数输出还能保持不变。比如：当识别一张图像是否是人脸时，我们需要知道人脸左边有一只眼睛，右边也有一只眼睛，而不需要知道眼睛的精确位置，这时候通过汇聚某一片区域的像素点来得到总体统计特征会显得很有用。这也就体现了汇聚层的<strong>平移不变</strong>特性。</p><blockquote><p>alec：</p><p>图像稍微平移之后，汇聚之后基本还是那张图像，体现了汇聚层的平移不变性。</p></blockquote><hr><h6 id="√-F-gt-汇聚层的参数量和计算量"><a href="#√-F-gt-汇聚层的参数量和计算量" class="headerlink" title="[√] F -&gt; 汇聚层的参数量和计算量"></a>[√] F -&gt; 汇聚层的参数量和计算量</h6><hr><p>由于汇聚层中没有参数，所以参数量为$0$；</p><p>最大汇聚中，没有乘加运算，所以计算量为$0$，</p><p>而平均汇聚中，输出特征图上每个点都对应了一次求平均运算。</p><p>使用飞桨实现一个简单的汇聚层，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Pool2D</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, size=(<span class="hljs-params"><span class="hljs-number">2</span>,<span class="hljs-number">2</span></span>), mode=<span class="hljs-string">&#x27;max&#x27;</span>, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(Pool2D, self).__init__()<br>        <span class="hljs-comment"># 汇聚方式</span><br>        self.mode = mode<br>        self.h, self.w = size<br>        self.stride = stride<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        output_w = (x.shape[<span class="hljs-number">2</span>] - self.w) // self.stride + <span class="hljs-number">1</span><br>        output_h = (x.shape[<span class="hljs-number">3</span>] - self.h) // self.stride + <span class="hljs-number">1</span><br>        <span class="hljs-comment">#alec: 池化只对长宽进行池化，其余的维度保持不变</span><br>        output = paddle.zeros([x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], output_w, output_h])<br>        <span class="hljs-comment"># 汇聚</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output.shape[<span class="hljs-number">2</span>]):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output.shape[<span class="hljs-number">3</span>]):<br>                <span class="hljs-comment"># 最大汇聚</span><br>                <span class="hljs-keyword">if</span> self.mode == <span class="hljs-string">&#x27;max&#x27;</span>:<br>                    output[:, :, i, j] = paddle.<span class="hljs-built_in">max</span>(<br>                        x[:, :, self.stride*i:self.stride*i+self.w, self.stride*j:self.stride*j+self.h], <br>                        axis=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>                <span class="hljs-comment"># 平均汇聚</span><br>                <span class="hljs-keyword">elif</span> self.mode == <span class="hljs-string">&#x27;avg&#x27;</span>:<br>                    output[:, :, i, j] = paddle.mean(<br>                        x[:, :, self.stride*i:self.stride*i+self.w, self.stride*j:self.stride*j+self.h], <br>                        axis=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>        <br>        <span class="hljs-keyword">return</span> output<br><br>inputs = paddle.to_tensor([[[[<span class="hljs-number">1.</span>,<span class="hljs-number">2.</span>,<span class="hljs-number">3.</span>,<span class="hljs-number">4.</span>],[<span class="hljs-number">5.</span>,<span class="hljs-number">6.</span>,<span class="hljs-number">7.</span>,<span class="hljs-number">8.</span>],[<span class="hljs-number">9.</span>,<span class="hljs-number">10.</span>,<span class="hljs-number">11.</span>,<span class="hljs-number">12.</span>],[<span class="hljs-number">13.</span>,<span class="hljs-number">14.</span>,<span class="hljs-number">15.</span>,<span class="hljs-number">16.</span>]]]])<br>pool2d = Pool2D(stride=<span class="hljs-number">2</span>)<br>outputs = pool2d(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input: &#123;&#125;, \noutput: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(inputs.shape, outputs.shape))<span class="hljs-comment">#(1,1,4,4),(1,1,2,2)</span><br><br><span class="hljs-comment"># 比较Maxpool2D与paddle API运算结果</span><br>maxpool2d_paddle = nn.MaxPool2D(kernel_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), stride=<span class="hljs-number">2</span>)<br>outputs_paddle = maxpool2d_paddle(inputs)<br><span class="hljs-comment"># 自定义算子运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Maxpool2D outputs:&#x27;</span>, outputs)<br><span class="hljs-comment"># paddle API运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;nn.Maxpool2D outputs:&#x27;</span>, outputs_paddle)<br><br><span class="hljs-comment"># 比较Avgpool2D与paddle API运算结果</span><br>avgpool2d_paddle = nn.AvgPool2D(kernel_size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), stride=<span class="hljs-number">2</span>)<br>outputs_paddle = avgpool2d_paddle(inputs)<br>pool2d = Pool2D(mode=<span class="hljs-string">&#x27;avg&#x27;</span>, stride=<span class="hljs-number">2</span>)<br>outputs = pool2d(inputs)<br><span class="hljs-comment"># 自定义算子运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Avgpool2D outputs:&#x27;</span>, outputs)<br><span class="hljs-comment"># paddle API运算结果</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;nn.Avgpool2D outputs:&#x27;</span>, outputs_paddle)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>], <br>output: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]<br>Maxpool2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[[<span class="hljs-number">6.</span> , <span class="hljs-number">8.</span> ],<br>          [<span class="hljs-number">14.</span>, <span class="hljs-number">16.</span>]]]])<br>nn.Maxpool2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[[<span class="hljs-number">6.</span> , <span class="hljs-number">8.</span> ],<br>          [<span class="hljs-number">14.</span>, <span class="hljs-number">16.</span>]]]])<br>Avgpool2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[[<span class="hljs-number">3.50000000</span> , <span class="hljs-number">5.50000000</span> ],<br>          [<span class="hljs-number">11.50000000</span>, <span class="hljs-number">13.50000000</span>]]]])<br>nn.Avgpool2D outputs: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[[<span class="hljs-number">3.50000000</span> , <span class="hljs-number">5.50000000</span> ],<br>          [<span class="hljs-number">11.50000000</span>, <span class="hljs-number">13.50000000</span>]]]])<br></code></pre></td></tr></table></figure><hr><h2 id="√-5-3-基于LeNet实现手写体数字识别实验"><a href="#√-5-3-基于LeNet实现手写体数字识别实验" class="headerlink" title="[√] 5.3 - 基于LeNet实现手写体数字识别实验"></a>[√] 5.3 - 基于LeNet实现手写体数字识别实验</h2><hr><p>在本节中，我们实现经典卷积网络LeNet-5，并进行手写体数字识别任务。</p><hr><h4 id="√-5-3-1-数据"><a href="#√-5-3-1-数据" class="headerlink" title="[√] 5.3.1 - 数据"></a>[√] 5.3.1 - 数据</h4><hr><p>手写体数字识别是计算机视觉中最常用的图像分类任务，让计算机识别出给定图片中的手写体数字（0-9共10个数字）。由于手写体风格差异很大，因此手写体数字识别是具有一定难度的任务。</p><p>我们采用常用的手写数字识别数据集：<strong>MNIST数据集</strong>。MNIST数据集是计算机视觉领域的经典入门数据集，包含了60,000个训练样本和10,000个测试样本。这些数字已经过尺寸标准化并位于图像中心，图像是固定大小($28\times28$像素)。<strong>图5.12</strong>给出了部分样本的示例。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218103843285.png" alt="image-20221218103120923"></p><p>为了节省训练时间，本节选取MNIST数据集的一个子集进行后续实验，数据集的划分为：</p><ul><li>训练集：1,000条样本</li><li>验证集：200条样本</li><li>测试集：200条样本</li></ul><p>MNIST数据集分为train_set、dev_set和test_set三个数据集，每个数据集含两个列表分别存放了图片数据以及标签数据。比如train_set包含：</p><ul><li><strong>图片数据</strong>：[1 000, 784]的二维列表，包含1 000张图片。每张图片用一个长度为784的向量表示，内容是 $28\times 28$ 尺寸的像素灰度值（黑白图片）。</li><li><strong>标签数据</strong>：[1 000, 1]的列表，表示这些图片对应的分类标签，即0~9之间的数字。</li></ul><p>观察数据集分布情况，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> gzip<br><br><span class="hljs-comment"># 打印并观察数据集分布情况</span><br>train_set, dev_set, test_set = json.load(gzip.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;./mnist.json.gz&#x27;</span>))<br>train_images, train_labels = train_set[<span class="hljs-number">0</span>][:<span class="hljs-number">1000</span>], train_set[<span class="hljs-number">1</span>][:<span class="hljs-number">1000</span>]<br>dev_images, dev_labels = dev_set[<span class="hljs-number">0</span>][:<span class="hljs-number">200</span>], dev_set[<span class="hljs-number">1</span>][:<span class="hljs-number">200</span>]<br>test_images, test_labels = test_set[<span class="hljs-number">0</span>][:<span class="hljs-number">200</span>], test_set[<span class="hljs-number">1</span>][:<span class="hljs-number">200</span>]<br>train_set, dev_set, test_set = [train_images, train_labels], [dev_images, dev_labels], [test_images, test_labels]<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Length of train/dev/test set:&#123;&#125;/&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(train_set[<span class="hljs-number">0</span>]), <span class="hljs-built_in">len</span>(dev_set[<span class="hljs-number">0</span>]), <span class="hljs-built_in">len</span>(test_set[<span class="hljs-number">0</span>])))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Length of train/dev/test <span class="hljs-built_in">set</span>:<span class="hljs-number">1000</span>/<span class="hljs-number">200</span>/<span class="hljs-number">200</span><br></code></pre></td></tr></table></figure><p>可视化观察其中的一张样本以及对应的标签，代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">image, label = train_set[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>], train_set[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>]<br>image, label = np.array(image).astype(<span class="hljs-string">&#x27;float32&#x27;</span>), <span class="hljs-built_in">int</span>(label)<br><span class="hljs-comment"># 原始图像数据为长度784的行向量，需要调整为[28,28]大小的图像</span><br>image = np.reshape(image, [<span class="hljs-number">28</span>,<span class="hljs-number">28</span>])<br>image = Image.fromarray(image.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>), mode=<span class="hljs-string">&#x27;L&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The number in the picture is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label))<br>plt.figure(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>plt.imshow(image)<br>plt.savefig(<span class="hljs-string">&#x27;conv-number5.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218172350294.png" alt="image-20221218103843285"></p><hr><h6 id="√-5-3-1-1-数据预处理"><a href="#√-5-3-1-1-数据预处理" class="headerlink" title="[√] 5.3.1.1 - 数据预处理"></a>[√] 5.3.1.1 - 数据预处理</h6><hr><p>图像分类网络对输入图片的格式、大小有一定的要求，数据输入模型前，需要对数据进行预处理操作，使图片满足网络训练以及预测的需要。本实验主要应用了如下方法：</p><ul><li>调整图片大小：LeNet网络对输入图片大小的要求为 $32\times 32$ ，而MNIST数据集中的原始图片大小却是 $28\times 28$ ，这里为了符合网络的结构设计，将其调整为$32 \times 32$；</li><li>规范化： 通过规范化手段，把输入图像的分布改变成均值为0，标准差为1的标准正态分布，使得最优解的寻优过程明显会变得平缓，训练过程更容易收敛。</li></ul><hr><p>在飞桨中，提供了部分视觉领域的高层API，可以直接调用API实现简单的图像处理操作。通过调用<code>paddle.vision.transforms.Resize</code>调整大小；调用<code>paddle.vision.transforms.Normalize</code>进行标准化处理；使用<code>paddle.vision.transforms.Compose</code>将两个预处理操作进行拼接。</p><hr><blockquote><p>alec：</p><ul><li>通过resize调整图像大小</li><li>通过normalize标准化处理图像，使得图像像素的分布变为标准正态分布</li><li>使用compose将两个操作拼接</li></ul></blockquote><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.vision.transforms <span class="hljs-keyword">import</span> Compose, Resize, Normalize<br><br><span class="hljs-comment"># 数据预处理</span><br>transforms = Compose([Resize(<span class="hljs-number">32</span>), Normalize(mean=[<span class="hljs-number">127.5</span>], std=[<span class="hljs-number">127.5</span>], data_format=<span class="hljs-string">&#x27;CHW&#x27;</span>)])<br></code></pre></td></tr></table></figure><p>将原始的数据集封装为Dataset类，以便DataLoader调用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle.io <span class="hljs-keyword">as</span> io<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MNIST_dataset</span>(io.Dataset):<br>    <span class="hljs-comment"># 将数据的预处理修改操作封装在Dataset类中，在Dataset返回数据的时候，顺便进行了数据的修改预处理操作</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dataset, transforms, mode=<span class="hljs-string">&#x27;train&#x27;</span></span>):<br>        self.mode = mode<br>        self.transforms =transforms<br>        self.dataset = dataset<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-comment"># 获取图像和标签</span><br>        image, label = self.dataset[<span class="hljs-number">0</span>][idx], self.dataset[<span class="hljs-number">1</span>][idx]<br>        image, label = np.array(image).astype(<span class="hljs-string">&#x27;float32&#x27;</span>), <span class="hljs-built_in">int</span>(label)<br>        image = np.reshape(image, [<span class="hljs-number">28</span>,<span class="hljs-number">28</span>])<br>        image = Image.fromarray(image.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>), mode=<span class="hljs-string">&#x27;L&#x27;</span>)<br>        image = self.transforms(image)<br><br>        <span class="hljs-keyword">return</span> image, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.dataset[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 固定随机种子</span><br>random.seed(<span class="hljs-number">0</span>)<br><span class="hljs-comment"># 加载 mnist 数据集</span><br>train_dataset = MNIST_dataset(dataset=train_set, transforms=transforms, mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>test_dataset = MNIST_dataset(dataset=test_set, transforms=transforms, mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br>dev_dataset = MNIST_dataset(dataset=dev_set, transforms=transforms, mode=<span class="hljs-string">&#x27;dev&#x27;</span>)<br></code></pre></td></tr></table></figure><hr><h4 id="√-5-3-2-模型构建"><a href="#√-5-3-2-模型构建" class="headerlink" title="[√] 5.3.2 - 模型构建"></a>[√] 5.3.2 - 模型构建</h4><hr><p>LeNet-5虽然提出的时间比较早，但它是一个非常成功的神经网络模型。</p><p>基于LeNet-5的手写数字识别系统在20世纪90年代被美国很多银行使用，用来识别支票上面的手写数字。LeNet-5的网络结构如<strong>图5.13</strong>所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218105333147.png" alt="image-20221218105333147"></p><p>我们使用上面定义的卷积层算子和汇聚层算子构建一个LeNet-5模型。</p><hr><p>  这里的LeNet-5和原始版本有4点不同：</p><ol><li>C3层没有使用连接表来减少卷积数量。</li><li>汇聚层使用了简单的平均汇聚，没有引入权重和偏置参数以及非线性激活函数。</li><li>卷积层的激活函数使用ReLU函数。</li><li>最后的输出层为一个全连接线性层。</li></ol><hr><p>网络共有7层，包含3个卷积层、2个汇聚层以及2个全连接层的简单卷积神经网络接，受输入图像大小为$32\times 32&#x3D;1, 024$，输出对应10个类别的得分。<br>具体实现如下：</p><blockquote><p>alec：</p><p>所谓的维度，不过就是一层层的每个箱子里面套了几个箱子，所以不要畏难。</p><p>比如，[1,1,32,32] 这个数据，</p><p>就是一个32x32的图片，放到一个箱子里，然后再放到一个箱子里</p><p>[2,3,32,32]</p><p>这个就是3个箱子里分别放一张32x32的图片，然后这个三个箱子在变成两份分别放到两个箱子里</p><hr><p>lenet5，3个卷积层、2个全连接层，一共5层</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_LeNet</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(Model_LeNet, self).__init__()<br>        <span class="hljs-comment"># 卷积层：输出通道数为6，卷积核大小为5×5</span><br>        <span class="hljs-comment"># 本层有6个卷积核提取6种特征</span><br>        self.conv1 = Conv2D(in_channels=in_channels, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, weight_attr=paddle.ParamAttr())<br>        <span class="hljs-comment"># 汇聚层：汇聚窗口为2×2，步长为2</span><br>        self.pool2 = Pool2D(size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), mode=<span class="hljs-string">&#x27;max&#x27;</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 本层有16个卷积核提取16种特征</span><br>        <span class="hljs-comment"># 卷积层：输入通道数为6，输出通道数为16，卷积核大小为5×5，步长为1</span><br>        self.conv3 = Conv2D(in_channels=<span class="hljs-number">6</span>, out_channels=<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, weight_attr=paddle.ParamAttr())<br>        <span class="hljs-comment"># 汇聚层：汇聚窗口为2×2，步长为2</span><br>        self.pool4 = Pool2D(size=(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), mode=<span class="hljs-string">&#x27;avg&#x27;</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 本层有120个卷积核提取120种特征</span><br>        <span class="hljs-comment"># 卷积层：输入通道数为16，输出通道数为120，卷积核大小为5×5</span><br>        self.conv5 = Conv2D(in_channels=<span class="hljs-number">16</span>, out_channels=<span class="hljs-number">120</span>, kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">1</span>, weight_attr=paddle.ParamAttr())<br>        <span class="hljs-comment"># 全连接层：输入神经元为120，输出神经元为84</span><br>        <span class="hljs-comment"># 将120种特征，推测得到84种特征</span><br>        <span class="hljs-comment"># 输出有84个神经元收集信息</span><br>        self.linear6 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)<br>        <span class="hljs-comment"># 84种特征，映射得到10个类别的概率</span><br>        <span class="hljs-comment"># 全连接层：输入神经元为84，输出神经元为类别数</span><br>        <span class="hljs-comment"># 输出有10个神经元收集信息</span><br>        self.linear7 = nn.Linear(<span class="hljs-number">84</span>, num_classes)<br>        <span class="hljs-comment">#alec：前面的卷积层+池化看做特征提取，后面的全连接层看做线性分类</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># C1：卷积层+激活函数</span><br>        output = F.relu(self.conv1(x))<br>        <span class="hljs-comment"># S2：汇聚层</span><br>        output = self.pool2(output)<br>        <span class="hljs-comment"># C3：卷积层+激活函数</span><br>        output = F.relu(self.conv3(output))<br>        <span class="hljs-comment"># S4：汇聚层</span><br>        output = self.pool4(output)<br>        <span class="hljs-comment"># C5：卷积层+激活函数</span><br>        output = F.relu(self.conv5(output))<br>        <span class="hljs-comment"># 输入层将数据拉平[B,C,H,W] -&gt; [B,CxHxW]</span><br>        <span class="hljs-comment"># 全连接层将数据拉平，每张图像提取到的所有的特征，组合连成一个一位特征向量，然后使用全连接层将这个向量分析映射推测</span><br>        output = paddle.squeeze(output, axis=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>        <span class="hljs-comment"># F6：全连接层</span><br>        <span class="hljs-comment"># 卷积层之后需要激活，全连接层之后也要激活，否则就是线性函数了</span><br>        output = F.relu(self.linear6(output))<br>        <span class="hljs-comment"># F7：全连接层</span><br>        <span class="hljs-comment"># 最后一层全连接之后不需要激活</span><br>        output = self.linear7(output)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>下面测试一下上面的LeNet-5模型，构造一个形状为 [1,1,32,32]的输入数据送入网络，观察每一层特征图的形状变化。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里用np.random创建一个随机数组作为输入数据</span><br>inputs = np.random.randn(*[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<br>inputs = inputs.astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br><span class="hljs-comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span><br>model = Model_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 通过调用LeNet从基类继承的sublayers()函数，查看LeNet中所包含的子层</span><br><span class="hljs-built_in">print</span>(model.sublayers())<br>x = paddle.to_tensor(inputs)<br><span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> model.sublayers():<br>    <span class="hljs-comment"># item是LeNet类中的一个子层</span><br>    <span class="hljs-comment"># 查看经过子层之后的输出数据形状</span><br>    <span class="hljs-keyword">try</span>:<br>        x = item(x)<br>    <span class="hljs-keyword">except</span>:<br>        <span class="hljs-comment"># 如果是最后一个卷积层输出，需要展平后才可以送入全连接层</span><br>        x = paddle.reshape(x, [x.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>]) <span class="hljs-comment"># alec：将数据展平，将一个图像的所有特征合为一维，然后再送入全连接层</span><br>        x = item(x)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(item.parameters())==<span class="hljs-number">2</span>:<br>        <span class="hljs-comment"># 查看卷积和全连接层的数据和参数的形状，</span><br>        <span class="hljs-comment"># 其中item.parameters()[0]是权重参数w，item.parameters()[1]是偏置参数b</span><br>        <span class="hljs-built_in">print</span>(item.full_name(), x.shape, item.parameters()[<span class="hljs-number">0</span>].shape, <br>                item.parameters()[<span class="hljs-number">1</span>].shape)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 汇聚层没有参数</span><br>        <span class="hljs-built_in">print</span>(item.full_name(), x.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型结构</span><br>[Conv2D(), Pool2D(), Conv2D(), Pool2D(), Conv2D(), Linear(in_features=<span class="hljs-number">120</span>, out_features=<span class="hljs-number">84</span>, dtype=float32), Linear(in_features=<span class="hljs-number">84</span>, out_features=<span class="hljs-number">10</span>, dtype=float32)]<br>conv2d_6 [<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>] [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>] [<span class="hljs-number">6</span>, <span class="hljs-number">1</span>]<br>pool2d_2 [<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]<br>conv2d_7 [<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>] [<span class="hljs-number">16</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>] [<span class="hljs-number">16</span>, <span class="hljs-number">1</span>]<br>pool2d_3 [<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]<br>conv2d_8 [<span class="hljs-number">1</span>, <span class="hljs-number">120</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>] [<span class="hljs-number">120</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>] [<span class="hljs-number">120</span>, <span class="hljs-number">1</span>]<br>linear_0 [<span class="hljs-number">1</span>, <span class="hljs-number">84</span>] [<span class="hljs-number">120</span>, <span class="hljs-number">84</span>] [<span class="hljs-number">84</span>]<br>linear_1 [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>] [<span class="hljs-number">84</span>, <span class="hljs-number">10</span>] [<span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><p>从输出结果看，</p><ul><li>对于大小为$32 \times32$的单通道图像，先用6个大小为$5 \times5$的卷积核对其进行卷积运算，输出为6个$28 \times28$大小的特征图；</li><li>6个$28 \times28$大小的特征图经过大小为$2 \times2$，步长为2的汇聚层后，输出特征图的大小变为$14 \times14$；</li><li>6个$14 \times14$大小的特征图再经过16个大小为$5 \times5$的卷积核对其进行卷积运算，得到16个$10 \times10$大小的输出特征图；</li><li>16个$10 \times10$大小的特征图经过大小为$2 \times2$，步长为2的汇聚层后，输出特征图的大小变为$5 \times5$；</li><li>16个$5 \times5$大小的特征图再经过120个大小为$5 \times5$的卷积核对其进行卷积运算，得到120个$1 \times1$大小的输出特征图；</li><li>此时，将特征图展平成1维，则有120个像素点，经过输入神经元个数为120，输出神经元个数为84的全连接层后，输出的长度变为84。</li><li>再经过一个全连接层的计算，最终得到了长度为类别数的输出结果。</li></ul><p>考虑到自定义的<code>Conv2D</code>和<code>Pool2D</code>算子中包含多个<code>for</code>循环，所以运算速度比较慢。飞桨框架中，针对卷积层算子和汇聚层算子进行了速度上的优化，这里基于<code>paddle.nn.Conv2D</code>、<code>paddle.nn.MaxPool2D</code>和<code>paddle.nn.AvgPool2D</code>构建LeNet-5模型，对比与上边实现的模型的运算速度。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Paddle_LeNet</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, num_classes=<span class="hljs-number">10</span></span>):<br>        <span class="hljs-built_in">super</span>(Paddle_LeNet, self).__init__()<br>        <span class="hljs-comment"># 卷积层：输出通道数为6，卷积核大小为5*5</span><br>        self.conv1 = nn.Conv2D(in_channels=in_channels, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 汇聚层：汇聚窗口为2*2，步长为2</span><br>        self.pool2 = nn.MaxPool2D(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 卷积层：输入通道数为6，输出通道数为16，卷积核大小为5*5</span><br>        self.conv3 = nn.Conv2D(in_channels=<span class="hljs-number">6</span>, out_channels=<span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 汇聚层：汇聚窗口为2*2，步长为2</span><br>        self.pool4 = nn.AvgPool2D(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 卷积层：输入通道数为16，输出通道数为120，卷积核大小为5*5</span><br>        self.conv5 = nn.Conv2D(in_channels=<span class="hljs-number">16</span>, out_channels=<span class="hljs-number">120</span>, kernel_size=<span class="hljs-number">5</span>)<br>        <span class="hljs-comment"># 全连接层：输入神经元为120，输出神经元为84</span><br>        self.linear6 = nn.Linear(in_features=<span class="hljs-number">120</span>, out_features=<span class="hljs-number">84</span>)<br>        <span class="hljs-comment"># 全连接层：输入神经元为84，输出神经元为类别数</span><br>        self.linear7 = nn.Linear(in_features=<span class="hljs-number">84</span>, out_features=num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># C1：卷积层+激活函数</span><br>        output = F.relu(self.conv1(x))<br>        <span class="hljs-comment"># S2：汇聚层</span><br>        output = self.pool2(output)<br>        <span class="hljs-comment"># C3：卷积层+激活函数</span><br>        output = F.relu(self.conv3(output))<br>        <span class="hljs-comment"># S4：汇聚层</span><br>        output = self.pool4(output)<br>        <span class="hljs-comment"># C5：卷积层+激活函数</span><br>        output = F.relu(self.conv5(output))<br>        <span class="hljs-comment"># 输入层将数据拉平[B,C,H,W] -&gt; [B,CxHxW]</span><br>        output = paddle.squeeze(output, axis=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>        <span class="hljs-comment"># F6：全连接层</span><br>        output = F.relu(self.linear6(output))<br>        <span class="hljs-comment"># F7：全连接层</span><br>        output = self.linear7(output)<br>        <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>测试两个网络的运算速度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 这里用np.random创建一个随机数组作为测试数据</span><br>inputs = np.random.randn(*[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<br>inputs = inputs.astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>x = paddle.to_tensor(inputs)<br><br><span class="hljs-comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span><br>model = Model_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 创建Paddle_LeNet类的实例，指定模型名称和分类的类别数目</span><br>paddle_model = Paddle_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 计算Model_LeNet类的运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">60</span>):<br>    strat_time = time.time()<br>    out = model(x)<br>    end_time = time.time()<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">50</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Model_LeNet speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br><span class="hljs-comment"># 计算Paddle_LeNet类的运算速度</span><br>paddle_model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">60</span>):<br>    strat_time = time.time()<br>    paddle_out = paddle_model(x)<br>    end_time = time.time()<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    paddle_model_time += (end_time - strat_time)<br>avg_paddle_model_time = paddle_model_time / <span class="hljs-number">50</span><br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Paddle_LeNet speed:&#x27;</span>, avg_paddle_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Model_LeNet speed: <span class="hljs-number">3.515837616920471</span> s<br>Paddle_LeNet speed: <span class="hljs-number">0.0010264301300048828</span> s<br></code></pre></td></tr></table></figure><p>可以看出，paddle中的模型速度，要远快于自定义的模型速度</p><p>这里还可以令两个网络加载同样的权重，测试一下两个网络的输出结果是否一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里用np.random创建一个随机数组作为测试数据</span><br>inputs = np.random.randn(*[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<br>inputs = inputs.astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>x = paddle.to_tensor(inputs)<br><br><span class="hljs-comment"># 创建Model_LeNet类的实例，指定模型名称和分类的类别数目</span><br>model = Model_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 获取网络的权重</span><br>params = model.state_dict()<br><span class="hljs-comment"># 自定义Conv2D算子的bias参数形状为[out_channels, 1]</span><br><span class="hljs-comment"># paddle API中Conv2D算子的bias参数形状为[out_channels]</span><br><span class="hljs-comment"># 需要进行调整后才可以赋值</span><br><span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> params:<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;bias&#x27;</span> <span class="hljs-keyword">in</span> key:<br>        params[key] = params[key].squeeze()<br><span class="hljs-comment"># 创建Paddle_LeNet类的实例，指定模型名称和分类的类别数目</span><br>paddle_model = Paddle_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 将Model_LeNet的权重参数赋予给Paddle_LeNet模型，保持两者一致</span><br>paddle_model.set_state_dict(params)<br><br><span class="hljs-comment"># 打印结果保留小数点后6位</span><br>paddle.set_printoptions(<span class="hljs-number">6</span>)<br><span class="hljs-comment"># 计算Model_LeNet的结果</span><br>output = model(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Model_LeNet output: &#x27;</span>, output)<br><span class="hljs-comment"># 计算Paddle_LeNet的结果</span><br>paddle_output = paddle_model(x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Paddle_LeNet output: &#x27;</span>, paddle_output)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">Model_LeNet output:  Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">10</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">False</span>,<br>       [[ <span class="hljs-number">0.102497</span>,  <span class="hljs-number">0.072102</span>,  <span class="hljs-number">0.024969</span>, -<span class="hljs-number">0.224923</span>, -<span class="hljs-number">0.019524</span>,  <span class="hljs-number">0.124833</span>,<br>          <span class="hljs-number">0.220763</span>,  <span class="hljs-number">0.095593</span>, -<span class="hljs-number">0.003534</span>,  <span class="hljs-number">0.108034</span>]])<br>Paddle_LeNet output:  Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">10</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">False</span>,<br>       [[ <span class="hljs-number">0.102497</span>,  <span class="hljs-number">0.072102</span>,  <span class="hljs-number">0.024969</span>, -<span class="hljs-number">0.224923</span>, -<span class="hljs-number">0.019524</span>,  <span class="hljs-number">0.124833</span>,<br>          <span class="hljs-number">0.220763</span>,  <span class="hljs-number">0.095593</span>, -<span class="hljs-number">0.003534</span>,  <span class="hljs-number">0.108034</span>]])<br></code></pre></td></tr></table></figure><p>可以看到，输出结果是一致的。</p><p>这里还可以统计一下LeNet-5模型的参数量和计算量。</p><hr><h6 id="√-参数量"><a href="#√-参数量" class="headerlink" title="[√] 参数量"></a>[√] 参数量</h6><hr><p>按照公式(5.18)进行计算，可以得到：</p><ul><li>第一个卷积层的参数量为：$6 \times 1 \times 5 \times 5 + 6 &#x3D; 156$；</li><li>第二个卷积层的参数量为：$16 \times 6 \times 5 \times 5 + 16 &#x3D; 2416$；</li><li>第三个卷积层的参数量为：$120 \times 16 \times 5 \times 5 + 120&#x3D; 48120$；</li><li>第一个全连接层的参数量为：$120 \times 84 + 84&#x3D; 10164$；</li><li>第二个全连接层的参数量为：$84 \times 10 + 10&#x3D; 850$；</li></ul><p>所以，LeNet-5总的参数量为$61706$。</p><p>在飞桨中，还可以使用<code>paddle.summary</code>API自动计算参数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model = Paddle_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br>params_info = paddle.summary(model, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br><span class="hljs-built_in">print</span>(params_info)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">---------------------------------------------------------------------------<br> Layer (<span class="hljs-built_in">type</span>)       Input Shape          Output Shape         Param <span class="hljs-comment">#    </span><br>===========================================================================<br>   Conv2D-<span class="hljs-number">22</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>]          <span class="hljs-number">156</span>      <br>  MaxPool2D-<span class="hljs-number">4</span>     [[<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]           <span class="hljs-number">0</span>       <br>   Conv2D-<span class="hljs-number">23</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]]     [<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>]         <span class="hljs-number">2</span>,<span class="hljs-number">416</span>     <br>  AvgPool2D-<span class="hljs-number">4</span>    [[<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]            <span class="hljs-number">0</span>       <br>   Conv2D-<span class="hljs-number">24</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">120</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]        <span class="hljs-number">48</span>,<span class="hljs-number">120</span>     <br>   Linear-<span class="hljs-number">11</span>         [[<span class="hljs-number">1</span>, <span class="hljs-number">120</span>]]            [<span class="hljs-number">1</span>, <span class="hljs-number">84</span>]            <span class="hljs-number">10</span>,<span class="hljs-number">164</span>     <br>   Linear-<span class="hljs-number">12</span>         [[<span class="hljs-number">1</span>, <span class="hljs-number">84</span>]]             [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>]              <span class="hljs-number">850</span>      <br>===========================================================================<br>Total params: <span class="hljs-number">61</span>,<span class="hljs-number">706</span><br>Trainable params: <span class="hljs-number">61</span>,<span class="hljs-number">706</span><br>Non-trainable params: <span class="hljs-number">0</span><br>---------------------------------------------------------------------------<br>Input size (MB): <span class="hljs-number">0.00</span><br>Forward/backward <span class="hljs-keyword">pass</span> size (MB): <span class="hljs-number">0.06</span><br>Params size (MB): <span class="hljs-number">0.24</span><br>Estimated Total Size (MB): <span class="hljs-number">0.30</span><br>---------------------------------------------------------------------------<br><br>&#123;<span class="hljs-string">&#x27;total_params&#x27;</span>: <span class="hljs-number">61706</span>, <span class="hljs-string">&#x27;trainable_params&#x27;</span>: <span class="hljs-number">61706</span>&#125;<br></code></pre></td></tr></table></figure><p>可以看到，结果与公式推导一致。</p><hr><h6 id="√-计算量"><a href="#√-计算量" class="headerlink" title="[√] 计算量"></a>[√] 计算量</h6><hr><p>按照公式(5.19)进行计算，可以得到：</p><ul><li>第一个卷积层的计算量为：$28\times 28\times 5\times 5\times 6\times 1 + 28\times 28\times 6&#x3D;122304$；</li><li>第二个卷积层的计算量为：$10\times 10\times 5\times 5\times 16\times 6 + 10\times 10\times 16&#x3D;241600$；</li><li>第三个卷积层的计算量为：$1\times 1\times 5\times 5\times 120\times 16 + 1\times 1\times 120&#x3D;48120$；</li><li>平均汇聚层的计算量为：$16\times 5\times 5&#x3D;400$</li><li>第一个全连接层的计算量为：$120 \times 84 &#x3D; 10080$；</li><li>第二个全连接层的计算量为：$84 \times 10 &#x3D; 840$；</li></ul><p>所以，LeNet-5总的计算量为$423344$。</p><p>在飞桨中，还可以使用<code>paddle.flops</code>API自动统计计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">FLOPs = paddle.flops(model, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>), print_detail=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(FLOPs)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python">&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.nn.layer.conv.Conv2D&#x27;</span>&gt;<span class="hljs-string">&#x27;s flops has been counted</span><br><span class="hljs-string">Cannot find suitable count function for &lt;class &#x27;</span>paddle.nn.layer.pooling.MaxPool2D<span class="hljs-string">&#x27;&gt;. Treat it as zero FLOPs.</span><br><span class="hljs-string">&lt;class &#x27;</span>paddle.nn.layer.pooling.AvgPool2D<span class="hljs-string">&#x27;&gt;&#x27;</span>s flops has been counted<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.nn.layer.common.Linear&#x27;</span>&gt;<span class="hljs-string">&#x27;s flops has been counted</span><br><span class="hljs-string">+--------------+-----------------+-----------------+--------+--------+</span><br><span class="hljs-string">|  Layer Name  |   Input Shape   |   Output Shape  | Params | Flops  |</span><br><span class="hljs-string">+--------------+-----------------+-----------------+--------+--------+</span><br><span class="hljs-string">|  conv2d_21   |  [1, 1, 32, 32] |  [1, 6, 28, 28] |  156   | 122304 |</span><br><span class="hljs-string">| max_pool2d_3 |  [1, 6, 28, 28] |  [1, 6, 14, 14] |   0    |   0    |</span><br><span class="hljs-string">|  conv2d_22   |  [1, 6, 14, 14] | [1, 16, 10, 10] |  2416  | 241600 |</span><br><span class="hljs-string">| avg_pool2d_3 | [1, 16, 10, 10] |  [1, 16, 5, 5]  |   0    |  400   |</span><br><span class="hljs-string">|  conv2d_23   |  [1, 16, 5, 5]  |  [1, 120, 1, 1] | 48120  | 48120  |</span><br><span class="hljs-string">|  linear_10   |     [1, 120]    |     [1, 84]     | 10164  | 10080  |</span><br><span class="hljs-string">|  linear_11   |     [1, 84]     |     [1, 10]     |  850   |  840   |</span><br><span class="hljs-string">+--------------+-----------------+-----------------+--------+--------+</span><br><span class="hljs-string">Total Flops: 423344     Total Params: 61706</span><br><span class="hljs-string">423344</span><br></code></pre></td></tr></table></figure><p>可以看到，结果与公式推导一致。</p><hr><h4 id="√-5-3-3-模型训练"><a href="#√-5-3-3-模型训练" class="headerlink" title="[√] 5.3.3 - 模型训练"></a>[√] 5.3.3 - 模型训练</h4><hr><p>使用交叉熵损失函数，并用随机梯度下降法作为优化器来训练LeNet-5网络。<br>用RunnerV3在训练集上训练5个epoch，并保存准确率最高的模型作为最佳模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> opt<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> RunnerV3, metric<br><br>paddle.seed(<span class="hljs-number">100</span>)<br><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.1</span>    <br><span class="hljs-comment"># 批次大小</span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size)<br><span class="hljs-comment"># 定义LeNet网络</span><br><span class="hljs-comment"># 自定义算子实现的LeNet-5</span><br>model = Model_LeNet(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>)<br><span class="hljs-comment"># 飞桨API实现的LeNet-5</span><br><span class="hljs-comment"># model = Paddle_LeNet(in_channels=1, num_classes=10)</span><br><span class="hljs-comment"># 定义优化器</span><br><span class="hljs-comment"># alec：优化器选择随机梯度下降方法，并设置优化器的学习率、并赋予模型的参数</span><br>optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = F.cross_entropy<br><span class="hljs-comment"># 定义评价指标</span><br>metric = metric.Accuracy(is_logist=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 实例化 RunnerV3 类，并传入训练配置。</span><br><span class="hljs-comment"># 传入模型、优化器、损失函数、评价指标</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">15</span><br>eval_steps = <span class="hljs-number">15</span><br>runner.train(train_loader, dev_loader, num_epochs=<span class="hljs-number">5</span>, log_steps=log_steps, <br>                eval_steps=eval_steps, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)          <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">2.28238</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">15</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">2.08087</span><br>[Evaluate]  dev score: <span class="hljs-number">0.56500</span>, dev loss: <span class="hljs-number">2.03516</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.56500</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">30</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">1.48617</span><br>[Evaluate]  dev score: <span class="hljs-number">0.58000</span>, dev loss: <span class="hljs-number">1.34470</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.56500</span> --&gt; <span class="hljs-number">0.58000</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">45</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">1.61891</span><br>[Evaluate]  dev score: <span class="hljs-number">0.61000</span>, dev loss: <span class="hljs-number">1.24050</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.58000</span> --&gt; <span class="hljs-number">0.61000</span><br>[Train] epoch: <span class="hljs-number">3</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">60</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.64030</span><br>[Evaluate]  dev score: <span class="hljs-number">0.81000</span>, dev loss: <span class="hljs-number">0.54010</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.61000</span> --&gt; <span class="hljs-number">0.81000</span><br>[Train] epoch: <span class="hljs-number">4</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">75</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.39027</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83000</span>, dev loss: <span class="hljs-number">0.46494</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.81000</span> --&gt; <span class="hljs-number">0.83000</span><br>[Evaluate]  dev score: <span class="hljs-number">0.82500</span>, dev loss: <span class="hljs-number">0.41483</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure><p>可视化观察训练集与验证集的损失变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> plot_training_loss_acc<br>plot_training_loss_acc(runner, <span class="hljs-string">&#x27;cnn-loss1.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218121636764.png" alt="image-20221218172300769"></p><hr><h4 id="√-5-3-4-模型评价"><a href="#√-5-3-4-模型评价" class="headerlink" title="[√] 5.3.4 - 模型评价"></a>[√] 5.3.4 - 模型评价</h4><hr><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br><span class="hljs-comment"># alec：模型加载参数，并传入dataloder进行评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] accuracy/loss: <span class="hljs-number">0.8600</span>/<span class="hljs-number">0.4435</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-5-3-5-模型预测"><a href="#√-5-3-5-模型预测" class="headerlink" title="[√] 5.3.5 - 模型预测"></a>[√] 5.3.5 - 模型预测</h4><hr><p>同样地，我们也可以使用保存好的模型，对测试集中的某一个数据进行模型预测，观察模型效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取测试集中第一条数据</span><br>X, label = <span class="hljs-built_in">next</span>(test_loader())<br>logits = runner.predict(X)<br><span class="hljs-comment"># 多分类，使用softmax计算预测概率</span><br>pred = F.softmax(logits)<br><span class="hljs-comment"># 获取概率最大的类别</span><br>pred_class = paddle.argmax(pred[<span class="hljs-number">1</span>]).numpy()<br>label = label[<span class="hljs-number">1</span>][<span class="hljs-number">0</span>].numpy()<br><span class="hljs-comment"># 输出真实类别与预测类别</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label[<span class="hljs-number">0</span>], pred_class[<span class="hljs-number">0</span>]))<br><span class="hljs-comment"># 可视化图片</span><br>plt.figure(figsize=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br>image, label = test_set[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>], test_set[<span class="hljs-number">1</span>][<span class="hljs-number">1</span>]<br>image= np.array(image).astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>image = np.reshape(image, [<span class="hljs-number">28</span>,<span class="hljs-number">28</span>])<br>image = Image.fromarray(image.astype(<span class="hljs-string">&#x27;uint8&#x27;</span>), mode=<span class="hljs-string">&#x27;L&#x27;</span>)<br>plt.imshow(image)<br>plt.savefig(<span class="hljs-string">&#x27;cnn-number2.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218173529787.png" alt="image-20221218172350294"></p><hr><h2 id="√-5-4-基于残差网络的手写体数字识别实验"><a href="#√-5-4-基于残差网络的手写体数字识别实验" class="headerlink" title="[√] 5.4 - 基于残差网络的手写体数字识别实验"></a>[√] 5.4 - 基于残差网络的手写体数字识别实验</h2><hr><blockquote><p>alec:</p><ul><li>残差网络是给非线性层增加直连边</li><li>目的地为了缓解网络层数很深的时候，梯度消失问题</li><li>残差单元在一个或多个神经层f()的输入和输出之间加上一个<strong>直连边</strong>。</li><li>在残差网络中，将目标函数$h(x)$拆为了两个部分：恒等函数$x$和残差函数$h(x)-x$，其中恒等函数直接将输入送到输出那里，然后和残差函数加起来</li><li>典型的残差单元是由多个级联的卷积层和一个直连边组成。</li><li>（网络层数加深，深层的特征图都是含有的高级特征，但是可能高层也需要底层的一些特征，因此通过残差直连边传过来，这样才能充分的利用数据去学习信息，否则高层拿到的都是前一层传过来的，信息获取不充分可能就会被蒙蔽导致数据分析、提取不正确）、</li><li>一个残差网络通常有很多个残差单元堆叠而成。</li><li>ResNet18是一个非常经典的残差网络。</li></ul></blockquote><p><strong>残差网络</strong>（Residual Network，ResNet）是在神经网络模型中给非线性层增加直连边的方式来缓解梯度消失问题，从而使训练深度神经网络变得更加容易。</p><p>在残差网络中，最基本的单位为<strong>残差单元</strong>。</p><p>假设$f(\mathbf x;\theta)$为一个或多个神经层，残差单元在$f()$的输入和输出之间加上一个<strong>直连边</strong>。</p><p>不同于传统网络结构中让网络$f(x;\theta)$去逼近一个目标函数$h(x)$，在残差网络中，将目标函数$h(x)$拆为了两个部分：恒等函数$x$和残差函数$h(x)-x$</p><p>$$<br>\mathrm{ResBlock}_f(\mathbf x) &#x3D; f(\mathbf x;\theta) + \mathbf x,（5.22）<br>$$</p><p>其中$\theta$为可学习的参数。</p><p>一个典型的残差单元如<strong>图5.14</strong>所示，由多个级联的卷积层和一个跨层的直连边组成。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218172300769.png" alt="image-20221218121636764"></p><p>一个残差网络通常有很多个残差单元堆叠而成。下面我们来构建一个在计算机视觉中非常典型的残差网络：ResNet18，并重复上一节中的手写体数字识别任务。</p><hr><h4 id="√-5-4-1-模型构建"><a href="#√-5-4-1-模型构建" class="headerlink" title="[√] 5.4.1 - 模型构建"></a>[√] 5.4.1 - 模型构建</h4><hr><p>在本节中，我们先构建ResNet18的残差单元，然后在组建完整的网络。</p><hr><h6 id="√-5-4-1-1-残差单元"><a href="#√-5-4-1-1-残差单元" class="headerlink" title="[√] 5.4.1.1 - 残差单元"></a>[√] 5.4.1.1 - 残差单元</h6><hr><blockquote><p>alec：</p><ul><li>残差单元包裹的非线性层的输入和输出形状大小应该一致。如果一个卷积层的输入特征图和输出特征图的通道数不一致，则其输出与输入特征图无法直接相加。</li><li>如何使得残差单元的输入输出大小形状一致：可以使用1×1大小的卷积将输入特征图的通道数映射为与级联卷积输出特征图的一致通道数。</li><li>1$\times$1 卷积的作用：<ul><li>不考虑输入数据局部信息之间的关系</li><li>实现信息的跨通道整合</li><li>实现数据的通道数降维或升维</li></ul></li></ul></blockquote><p>这里，我们实现一个算子<code>ResBlock</code>来构建残差单元，其中定义了<code>use_residual</code>参数，用于在后续实验中控制是否使用残差连接。</p><hr><p>残差单元包裹的非线性层的输入和输出形状大小应该一致。如果一个卷积层的输入特征图和输出特征图的通道数不一致，则其输出与输入特征图无法直接相加。为了解决上述问题，我们可以使用$1 \times 1$大小的卷积将输入特征图的通道数映射为与级联卷积输出特征图的一致通道数。</p><p>$1 \times 1$卷积：与标准卷积完全一样，唯一的特殊点在于卷积核的尺寸是$1 \times 1$，也就是不去考虑输入数据局部信息之间的关系，而把关注点放在不同通道间。通过使用$1 \times 1$卷积，可以起到如下作用：</p><ul><li>实现信息的跨通道交互与整合。考虑到卷积运算的输入输出都是3个维度（宽、高、多通道），所以$1 \times 1$卷积实际上就是对每个像素点，在不同的通道上进行线性组合，从而整合不同通道的信息；</li><li>对卷积核通道数进行降维和升维，减少参数量。经过$1 \times 1$卷积后的输出保留了输入数据的原有平面结构，通过调控通道数，从而完成升维或降维的作用；</li><li>利用$1 \times 1$卷积后的非线性激活函数，在保持特征图尺寸不变的前提下，大幅增加非线性。</li></ul><blockquote><p>alec：</p><ul><li>在残差单元中，如果残差函数的输出和输入形状不一致，则通过1x1卷积，改变直连边输入数据的形状，使得残差函数的输出和输入的形状一致，然后才能相加。注意使用1x1改变的是输入在直连边的形状，不是改变的残差函数的输出的形状。</li><li>残差块中每个卷积层的后面，接一个批量规范化层</li></ul></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 残差单元的定义</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, stride=<span class="hljs-number">1</span>, use_residual=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        残差单元</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - in_channels：输入通道数</span><br><span class="hljs-string">            - out_channels：输出通道数</span><br><span class="hljs-string">            - stride：残差单元的步长，通过调整残差单元中第一个卷积层的步长来控制</span><br><span class="hljs-string">            - use_residual：用于控制是否使用残差连接</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(ResBlock, self).__init__()<br>        self.stride = stride<br>        self.use_residual = use_residual<br>        <span class="hljs-comment"># 等宽卷积，不改变长宽</span><br>        <span class="hljs-comment"># 第一个卷积层，卷积核大小为3×3，可以设置不同输出通道数以及步长</span><br>        self.conv1 = nn.Conv2D(in_channels, out_channels, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=self.stride, bias_attr=<span class="hljs-literal">False</span>)<br>        <span class="hljs-comment"># 第二个卷积层，卷积核大小为3×3，不改变输入特征图的形状，步长为1</span><br>        self.conv2 = nn.Conv2D(out_channels, out_channels, <span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, bias_attr=<span class="hljs-literal">False</span>)<br>        <br>        <span class="hljs-comment"># 如果输出和输入的形状不一致，则使用1x1卷积，来改变输入的通道数，使得形状一致</span><br>        <span class="hljs-comment"># 如果conv2的输出和此残差块的输入数据形状不一致，则use_1x1conv = True</span><br>        <span class="hljs-comment"># 当use_1x1conv = True，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致</span><br>        <span class="hljs-comment"># 如果输入和输出通道数不一致，则通过1x1卷积使得通道数保持一致。</span><br>        <span class="hljs-keyword">if</span> in_channels != out_channels <span class="hljs-keyword">or</span> stride != <span class="hljs-number">1</span>:<br>            self.use_1x1conv = <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            self.use_1x1conv = <span class="hljs-literal">False</span><br>        <span class="hljs-comment"># 当残差单元包裹的非线性层输入和输出通道数不一致时，需要用1×1卷积调整通道数后再进行相加运算</span><br>        <span class="hljs-keyword">if</span> self.use_1x1conv:<br>            self.shortcut = nn.Conv2D(in_channels, out_channels, <span class="hljs-number">1</span>, stride=self.stride, bias_attr=<span class="hljs-literal">False</span>)<br><br>        <span class="hljs-comment"># 每个卷积层后会接一个批量规范化层，批量规范化的内容在7.5.1中会进行详细介绍</span><br>        self.bn1 = nn.BatchNorm2D(out_channels)<br>        self.bn2 = nn.BatchNorm2D(out_channels)<br>        <span class="hljs-keyword">if</span> self.use_1x1conv:<br>            self.bn3 = nn.BatchNorm2D(out_channels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        y = F.relu(self.bn1(self.conv1(inputs))) <span class="hljs-comment"># 卷积、批量规范化、激活函数</span><br>        y = self.bn2(self.conv2(y)) <span class="hljs-comment"># 卷积、批量规范化</span><br>        <span class="hljs-keyword">if</span> self.use_residual:<br>            <span class="hljs-keyword">if</span> self.use_1x1conv:  <span class="hljs-comment"># 如果为真，对inputs进行1×1卷积，将形状调整成跟conv2的输出y一致</span><br>                shortcut = self.shortcut(inputs) <span class="hljs-comment"># 1x1卷积调整形状、批量规范化</span><br>                shortcut = self.bn3(shortcut)<br>            <span class="hljs-keyword">else</span>: <span class="hljs-comment"># 否则直接将inputs和conv2的输出y相加</span><br>                shortcut = inputs<br>            y = paddle.add(shortcut, y) <span class="hljs-comment"># 将残差函数的输入 + 输入</span><br>        out = F.relu(y) <span class="hljs-comment"># 激活</span><br>        <span class="hljs-keyword">return</span> out<br></code></pre></td></tr></table></figure><blockquote><p>alec：</p><p>残差单元的基本结构：</p><p>Y &#x3D; A + B，A是残差函数的输出，B是直连边的输出</p><p>A’ &#x3D; 输入 -&gt; 等宽卷积 -&gt; 批量规范化 -&gt; 激活函数 -&gt; 等宽卷积 -&gt; 批量规范化</p><p>B’ &#x3D; 输入 （-&gt; 1x1卷积调整形状 -&gt; 批量规范化）</p><p>Y &#x3D; （A’ + B’）-&gt; 激活函数</p></blockquote><hr><h6 id="√-5-4-1-2-残差网络的整体结构"><a href="#√-5-4-1-2-残差网络的整体结构" class="headerlink" title="[√] 5.4.1.2 - 残差网络的整体结构"></a>[√] 5.4.1.2 - 残差网络的整体结构</h6><hr><blockquote><p>alec：</p><p>残差网络就是将多个残差单元串联起来的深网络。</p></blockquote><p>残差网络就是将很多个残差单元串联起来构成的一个非常深的网络。ResNet18 的网络结构如<strong>图5.16</strong>所示。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218180923475.png" alt="image-20221218173529787"></p><p>其中为了便于理解，可以将ResNet18网络划分为6个模块：</p><ul><li>第一模块：包含了一个步长为2，大小为$7 \times 7$的卷积层，卷积层的输出通道数为64，卷积层的输出经过批量归一化、ReLU激活函数的处理后，接了一个步长为2的$3 \times 3$的最大汇聚层；</li><li>第二模块：包含了两个残差单元，经过运算后，输出通道数为64，特征图的尺寸保持不变；</li><li>第三模块：包含了两个残差单元，经过运算后，输出通道数为128，特征图的尺寸缩小一半；</li><li>第四模块：包含了两个残差单元，经过运算后，输出通道数为256，特征图的尺寸缩小一半；</li><li>第五模块：包含了两个残差单元，经过运算后，输出通道数为512，特征图的尺寸缩小一半；</li><li>第六模块：包含了一个全局平均汇聚层，将特征图变为$1 \times 1$的大小，最终经过全连接层计算出最后的输出。</li></ul><p>ResNet18模型的代码实现如下：</p><p>定义模块1：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 卷积+批量归一化+激活+最大汇聚</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_first_module</span>(<span class="hljs-params">in_channels</span>):<br>    <span class="hljs-comment"># 模块一：7*7卷积、批量规范化、汇聚</span><br>    <span class="hljs-comment"># 64个卷积核，得到64张特征图，</span><br>    m1 = nn.Sequential(nn.Conv2D(in_channels, <span class="hljs-number">64</span>, <span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                    nn.BatchNorm2D(<span class="hljs-number">64</span>), nn.ReLU(),<br>                    nn.MaxPool2D(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br>    <span class="hljs-keyword">return</span> m1<br></code></pre></td></tr></table></figure><p>定义模块二到模块五。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet_module</span>(<span class="hljs-params">input_channels, out_channels, num_res_blocks, stride=<span class="hljs-number">1</span>, use_residual=<span class="hljs-literal">True</span></span>):<br>    blk = []<br>    <span class="hljs-comment"># 根据num_res_blocks，循环生成残差单元</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_res_blocks):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>: <span class="hljs-comment"># 创建模块中的第一个残差单元</span><br>            blk.append(ResBlock(input_channels, out_channels,<br>                                stride=stride, use_residual=use_residual))<br>        <span class="hljs-keyword">else</span>:      <span class="hljs-comment"># 创建模块中的其他残差单元</span><br>            blk.append(ResBlock(out_channels, out_channels, use_residual=use_residual))<br>    <span class="hljs-keyword">return</span> blk<br></code></pre></td></tr></table></figure><p>封装模块二到模块五。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_modules</span>(<span class="hljs-params">use_residual</span>):<br>    <span class="hljs-comment"># 模块二：包含两个残差单元，输入通道数为64，输出通道数为64，步长为1，特征图大小保持不变</span><br>    m2 = nn.Sequential(*resnet_module(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, use_residual=use_residual))<br>    <span class="hljs-comment"># 模块三：包含两个残差单元，输入通道数为64，输出通道数为128，步长为2，特征图大小缩小一半。</span><br>    m3 = nn.Sequential(*resnet_module(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, use_residual=use_residual))<br>    <span class="hljs-comment"># 模块四：包含两个残差单元，输入通道数为128，输出通道数为256，步长为2，特征图大小缩小一半。</span><br>    m4 = nn.Sequential(*resnet_module(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, use_residual=use_residual))<br>    <span class="hljs-comment"># 模块五：包含两个残差单元，输入通道数为256，输出通道数为512，步长为2，特征图大小缩小一半。</span><br>    m5 = nn.Sequential(*resnet_module(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>, use_residual=use_residual))<br>    <span class="hljs-keyword">return</span> m2, m3, m4, m5<br></code></pre></td></tr></table></figure><p>定义完整网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义完整网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_ResNet18</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels=<span class="hljs-number">3</span>, num_classes=<span class="hljs-number">10</span>, use_residual=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-built_in">super</span>(Model_ResNet18,self).__init__()<br>        m1 = make_first_module(in_channels)<br>        m2, m3, m4, m5 = make_modules(use_residual)<br>        <span class="hljs-comment"># 封装模块一到模块6</span><br>        self.net = nn.Sequential(m1, m2, m3, m4, m5,<br>                        <span class="hljs-comment"># 模块六：汇聚层、全连接层</span><br>                        nn.AdaptiveAvgPool2D(<span class="hljs-number">1</span>), nn.Flatten(), nn.Linear(<span class="hljs-number">512</span>, num_classes) )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.net(x)<br></code></pre></td></tr></table></figure><p>这里同样可以使用<code>paddle.summary</code>统计模型的参数量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">model = Model_ResNet18(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>, use_residual=<span class="hljs-literal">True</span>)<br>params_info = paddle.summary(model, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br><span class="hljs-built_in">print</span>(params_info)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs python">-------------------------------------------------------------------------------<br>   Layer (<span class="hljs-built_in">type</span>)         Input Shape          Output Shape         Param <span class="hljs-comment">#    </span><br>===============================================================================<br>     Conv2D-<span class="hljs-number">28</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>]]     [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]         <span class="hljs-number">3</span>,<span class="hljs-number">200</span>     <br>   BatchNorm2D-<span class="hljs-number">1</span>     [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]]     [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]          <span class="hljs-number">256</span>      <br>      ReLU-<span class="hljs-number">1</span>         [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]]     [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]           <span class="hljs-number">0</span>       <br>    MaxPool2D-<span class="hljs-number">5</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">16</span>, <span class="hljs-number">16</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]            <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">29</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]         <span class="hljs-number">36</span>,<span class="hljs-number">864</span>     <br>   BatchNorm2D-<span class="hljs-number">2</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]           <span class="hljs-number">256</span>      <br>     Conv2D-<span class="hljs-number">30</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]         <span class="hljs-number">36</span>,<span class="hljs-number">864</span>     <br>   BatchNorm2D-<span class="hljs-number">3</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]           <span class="hljs-number">256</span>      <br>    ResBlock-<span class="hljs-number">1</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]            <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">31</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]         <span class="hljs-number">36</span>,<span class="hljs-number">864</span>     <br>   BatchNorm2D-<span class="hljs-number">4</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]           <span class="hljs-number">256</span>      <br>     Conv2D-<span class="hljs-number">32</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]         <span class="hljs-number">36</span>,<span class="hljs-number">864</span>     <br>   BatchNorm2D-<span class="hljs-number">5</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]           <span class="hljs-number">256</span>      <br>    ResBlock-<span class="hljs-number">2</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]            <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">33</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]        <span class="hljs-number">73</span>,<span class="hljs-number">728</span>     <br>   BatchNorm2D-<span class="hljs-number">6</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]          <span class="hljs-number">512</span>      <br>     Conv2D-<span class="hljs-number">34</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]        <span class="hljs-number">147</span>,<span class="hljs-number">456</span>    <br>   BatchNorm2D-<span class="hljs-number">7</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]          <span class="hljs-number">512</span>      <br>     Conv2D-<span class="hljs-number">35</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]         <span class="hljs-number">8</span>,<span class="hljs-number">192</span>     <br>   BatchNorm2D-<span class="hljs-number">8</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]          <span class="hljs-number">512</span>      <br>    ResBlock-<span class="hljs-number">3</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]]       [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]           <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">36</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]        <span class="hljs-number">147</span>,<span class="hljs-number">456</span>    <br>   BatchNorm2D-<span class="hljs-number">9</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]          <span class="hljs-number">512</span>      <br>     Conv2D-<span class="hljs-number">37</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]        <span class="hljs-number">147</span>,<span class="hljs-number">456</span>    <br>  BatchNorm2D-<span class="hljs-number">10</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]          <span class="hljs-number">512</span>      <br>    ResBlock-<span class="hljs-number">4</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]           <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">38</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]        <span class="hljs-number">294</span>,<span class="hljs-number">912</span>    <br>  BatchNorm2D-<span class="hljs-number">11</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]         <span class="hljs-number">1</span>,024     <br>     Conv2D-<span class="hljs-number">39</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]        <span class="hljs-number">589</span>,<span class="hljs-number">824</span>    <br>  BatchNorm2D-<span class="hljs-number">12</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]         <span class="hljs-number">1</span>,024     <br>     Conv2D-<span class="hljs-number">40</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]        <span class="hljs-number">32</span>,<span class="hljs-number">768</span>     <br>  BatchNorm2D-<span class="hljs-number">13</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]         <span class="hljs-number">1</span>,024     <br>    ResBlock-<span class="hljs-number">5</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">4</span>, <span class="hljs-number">4</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]           <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">41</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]        <span class="hljs-number">589</span>,<span class="hljs-number">824</span>    <br>  BatchNorm2D-<span class="hljs-number">14</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]         <span class="hljs-number">1</span>,024     <br>     Conv2D-<span class="hljs-number">42</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]        <span class="hljs-number">589</span>,<span class="hljs-number">824</span>    <br>  BatchNorm2D-<span class="hljs-number">15</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]         <span class="hljs-number">1</span>,024     <br>    ResBlock-<span class="hljs-number">6</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]           <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">43</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]       <span class="hljs-number">1</span>,<span class="hljs-number">179</span>,<span class="hljs-number">648</span>   <br>  BatchNorm2D-<span class="hljs-number">16</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]         <span class="hljs-number">2</span>,048     <br>     Conv2D-<span class="hljs-number">44</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]       <span class="hljs-number">2</span>,<span class="hljs-number">359</span>,<span class="hljs-number">296</span>   <br>  BatchNorm2D-<span class="hljs-number">17</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]         <span class="hljs-number">2</span>,048     <br>     Conv2D-<span class="hljs-number">45</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]        <span class="hljs-number">131</span>,072    <br>  BatchNorm2D-<span class="hljs-number">18</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]         <span class="hljs-number">2</span>,048     <br>    ResBlock-<span class="hljs-number">7</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]           <span class="hljs-number">0</span>       <br>     Conv2D-<span class="hljs-number">46</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]       <span class="hljs-number">2</span>,<span class="hljs-number">359</span>,<span class="hljs-number">296</span>   <br>  BatchNorm2D-<span class="hljs-number">19</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]         <span class="hljs-number">2</span>,048     <br>     Conv2D-<span class="hljs-number">47</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]       <span class="hljs-number">2</span>,<span class="hljs-number">359</span>,<span class="hljs-number">296</span>   <br>  BatchNorm2D-<span class="hljs-number">20</span>      [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]         <span class="hljs-number">2</span>,048     <br>    ResBlock-<span class="hljs-number">8</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]           <span class="hljs-number">0</span>       <br>AdaptiveAvgPool2D-<span class="hljs-number">1</span>   [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]      [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]           <span class="hljs-number">0</span>       <br>     Flatten-<span class="hljs-number">1</span>        [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]         [<span class="hljs-number">1</span>, <span class="hljs-number">512</span>]              <span class="hljs-number">0</span>       <br>     Linear-<span class="hljs-number">15</span>           [[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>]]            [<span class="hljs-number">1</span>, <span class="hljs-number">10</span>]             <span class="hljs-number">5</span>,<span class="hljs-number">130</span>     <br>===============================================================================<br>Total params: <span class="hljs-number">11</span>,<span class="hljs-number">185</span>,034<br>Trainable params: <span class="hljs-number">11</span>,<span class="hljs-number">165</span>,<span class="hljs-number">834</span><br>Non-trainable params: <span class="hljs-number">19</span>,<span class="hljs-number">200</span><br>-------------------------------------------------------------------------------<br>Input size (MB): <span class="hljs-number">0.00</span><br>Forward/backward <span class="hljs-keyword">pass</span> size (MB): <span class="hljs-number">1.05</span><br>Params size (MB): <span class="hljs-number">42.67</span><br>Estimated Total Size (MB): <span class="hljs-number">43.73</span><br>-------------------------------------------------------------------------------<br><br>&#123;<span class="hljs-string">&#x27;total_params&#x27;</span>: <span class="hljs-number">11185034</span>, <span class="hljs-string">&#x27;trainable_params&#x27;</span>: <span class="hljs-number">11165834</span>&#125;<br></code></pre></td></tr></table></figure><p>使用<code>paddle.flops</code>统计模型的计算量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">FLOPs = paddle.flops(model, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>), print_detail=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(FLOPs)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python">&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.nn.layer.conv.Conv2D&#x27;</span>&gt;<span class="hljs-string">&#x27;s flops has been counted</span><br><span class="hljs-string">&lt;class &#x27;</span>paddle.nn.layer.norm.BatchNorm2D<span class="hljs-string">&#x27;&gt;&#x27;</span>s flops has been counted<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.nn.layer.activation.ReLU&#x27;</span>&gt;<span class="hljs-string">&#x27;s flops has been counted</span><br><span class="hljs-string">Cannot find suitable count function for &lt;class &#x27;</span>paddle.nn.layer.pooling.MaxPool2D<span class="hljs-string">&#x27;&gt;. Treat it as zero FLOPs.</span><br><span class="hljs-string">&lt;class &#x27;</span>paddle.nn.layer.pooling.AdaptiveAvgPool2D<span class="hljs-string">&#x27;&gt;&#x27;</span>s flops has been counted<br>Cannot find suitable count function <span class="hljs-keyword">for</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.fluid.dygraph.nn.Flatten&#x27;</span>&gt;. Treat it <span class="hljs-keyword">as</span> zero FLOPs.<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;paddle.nn.layer.common.Linear&#x27;</span>&gt;<span class="hljs-string">&#x27;s flops has been counted</span><br><span class="hljs-string">+-----------------------+-----------------+-----------------+---------+---------+</span><br><span class="hljs-string">|       Layer Name      |   Input Shape   |   Output Shape  |  Params |  Flops  |</span><br><span class="hljs-string">+-----------------------+-----------------+-----------------+---------+---------+</span><br><span class="hljs-string">|       conv2d_27       |  [1, 1, 32, 32] | [1, 64, 16, 16] |   3200  |  819200 |</span><br><span class="hljs-string">|     batch_norm2d_0    | [1, 64, 16, 16] | [1, 64, 16, 16] |   256   |  32768  |</span><br><span class="hljs-string">|        re_lu_0        | [1, 64, 16, 16] | [1, 64, 16, 16] |    0    |    0    |</span><br><span class="hljs-string">|      max_pool2d_4     | [1, 64, 16, 16] |  [1, 64, 8, 8]  |    0    |    0    |</span><br><span class="hljs-string">|       conv2d_28       |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |  36864  | 2359296 |</span><br><span class="hljs-string">|       conv2d_29       |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |  36864  | 2359296 |</span><br><span class="hljs-string">|     batch_norm2d_1    |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |   256   |   8192  |</span><br><span class="hljs-string">|     batch_norm2d_2    |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |   256   |   8192  |</span><br><span class="hljs-string">|       conv2d_30       |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |  36864  | 2359296 |</span><br><span class="hljs-string">|       conv2d_31       |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |  36864  | 2359296 |</span><br><span class="hljs-string">|     batch_norm2d_3    |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |   256   |   8192  |</span><br><span class="hljs-string">|     batch_norm2d_4    |  [1, 64, 8, 8]  |  [1, 64, 8, 8]  |   256   |   8192  |</span><br><span class="hljs-string">|       conv2d_32       |  [1, 64, 8, 8]  |  [1, 128, 4, 4] |  73728  | 1179648 |</span><br><span class="hljs-string">|       conv2d_33       |  [1, 128, 4, 4] |  [1, 128, 4, 4] |  147456 | 2359296 |</span><br><span class="hljs-string">|       conv2d_34       |  [1, 64, 8, 8]  |  [1, 128, 4, 4] |   8192  |  131072 |</span><br><span class="hljs-string">|     batch_norm2d_5    |  [1, 128, 4, 4] |  [1, 128, 4, 4] |   512   |   4096  |</span><br><span class="hljs-string">|     batch_norm2d_6    |  [1, 128, 4, 4] |  [1, 128, 4, 4] |   512   |   4096  |</span><br><span class="hljs-string">|     batch_norm2d_7    |  [1, 128, 4, 4] |  [1, 128, 4, 4] |   512   |   4096  |</span><br><span class="hljs-string">|       conv2d_35       |  [1, 128, 4, 4] |  [1, 128, 4, 4] |  147456 | 2359296 |</span><br><span class="hljs-string">|       conv2d_36       |  [1, 128, 4, 4] |  [1, 128, 4, 4] |  147456 | 2359296 |</span><br><span class="hljs-string">|     batch_norm2d_8    |  [1, 128, 4, 4] |  [1, 128, 4, 4] |   512   |   4096  |</span><br><span class="hljs-string">|     batch_norm2d_9    |  [1, 128, 4, 4] |  [1, 128, 4, 4] |   512   |   4096  |</span><br><span class="hljs-string">|       conv2d_37       |  [1, 128, 4, 4] |  [1, 256, 2, 2] |  294912 | 1179648 |</span><br><span class="hljs-string">|       conv2d_38       |  [1, 256, 2, 2] |  [1, 256, 2, 2] |  589824 | 2359296 |</span><br><span class="hljs-string">|       conv2d_39       |  [1, 128, 4, 4] |  [1, 256, 2, 2] |  32768  |  131072 |</span><br><span class="hljs-string">|    batch_norm2d_10    |  [1, 256, 2, 2] |  [1, 256, 2, 2] |   1024  |   2048  |</span><br><span class="hljs-string">|    batch_norm2d_11    |  [1, 256, 2, 2] |  [1, 256, 2, 2] |   1024  |   2048  |</span><br><span class="hljs-string">|    batch_norm2d_12    |  [1, 256, 2, 2] |  [1, 256, 2, 2] |   1024  |   2048  |</span><br><span class="hljs-string">|       conv2d_40       |  [1, 256, 2, 2] |  [1, 256, 2, 2] |  589824 | 2359296 |</span><br><span class="hljs-string">|       conv2d_41       |  [1, 256, 2, 2] |  [1, 256, 2, 2] |  589824 | 2359296 |</span><br><span class="hljs-string">|    batch_norm2d_13    |  [1, 256, 2, 2] |  [1, 256, 2, 2] |   1024  |   2048  |</span><br><span class="hljs-string">|    batch_norm2d_14    |  [1, 256, 2, 2] |  [1, 256, 2, 2] |   1024  |   2048  |</span><br><span class="hljs-string">|       conv2d_42       |  [1, 256, 2, 2] |  [1, 512, 1, 1] | 1179648 | 1179648 |</span><br><span class="hljs-string">|       conv2d_43       |  [1, 512, 1, 1] |  [1, 512, 1, 1] | 2359296 | 2359296 |</span><br><span class="hljs-string">|       conv2d_44       |  [1, 256, 2, 2] |  [1, 512, 1, 1] |  131072 |  131072 |</span><br><span class="hljs-string">|    batch_norm2d_15    |  [1, 512, 1, 1] |  [1, 512, 1, 1] |   2048  |   1024  |</span><br><span class="hljs-string">|    batch_norm2d_16    |  [1, 512, 1, 1] |  [1, 512, 1, 1] |   2048  |   1024  |</span><br><span class="hljs-string">|    batch_norm2d_17    |  [1, 512, 1, 1] |  [1, 512, 1, 1] |   2048  |   1024  |</span><br><span class="hljs-string">|       conv2d_45       |  [1, 512, 1, 1] |  [1, 512, 1, 1] | 2359296 | 2359296 |</span><br><span class="hljs-string">|       conv2d_46       |  [1, 512, 1, 1] |  [1, 512, 1, 1] | 2359296 | 2359296 |</span><br><span class="hljs-string">|    batch_norm2d_18    |  [1, 512, 1, 1] |  [1, 512, 1, 1] |   2048  |   1024  |</span><br><span class="hljs-string">|    batch_norm2d_19    |  [1, 512, 1, 1] |  [1, 512, 1, 1] |   2048  |   1024  |</span><br><span class="hljs-string">| adaptive_avg_pool2d_0 |  [1, 512, 1, 1] |  [1, 512, 1, 1] |    0    |   1024  |</span><br><span class="hljs-string">|       flatten_0       |  [1, 512, 1, 1] |     [1, 512]    |    0    |    0    |</span><br><span class="hljs-string">|       linear_14       |     [1, 512]    |     [1, 10]     |   5130  |   5120  |</span><br><span class="hljs-string">+-----------------------+-----------------+-----------------+---------+---------+</span><br><span class="hljs-string">Total Flops: 35529728     Total Params: 11185034</span><br><span class="hljs-string">35529728</span><br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;验证残差连接对卷积神经网络的促进作用：&#x3D;&#x3D;</p><p>为了验证残差连接对深层卷积神经网络的训练可以起到促进作用，接下来先使用ResNet18（use_residual设置为False）进行手写数字识别实验，再添加残差连接（use_residual设置为True），观察实验对比效果。</p><blockquote><p>alec：</p><p>resnet18，包含输入1个卷积层、8个残差单元（16个卷积层）、1个全连接层，合计18层</p></blockquote><hr><h4 id="√-5-4-2-没有残差连接的ResNet18"><a href="#√-5-4-2-没有残差连接的ResNet18" class="headerlink" title="[√] 5.4.2 - 没有残差连接的ResNet18"></a>[√] 5.4.2 - 没有残差连接的ResNet18</h4><hr><p>为了验证残差连接的效果，先使用没有残差连接的ResNet18进行实验。</p><hr><h6 id="√-5-4-2-1-模型训练"><a href="#√-5-4-2-1-模型训练" class="headerlink" title="[√] 5.4.2.1 - 模型训练"></a>[√] 5.4.2.1 - 模型训练</h6><hr><p>使用训练集和验证集进行模型训练，共训练5个epoch。在实验中，保存准确率最高的模型作为最佳模型。代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> plot<br><br>paddle.seed(<span class="hljs-number">100</span>)<br><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.005</span>  <br><span class="hljs-comment"># 批次大小</span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size)<br><span class="hljs-comment"># 定义网络，不使用残差结构的深层网络</span><br>model = Model_ResNet18(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>, use_residual=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 定义优化器</span><br>optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())<br><span class="hljs-comment"># 实例化RunnerV3</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">15</span><br>eval_steps = <span class="hljs-number">15</span><br>runner.train(train_loader, dev_loader, num_epochs=<span class="hljs-number">5</span>, log_steps=log_steps, <br>            eval_steps=eval_steps, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><span class="hljs-comment"># 可视化观察训练集与验证集的Loss变化情况</span><br>plot_training_loss_acc(runner, <span class="hljs-string">&#x27;cnn-loss2.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">2.66104</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">15</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">1.84262</span><br>[Evaluate]  dev score: <span class="hljs-number">0.21500</span>, dev loss: <span class="hljs-number">2.21904</span><br>/opt/conda/envs/python35-paddle120-env/lib/python3<span class="hljs-number">.7</span>/site-packages/paddle/nn/layer/norm.py:<span class="hljs-number">653</span>: UserWarning: When training, we now always track <span class="hljs-keyword">global</span> mean <span class="hljs-keyword">and</span> variance.<br>  <span class="hljs-string">&quot;When training, we now always track global mean and variance.&quot;</span>)<br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.21500</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">30</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">1.14590</span><br>[Evaluate]  dev score: <span class="hljs-number">0.57500</span>, dev loss: <span class="hljs-number">1.42718</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.21500</span> --&gt; <span class="hljs-number">0.57500</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">45</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.51383</span><br>[Evaluate]  dev score: <span class="hljs-number">0.77500</span>, dev loss: <span class="hljs-number">0.80278</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.57500</span> --&gt; <span class="hljs-number">0.77500</span><br>[Train] epoch: <span class="hljs-number">3</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">60</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.39311</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80500</span>, dev loss: <span class="hljs-number">0.60274</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.77500</span> --&gt; <span class="hljs-number">0.80500</span><br>[Train] epoch: <span class="hljs-number">4</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">75</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.12544</span><br>[Evaluate]  dev score: <span class="hljs-number">0.86500</span>, dev loss: <span class="hljs-number">0.53689</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80500</span> --&gt; <span class="hljs-number">0.86500</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80500</span>, dev loss: <span class="hljs-number">0.54756</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218175932196.png" alt="image-20221218175932196"></p><hr><h6 id="√-5-4-2-2-模型评价"><a href="#√-5-4-2-2-模型评价" class="headerlink" title="[√] 5.4.2.2 - 模型评价"></a>[√] 5.4.2.2 - 模型评价</h6><hr><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失情况。代码实现如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] accuracy/loss: <span class="hljs-number">0.8300</span>/<span class="hljs-number">0.5282</span><br></code></pre></td></tr></table></figure><p>从输出结果看，对比LeNet-5模型评价实验结果，准确率下降、损失升高。得出结论，网络层级加深后，训练效果不升反降。</p><blockquote><p>lenet5的评价分数：</p><p>[Test] accuracy&#x2F;loss: 0.8600&#x2F;0.4435</p></blockquote><hr><h4 id="√-5-4-3-带残差连接的ResNet18"><a href="#√-5-4-3-带残差连接的ResNet18" class="headerlink" title="[√] 5.4.3 - 带残差连接的ResNet18"></a>[√] 5.4.3 - 带残差连接的ResNet18</h4><hr><h6 id="√-5-4-3-1-模型训练"><a href="#√-5-4-3-1-模型训练" class="headerlink" title="[√] 5.4.3.1 - 模型训练"></a>[√] 5.4.3.1 - 模型训练</h6><hr><p>使用带残差连接的ResNet18重复上面的实验，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.01</span>  <br><span class="hljs-comment"># 批次大小</span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size)<br><span class="hljs-comment"># 定义网络，通过指定use_residual为True，使用残差结构的深层网络</span><br>model = Model_ResNet18(in_channels=<span class="hljs-number">1</span>, num_classes=<span class="hljs-number">10</span>, use_residual=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 定义优化器</span><br>optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())<br><span class="hljs-comment"># 实例化RunnerV3</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">15</span><br>eval_steps = <span class="hljs-number">15</span><br>runner.train(train_loader, dev_loader, num_epochs=<span class="hljs-number">5</span>, log_steps=log_steps, <br>            eval_steps=eval_steps, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br><span class="hljs-comment"># 可视化观察训练集与验证集的Loss变化情况</span><br>plot_training_loss_acc(runner, <span class="hljs-string">&#x27;cnn-loss3.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">2.93406</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">15</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.74154</span><br>[Evaluate]  dev score: <span class="hljs-number">0.56500</span>, dev loss: <span class="hljs-number">1.40920</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.56500</span><br>[Train] epoch: <span class="hljs-number">1</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">30</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.20007</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80000</span>, dev loss: <span class="hljs-number">0.55951</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.56500</span> --&gt; <span class="hljs-number">0.80000</span><br>[Train] epoch: <span class="hljs-number">2</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">45</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.02310</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84500</span>, dev loss: <span class="hljs-number">0.41989</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80000</span> --&gt; <span class="hljs-number">0.84500</span><br>[Train] epoch: <span class="hljs-number">3</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">60</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.01396</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84500</span>, dev loss: <span class="hljs-number">0.41457</span><br>[Train] epoch: <span class="hljs-number">4</span>/<span class="hljs-number">5</span>, step: <span class="hljs-number">75</span>/<span class="hljs-number">80</span>, loss: <span class="hljs-number">0.01147</span><br>[Evaluate]  dev score: <span class="hljs-number">0.83500</span>, dev loss: <span class="hljs-number">0.40551</span><br>[Evaluate]  dev score: <span class="hljs-number">0.84500</span>, dev loss: <span class="hljs-number">0.40260</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218103120923.png" alt="image-20221218180737876"></p><hr><h6 id="√-5-4-3-2-模型评价"><a href="#√-5-4-3-2-模型评价" class="headerlink" title="[√] 5.4.3.2 - 模型评价"></a>[√] 5.4.3.2 - 模型评价</h6><hr><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及损失情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">[Test] accuracy/loss: 0.8950/0.4043<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221218180737876.png" alt="image-20221218180923475"></p><blockquote><p>对比lenet5和不加残差连接的resnet18，模型的准确率和损失都有提升。</p></blockquote><hr><h4 id="√-5-4-4-与高层API实现版本的对比实验"><a href="#√-5-4-4-与高层API实现版本的对比实验" class="headerlink" title="[√] 5.4.4 - 与高层API实现版本的对比实验"></a>[√] 5.4.4 - 与高层API实现版本的对比实验</h4><hr><p>对于Reset18这种比较经典的图像分类网络，飞桨高层API中都为大家提供了实现好的版本，大家可以不再从头开始实现。这里为高层API版本的resnet18模型和自定义的resnet18模型赋予相同的权重，并使用相同的输入数据，观察输出结果是否一致。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.vision.models <span class="hljs-keyword">import</span> resnet18<br><span class="hljs-keyword">import</span> warnings<br><span class="hljs-comment">#warnings.filterwarnings(&quot;ignore&quot;)</span><br><br><span class="hljs-comment"># 使用飞桨HAPI中实现的resnet18模型，该模型默认输入通道数为3，输出类别数1000</span><br>hapi_model = resnet18()<br><span class="hljs-comment"># 自定义的resnet18模型</span><br>model = Model_ResNet18(in_channels=<span class="hljs-number">3</span>, num_classes=<span class="hljs-number">1000</span>, use_residual=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 获取网络的权重</span><br>params = hapi_model.state_dict()<br><span class="hljs-comment"># 用来保存参数名映射后的网络权重</span><br>new_params = &#123;&#125;<br><span class="hljs-comment"># 将参数名进行映射</span><br><span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> params:<br>    <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;layer&#x27;</span> <span class="hljs-keyword">in</span> key:<br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;downsample.0&#x27;</span> <span class="hljs-keyword">in</span> key:<br>            new_params[<span class="hljs-string">&#x27;net.&#x27;</span> + key[<span class="hljs-number">5</span>:<span class="hljs-number">8</span>] + <span class="hljs-string">&#x27;.shortcut&#x27;</span> + key[-<span class="hljs-number">7</span>:]] = params[key]<br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;downsample.1&#x27;</span> <span class="hljs-keyword">in</span> key:<br>            new_params[<span class="hljs-string">&#x27;net.&#x27;</span> + key[<span class="hljs-number">5</span>:<span class="hljs-number">8</span>] + <span class="hljs-string">&#x27;.shorcutt&#x27;</span> + key[<span class="hljs-number">23</span>:]] = params[key]<br>        <span class="hljs-keyword">else</span>:<br>            new_params[<span class="hljs-string">&#x27;net.&#x27;</span> + key[<span class="hljs-number">5</span>:]] = params[key]<br>    <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;conv1.weight&#x27;</span> == key:<br>        new_params[<span class="hljs-string">&#x27;net.0.0.weight&#x27;</span>] = params[key]<br>    <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;bn1&#x27;</span> <span class="hljs-keyword">in</span> key:<br>        new_params[<span class="hljs-string">&#x27;net.0.1&#x27;</span> + key[<span class="hljs-number">3</span>:]] = params[key]<br>    <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;fc&#x27;</span> <span class="hljs-keyword">in</span> key:<br>        new_params[<span class="hljs-string">&#x27;net.7&#x27;</span> + key[<span class="hljs-number">2</span>:]] = params[key]<br><br><span class="hljs-comment"># 将飞桨HAPI中实现的resnet18模型的权重参数赋予自定义的resnet18模型，保持两者一致</span><br>model.set_state_dict(new_params)<br><br><span class="hljs-comment"># 这里用np.random创建一个随机数组作为测试数据</span><br>inputs = np.random.randn(*[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>])<br>inputs = inputs.astype(<span class="hljs-string">&#x27;float32&#x27;</span>)<br>x = paddle.to_tensor(inputs)<br><br>output = model(x)<br>hapi_out = hapi_model(x)<br><br><span class="hljs-comment"># 计算两个模型输出的差异</span><br>diff = output - hapi_out<br><span class="hljs-comment"># 取差异最大的值</span><br>max_diff = paddle.<span class="hljs-built_in">max</span>(diff)<br><span class="hljs-built_in">print</span>(max_diff)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [<span class="hljs-number">0.</span>])<br></code></pre></td></tr></table></figure><p>可以看到，高层API版本的resnet18模型和自定义的resnet18模型输出结果是一致的，也就说明两个模型的实现完全一样。</p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第4章 - 前馈神经网络 - 视频</title>
    <link href="/posts/1943031638/"/>
    <url>/posts/1943031638/</url>
    
    <content type="html"><![CDATA[<h2 id="√-4-1-神经元和前馈神经网络"><a href="#√-4-1-神经元和前馈神经网络" class="headerlink" title="[√] 4.1 - 神经元和前馈神经网络"></a>[√] 4.1 - 神经元和前馈神经网络</h2><hr><h4 id="√-D-x3D-gt-神经元-净活性值"><a href="#√-D-x3D-gt-神经元-净活性值" class="headerlink" title="[√] D &#x3D;&gt; 神经元 - 净活性值"></a>[√] D &#x3D;&gt; 神经元 - 净活性值</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150624226.png" alt="image-20221217150208903"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150505118.png" alt="image-20221217150410382"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150718241.png" alt="image-20221217150505118"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217153207228.png" alt="image-20221217150555919"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150410382.png" alt="image-20221217150624226"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217153051359.png" alt="image-20221217150718241"></p><hr><h6 id="√-F-gt-ReLU函数绘制"><a href="#√-F-gt-ReLU函数绘制" class="headerlink" title="[√] F -&gt; ReLU函数绘制"></a>[√] F -&gt; ReLU函数绘制</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217153529772.png" alt="image-20221217151601341"></p><hr><h4 id="√-D-x3D-gt-前馈神经网络"><a href="#√-D-x3D-gt-前馈神经网络" class="headerlink" title="[√] D &#x3D;&gt; 前馈神经网络"></a>[√] D &#x3D;&gt; 前馈神经网络</h4><hr><hr><h6 id="√-F-gt-全连接神经网络、多层感知机"><a href="#√-F-gt-全连接神经网络、多层感知机" class="headerlink" title="[√] F -&gt; 全连接神经网络、多层感知机"></a>[√] F -&gt; 全连接神经网络、多层感知机</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217153708226.png" alt="image-20221217152359647"></p><blockquote><p>alec: 输出层什么时候使用激活函数？</p></blockquote><p>分类问题，输出层一般有激活函数，因此要做成概率来表示分类的可能性</p><p>回归问题，一般输出层不要激活函数，因为回归问题是直接预测的输出值</p><blockquote><p>思考：如果隐藏层没有激活函数会怎样？</p></blockquote><p>没有激活函数，函数计算的数值可能会随着层数的加深变得非常大从而溢出</p><p>没有激活函数那么全都是线性的，无法解决非线性问题</p><h2 id="√-4-2-基于前馈神经网络的二分类任务"><a href="#√-4-2-基于前馈神经网络的二分类任务" class="headerlink" title="[√] 4.2 - 基于前馈神经网络的二分类任务"></a>[√] 4.2 - 基于前馈神经网络的二分类任务</h2><hr><h4 id="√-D-x3D-gt-机器学习实践5要素"><a href="#√-D-x3D-gt-机器学习实践5要素" class="headerlink" title="[√] D &#x3D;&gt; 机器学习实践5要素"></a>[√] D &#x3D;&gt; 机器学习实践5要素</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160129023.png" alt="image-20221217153051359"></p><hr><h4 id="√-D-x3D-gt-数据集构建"><a href="#√-D-x3D-gt-数据集构建" class="headerlink" title="[√] D &#x3D;&gt; 数据集构建"></a>[√] D &#x3D;&gt; 数据集构建</h4><hr><hr><h6 id="√-F-gt-二分类数据集"><a href="#√-F-gt-二分类数据集" class="headerlink" title="[√] F -&gt; 二分类数据集"></a>[√] F -&gt; 二分类数据集</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217154445689.png" alt="image-20221217153207228"></p><hr><h6 id="√-F-gt-线性层算子"><a href="#√-F-gt-线性层算子" class="headerlink" title="[√] F -&gt; 线性层算子"></a>[√] F -&gt; 线性层算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160300062.png" alt="image-20221217153529772"></p><hr><h6 id="√-F-gt-logistic激活函数算子"><a href="#√-F-gt-logistic激活函数算子" class="headerlink" title="[√] F -&gt; logistic激活函数算子"></a>[√] F -&gt; logistic激活函数算子</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160350834.png" alt="image-20221217153624034"></p><hr><h6 id="√-F-gt-层的串行组合"><a href="#√-F-gt-层的串行组合" class="headerlink" title="[√] F -&gt; 层的串行组合"></a>[√] F -&gt; 层的串行组合</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160325924.png" alt="image-20221217153708226"></p><hr><h4 id="√-D-x3D-gt-前馈神经网络的模型优化"><a href="#√-D-x3D-gt-前馈神经网络的模型优化" class="headerlink" title="[√] D &#x3D;&gt; 前馈神经网络的模型优化"></a>[√] D &#x3D;&gt; 前馈神经网络的模型优化</h4><hr><h6 id="√-F-gt-参数优化-反向传播算法"><a href="#√-F-gt-参数优化-反向传播算法" class="headerlink" title="[√] F -&gt; 参数优化 + 反向传播算法"></a>[√] F -&gt; 参数优化 + 反向传播算法</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160721891.png" alt="image-20221217154247581"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217163305414.png" alt="image-20221217154445689"></p><hr><h6 id="√-F-gt-损失函数-损失函数反向传播"><a href="#√-F-gt-损失函数-损失函数反向传播" class="headerlink" title="[√] F -&gt; 损失函数 + 损失函数反向传播"></a>[√] F -&gt; 损失函数 + 损失函数反向传播</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160630996.png" alt="image-20221217154635469"></p><p>这是交叉熵损失函数的backward↑</p><hr><h6 id="√-F-gt-激活函数层的反向传播"><a href="#√-F-gt-激活函数层的反向传播" class="headerlink" title="[√] F -&gt; 激活函数层的反向传播"></a>[√] F -&gt; 激活函数层的反向传播</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160800466.png" alt="image-20221217155133070"></p><p>这是logistic激活函数的backward↑</p><hr><h6 id="√-F-gt-线性层-amp-线性层输入的梯度-amp-线性层参数的梯度"><a href="#√-F-gt-线性层-amp-线性层输入的梯度-amp-线性层参数的梯度" class="headerlink" title="[√] F -&gt; 线性层 &amp; 线性层输入的梯度 &amp; 线性层参数的梯度"></a>[√] F -&gt; 线性层 &amp; 线性层输入的梯度 &amp; 线性层参数的梯度</h6><hr><blockquote><p>alec：</p><p>每个层的算子都有一个backward，线性层、激活函数层、损失函数层</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217163859825.png" alt="image-20221217155456677"></p><hr><h6 id="√-F-gt-整个网络的反向传播"><a href="#√-F-gt-整个网络的反向传播" class="headerlink" title="[√] F -&gt; 整个网络的反向传播"></a>[√] F -&gt; 整个网络的反向传播</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217164302567.png" alt="image-20221217155933754"></p><p>第一个backward的输入是损失函数对输出a2的梯度<code>δ~a2~ </code>，然后逐层的往前反向传播。</p><p>其中fc指的是线性层。</p><blockquote><p>alec：</p><p>先反向传播，把需要的这些梯度计算出来</p><p>然后后面调用优化方法的时候，会用到这些梯度里进行参数的更新</p></blockquote><hr><h6 id="√-F-gt-优化器"><a href="#√-F-gt-优化器" class="headerlink" title="[√] F -&gt; 优化器"></a>[√] F -&gt; 优化器</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217153624034.png" alt="image-20221217160129023"></p><p>优化器的step方法用来进行参数的更新</p><p>遍历所有层，进行每一层的参数的更新</p><hr><h4 id="√-D-x3D-gt-Runner类"><a href="#√-D-x3D-gt-Runner类" class="headerlink" title="[√] D &#x3D;&gt; Runner类"></a>[√] D &#x3D;&gt; Runner类</h4><hr><h6 id="√-F-gt-完善Runner类"><a href="#√-F-gt-完善Runner类" class="headerlink" title="[√] F -&gt; 完善Runner类"></a>[√] F -&gt; 完善Runner类</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217171436228.png" alt="image-20221217160300062"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170723956.png" alt="image-20221217160325924"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170841827.png" alt="image-20221217160350834"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217173925371.png" alt="image-20221217160527263"></p><hr><h6 id="√-F-gt-模型训练"><a href="#√-F-gt-模型训练" class="headerlink" title="[√] F -&gt; 模型训练"></a>[√] F -&gt; 模型训练</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217171755802.png" alt="image-20221217160630996"></p><hr><h6 id="√-F-gt-性能评价"><a href="#√-F-gt-性能评价" class="headerlink" title="[√] F -&gt; 性能评价"></a>[√] F -&gt; 性能评价</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170938635.png" alt="image-20221217160721891"></p><hr><h4 id="√-D-x3D-gt-可视化"><a href="#√-D-x3D-gt-可视化" class="headerlink" title="[√] D &#x3D;&gt; 可视化"></a>[√] D &#x3D;&gt; 可视化</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217172823148.png" alt="image-20221217160800466"></p><p>可以看出，模型学习出的分界线是一个非线性的分界线。模型的分类效果更好。</p><hr><h2 id="√-4-3-自动梯度计算和预定义算子"><a href="#√-4-3-自动梯度计算和预定义算子" class="headerlink" title="[√] 4.3 - 自动梯度计算和预定义算子"></a>[√] 4.3 - 自动梯度计算和预定义算子</h2><h4 id="√-D-x3D-gt-重新实现前馈神经网络"><a href="#√-D-x3D-gt-重新实现前馈神经网络" class="headerlink" title="[√] D &#x3D;&gt; 重新实现前馈神经网络"></a>[√] D &#x3D;&gt; 重新实现前馈神经网络</h4><hr><h6 id="√-F-gt-自动梯度计算-amp-nn-Layer-amp-nn-Linear-amp-nn"><a href="#√-F-gt-自动梯度计算-amp-nn-Layer-amp-nn-Linear-amp-nn" class="headerlink" title="[√] F -&gt; 自动梯度计算 &amp; nn.Layer &amp; nn.Linear &amp; nn"></a>[√] F -&gt; 自动梯度计算 &amp; nn.Layer &amp; nn.Linear &amp; nn</h6><hr><blockquote><p>alec - nn.Layer和nn.Linear的区别：</p><ul><li>Layer指的是一个模型</li><li>Linear指的是模型中的一层，Layer和Linear是包含与被包含关系</li></ul></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217151601341.png" alt="image-20221217163305414"></p><hr><h6 id="√-F-gt-代码实现"><a href="#√-F-gt-代码实现" class="headerlink" title="[√] F -&gt; 代码实现"></a>[√] F -&gt; 代码实现</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217152359647.png" alt="image-20221217163655327"></p><hr><h4 id="√-D-x3D-gt-完善Runner类"><a href="#√-D-x3D-gt-完善Runner类" class="headerlink" title="[√] D &#x3D;&gt; 完善Runner类"></a>[√] D &#x3D;&gt; 完善Runner类</h4><hr><h6 id="√-F-gt-RunnerV2-2"><a href="#√-F-gt-RunnerV2-2" class="headerlink" title="[√] F -&gt; RunnerV2_2"></a>[√] F -&gt; RunnerV2_2</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217154247581.png" alt="image-20221217163859825"></p><hr><h6 id="√-F-gt-版本对比"><a href="#√-F-gt-版本对比" class="headerlink" title="[√] F -&gt; 版本对比"></a>[√] F -&gt; 版本对比</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217155133070.png" alt="image-20221217163940010"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217154635469.png" alt="image-20221217164053403"></p><p>自动进行梯度的计算和参数的更新</p><p>注意，参数优化更新之后需要清空梯度，顺序是：</p><ul><li>backward 反向传播计算需要的梯度</li><li>step 利用计算的梯度进行参数的更新</li><li>clear 清空梯度</li></ul><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217155456677.png" alt="image-20221217164124581"></p><p>模型的评估和预测阶段，关闭梯度计算和存储</p><hr><h6 id="√-F-gt-模型的训练"><a href="#√-F-gt-模型的训练" class="headerlink" title="[√] F -&gt; 模型的训练"></a>[√] F -&gt; 模型的训练</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217155933754.png" alt="image-20221217164233292"></p><hr><h4 id="√-D-x3D-gt-性能评价"><a href="#√-D-x3D-gt-性能评价" class="headerlink" title="[√] D &#x3D;&gt; 性能评价"></a>[√] D &#x3D;&gt; 性能评价</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217160527263.png" alt="image-20221217164302567"></p><hr><h2 id="√-4-4-优化问题"><a href="#√-4-4-优化问题" class="headerlink" title="[√] 4.4 - 优化问题"></a>[√] 4.4 - 优化问题</h2><hr><h4 id="√-D-x3D-gt-优化问题1-参数初始化"><a href="#√-D-x3D-gt-优化问题1-参数初始化" class="headerlink" title="[√] D &#x3D;&gt; 优化问题1 - 参数初始化"></a>[√] D &#x3D;&gt; 优化问题1 - 参数初始化</h4><hr><h6 id="√-F-gt-0初始化"><a href="#√-F-gt-0初始化" class="headerlink" title="[√] F -&gt; 0初始化"></a>[√] F -&gt; 0初始化</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217183719028.png" alt="image-20221217165833209"></p><p>0初始化的对称权重现象：所有隐藏层的激活值相同</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217164124581.png" alt="image-20221217170022786"></p><p>参数的初始化一般是使用非零的初始化，一般是使用高斯分布和均匀分布来初始化</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217164053403.png" alt="image-20221217170125398"></p><hr><h4 id="√-D-x3D-gt-优化问题2-梯度消失问题"><a href="#√-D-x3D-gt-优化问题2-梯度消失问题" class="headerlink" title="[√] D &#x3D;&gt; 优化问题2 - 梯度消失问题"></a>[√] D &#x3D;&gt; 优化问题2 - 梯度消失问题</h4><hr><h6 id="√-F-gt-反向传播-amp-amp-梯度消失问题-amp-amp-减轻梯度消失问题"><a href="#√-F-gt-反向传播-amp-amp-梯度消失问题-amp-amp-减轻梯度消失问题" class="headerlink" title="[√] F -&gt; 反向传播 &amp;&amp; 梯度消失问题 &amp;&amp; 减轻梯度消失问题"></a>[√] F -&gt; 反向传播 &amp;&amp; 梯度消失问题 &amp;&amp; 减轻梯度消失问题</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217163655327.png" alt="image-20221217170358022"></p><p>使用梯度比较稳定的激活函数缓解梯度消失问题</p><hr><h6 id="√-F-gt-模型构建"><a href="#√-F-gt-模型构建" class="headerlink" title="[√] F -&gt; 模型构建"></a>[√] F -&gt; 模型构建</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217164233292.png" alt="image-20221217170528466"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170022786.png" alt="image-20221217170612514"></p><p>从后往前，梯度逐渐衰减，最前面的几层梯度就很小了。</p><hr><h6 id="√-F-gt-网络每层的梯度变化趋势"><a href="#√-F-gt-网络每层的梯度变化趋势" class="headerlink" title="[√] F -&gt; 网络每层的梯度变化趋势"></a>[√] F -&gt; 网络每层的梯度变化趋势</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217165833209.png" alt="image-20221217170723956"></p><hr><h4 id="√-D-x3D-gt-优化问题3-死亡ReLU"><a href="#√-D-x3D-gt-优化问题3-死亡ReLU" class="headerlink" title="[√] D &#x3D;&gt; 优化问题3 - 死亡ReLU"></a>[√] D &#x3D;&gt; 优化问题3 - 死亡ReLU</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217163940010.png" alt="image-20221217170841827"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170358022.png" alt="image-20221217170938635"></p><h2 id="√-4-5-前馈神经网络总结"><a href="#√-4-5-前馈神经网络总结" class="headerlink" title="[√] 4.5 - 前馈神经网络总结"></a>[√] 4.5 - 前馈神经网络总结</h2><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170125398.png" alt="image-20221217171436228"></p><h2 id="√-4-6-基于前馈神经网络的鸢尾花分类任务"><a href="#√-4-6-基于前馈神经网络的鸢尾花分类任务" class="headerlink" title="[√] 4.6 - 基于前馈神经网络的鸢尾花分类任务"></a>[√] 4.6 - 基于前馈神经网络的鸢尾花分类任务</h2><hr><h4 id="√-D-x3D-gt-机器学习实践5要素-1"><a href="#√-D-x3D-gt-机器学习实践5要素-1" class="headerlink" title="[√] D &#x3D;&gt; 机器学习实践5要素"></a>[√] D &#x3D;&gt; 机器学习实践5要素</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170612514.png" alt="image-20221217171755802"></p><hr><h4 id="√-D-x3D-gt-小批量梯度下降法"><a href="#√-D-x3D-gt-小批量梯度下降法" class="headerlink" title="[√] D &#x3D;&gt; 小批量梯度下降法"></a>[√] D &#x3D;&gt; 小批量梯度下降法</h4><hr><h6 id="√-F-gt-BGD批量梯度下降法-amp-amp-mini-BGD-小批量梯度下降法-amp-amp-参数更新"><a href="#√-F-gt-BGD批量梯度下降法-amp-amp-mini-BGD-小批量梯度下降法-amp-amp-参数更新" class="headerlink" title="[√] F -&gt; BGD批量梯度下降法 &amp;&amp; mini BGD 小批量梯度下降法 &amp;&amp; 参数更新"></a>[√] F -&gt; BGD批量梯度下降法 &amp;&amp; mini BGD 小批量梯度下降法 &amp;&amp; 参数更新</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217172014449.png" alt="image-20221217172014449"></p><hr><h4 id="√-D-x3D-gt-数据处理"><a href="#√-D-x3D-gt-数据处理" class="headerlink" title="[√] D &#x3D;&gt; 数据处理"></a>[√] D &#x3D;&gt; 数据处理</h4><hr><h6 id="√-F-gt-数据分组-amp-amp-Dataset类-amp-amp-DataLoader类"><a href="#√-F-gt-数据分组-amp-amp-Dataset类-amp-amp-DataLoader类" class="headerlink" title="[√] F -&gt; 数据分组 &amp;&amp; Dataset类 &amp;&amp; DataLoader类"></a>[√] F -&gt; 数据分组 &amp;&amp; Dataset类 &amp;&amp; DataLoader类</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217170528466.png" alt="image-20221217172823148"></p><hr><h6 id="√-F-gt-构建IrisDataset"><a href="#√-F-gt-构建IrisDataset" class="headerlink" title="[√] F -&gt; 构建IrisDataset"></a>[√] F -&gt; 构建IrisDataset</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217173120736.png" alt="image-20221217173120736"></p><hr><h6 id="√-F-gt-用DataLoader进行封装"><a href="#√-F-gt-用DataLoader进行封装" class="headerlink" title="[√] F -&gt; 用DataLoader进行封装"></a>[√] F -&gt; 用DataLoader进行封装</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181115279.png" alt="image-20221217173925371"></p><hr><h4 id="√-D-x3D-gt-模型构建"><a href="#√-D-x3D-gt-模型构建" class="headerlink" title="[√] D &#x3D;&gt; 模型构建"></a>[√] D &#x3D;&gt; 模型构建</h4><hr><h6 id="√-F-gt-前馈神经网络-amp-amp-注意"><a href="#√-F-gt-前馈神经网络-amp-amp-注意" class="headerlink" title="[√] F -&gt; 前馈神经网络 &amp;&amp; 注意"></a>[√] F -&gt; 前馈神经网络 &amp;&amp; 注意</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217180742986.png" alt="image-20221217180742986"></p><p>飞桨的损失函数中集成了softmax激活函数，因此前向传播这里最后没有加激活函数</p><hr><h4 id="√-D-x3D-gt-RunnerV3"><a href="#√-D-x3D-gt-RunnerV3" class="headerlink" title="[√] D &#x3D;&gt; RunnerV3"></a>[√] D &#x3D;&gt; RunnerV3</h4><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181211275.png" alt="image-20221217181115279"></p><hr><h6 id="√-F-gt-RunnerV3"><a href="#√-F-gt-RunnerV3" class="headerlink" title="[√] F -&gt; RunnerV3"></a>[√] F -&gt; RunnerV3</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181306399.png" alt="image-20221217181211275"></p><hr><h6 id="√-F-gt-RunnerV3-train"><a href="#√-F-gt-RunnerV3-train" class="headerlink" title="[√] F -&gt; RunnerV3 - train"></a>[√] F -&gt; RunnerV3 - train</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181406357.png" alt="image-20221217181306399"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181954787.png" alt="image-20221217181317314"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181627593.png" alt="image-20221217181406357"></p><hr><h4 id="√-D-x3D-gt-模型训练"><a href="#√-D-x3D-gt-模型训练" class="headerlink" title="[√] D &#x3D;&gt; 模型训练"></a>[√] D &#x3D;&gt; 模型训练</h4><hr><h6 id="√-F-gt-模型训练-1"><a href="#√-F-gt-模型训练-1" class="headerlink" title="[√] F -&gt; 模型训练"></a>[√] F -&gt; 模型训练</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217181317314.png" alt="image-20221217181627593"></p><p>损失函数要计算梯度、前向传播网络也要计算损失函数，损失函数只是反向传播的第一步，并不是计算了损失就可以直接进行梯度更新了，而是要计算损失，然后计算损失函数的梯度，然后从这里开始，逐层的往前计算每一层的梯度并根据链式法则计算梯度并保存，然后再根据计算的梯度来更新参数。循环往复。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217183658601.png" alt="image-20221217181954787"></p><hr><h6 id="√-F-gt-可视化"><a href="#√-F-gt-可视化" class="headerlink" title="[√] F -&gt; 可视化"></a>[√] F -&gt; 可视化</h6><hr><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150208903.png" alt="image-20221217183658601"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221217150555919.png" alt="image-20221217183719028"></p>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第4章 - 前馈神经网络 - 书籍</title>
    <link href="/posts/75512103/"/>
    <url>/posts/75512103/</url>
    
    <content type="html"><![CDATA[<h2 id="√-第4章-前馈神经网络"><a href="#√-第4章-前馈神经网络" class="headerlink" title="[√] 第4章 - 前馈神经网络"></a>[√] 第4章 - 前馈神经网络</h2><p>神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。<br>前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。</p><p>在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 <strong>图4.1</strong> 所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55" width=500></center><center>图4.1 《神经网络与深度学习》关键知识点回顾</center><p>本实践基于 <strong>《神经网络与深度学习》第4章：前馈神经网络</strong> 相关内容进行设计，主要包含两部分：</p><ul><li><strong>模型解读</strong>：介绍前馈神经网络的基本概念、网络结构及代码实现，利用前馈神经网络完成一个分类任务，并通过两个简单的实验，观察前馈神经网络的梯度消失问题和死亡ReLU问题，以及对应的优化策略；</li><li><strong>案例与实践</strong>：基于前馈神经网络完成鸢尾花分类任务。</li></ul><hr><h3 id="√-4-1-神经元"><a href="#√-4-1-神经元" class="headerlink" title="[√] 4.1 - 神经元"></a>[√] 4.1 - 神经元</h3><p>神经网络的基本组成单元为带有非线性激活函数的神经元，其结构如如<strong>图4.2</strong>所示。神经元是对生物神经元的结构和特性的一种简化建模，接收一组输入信号并产生输出。</p><p>&#x3D;&#x3D;带有非线性激活函数&#x3D;&#x3D;</p><p>&#x3D;&#x3D;接收一组信号并产生输出&#x3D;&#x3D;</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214165655381.png" alt="image-20221214165655381"></p><hr><h4 id="√-4-1-1-净活性值"><a href="#√-4-1-1-净活性值" class="headerlink" title="[√] 4.1.1 - 净活性值"></a>[√] 4.1.1 - 净活性值</h4><p>&#x3D;&#x3D;z&#x3D;wx+b 这个公司计算的是净活性值&#x3D;&#x3D;</p><p>假设一个神经元接收的输入为$\mathbf{x}\in \mathbb{R}^D$，其权重向量为$\mathbf{w}\in \mathbb{R}^D$，神经元所获得的输入信号，即净活性值$z$的计算方法为</p><p>$$<br>z &#x3D;\mathbf{w}^T\mathbf{x}+b，（4.1）<br>$$</p><p>其中$b$为偏置。</p><p>为了提高预测样本的效率，我们通常会将$N$个样本归为一组进行成批地预测。</p><p>$$<br>\boldsymbol{z} &#x3D;\boldsymbol{X} \boldsymbol{w} + b, (4.2)<br>$$</p><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{z}\in \mathbb{R}^N$为$N$个预测值组成的列向量。</p><p>使用Paddle计算一组输入的净活性值。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-comment"># 2个特征数为5的样本</span><br>X = paddle.rand(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>])<br><br><span class="hljs-comment"># 含有5个参数的权重向量</span><br>w = paddle.rand(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">1</span>])<br><span class="hljs-comment"># 偏置项</span><br>b = paddle.rand(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 使用&#x27;paddle.matmul&#x27;实现矩阵相乘</span><br>z = paddle.matmul(X, w) + b<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input X:&quot;</span>, X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight w:&quot;</span>, w, <span class="hljs-string">&quot;\nbias b:&quot;</span>, b)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output z:&quot;</span>, z)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> X: Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.79964578</span>, <span class="hljs-number">0.80879998</span>, <span class="hljs-number">0.94919258</span>, <span class="hljs-number">0.90140802</span>, <span class="hljs-number">0.99157101</span>],<br>        [<span class="hljs-number">0.68319607</span>, <span class="hljs-number">0.18029618</span>, <span class="hljs-number">0.31775340</span>, <span class="hljs-number">0.69175428</span>, <span class="hljs-number">0.23035321</span>]])<br>weight w: Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.40206695</span>],<br>        [<span class="hljs-number">0.33055961</span>],<br>        [<span class="hljs-number">0.66693956</span>],<br>        [<span class="hljs-number">0.91678756</span>],<br>        [<span class="hljs-number">0.37521145</span>]]) <br>bias b: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.04227288</span>]])<br>output z: Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">2.46264338</span>],<br>        [<span class="hljs-number">1.30910742</span>]])<br><br></code></pre></td></tr></table></figure><p><strong>说明</strong></p><p>在飞桨中，可以使用<strong>nn.Linear</strong>完成输入张量的上述变换。</p><hr><h4 id="√-4-1-2-激活函数"><a href="#√-4-1-2-激活函数" class="headerlink" title="[√] 4.1.2 - 激活函数"></a>[√] 4.1.2 - 激活函数</h4><p>&#x3D;&#x3D;激活函数通常为非线性函数，可以增强神经网络的表示能力和学习能力。常用的激活函数有S型函数和ReLU函数。&#x3D;&#x3D;</p><p>净活性值$z$再经过一个非线性函数$f(·)$后，得到神经元的活性值$a$。</p><p>$$<br>a &#x3D; f(z)，（4.3）<br>$$</p><p>激活函数通常为非线性函数，可以增强神经网络的表示能力和学习能力。常用的激活函数有S型函数和ReLU函数。</p><hr><h5 id="√-4-1-2-1-Sigmoid-型函数"><a href="#√-4-1-2-1-Sigmoid-型函数" class="headerlink" title="[√] 4.1.2.1 - Sigmoid 型函数"></a>[√] 4.1.2.1 - Sigmoid 型函数</h5><p>&#x3D;&#x3D;Sigmoid 型函数是指一类S型曲线函数，为两端饱和函数。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;常用的 Sigmoid 型函数有 Logistic 函数和 Tanh 函数&#x3D;&#x3D;</p><p>其数学表达式为：</p><p>Logistic 函数：</p><p>$$<br>\sigma(z) &#x3D; \frac{1}{1+\exp(-z)}。（4.4）<br>$$</p><p>Tanh 函数：</p><p>$$<br>\mathrm{tanh}(z) &#x3D; \frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}。（4.5）<br>$$</p><p>Logistic函数和Tanh函数的代码实现和可视化如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Logistic函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">logistic</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-z))<br><br><span class="hljs-comment"># Tanh函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tanh</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> (paddle.exp(z) - paddle.exp(-z)) / (paddle.exp(z) + paddle.exp(-z))<br><br><span class="hljs-comment"># 在[-10,10]的范围内生成10000个输入值，用于绘制函数曲线</span><br>z = paddle.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10000</span>)<br><br>plt.figure()<br>plt.plot(z.tolist(), logistic(z).tolist(), color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Logistic Function&quot;</span>)<br>plt.plot(z.tolist(), tanh(z).tolist(), color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle =<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Tanh Function&quot;</span>)<br><br>ax = plt.gca() <span class="hljs-comment"># 获取轴，默认有4个</span><br><span class="hljs-comment"># 隐藏两个轴，通过把颜色设置成none</span><br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br><span class="hljs-comment"># 调整坐标轴位置   </span><br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br><br>plt.savefig(<span class="hljs-string">&#x27;fw-logistic-tanh.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172332002.png" alt="image-20221214172332002"></p><p><strong>说明</strong></p><p>在飞桨中，可以通过调用<code>paddle.nn.functional.sigmoid</code>和<code>paddle.nn.functional.tanh</code>实现对张量的Logistic和Tanh计算。</p><hr><h5 id="√-4-1-2-2-ReLU-型函数"><a href="#√-4-1-2-2-ReLU-型函数" class="headerlink" title="[√] 4.1.2.2 - ReLU 型函数"></a>[√] 4.1.2.2 - ReLU 型函数</h5><p>常见的ReLU函数有ReLU和带泄露的ReLU（Leaky ReLU），数学表达式分别为：</p><p>$$<br>\mathrm{ReLU}(z) &#x3D; \max(0,z),（4.6）<br>$$</p><p>$$<br>\mathrm{LeakyReLU}(z) &#x3D; \max(0,z)+\lambda \min(0,z),（4.7）<br>$$</p><p>其中$\lambda$为超参数。</p><p>可视化ReLU和带泄露的ReLU的函数的代码实现和可视化如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ReLU</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> paddle.maximum(z, paddle.to_tensor(<span class="hljs-number">0.</span>))<br><br><span class="hljs-comment"># 带泄露的ReLU</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">leaky_relu</span>(<span class="hljs-params">z, negative_slope=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-comment"># 当前版本paddle暂不支持直接将bool类型转成int类型，因此调用了paddle的cast函数来进行显式转换</span><br>    a1 = (paddle.cast((z &gt; <span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) * z) <br>    a2 = (paddle.cast((z &lt;= <span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) * (negative_slope * z))<br>    <span class="hljs-keyword">return</span> a1 + a2<br><br><span class="hljs-comment"># 在[-10,10]的范围内生成一系列的输入值，用于绘制relu、leaky_relu的函数曲线</span><br>z = paddle.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10000</span>)<br><br>plt.figure()<br>plt.plot(z.tolist(), relu(z).tolist(), color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;ReLU Function&quot;</span>)<br>plt.plot(z.tolist(), leaky_relu(z).tolist(), color=<span class="hljs-string">&quot;#E20079&quot;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>, label=<span class="hljs-string">&quot;LeakyReLU Function&quot;</span>)<br><br>ax = plt.gca()<br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>plt.legend(loc=<span class="hljs-string">&#x27;upper left&#x27;</span>, fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-relu-leakyrelu.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172608024.png" alt="image-20221214172608024"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214204504748.png" alt="image-20221214172620106"></p><p><strong>说明</strong></p><p>在飞桨中，可以通过调用<code>paddle.nn.functional.relu</code>和<code>paddle.nn.functional.leaky_relu</code>完成ReLU与带泄露的ReLU的计算。</p><hr><p><strong>动手练习</strong><br>本节重点介绍和实现了几个经典的Sigmoid函数和ReLU函数。<br>请动手实现《神经网络与深度学习》4.1节中提到的其他激活函数，如：Hard-Logistic、Hard-Tanh、ELU、Softplus、Swish等。</p><hr><h3 id="√-4-2-基于前馈神经网络的二分类任务"><a href="#√-4-2-基于前馈神经网络的二分类任务" class="headerlink" title="[√] 4.2 - 基于前馈神经网络的二分类任务"></a>[√] 4.2 - 基于前馈神经网络的二分类任务</h3><p>前馈神经网络的网络结构如<strong>图4.3</strong>所示。每一层获取前一层神经元的活性值，并重复上述计算得到该层的活性值，传入到下一层。整个网络中无反馈，信号从输入层向输出层逐层的单向传播，得到网络最后的输出 $\boldsymbol{a}^{(L)}$。</p><center><img src="https://ai-studio-static-online.cdn.bcebos.com/dbcf147a4e00446792eb2b93834e0f3154936e08ea124242af8631fde204381c" ></center><center><br>图4.3: 前馈神经网络结构</br></center><hr><h4 id="√-4-2-1-数据集构建"><a href="#√-4-2-1-数据集构建" class="headerlink" title="[√] 4.2.1 - 数据集构建"></a>[√] 4.2.1 - 数据集构建</h4><p>这里，我们使用第3.1.1节中构建的二分类数据集：Moon1000数据集，其中训练集640条、验证集160条、测试集200条。 该数据集的数据是从两个带噪音的弯月形状数据分布中采样得到，每个样本包含2个特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.dataset <span class="hljs-keyword">import</span> make_moons<br><br><span class="hljs-comment"># 采样1000个样本</span><br>n_samples = <span class="hljs-number">1000</span><br>X, y = make_moons(n_samples=n_samples, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">0.5</span>)<br><br>num_train = <span class="hljs-number">640</span><br>num_dev = <span class="hljs-number">160</span><br>num_test = <span class="hljs-number">200</span><br><br>X_train, y_train = X[:num_train], y[:num_train]<br>X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]<br><br>y_train = y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_dev = y_dev.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_test = y_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">outer_circ_x.shape: [<span class="hljs-number">500</span>] outer_circ_y.shape: [<span class="hljs-number">500</span>]<br>inner_circ_x.shape: [<span class="hljs-number">500</span>] inner_circ_y.shape: [<span class="hljs-number">500</span>]<br>after concat shape: [<span class="hljs-number">1000</span>]<br>X shape: [<span class="hljs-number">1000</span>, <span class="hljs-number">2</span>]<br>y shape: [<span class="hljs-number">1000</span>]<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-2-2-模型构建"><a href="#√-4-2-2-模型构建" class="headerlink" title="[√] 4.2.2 - 模型构建"></a>[√] 4.2.2 - 模型构建</h4><p>&#x3D;&#x3D;经过仿射变换，得到该层神经元的净活性值z&#x3D;&#x3D;</p><p>&#x3D;&#x3D;再输入到激活函数得到该层神经元的活性值a&#x3D;&#x3D;</p><p>&#x3D;&#x3D;在实践中，为了提高模型的处理效率，通常将N个样本归为一组进行成批地计算。&#x3D;&#x3D;</p><p>为了更高效的构建前馈神经网络，我们先定义每一层的算子，然后再通过算子组合构建整个前馈神经网络。</p><p>假设网络的第$l$层的输入为第$l-1$层的神经元活性值$\boldsymbol{a}^{(l-1)}$，经过一个仿射变换，得到该层神经元的净活性值$\boldsymbol{z}$，再输入到激活函数得到该层神经元的活性值$\boldsymbol{a}$。</p><p>在实践中，为了提高模型的处理效率，通常将$N$个样本归为一组进行成批地计算。假设网络第$l$层的输入为$\boldsymbol{A}^{(l-1)}\in \mathbb{R}^{N\times M_{l-1}}$，其中每一行为一个样本，则前馈网络中第$l$层的计算公式为</p><p>$$<br>\mathbf Z^{(l)}&#x3D;\mathbf A^{(l-1)} \mathbf W^{(l)} +\mathbf b^{(l)}  \in \mathbb{R}^{N\times M_{l}}, (4.8)<br>$$<br>$$<br>\mathbf A^{(l)}&#x3D;f_l(\mathbf Z^{(l)}) \in \mathbb{R}^{N\times M_{l}}, (4.9)<br>$$<br>其中$\mathbf Z^{(l)}$为$N$个样本第$l$层神经元的净活性值，$\mathbf A^{(l)}$为$N$个样本第$l$层神经元的活性值，$\boldsymbol{W}^{(l)}\in \mathbb{R}^{M_{l-1}\times M_{l}}$为第$l$层的权重矩阵，$\boldsymbol{b}^{(l)}\in \mathbb{R}^{1\times M_{l}}$为第$l$层的偏置。</p><hr><p>为了和代码的实现保存一致性，这里使用形状为$(样本数量\times 特征维度)$的张量来表示一组样本。样本的矩阵$\boldsymbol{X}$是由$N$个$\boldsymbol{x}$的<strong>行向量</strong>组成。而《神经网络与深度学习》中$\boldsymbol{x}$为列向量，因此这里的权重矩阵$\boldsymbol{W}$和偏置$\boldsymbol{b}$和《神经网络与深度学习》中的表示刚好为转置关系。</p><hr><p>为了使后续的模型搭建更加便捷，我们将神经层的计算，即公式(4.8)和(4.9)，都封装成算子，这些算子都继承<code>Op</code>基类。</p><hr><h5 id="√-4-2-2-1-线性层算子"><a href="#√-4-2-2-1-线性层算子" class="headerlink" title="[√] 4.2.2.1 - 线性层算子"></a>[√] 4.2.2.1 - 线性层算子</h5><p>公式（4.8）对应一个线性层算子，权重参数采用默认的随机初始化，偏置采用默认的零初始化。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.op <span class="hljs-keyword">import</span> Op<br><br><span class="hljs-comment"># 实现线性层算子</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - input_size：输入数据维度</span><br><span class="hljs-string">            - output_size：输出数据维度</span><br><span class="hljs-string">            - name：算子名称</span><br><span class="hljs-string">            - weight_init：权重初始化方式，默认使用&#x27;paddle.standard_normal&#x27;进行标准正态分布初始化</span><br><span class="hljs-string">            - bias_init：偏置初始化方式，默认使用全0初始化</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 初始化权重</span><br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size,output_size])<br>        <span class="hljs-comment"># 初始化偏置</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="hljs-number">1</span>,output_size])<br>        self.inputs = <span class="hljs-literal">None</span><br><br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs：shape=[N,input_size], N是样本数量</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：预测值，shape=[N,output_size]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.inputs = inputs<br><br>        outputs = paddle.matmul(self.inputs, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-2-2-Logistic算子"><a href="#√-4-2-2-2-Logistic算子" class="headerlink" title="[√] 4.2.2.2 - Logistic算子"></a>[√] 4.2.2.2 - Logistic算子</h5><p>本节我们采用Logistic函数来作为公式(4.9)中的激活函数。这里也将Logistic函数实现一个算子，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Logistic</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.inputs = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs: shape=[N,D]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：shape=[N,D]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        outputs = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-inputs))<br>        self.outputs = outputs<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-2-3-层的串行组合"><a href="#√-4-2-2-3-层的串行组合" class="headerlink" title="[√] 4.2.2.3 - 层的串行组合"></a>[√] 4.2.2.3 - 层的串行组合</h5><p>&#x3D;&#x3D;在定义了神经层的线性层算子和激活函数算子之后，我们可以不断交叉重复使用它们来构建一个多层的神经网络。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;使用激活函数的作用是为了激活特征，让特征更有活力。将特征归一化，这样可以防止过拟合，通过这种正则化方式抑制模型的能力。通过归一化特征值，可以防止特征值量纲的不同导致的差异。&#x3D;&#x3D;</p><p>下面我们实现一个两层的用于二分类任务的前馈神经网络，选用Logistic作为激活函数，可以利用上面实现的线性层和激活函数算子来组装。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现一个两层前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - input_size：输入维度</span><br><span class="hljs-string">            - hidden_size：隐藏层神经元数量</span><br><span class="hljs-string">            - output_size：输出维度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.fc1 = Linear(input_size, hidden_size, name=<span class="hljs-string">&quot;fc1&quot;</span>)<br>        self.act_fn1 = Logistic()<br>        self.fc2 = Linear(hidden_size, output_size, name=<span class="hljs-string">&quot;fc2&quot;</span>)<br>        self.act_fn2 = Logistic()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - X：shape=[N,input_size], N是样本数量</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - a2：预测值，shape=[N,output_size]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        z1 = self.fc1(X)<br>        a1 = self.act_fn1(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn2(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure><p><strong>测试一下</strong></p><p>现在，我们实例化一个两层的前馈网络，令其输入层维度为5，隐藏层维度为10，输出层维度为1。 并随机生成一条长度为5的数据输入两层神经网络，观察输出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化模型</span><br>model = Model_MLP_L2(input_size=<span class="hljs-number">5</span>, hidden_size=<span class="hljs-number">10</span>, output_size=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 随机生成1条长度为5的数据</span><br>X = paddle.rand(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>])<br>result = model(X)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;result: &quot;</span>, result)<br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">result:  Tensor(shape=[1, 1], <span class="hljs-attribute">dtype</span>=float32, <span class="hljs-attribute">place</span>=CPUPlace, <span class="hljs-attribute">stop_gradient</span>=<span class="hljs-literal">True</span>,<br>       [[0.65805507]])<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-2-3-损失函数"><a href="#√-4-2-3-损失函数" class="headerlink" title="[√] 4.2.3 - 损失函数"></a>[√] 4.2.3 - 损失函数</h4><p>二分类交叉熵损失函数见第三章，这里不再赘述。</p><hr><h4 id="√-4-2-4-模型优化"><a href="#√-4-2-4-模型优化" class="headerlink" title="[√] 4.2.4 - 模型优化"></a>[√] 4.2.4 - 模型优化</h4><p>&#x3D;&#x3D;神经网络的参数主要是通过<strong>梯度下降法</strong>进行优化的，因此需要计算最终损失对每个参数的梯度。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;由于神经网络的层数通常比较深，其梯度计算和上一章中的线性分类模型的不同的点在于：线性模型通常比较简单可以直接计算梯度，而神经网络相当于一个复合函数，需要利用链式法则进行反向传播来计算梯度。&#x3D;&#x3D;</p><hr><h5 id="√-4-2-4-1-反向传播算法"><a href="#√-4-2-4-1-反向传播算法" class="headerlink" title="[√] 4.2.4.1 - 反向传播算法"></a>[√] 4.2.4.1 - 反向传播算法</h5><p>前馈神经网络的参数梯度通常使用<strong>误差反向传播</strong>算法来计算。使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步：</p><ol><li>前馈计算每一层的净活性值$\boldsymbol{Z}^{(l)}$和激活值$\boldsymbol{A}^ {(l)}$，直到最后一层；</li><li>反向传播计算每一层的误差项$\delta^{(l)}&#x3D;\frac{\partial R}{\partial \boldsymbol{Z}^{(l)}}$；</li><li>计算每一层参数的梯度，并更新参数。</li></ol><p>在上面实现算子的基础上，来实现误差反向传播算法。在上面的三个步骤中，</p><ol><li>第1步是前向计算，可以利用算子的<code>forward()</code>方法来实现；</li><li>第2步是反向计算梯度，可以利用算子的<code>backward()</code>方法来实现；</li><li>第3步中的计算参数梯度也放到<code>backward()</code>中实现，更新参数放到另外的<strong>优化器</strong>中专门进行。</li></ol><p>这样，在模型训练过程中，我们首先执行模型的<code>forward()</code>，再执行模型的<code>backward()</code>，就得到了所有参数的梯度，之后再利用优化器迭代更新参数。</p><p>以这我们这节中构建的两层全连接前馈神经网络<code>Model_MLP_L2</code>为例，下图给出了其前向和反向计算过程：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172620106.png"></p><p>下面我们按照反向的梯度传播顺序，为每个算子添加<code>backward()</code>方法，并在其中实现每一层参数的梯度的计算。</p><hr><h5 id="√-4-2-4-2-损失函数"><a href="#√-4-2-4-2-损失函数" class="headerlink" title="[√] 4.2.4.2 - 损失函数"></a>[√] 4.2.4.2 - 损失函数</h5><p>二分类交叉熵损失函数对神经网络的输出$\hat{\boldsymbol{y}}$的偏导数为:<br>$$<br>\frac{\partial R}{\partial \hat{\boldsymbol{y}}} &#x3D;  -\frac{1}{N}(\mathrm{dialog}(\frac{1}{\hat{\boldsymbol{y}}})\boldsymbol{y}-\mathrm{dialog}(\frac{1}{1-\hat{\boldsymbol{y}}})(1-\boldsymbol{y})) (4.10) \<br>&#x3D; -\frac{1}{N}(\frac{1}{\hat{\boldsymbol{y}}}\odot\boldsymbol{y}-\frac{1}{1-\hat{\boldsymbol{y}}}\odot(1-\boldsymbol{y})), (4.11)<br>$$<br>其中$dialog(\boldsymbol{x})$表示以向量$\boldsymbol{x}$为对角元素的对角阵，$\frac{1}{\boldsymbol{x}}&#x3D;\frac{1}{x_1},…,\frac{1}{x_N}$表示逐元素除，$\odot$表示逐元素积。</p><ul><li>实现损失函数的<code>backward()</code>，代码实现如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现交叉熵损失函数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BinaryCrossEntropyLoss</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model</span>):<br>        self.predicts = <span class="hljs-literal">None</span><br>        self.labels = <span class="hljs-literal">None</span><br>        self.num = <span class="hljs-literal">None</span><br><br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-keyword">return</span> self.forward(predicts, labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失值：shape=[1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.predicts = predicts<br>        self.labels = labels<br>        self.num = self.predicts.shape[<span class="hljs-number">0</span>]<br>        loss = -<span class="hljs-number">1.</span> / self.num * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) <br>                + paddle.matmul((<span class="hljs-number">1</span>-self.labels.t()), paddle.log(<span class="hljs-number">1</span>-self.predicts)))<br><br>        loss = paddle.squeeze(loss, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 计算损失函数对模型预测的导数</span><br>        loss_grad_predicts = -<span class="hljs-number">1.0</span> * (self.labels / self.predicts - <br>                       (<span class="hljs-number">1</span> - self.labels) / (<span class="hljs-number">1</span> - self.predicts)) / self.num<br>        <br>        <span class="hljs-comment"># 梯度反向传播</span><br>        self.model.backward(loss_grad_predicts)<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-4-3-Logistic算子"><a href="#√-4-2-4-3-Logistic算子" class="headerlink" title="[√] 4.2.4.3 - Logistic算子"></a>[√] 4.2.4.3 - Logistic算子</h5><p>在本节中，我们使用Logistic激活函数，所以这里为Logistic算子增加的反向函数。</p><p>Logistic算子的前向过程表示为$\boldsymbol{A}&#x3D;\sigma(\boldsymbol{Z})$，其中$\sigma$为Logistic函数，$\boldsymbol{Z} \in R^{N \times D}$和$\boldsymbol{A} \in R^{N \times D}$的每一行表示一个样本。</p><p>为了简便起见，我们分别用向量$\boldsymbol{a} \in R^D$ 和 $\boldsymbol{z} \in R^D$表示同一个样本在激活函数前后的表示，则$\boldsymbol{a}$对$\boldsymbol{z}$的偏导数为：<br>$$<br>\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}&#x3D;diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\in R^{D \times D}, (4.12)<br>$$<br>按照反向传播算法，令$\delta_{\boldsymbol{a}}&#x3D;\frac{\partial R}{\partial \boldsymbol{a}} \in R^D$表示最终损失$R$对Logistic算子的单个输出$\boldsymbol{a}$的梯度，则<br>$$<br>\delta_{\boldsymbol{z}} \triangleq \frac{\partial R}{\partial \boldsymbol{z}} &#x3D; \frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}\delta_{\boldsymbol{a}}  (4.13) \<br>&#x3D; diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\delta_{\boldsymbol(a)}, (4.14) \<br>&#x3D; \boldsymbol{a}\odot(1-\boldsymbol{a})\odot\delta_{\boldsymbol(a)}。 (4.15)<br>$$</p><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{A}} &#x3D;\frac{\partial R}{\partial \boldsymbol{A}} \in R^{N \times D}$表示最终损失$R$对Logistic算子输出$A$的梯度，损失函数对Logistic函数输入$\boldsymbol{Z}$的导数为<br>$$<br>\delta_{\boldsymbol{Z}}&#x3D;\boldsymbol{A} \odot (1-\boldsymbol{A})\odot \delta_{\boldsymbol{A}} \in R^{N \times D},(4.16)<br>$$<br>$\delta_{\boldsymbol{Z}}$为Logistic算子反向传播的输出。</p><p>由于Logistic函数中没有参数，这里不需要在<code>backward()</code>方法中计算该算子参数的梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Logistic</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.inputs = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br>        self.params = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-inputs))<br>        self.outputs = outputs<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        <span class="hljs-comment"># 计算Logistic激活函数对输入的导数</span><br>        outputs_grad_inputs = paddle.multiply(self.outputs, (<span class="hljs-number">1.0</span> - self.outputs))<br>        <span class="hljs-keyword">return</span> paddle.multiply(grads,outputs_grad_inputs)<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-4-4-线性层"><a href="#√-4-2-4-4-线性层" class="headerlink" title="[√] 4.2.4.4 - 线性层"></a>[√] 4.2.4.4 - 线性层</h5><p>线性层算子Linear的前向过程表示为$\boldsymbol{Y}&#x3D;\boldsymbol{X}\boldsymbol{W}+\boldsymbol{b}$，其中输入为$\boldsymbol{X} \in R^{N \times M}$，输出为$\boldsymbol{Y} \in R^{N \times D}$，参数为权重矩阵$\boldsymbol{W} \in R^{M \times D}$和偏置$\boldsymbol{b} \in R^{1 \times D}$。$\boldsymbol{X}$和$\boldsymbol{Y}$中的每一行表示一个样本。</p><p>为了简便起见，我们用向量$\boldsymbol{x}\in R^M$和$\boldsymbol{y}\in R^D$表示同一个样本在线性层算子中的输入和输出，则有$\boldsymbol{y}&#x3D;\boldsymbol{W}^T\boldsymbol{x}+\boldsymbol{b}^T$。$\boldsymbol{y}$对输入$\boldsymbol{x}$的偏导数为<br>$$<br>\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} &#x3D; \boldsymbol{W}\in R^{D \times M}。(4.17)<br>$$</p><p>&#x3D;&#x3D;线性层输入的梯度&#x3D;&#x3D; </p><p>按照反向传播算法，令$\delta_{\boldsymbol{y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{y}}\in R^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则<br>$$<br>\delta_{\boldsymbol{x}} \triangleq \frac{\partial R}{\partial \boldsymbol{x}}&#x3D; \boldsymbol{W} \delta_{\boldsymbol{y}}。(4.18)<br>$$</p><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，公式可以重写为<br>$$<br>\delta_{\boldsymbol{X}} &#x3D;\delta_{\boldsymbol{Y}} \boldsymbol{W}^T,(4.19)<br>$$<br>其中$\delta_{\boldsymbol{X}}$为线性层算子反向函数的输出。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213744716.png" alt="image-20221214204504748"></p><p>&#x3D;&#x3D;线性层参数的梯度&#x3D;&#x3D;</p><p><strong>计算线性层参数的梯度</strong> 由于线性层算子中包含有可学习的参数$\boldsymbol{W}$和$\boldsymbol{b}$，因此<code>backward()</code>除了实现梯度反传外，还需要计算算子内部的参数的梯度。</p><p>令$\delta_{\boldsymbol{y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{y}}\in \mathbb{R}^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则<br>$$<br>\delta_{\boldsymbol{W}} \triangleq \frac{\partial R}{\partial \boldsymbol{W}} &#x3D; \boldsymbol{x}\delta_{\boldsymbol{y}}^T,(4.20) \<br>\delta_{\boldsymbol{b}} \triangleq \frac{\partial R}{\partial \boldsymbol{b}} &#x3D; \delta_{\boldsymbol{y}}^T。(4.21)<br>$$</p><p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，则公式可以重写为<br>$$<br>\delta_{\boldsymbol{W}} &#x3D; \boldsymbol{X}^T \delta_{\boldsymbol{Y}},(4.22) \<br>\delta_{\boldsymbol{b}} &#x3D; \mathbf{1}^T \delta_{\boldsymbol{Y}}。(4.23)<br>$$</p><p>具体实现代码如下：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/8562dfb10d464396948d05ee3620cec1d057025dddee43ff92dae3fbb72e8f65.png" alt="image-20221214205337207"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):<br>        self.params = &#123;&#125;<br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size, output_size])<br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="hljs-number">1</span>, output_size])<br><br>        self.inputs = <span class="hljs-literal">None</span><br>        self.grads = &#123;&#125;<br><br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        self.inputs = inputs<br>        outputs = paddle.matmul(self.inputs, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - grads：损失函数对当前层输出的导数</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失函数对当前层输入的导数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.grads[<span class="hljs-string">&#x27;W&#x27;</span>] = paddle.matmul(self.inputs.T, grads)<br>        self.grads[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.<span class="hljs-built_in">sum</span>(grads, axis=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 线性层输入的梯度</span><br>        <span class="hljs-keyword">return</span> paddle.matmul(grads, self.params[<span class="hljs-string">&#x27;W&#x27;</span>].T)<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-4-5-整个网络"><a href="#√-4-2-4-5-整个网络" class="headerlink" title="[√] 4.2.4.5 - 整个网络"></a>[√] 4.2.4.5 - 整个网络</h5><p>实现完整的两层神经网络的前向和反向计算。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-comment"># 线性层</span><br>        self.fc1 = Linear(input_size, hidden_size, name=<span class="hljs-string">&quot;fc1&quot;</span>)<br>        <span class="hljs-comment"># Logistic激活函数层</span><br>        self.act_fn1 = Logistic()<br>        self.fc2 = Linear(hidden_size, output_size, name=<span class="hljs-string">&quot;fc2&quot;</span>)<br>        self.act_fn2 = Logistic()<br><br>        self.layers = [self.fc1, self.act_fn1, self.fc2, self.act_fn2]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        z1 = self.fc1(X)<br>        a1 = self.act_fn1(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn2(z2)<br>        <span class="hljs-keyword">return</span> a2<br>        <br>    <span class="hljs-comment"># 反向计算，因为是反向传播，因此对输入的倒数，通过对输出的倒数推得，←</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, loss_grad_a2</span>):<br>        loss_grad_z2 = self.act_fn2.backward(loss_grad_a2)<br>        loss_grad_a1 = self.fc2.backward(loss_grad_z2)<br>        loss_grad_z1 = self.act_fn1.backward(loss_grad_a1)<br>        loss_grad_inputs = self.fc1.backward(loss_grad_z1)<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-2-4-6-优化器"><a href="#√-4-2-4-6-优化器" class="headerlink" title="[√] 4.2.4.6 - 优化器"></a>[√] 4.2.4.6 - 优化器</h5><p>在计算好神经网络参数的梯度之后，我们将梯度下降法中参数的更新过程实现在优化器中。</p><p>与第3章中实现的梯度下降优化器<code>SimpleBatchGD</code>不同的是，此处的优化器需要遍历每层，对每层的参数分别做更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.opitimizer <span class="hljs-keyword">import</span> Optimizer<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BatchGD</span>(<span class="hljs-title class_ inherited__">Optimizer</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, init_lr, model</span>):<br>        <span class="hljs-built_in">super</span>(BatchGD, self).__init__(init_lr=init_lr, model=model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 参数更新</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> layer.params.keys():<br>                    layer.params[key] = layer.params[key] - self.init_lr * layer.grads[key]<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-2-5-完善Runner类：RunnerV2-1"><a href="#√-4-2-5-完善Runner类：RunnerV2-1" class="headerlink" title="[√] 4.2.5 - 完善Runner类：RunnerV2_1"></a>[√] 4.2.5 - 完善Runner类：RunnerV2_1</h4><p>基于3.1.6实现的 RunnerV2 类主要针对比较简单的模型。而在本章中，模型由多个算子组合而成，通常比较复杂，因此本节继续完善并实现一个改进版： <code>RunnerV2_1</code>类，其主要加入的功能有：</p><ol><li>支持自定义算子的梯度计算，在训练过程中调用<code>self.loss_fn.backward()</code>从损失函数开始反向计算梯度；</li><li>每层的模型保存和加载，将每一层的参数分别进行保存和加载。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV2_1</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, metric, loss_fn, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric<br><br>        <span class="hljs-comment"># 记录训练过程中的评估指标变化情况</span><br>        self.train_scores = []<br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.train_loss = []<br>        self.dev_loss = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_set, dev_set, **kwargs</span>):<br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_epochs = kwargs.get(<span class="hljs-string">&quot;log_epochs&quot;</span>, <span class="hljs-number">100</span>)<br><br>        <span class="hljs-comment"># 传入模型保存路径</span><br>        save_dir = kwargs.get(<span class="hljs-string">&quot;save_dir&quot;</span>, <span class="hljs-literal">None</span>)<br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        best_score = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            X, y = train_set<br>            <span class="hljs-comment"># 获取模型预测</span><br>            logits = self.model(X)<br>            <span class="hljs-comment"># 计算交叉熵损失</span><br>            trn_loss = self.loss_fn(logits, y) <span class="hljs-comment"># return a tensor</span><br>            <br>            self.train_loss.append(trn_loss.item())<br>            <span class="hljs-comment"># 计算评估指标</span><br>            trn_score = self.metric(logits, y).item()<br>            self.train_scores.append(trn_score)<br><br>            <span class="hljs-comment"># alec：此处是主动计算梯度</span><br>            self.loss_fn.backward() <span class="hljs-comment"># 反向传播计算梯度</span><br><br>            <span class="hljs-comment"># 参数更新</span><br>            self.optimizer.step() <span class="hljs-comment"># 梯度下降方法更新梯度</span><br>           <br>            dev_score, dev_loss = self.evaluate(dev_set)<br>            <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>            <span class="hljs-keyword">if</span> dev_score &gt; best_score:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                best_score = dev_score<br>                <span class="hljs-keyword">if</span> save_dir:<br>                    self.save_model(save_dir)<br><br>            <span class="hljs-keyword">if</span> log_epochs <span class="hljs-keyword">and</span> epoch % log_epochs == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, loss: <span class="hljs-subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 验证不需要计算梯度和更新参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set</span>):<br>        X, y = data_set<br>        <span class="hljs-comment"># 计算模型输出</span><br>        logits = self.model(X)<br>        <span class="hljs-comment"># 计算损失函数</span><br>        loss = self.loss_fn(logits, y).item()<br>        self.dev_loss.append(loss)<br>        <span class="hljs-comment"># 计算评估指标</span><br>        score = self.metric(logits, y).item()<br>        self.dev_scores.append(score)<br>        <span class="hljs-keyword">return</span> score, loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.model(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_dir</span>):<br>        <span class="hljs-comment"># 对模型每层参数分别进行保存，保存文件名称与该层名称相同</span><br>        <span class="hljs-comment"># 按层保存模型</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                paddle.save(layer.params, os.path.join(save_dir, layer.name+<span class="hljs-string">&quot;.pdparams&quot;</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_dir</span>):<br>        <span class="hljs-comment"># 获取所有层参数名称和保存路径之间的对应关系</span><br>        model_file_names = os.listdir(model_dir)<br>        name_file_dict = &#123;&#125;<br>        <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> model_file_names:<br>            name = file_name.replace(<span class="hljs-string">&quot;.pdparams&quot;</span>,<span class="hljs-string">&quot;&quot;</span>)<br>            name_file_dict[name] = os.path.join(model_dir, file_name)<br><br>        <span class="hljs-comment"># 加载每层参数</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                name = layer.name<br>                file_path = name_file_dict[name]<br>                layer.params = paddle.load(file_path)<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-2-6-模型训练"><a href="#√-4-2-6-模型训练" class="headerlink" title="[√] 4.2.6 - 模型训练"></a>[√] 4.2.6 - 模型训练</h4><p>基于<code>RunnerV2_1</code>，使用训练集和验证集进行模型训练，共训练2000个epoch。评价指标为第章介绍的<code>accuracy</code>。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.metric <span class="hljs-keyword">import</span> accuracy<br>paddle.seed(<span class="hljs-number">123</span>)<br>epoch_num = <span class="hljs-number">1000</span><br><br>model_saved_dir = <span class="hljs-string">&quot;model&quot;</span><br><br><span class="hljs-comment"># 输入层维度为2</span><br>input_size = <span class="hljs-number">2</span><br><span class="hljs-comment"># 隐藏层维度为5</span><br>hidden_size = <span class="hljs-number">5</span><br><span class="hljs-comment"># 输出层维度为1</span><br>output_size = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 定义网络</span><br>model = Model_MLP_L2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = BinaryCrossEntropyLoss(model)<br><br><span class="hljs-comment"># 优化器</span><br>learning_rate = <span class="hljs-number">0.2</span><br>optimizer = BatchGD(learning_rate, model)<br><br><span class="hljs-comment"># 评价方法</span><br>metric = accuracy<br><br><span class="hljs-comment"># 实例化RunnerV2_1类，并传入训练配置</span><br>runner = RunnerV2_1(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="hljs-number">50</span>, save_dir=model_saved_dir)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python">运行耗时: <span class="hljs-number">2</span>秒<span class="hljs-number">843</span>毫秒<br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.20000</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.7360124588012695</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.20000</span> --&gt; <span class="hljs-number">0.21875</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.21875</span> --&gt; <span class="hljs-number">0.29375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.29375</span> --&gt; <span class="hljs-number">0.32500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.32500</span> --&gt; <span class="hljs-number">0.39375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.39375</span> --&gt; <span class="hljs-number">0.44375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.44375</span> --&gt; <span class="hljs-number">0.67500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.67500</span> --&gt; <span class="hljs-number">0.70000</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.70000</span> --&gt; <span class="hljs-number">0.71250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.71250</span> --&gt; <span class="hljs-number">0.72500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.72500</span> --&gt; <span class="hljs-number">0.73125</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.73125</span> --&gt; <span class="hljs-number">0.74375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.74375</span> --&gt; <span class="hljs-number">0.75000</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.75000</span> --&gt; <span class="hljs-number">0.75625</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.75625</span> --&gt; <span class="hljs-number">0.76250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.76250</span> --&gt; <span class="hljs-number">0.76875</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.76875</span> --&gt; <span class="hljs-number">0.77500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.77500</span> --&gt; <span class="hljs-number">0.78125</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.78125</span> --&gt; <span class="hljs-number">0.78750</span><br>[Train] epoch: <span class="hljs-number">50</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.6630627512931824</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.78750</span> --&gt; <span class="hljs-number">0.79375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.79375</span> --&gt; <span class="hljs-number">0.80000</span><br>[Train] epoch: <span class="hljs-number">100</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.5919685959815979</span><br>[Train] epoch: <span class="hljs-number">150</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.5248624086380005</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80000</span> --&gt; <span class="hljs-number">0.80625</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80625</span> --&gt; <span class="hljs-number">0.81250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.81250</span> --&gt; <span class="hljs-number">0.81875</span><br>[Train] epoch: <span class="hljs-number">200</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.48363637924194336</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.81875</span> --&gt; <span class="hljs-number">0.82500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.82500</span> --&gt; <span class="hljs-number">0.83125</span><br>[Train] epoch: <span class="hljs-number">250</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.46238335967063904</span><br>[Train] epoch: <span class="hljs-number">300</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4515562951564789</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.83125</span> --&gt; <span class="hljs-number">0.83750</span><br>[Train] epoch: <span class="hljs-number">350</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44589540362358093</span><br>[Train] epoch: <span class="hljs-number">400</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44286662340164185</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.83750</span> --&gt; <span class="hljs-number">0.84375</span><br>[Train] epoch: <span class="hljs-number">450</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44121456146240234</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.84375</span> --&gt; <span class="hljs-number">0.85000</span><br>[Train] epoch: <span class="hljs-number">500</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44029098749160767</span><br>[Train] epoch: <span class="hljs-number">550</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43975430727005005</span><br>[Train] epoch: <span class="hljs-number">600</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4394233822822571</span><br>[Train] epoch: <span class="hljs-number">650</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43920236825942993</span><br>[Train] epoch: <span class="hljs-number">700</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4390408992767334</span><br>[Train] epoch: <span class="hljs-number">750</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4389124810695648</span><br>[Train] epoch: <span class="hljs-number">800</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43880319595336914</span><br>[Train] epoch: <span class="hljs-number">850</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4387057423591614</span><br>[Train] epoch: <span class="hljs-number">900</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43861618638038635</span><br>[Train] epoch: <span class="hljs-number">950</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43853235244750977</span><br></code></pre></td></tr></table></figure><p>可视化观察训练集与验证集的损失函数变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印训练集和验证集的损失</span><br>plt.figure()<br>plt.plot(<span class="hljs-built_in">range</span>(epoch_num), runner.train_loss, color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>plt.plot(<span class="hljs-built_in">range</span>(epoch_num), runner.dev_loss, color=<span class="hljs-string">&quot;#E20079&quot;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.legend(fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-loss2.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214205337207.png" alt="image-20221214213744716"></p><hr><h4 id="√-4-2-7-性能评价"><a href="#√-4-2-7-性能评价" class="headerlink" title="[√] 4.2.7 - 性能评价"></a>[√] 4.2.7 - 性能评价</h4><p>使用测试集对训练中的最优模型进行评价，观察模型的评价指标。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载训练好的模型</span><br>runner.load_model(model_saved_dir)<br><span class="hljs-comment"># 在测试集上对模型进行评价</span><br>score, loss = runner.evaluate([X_test, y_test])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] score/loss: <span class="hljs-number">0.8350</span>/<span class="hljs-number">0.4016</span><br></code></pre></td></tr></table></figure><p>从结果来看，模型在测试集上取得了较高的准确率。</p><p>下面对结果进行可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 均匀生成40000个数据点</span><br>x1, x2 = paddle.meshgrid(paddle.linspace(-math.pi, math.pi, <span class="hljs-number">200</span>), paddle.linspace(-math.pi, math.pi, <span class="hljs-number">200</span>))<br>x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 预测对应类别</span><br>y = runner.predict(x)<br>y = paddle.squeeze(paddle.cast((y&gt;=<span class="hljs-number">0.5</span>),dtype=<span class="hljs-string">&#x27;float32&#x27;</span>),axis=-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 绘制类别区域</span><br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.scatter(x[:,<span class="hljs-number">0</span>].tolist(), x[:,<span class="hljs-number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)<br><br>plt.scatter(X_train[:, <span class="hljs-number">0</span>].tolist(), X_train[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_train,axis=-<span class="hljs-number">1</span>).tolist())<br>plt.scatter(X_dev[:, <span class="hljs-number">0</span>].tolist(), X_dev[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_dev,axis=-<span class="hljs-number">1</span>).tolist())<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>].tolist(), X_test[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_test,axis=-<span class="hljs-number">1</span>).tolist())<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222637659.png" alt="image-20221214213927961"></p><hr><h3 id="√-4-3-自动梯度计算和预定义算子"><a href="#√-4-3-自动梯度计算和预定义算子" class="headerlink" title="[√] 4.3 - 自动梯度计算和预定义算子"></a>[√] 4.3 - 自动梯度计算和预定义算子</h3><p>虽然我们能够通过模块化的方式比较好地对神经网络进行组装，但是每个模块的梯度计算过程仍然十分繁琐且容易出错。在深度学习框架中，已经封装了自动梯度计算的功能，我们只需要聚焦模型架构，不再需要耗费精力进行计算梯度。</p><p>飞桨提供了<code>paddle.nn.Layer</code>类，来方便快速的实现自己的层和模型。模型和层都可以基于<code>paddle.nn.Layer</code>扩充实现，模型只是一种特殊的层。</p><p>继承了<code>paddle.nn.Layer</code>类的算子中，可以在内部直接调用其它继承<code>paddle.nn.Layer</code>类的算子，飞桨框架会自动识别算子中内嵌的<code>paddle.nn.Layer</code>类算子，并自动计算它们的梯度，并在优化时更新它们的参数。</p><hr><h4 id="√-4-3-1-利用预定义算子重新实现前馈神经网络"><a href="#√-4-3-1-利用预定义算子重新实现前馈神经网络" class="headerlink" title="[√] 4.3.1 - 利用预定义算子重新实现前馈神经网络"></a>[√] 4.3.1 - 利用预定义算子重新实现前馈神经网络</h4><p>下面我们使用Paddle的预定义算子来重新实现二分类任务。 主要使用到的预定义算子为<code>paddle.nn.Linear</code>：</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">class</span> paddle.nn.<span class="hljs-constructor">Linear(<span class="hljs-params">in_features</span>, <span class="hljs-params">out_features</span>, <span class="hljs-params">weight_attr</span>=None, <span class="hljs-params">bias_attr</span>=None, <span class="hljs-params">name</span>=None)</span><br></code></pre></td></tr></table></figure><p><code>paddle.nn.Linear</code>算子可以接受一个形状为[batch_size,∗,in_features]的<strong>输入张量</strong>，其中”∗”表示张量中可以有任意的其它额外维度，并计算它与形状为[in_features, out_features]的<strong>权重矩阵</strong>的乘积，然后生成形状为[batch_size,∗,out_features]的<strong>输出张量</strong>。 <code>paddle.nn.Linear</code>算子默认有偏置参数，可以通过<code>bias_attr=False</code>设置不带偏置。</p><hr><p>paddle.nn 目录下包含飞桨框架支持的神经网络层和相关函数的相关API。paddle.nn.functional下都是一些函数实现。</p><hr><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> paddle.nn.initializer <span class="hljs-keyword">import</span> Constant, Normal, Uniform<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V2</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V2, self).__init__()<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span><br>        <span class="hljs-comment"># 其中第一个参数（in_features）为线性层输入维度；第二个参数（out_features）为线性层输出维度</span><br>        <span class="hljs-comment"># weight_attr为权重参数属性，这里使用&#x27;paddle.nn.initializer.Normal&#x27;进行随机高斯分布初始化</span><br>        <span class="hljs-comment"># bias_attr为偏置参数属性，这里使用&#x27;paddle.nn.initializer.Constant&#x27;进行常量初始化</span><br>        self.fc1 = nn.Linear(input_size, hidden_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="hljs-number">0.</span>, std=<span class="hljs-number">1.</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        self.fc2 = nn.Linear(hidden_size, output_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="hljs-number">0.</span>, std=<span class="hljs-number">1.</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span><br>        self.act_fn = F.sigmoid<br>        <br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        z1 = self.fc1(inputs)<br>        a1 = self.act_fn(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-3-2-完善Runner类"><a href="#√-4-3-2-完善Runner类" class="headerlink" title="[√] 4.3.2 - 完善Runner类"></a>[√] 4.3.2 - 完善Runner类</h4><p>基于上一节实现的 <code>RunnerV2_1</code> 类，本节的 RunnerV2_2 类在训练过程中使用自动梯度计算；模型保存时，使用<code>state_dict</code>方法获取模型参数；模型加载时，使用<code>set_state_dict</code>方法加载模型参数.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV2_2</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, metric, loss_fn, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric<br><br>        <span class="hljs-comment"># 记录训练过程中的评估指标变化情况</span><br>        self.train_scores = []<br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.train_loss = []<br>        self.dev_loss = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_set, dev_set, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型切换为训练模式</span><br>        self.model.train()<br><br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_epochs = kwargs.get(<span class="hljs-string">&quot;log_epochs&quot;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span><br>        save_path = kwargs.get(<span class="hljs-string">&quot;save_path&quot;</span>, <span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br>        <span class="hljs-comment"># log打印函数，如果没有传入则默认为&quot;None&quot;</span><br>        custom_print_log = kwargs.get(<span class="hljs-string">&quot;custom_print_log&quot;</span>, <span class="hljs-literal">None</span>) <br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        best_score = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            X, y = train_set<br>            <span class="hljs-comment"># 获取模型预测</span><br>            logits = self.model(X)<br>            <span class="hljs-comment"># 计算交叉熵损失</span><br>            trn_loss = self.loss_fn(logits, y)<br>            self.train_loss.append(trn_loss.item())<br>            <span class="hljs-comment"># 计算评估指标</span><br>            trn_score = self.metric(logits, y).item()<br>            self.train_scores.append(trn_score)<br><br>            <span class="hljs-comment"># 自动计算参数梯度</span><br>            <span class="hljs-comment"># alec：此处通过trn_loss自动计算梯度</span><br>            trn_loss.backward()<br>            <span class="hljs-keyword">if</span> custom_print_log <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># 打印每一层的梯度</span><br>                custom_print_log(self)<br><br>            <span class="hljs-comment"># 参数更新</span><br>            self.optimizer.step()<br>            <span class="hljs-comment"># 清空梯度</span><br>            self.optimizer.clear_grad()<br><br>            dev_score, dev_loss = self.evaluate(dev_set)<br>            <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>            <span class="hljs-keyword">if</span> dev_score &gt; best_score:<br>                self.save_model(save_path)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                best_score = dev_score<br><br>            <span class="hljs-keyword">if</span> log_epochs <span class="hljs-keyword">and</span> epoch % log_epochs == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, loss: <span class="hljs-subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)<br>                <br>    <span class="hljs-comment"># 模型评估阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set</span>):<br>        <span class="hljs-comment"># 将模型切换为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br><br>        X, y = data_set<br>        <span class="hljs-comment"># 计算模型输出</span><br>        logits = self.model(X)<br>        <span class="hljs-comment"># 计算损失函数</span><br>        loss = self.loss_fn(logits, y).item()<br>        self.dev_loss.append(loss)<br>        <span class="hljs-comment"># 计算评估指标</span><br>        score = self.metric(logits, y).item()<br>        self.dev_scores.append(score)<br>        <span class="hljs-keyword">return</span> score, loss<br>    <br>    <span class="hljs-comment"># 模型测试阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 将模型切换为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">return</span> self.model(X)<br><br>    <span class="hljs-comment"># 使用&#x27;model.state_dict()&#x27;获取模型参数，并进行保存</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, saved_path</span>):<br>        paddle.save(self.model.state_dict(), saved_path)<br><br>    <span class="hljs-comment"># 使用&#x27;model.set_state_dict&#x27;加载模型参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        state_dict = paddle.load(model_path)<br>        self.model.set_state_dict(state_dict)<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-3-3-模型训练"><a href="#√-4-3-3-模型训练" class="headerlink" title="[√] 4.3.3 - 模型训练"></a>[√] 4.3.3 - 模型训练</h4><p>实例化RunnerV2类，并传入训练配置，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置模型</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size = <span class="hljs-number">5</span><br>output_size = <span class="hljs-number">1</span><br>model = Model_MLP_L2_V2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 设置损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 设置优化器</span><br>learning_rate = <span class="hljs-number">0.2</span><br>optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())<br><br><span class="hljs-comment"># 设置评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 其他参数</span><br>epoch_num = <span class="hljs-number">1000</span><br>saved_path = <span class="hljs-string">&#x27;best_model.pdparams&#x27;</span><br><br><span class="hljs-comment"># 实例化RunnerV2类，并传入训练配置</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="hljs-number">50</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure><p>将训练过程中训练集与验证集的准确率变化情况进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可视化观察训练集与验证集的指标变化情况</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">runner, fig_name</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>    epochs = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(runner.train_scores))]<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>    plt.plot(epochs, runner.train_loss, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    plt.plot(epochs, runner.dev_loss, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>    plt.plot(epochs, runner.train_scores, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train accuracy&quot;</span>)<br>    plt.plot(epochs, runner.dev_scores, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev accuracy&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;score&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    <br>    plt.savefig(fig_name)<br>    plt.show()<br><br>plot(runner, <span class="hljs-string">&#x27;fw-acc.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214224725288.png" alt="image-20221214221440242"></p><hr><h4 id="√-4-3-4-性能评价"><a href="#√-4-3-4-性能评价" class="headerlink" title="[√] 4.3.4 - 性能评价"></a>[√] 4.3.4 - 性能评价</h4><p>使用测试数据对训练完成后的最优模型进行评价，观察模型在测试集上的准确率以及loss情况。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型评价</span><br>runner.load_model(<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br>score, loss = runner.evaluate([X_test, y_test])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">运行耗时: <span class="hljs-number">7</span>毫秒<br>[Test] score/loss: <span class="hljs-number">0.8300</span>/<span class="hljs-number">0.4035</span><br></code></pre></td></tr></table></figure><p>从结果来看，模型在测试集上取得了较高的准确率。</p><hr><h3 id="√-4-4-优化问题"><a href="#√-4-4-优化问题" class="headerlink" title="[√] 4.4 - 优化问题"></a>[√] 4.4 - 优化问题</h3><p>在本节中，我们通过实践来发现神经网络模型的优化问题，并思考如何改进。</p><hr><h4 id="√-4-4-1-参数初始化"><a href="#√-4-4-1-参数初始化" class="headerlink" title="[√] 4.4.1 - 参数初始化"></a>[√] 4.4.1 - 参数初始化</h4><p>实现一个神经网络前，需要先初始化模型参数。如果对每一层的权重和偏置都用0初始化，那么通过第一遍前向计算，所有隐藏层神经元的激活值都相同；在反向传播时，所有权重的更新也都相同，这样会导致隐藏层神经元没有差异性，出现<strong>对称权重现象</strong>。</p><p>接下来，将模型参数全都初始化为0，看实验结果。这里重新定义了一个类<code>TwoLayerNet_Zeros</code>，两个线性层的参数全都初始化为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> paddle.nn.initializer <span class="hljs-keyword">import</span> Constant, Normal, Uniform<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V4</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V4, self).__init__()<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span><br>        <span class="hljs-comment"># 其中in_features为线性层输入维度；out_features为线性层输出维度</span><br>        <span class="hljs-comment"># weight_attr为权重参数属性</span><br>        <span class="hljs-comment"># bias_attr为偏置参数属性</span><br>        self.fc1 = nn.Linear(input_size, hidden_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        self.fc2 = nn.Linear(hidden_size, output_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span><br>        self.act_fn = F.sigmoid<br>        <br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        z1 = self.fc1(inputs)<br>        a1 = self.act_fn(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_weights</span>(<span class="hljs-params">runner</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The weights of the Layers：&#x27;</span>)<br>    <br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> runner.model.sublayers():<br>        <span class="hljs-built_in">print</span>(item.full_name())<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> item.parameters():<br>            <span class="hljs-built_in">print</span>(param.numpy())<br>        <br></code></pre></td></tr></table></figure><p>利用Runner类训练模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置模型</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size = <span class="hljs-number">5</span><br>output_size = <span class="hljs-number">1</span><br>model = Model_MLP_L2_V4(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 设置损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 设置优化器</span><br>learning_rate = <span class="hljs-number">0.2</span> <span class="hljs-comment">#5e-2</span><br>optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())<br><br><span class="hljs-comment"># 设置评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 其他参数</span><br>epoch = <span class="hljs-number">2000</span><br>saved_path = <span class="hljs-string">&#x27;best_model.pdparams&#x27;</span><br><br><span class="hljs-comment"># 实例化RunnerV2类，并传入训练配置</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">5</span>, log_epochs=<span class="hljs-number">50</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>,custom_print_log=print_weights)<br></code></pre></td></tr></table></figure><p>可视化训练和验证集上的主准确率和loss变化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot(runner, <span class="hljs-string">&quot;fw-zero.pdf&quot;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214221440242.png" alt="image-20221214222430247"></p><p>&#x3D;&#x3D;从输出结果看，二分类准确率为50%左右，说明模型没有学到任何内容。训练和验证loss几乎没有怎么下降。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;为了避免对称权重现象，可以使用高斯分布或均匀分布初始化神经网络的参数。&#x3D;&#x3D;</p><p>高斯分布和均匀分布采样的实现和可视化代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用&#x27;paddle.normal&#x27;实现高斯分布采样，其中&#x27;mean&#x27;为高斯分布的均值，&#x27;std&#x27;为高斯分布的标准差，&#x27;shape&#x27;为输出形状</span><br>gausian_weights = paddle.normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>, shape=[<span class="hljs-number">10000</span>])<br><span class="hljs-comment"># 使用&#x27;paddle.uniform&#x27;实现在[min,max)范围内的均匀分布采样，其中&#x27;shape&#x27;为输出形状</span><br>uniform_weights = paddle.uniform(shape=[<span class="hljs-number">10000</span>], <span class="hljs-built_in">min</span>=- <span class="hljs-number">1.0</span>, <span class="hljs-built_in">max</span>=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-comment"># 绘制两种参数分布</span><br>plt.figure()<br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>plt.title(<span class="hljs-string">&#x27;Gausian Distribution&#x27;</span>)<br>plt.hist(gausian_weights, bins=<span class="hljs-number">200</span>, density=<span class="hljs-literal">True</span>, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>)<br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>plt.title(<span class="hljs-string">&#x27;Uniform Distribution&#x27;</span>)<br>plt.hist(uniform_weights, bins=<span class="hljs-number">200</span>, density=<span class="hljs-literal">True</span>, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-gausian-uniform.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160241372.png" alt="image-20221214222637659"></p><blockquote><p>alec总结：</p><p>如果网络的参数初始化的时候，将其全部初始化为0，那么将学不到任何东西。因此需要通过高斯分布、均匀分布等初始化，而不能直接全零初始化。</p></blockquote><hr><h4 id="√-4-4-2-梯度消失问题"><a href="#√-4-4-2-梯度消失问题" class="headerlink" title="[√] 4.4.2 - 梯度消失问题"></a>[√] 4.4.2 - 梯度消失问题</h4><blockquote><p>alec记录：</p><p>（1）随着网络层数的加深，容易出现梯度消失的问题。</p><p>（2）对于sigmoid型的激活函数，这种函数在饱和区（x趋向于很大或者很小）的时候，梯度很小，再加上网络层数很深，误差不断的衰减，因此最终梯度很小，就出现了梯度消失问题。解决这种问题的简单有效的方法之一是使用导数比较大的激活函数，比如ReLU激活函数。（ReLU激活函数在x为正的时候，梯度始终等于1，因此能够处理梯度消失问题。）</p></blockquote><p>在神经网络的构建过程中，随着网络层数的增加，理论上网络的拟合能力也应该是越来越好的。但是随着网络变深，参数学习更加困难，容易出现梯度消失问题。</p><p>由于Sigmoid型函数的饱和性，饱和区的导数更接近于0，误差经过每一层传递都会不断衰减。当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练，这就是所谓的梯度消失问题。 在深度神经网络中，减轻梯度消失问题的方法有很多种，一种简单有效的方式就是使用导数比较大的激活函数，如：ReLU。</p><p>下面通过一个简单的实验观察前馈神经网络的梯度消失现象和改进方法。</p><hr><p>#####[√] 4.4.2.1 - 模型构建</p><p>定义一个前馈神经网络，包含4个隐藏层和1个输出层，通过传入的参数指定激活函数。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义多层前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L5</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, act=<span class="hljs-string">&#x27;sigmoid&#x27;</span>, w_init=Normal(<span class="hljs-params">mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span></span>), b_init=Constant(<span class="hljs-params">value=<span class="hljs-number">1.0</span></span>)</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L5, self).__init__()<br>        self.fc1 = paddle.nn.Linear(input_size, <span class="hljs-number">3</span>)<br>        self.fc2 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc3 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc4 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc5 = paddle.nn.Linear(<span class="hljs-number">3</span>, output_size)<br>        <span class="hljs-comment"># 定义网络使用的激活函数</span><br>        <span class="hljs-keyword">if</span> act == <span class="hljs-string">&#x27;sigmoid&#x27;</span>:<br>            self.act = F.sigmoid<br>        <span class="hljs-keyword">elif</span> act == <span class="hljs-string">&#x27;relu&#x27;</span>:<br>            self.act = F.relu<br>        <span class="hljs-keyword">elif</span> act == <span class="hljs-string">&#x27;lrelu&#x27;</span>:<br>            self.act = F.leaky_relu<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Please enter sigmoid, relu or lrelu!&quot;</span>)<br>        <span class="hljs-comment"># 初始化线性层权重和偏置参数</span><br>        self.init_weights(w_init, b_init)<br><br>    <span class="hljs-comment"># 初始化线性层权重和偏置参数（初始化的是线性层的参数，不含激活层（非线性层）的参数）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">self, w_init, b_init</span>):<br>        <span class="hljs-comment"># 使用&#x27;named_sublayers&#x27;遍历所有网络层</span><br>        <span class="hljs-keyword">for</span> n, m <span class="hljs-keyword">in</span> self.named_sublayers():<br>            <span class="hljs-comment"># 如果是线性层，则使用指定方式进行参数初始化</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                w_init(m.weight)<br>                b_init(m.bias)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc1(inputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc2(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc3(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc4(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc5(outputs)<br>        outputs = F.sigmoid(outputs)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-4-2-2-使用Sigmoid型函数进行训练"><a href="#√-4-4-2-2-使用Sigmoid型函数进行训练" class="headerlink" title="[√] 4.4.2.2 - 使用Sigmoid型函数进行训练"></a>[√] 4.4.2.2 - 使用Sigmoid型函数进行训练</h5><p>使用Sigmoid型函数作为激活函数，为了便于观察梯度消失现象，只进行一轮网络优化。代码实现如下：</p><p>定义梯度打印函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_grads</span>(<span class="hljs-params">runner</span>):<br>    <span class="hljs-comment"># 打印每一层的权重的模</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The gradient of the Layers：&#x27;</span>)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> runner.model.sublayers():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(item.parameters())==<span class="hljs-number">2</span>:<br>            <span class="hljs-built_in">print</span>(item.full_name(), paddle.norm(item.parameters()[<span class="hljs-number">0</span>].grad, p=<span class="hljs-number">2.</span>).numpy()[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">102</span>)<br><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 定义网络，激活函数使用sigmoid</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数，使用交叉熵损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 定义评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 指定梯度打印函数</span><br>custom_print_log=print_grads<br></code></pre></td></tr></table></figure><p>实例化RunnerV2_2类，并传入训练配置。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br></code></pre></td></tr></table></figure><p>模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_2 <span class="hljs-number">0.0007373723</span><br>linear_3 <span class="hljs-number">0.006756582</span><br></code></pre></td></tr></table></figure><p>观察实验结果可以发现，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。</p><blockquote><p>alec分析：</p><p>使用sigmoid型的激活函数，随着网络的加深，出现了梯度消失的现象，这可能导致网络无法继续学习下去。因此梯度为0，无法更新参数了。</p></blockquote><hr><h5 id="√-4-4-2-3-使用ReLU函数进行模型训练"><a href="#√-4-4-2-3-使用ReLU函数进行模型训练" class="headerlink" title="[√] 4.4.2.3 - 使用ReLU函数进行模型训练"></a>[√] 4.4.2.3 - 使用ReLU函数进行模型训练</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">102</span>)<br>lr = <span class="hljs-number">0.01</span>  <span class="hljs-comment"># 学习率大小</span><br><br><span class="hljs-comment"># 定义网络，激活函数使用relu</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;relu&#x27;</span>) <span class="hljs-comment"># 使用ReLU作为激活函数</span><br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-comment"># 定义损失函数，这里使用交叉熵损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 定义评估指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_9 <span class="hljs-number">4.828196e-08</span><br>linear_10 <span class="hljs-number">3.2609435e-06</span><br>linear_11 <span class="hljs-number">0.0001859922</span><br>linear_12 <span class="hljs-number">0.011442569</span><br>linear_13 <span class="hljs-number">0.43247733</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.53750</span><br></code></pre></td></tr></table></figure><p><strong>图4.4</strong> 展示了使用不同激活函数时，网络每层梯度值的ℓ2\ell_2ℓ2范数情况。从结果可以看到，5层的全连接前馈神经网络使用Sigmoid型函数作为激活函数时，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。改为ReLU激活函数后，梯度消失现象得到了缓解，每一层的参数都具有梯度值。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213927961.png" alt="image-20221214224725288"></p><blockquote><p>alec结论：</p><p>使用ReLU激活函数能够较好的解决使用sigmoid激活函数导致的梯度消失问题。</p></blockquote><hr><h4 id="√-4-4-3-死亡-ReLU-问题"><a href="#√-4-4-3-死亡-ReLU-问题" class="headerlink" title="[√] 4.4.3 - 死亡 ReLU 问题"></a>[√] 4.4.3 - 死亡 ReLU 问题</h4><p>&#x3D;&#x3D;ReLU函数的利与弊：ReLU激活函数可以一定程度上改善梯度消失问题，但是ReLU函数在某些情况下容易出现死亡 ReLU问题，使得网络难以训练。&#x3D;&#x3D;</p><p>这是由于当$x&lt;0$时，ReLU函数的输出恒为0。在训练过程中，如果参数在一次不恰当的更新后，某个ReLU神经元在所有训练数据上都不能被激活（即输出为0），那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远都不能被激活。而一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU的变种。</p><blockquote><p>alec：</p><p>因为ReLU函数在x轴的负数部分恒为0，因此这就导致梯度永远是0，出现了死亡梯度问题。</p><p>解决这个问题的方法就是，将负半轴改变，例如可以使用Leaky ReLU、ELU等ReLU的变种。</p></blockquote><hr><h5 id="√-4-4-3-1-使用ReLU进行模型训练"><a href="#√-4-4-3-1-使用ReLU进行模型训练" class="headerlink" title="[√] 4.4.3.1 - 使用ReLU进行模型训练"></a>[√] 4.4.3.1 - 使用ReLU进行模型训练</h5><p>使用第4.4.2节中定义的多层全连接前馈网络进行实验，使用ReLU作为激活函数，观察死亡ReLU现象和优化方法。当神经层的偏置被初始化为一个相对于权重较大的负值时，可以想像，输入经过神经层的处理，最终的输出会为负值，从而导致死亡ReLU现象。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义网络，并使用较大的负值来初始化偏置</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;relu&#x27;</span>, b_init=Constant(value=-<span class="hljs-number">8.0</span>))<br></code></pre></td></tr></table></figure><p>实例化RunnerV2类，启动模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-number">0</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_14 <span class="hljs-number">0.0</span><br>linear_15 <span class="hljs-number">0.0</span><br>linear_16 <span class="hljs-number">0.0</span><br>linear_17 <span class="hljs-number">0.0</span><br>linear_18 <span class="hljs-number">0.0</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.50625</span><br></code></pre></td></tr></table></figure><p>从输出结果可以发现，使用 ReLU 作为激活函数，当满足条件时，会发生死亡ReLU问题，网络训练过程中 ReLU 神经元的梯度始终为0，参数无法更新。</p><p>针对死亡ReLU问题，一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU 的变种。接下来，观察将激活函数更换为 Leaky ReLU时的梯度情况。</p><hr><h5 id="√-4-4-3-2-使用Leaky-ReLU进行模型训练"><a href="#√-4-4-3-2-使用Leaky-ReLU进行模型训练" class="headerlink" title="[√] 4.4.3.2 - 使用Leaky ReLU进行模型训练"></a>[√] 4.4.3.2 - 使用Leaky ReLU进行模型训练</h5><p>将激活函数更换为Leaky ReLU进行模型训练，观察梯度情况。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 重新定义网络，使用Leaky ReLU激活函数</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;lrelu&#x27;</span>, b_init=Constant(value=-<span class="hljs-number">8.0</span>))<br><br><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochps=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_19 <span class="hljs-number">2.0111758e-16</span><br>linear_20 <span class="hljs-number">1.8527564e-13</span><br>linear_21 <span class="hljs-number">1.6659853e-09</span><br>linear_22 <span class="hljs-number">1.1705935e-05</span><br>linear_23 <span class="hljs-number">0.06902571</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.50625</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">1</span>, loss: <span class="hljs-number">3.988154649734497</span><br></code></pre></td></tr></table></figure><p>从输出结果可以看到，将激活函数更换为Leaky ReLU后，死亡ReLU问题得到了改善，梯度恢复正常，参数也可以正常更新。但是由于 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率默认只有0.01，所以反向传播时，随着网络层数的加深，梯度值越来越小。如果想要改善这一现象，将 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率调大即可。</p><h3 id="√-4-5-实践：基于前馈神经网络完成鸢尾花分类"><a href="#√-4-5-实践：基于前馈神经网络完成鸢尾花分类" class="headerlink" title="[√] 4.5 - 实践：基于前馈神经网络完成鸢尾花分类"></a>[√] 4.5 - 实践：基于前馈神经网络完成鸢尾花分类</h3><p>在本实践中，我们继续使用第三章中的鸢尾花分类任务，将Softmax分类器替换为本章介绍的前馈神经网络。 在本实验中，我们使用的损失函数为交叉熵损失；优化器为随机梯度下降法；评价指标为准确率。</p><hr><h4 id="√-4-5-1-小批量梯度下降法"><a href="#√-4-5-1-小批量梯度下降法" class="headerlink" title="[√] 4.5.1 - 小批量梯度下降法"></a>[√] 4.5.1 - 小批量梯度下降法</h4><p>在梯度下降法中，目标函数是整个训练集上的风险函数，这种方式称为<strong>批量梯度下降法（Batch Gradient Descent，BGD）</strong>。 批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和。当训练集中的样本数量NN<em>N</em>很大时，空间复杂度比较高，每次迭代的计算开销也很大。</p><p>为了减少每次迭代的计算复杂度，我们可以在每次迭代时只采集一小部分样本，计算在这组样本上损失函数的梯度并更新参数，这种优化方式称为 小批量梯度下降法（Mini-Batch Gradient Descent，Mini-Batch GD）。</p><p>第$t$次迭代时，随机选取一个包含$K$个样本的子集$\mathcal{B}<em>t$，计算这个子集上每个样本损失函数的梯度并进行平均，然后再进行参数更新。<br>$$<br>\theta</em>{t+1} \leftarrow \theta_t  - \alpha \frac{1}{K} \sum_{(\boldsymbol{x},y)\in \mathcal{S}_t} \frac{\partial \mathcal{L}\Big(y,f(\boldsymbol{x};\theta)\Big)}{\partial \theta},<br>$$<br>&#x3D;&#x3D;其中$K$为**批量大小(Batch Size)**。$K$通常不会设置很大，一般在$1\sim100$之间。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;在实际应用中为了提高计算效率，通常设置为2的幂$2^n$。&#x3D;&#x3D;</p><p>&#x3D;&#x3D;在实际应用中，小批量随机梯度下降法有收敛快、计算开销小的优点，因此逐渐成为大规模的机器学习中的主要优化算法。&#x3D;&#x3D;<br>&#x3D;&#x3D;此外，随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声。在非凸优化问题中，随机梯度下降更容易逃离局部最优点。&#x3D;&#x3D;</p><blockquote><p>alec:</p><p>小批量梯度下降收敛快、开销小。</p><p>随机梯度下降在梯度下降的基础上引入了随机噪声，因此更加容易逃离局部最优点。</p></blockquote><p>小批量随机梯度下降法的训练过程如下：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215175316781.png" alt="image-20221215160106452"></p><hr><h5 id="√-4-5-1-1-数据分组"><a href="#√-4-5-1-1-数据分组" class="headerlink" title="[√] 4.5.1.1 - 数据分组"></a>[√] 4.5.1.1 - 数据分组</h5><p>&#x3D;&#x3D;为了小批量梯度下降法，我们需要对数据进行随机分组。目前，机器学习中通常做法是构建一个数据迭代器，每个迭代过程中从全部数据集中获取一批指定数量的数据。&#x3D;&#x3D;</p><p>数据迭代器的实现原理如下图所示：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160106452.png" alt="image-20221215160241372"></p><ol><li>首先，将数据集封装为Dataset类，传入一组索引值，根据索引从数据集合中获取数据；</li><li>其次，构建DataLoader类，需要指定数据批量的大小和是否需要对数据进行乱序，通过该类即可批量获取数据。</li></ol><p>在实践过程中，通常使用进行参数优化。在飞桨中，使用<code>paddle.io.DataLoader</code>加载minibatch的数据， <code>paddle.io.DataLoader</code> API可以生成一个迭代器，其中通过设置<code>batch_size</code>参数来指定minibatch的长度，通过设置shuffle参数为True，可以在生成<code>minibatch</code>的索引列表时将索引顺序打乱。</p><hr><h4 id="√-4-5-2-数据处理"><a href="#√-4-5-2-数据处理" class="headerlink" title="[√] 4.5.2 - 数据处理"></a>[√] 4.5.2 - 数据处理</h4><p>构造IrisDataset类进行数据读取，继承自<code>paddle.io.Dataset</code>类。<code>paddle.io.Dataset</code>是用来封装 Dataset的方法和行为的抽象类，通过一个索引获取指定的样本，同时对该样本进行数据处理。当继承<code>paddle.io.Dataset</code>来定义数据读取类时，实现如下方法：</p><ul><li><code>__getitem__</code>：根据给定索引获取数据集中指定样本，并对样本进行数据处理；</li><li><code>__len__</code>：返回数据集样本个数。</li></ul><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">from</span> nndl.dataset <span class="hljs-keyword">import</span> load_data<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IrisDataset</span>(io.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&#x27;train&#x27;</span>, num_train=<span class="hljs-number">120</span>, num_dev=<span class="hljs-number">15</span></span>):<br>        <span class="hljs-built_in">super</span>(IrisDataset, self).__init__()<br>        <span class="hljs-comment"># 调用第三章中的数据读取函数，其中不需要将标签转成one-hot类型</span><br>        X, y = load_data(shuffle=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.X, self.y = X[:num_train], y[:num_train]<br>        <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;dev&#x27;</span>:<br>            self.X, self.y = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>        <span class="hljs-keyword">else</span>:<br>            self.X, self.y = X[num_train + num_dev:], y[num_train + num_dev:]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.X[idx], self.y[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.y)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">12</span>)<br>train_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>dev_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;dev&#x27;</span>)<br>test_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印训练集长度</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;length of train set: &quot;</span>, <span class="hljs-built_in">len</span>(train_dataset))<br></code></pre></td></tr></table></figure><hr><h5 id="√-4-5-2-2-用DataLoader进行封装"><a href="#√-4-5-2-2-用DataLoader进行封装" class="headerlink" title="[√] 4.5.2.2 - 用DataLoader进行封装"></a>[√] 4.5.2.2 - 用DataLoader进行封装</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批量大小</span><br>batch_size = <span class="hljs-number">16</span><br><br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size)<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-5-3-模型构建"><a href="#√-4-5-3-模型构建" class="headerlink" title="[√] 4.5.3 - 模型构建"></a>[√] 4.5.3 - 模型构建</h4><p>构建一个简单的前馈神经网络进行鸢尾花分类实验。其中输入层神经元个数为4，输出层神经元个数为3，隐含层神经元个数为6。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment"># 定义前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V3</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, hidden_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V3, self).__init__()<br>        <span class="hljs-comment"># 构建第一个全连接层</span><br>        self.fc1 = nn.Linear(<br>            input_size,<br>            hidden_size,<br>            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)),<br>            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">1.0</span>))<br>        )<br>        <span class="hljs-comment"># 构建第二全连接层</span><br>        self.fc2 = nn.Linear(<br>            hidden_size,<br>            output_size,<br>            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)),<br>            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">1.0</span>))<br>        )<br>        <span class="hljs-comment"># 定义网络使用的激活函数</span><br>        self.act = nn.Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc1(inputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc2(outputs)<br>        <span class="hljs-keyword">return</span> outputs<br><br>fnn_model = Model_MLP_L2_V3(input_size=<span class="hljs-number">4</span>, output_size=<span class="hljs-number">3</span>, hidden_size=<span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-5-4-完善Runner类"><a href="#√-4-5-4-完善Runner类" class="headerlink" title="[√] 4.5.4 - 完善Runner类"></a>[√] 4.5.4 - 完善Runner类</h4><p>&#x3D;&#x3D;基于RunnerV2类进行完善实现了RunnerV3类。其中训练过程使用自动梯度计算&#x3D;&#x3D;</p><p>&#x3D;&#x3D;使用<code>DataLoader</code>加载批量数据&#x3D;&#x3D;</p><p>&#x3D;&#x3D;使用随机梯度下降法进行参数优化&#x3D;&#x3D;</p><p>模型保存时，使用<code>state_dict</code>方法获取模型参数</p><p>模型加载时，使用<code>set_state_dict</code>方法加载模型参数.</p><p>&#x3D;&#x3D;由于这里使用随机梯度下降法对参数优化，所以数据以批次的形式输入到模型中进行训练&#x3D;&#x3D;</p><p>&#x3D;&#x3D;那么评价指标计算也是分别在每个批次进行的&#x3D;&#x3D;</p><p>&#x3D;&#x3D;要想获得每个epoch整体的评价结果，需要对历史评价结果进行累积。&#x3D;&#x3D;</p><p>这里定义<code>Accuracy</code>类实现该功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.metric <span class="hljs-keyword">import</span> Metric<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Accuracy</span>(<span class="hljs-title class_ inherited__">Metric</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, is_logist=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - is_logist: outputs是logist还是激活后的值</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 用于统计正确的样本个数</span><br>        self.num_correct = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 用于统计样本的总数</span><br>        self.num_count = <span class="hljs-number">0</span><br><br>        self.is_logist = is_logist<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, outputs, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - outputs: 预测值, shape=[N,class_num]</span><br><span class="hljs-string">           - labels: 标签值, shape=[N,1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 判断是二分类任务还是多分类任务，shape[1]=1时为二分类任务，shape[1]&gt;1时为多分类任务</span><br>        <span class="hljs-keyword">if</span> outputs.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>: <span class="hljs-comment"># 二分类</span><br>            outputs = paddle.squeeze(outputs, axis=-<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.is_logist:<br>                <span class="hljs-comment"># logist判断是否大于0</span><br>                preds = paddle.cast((outputs&gt;=<span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 如果不是logist，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span><br>                preds = paddle.cast((outputs&gt;=<span class="hljs-number">0.5</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span><br>            preds = paddle.argmax(outputs, axis=<span class="hljs-number">1</span>, dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br><br>        <span class="hljs-comment"># 获取本批数据中预测正确的样本个数</span><br>        labels = paddle.squeeze(labels, axis=-<span class="hljs-number">1</span>)<br>        batch_correct = paddle.<span class="hljs-built_in">sum</span>(paddle.cast(preds==labels, dtype=<span class="hljs-string">&quot;float32&quot;</span>)).numpy()[<span class="hljs-number">0</span>]<br>        batch_count = <span class="hljs-built_in">len</span>(labels)<br><br>        <span class="hljs-comment"># 更新num_correct 和 num_count</span><br>        self.num_correct += batch_correct<br>        self.num_count += batch_count<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accumulate</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 使用累计的数据，计算总的指标</span><br>        <span class="hljs-keyword">if</span> self.num_count == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> self.num_correct / self.num_count<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 重置正确的数目和总数</span><br>        self.num_correct = <span class="hljs-number">0</span><br>        self.num_count = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Accuracy&quot;</span><br><br></code></pre></td></tr></table></figure><p>RunnerV3类的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV3</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, loss_fn, metric, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric <span class="hljs-comment"># 只用于计算评价指标</span><br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的损失函数变化情况</span><br>        self.train_epoch_losses = [] <span class="hljs-comment"># 一个epoch记录一次loss</span><br>        self.train_step_losses = []  <span class="hljs-comment"># 一个step记录一次loss</span><br>        self.dev_losses = []<br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        self.best_score = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_loader, dev_loader=<span class="hljs-literal">None</span>, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型切换为训练模式</span><br>        self.model.train()<br><br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_steps = kwargs.get(<span class="hljs-string">&quot;log_steps&quot;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-comment"># 评价频率</span><br>        eval_steps = kwargs.get(<span class="hljs-string">&quot;eval_steps&quot;</span>, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span><br>        save_path = kwargs.get(<span class="hljs-string">&quot;save_path&quot;</span>, <span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br>        custom_print_log = kwargs.get(<span class="hljs-string">&quot;custom_print_log&quot;</span>, <span class="hljs-literal">None</span>) <br>       <br>        <span class="hljs-comment"># 训练总的步数</span><br>        <span class="hljs-comment"># len(train_loader)为 mini-batch 的数量</span><br>        num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_loader)<br><br>        <span class="hljs-keyword">if</span> eval_steps:<br>            <span class="hljs-keyword">if</span> self.metric <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Error: Metric can not be None!&#x27;</span>)<br>            <span class="hljs-keyword">if</span> dev_loader <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Error: dev_loader can not be None!&#x27;</span>)<br>            <br>        <span class="hljs-comment"># 运行的step数目</span><br>        global_step = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            <span class="hljs-comment"># 用于统计训练集的损失</span><br>            total_loss = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>                X, y = data<br>                <span class="hljs-comment"># 获取模型预测</span><br>                logits = self.model(X)<br>                loss = self.loss_fn(logits, y) <span class="hljs-comment"># 默认求mean</span><br>                total_loss += loss <span class="hljs-comment"># 统计累加损失</span><br><br>                <span class="hljs-comment"># 训练过程中，每个step的loss进行保存</span><br>                self.train_step_losses.append((global_step,loss.item()))<br><br>                <span class="hljs-keyword">if</span> log_steps <span class="hljs-keyword">and</span> global_step%log_steps==<span class="hljs-number">0</span>:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, step: <span class="hljs-subst">&#123;global_step&#125;</span>/<span class="hljs-subst">&#123;num_training_steps&#125;</span>, loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                <br>                <span class="hljs-comment"># 梯度反向传播，计算每个参数的梯度值</span><br>                <span class="hljs-comment"># alec：计算每个参数的梯度</span><br>                loss.backward() <br><br>                <span class="hljs-keyword">if</span> custom_print_log:<br>                   custom_print_log(self)<br>                <br>                <span class="hljs-comment"># 小批量梯度下降进行参数更新</span><br>                <span class="hljs-comment"># 更新参数</span><br>                self.optimizer.step()<br>                <span class="hljs-comment"># 梯度归零</span><br>                <span class="hljs-comment"># 梯度归零</span><br>                self.optimizer.clear_grad()<br><br>                <span class="hljs-comment"># 判断是否需要评价</span><br>                <span class="hljs-keyword">if</span> eval_steps&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> global_step&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> \<br>                    (global_step%eval_steps == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> global_step==(num_training_steps-<span class="hljs-number">1</span>)):<br><br>                    dev_score, dev_loss = self.evaluate(dev_loader, global_step=global_step)<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate]  dev score: <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>, dev loss: <span class="hljs-subst">&#123;dev_loss:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>) <br><br>                    <span class="hljs-comment"># 将模型切换为训练模式</span><br>                    self.model.train()<br><br>                    <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>                    <span class="hljs-comment"># 通过计算在验证集上的验证得分来判断是否是最优的模型</span><br>                    <span class="hljs-keyword">if</span> dev_score &gt; self.best_score:<br>                        self.save_model(save_path)<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;self.best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                        self.best_score = dev_score<br><br>                global_step += <span class="hljs-number">1</span><br>            <br>            <span class="hljs-comment"># 当前epoch 训练loss累计值 </span><br>            <span class="hljs-comment"># 当前epoch的损失的平均值</span><br>            trn_loss = (total_loss / <span class="hljs-built_in">len</span>(train_loader)).item()<br>            <span class="hljs-comment"># epoch粒度的训练loss保存</span><br>            self.train_epoch_losses.append(trn_loss)<br>            <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Train] Training done!&quot;</span>)<br><br>    <span class="hljs-comment"># 模型评估阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br>    <span class="hljs-comment"># 评估阶段，不需要计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, dev_loader, **kwargs</span>):<br>        <span class="hljs-keyword">assert</span> self.metric <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 将模型设置为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br><br>        global_step = kwargs.get(<span class="hljs-string">&quot;global_step&quot;</span>, -<span class="hljs-number">1</span>) <br><br>        <span class="hljs-comment"># 用于统计训练集的损失</span><br>        total_loss = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 重置评价</span><br>        self.metric.reset() <br>        <br>        <span class="hljs-comment"># 遍历验证集每个批次  </span><br>        <span class="hljs-comment"># alec：第一个参数是指第几个mini-batch，第二个参数是存储该批次的数据对</span><br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dev_loader):<br>            X, y = data<br>    <br>            <span class="hljs-comment"># 计算模型输出</span><br>            logits = self.model(X)<br>            <br>            <span class="hljs-comment"># 计算损失函数</span><br>            loss = self.loss_fn(logits, y).item()<br>            <span class="hljs-comment"># 累积损失</span><br>            total_loss += loss <br><br>            <span class="hljs-comment"># 累积评价</span><br>            self.metric.update(logits, y)<br><br>        dev_loss = (total_loss/<span class="hljs-built_in">len</span>(dev_loader))<br>        dev_score = self.metric.accumulate() <br><br>        <span class="hljs-comment"># 记录验证集loss</span><br>        <span class="hljs-keyword">if</span> global_step!=-<span class="hljs-number">1</span>:<br>            self.dev_losses.append((global_step, dev_loss))<br>            self.dev_scores.append(dev_score)<br>        <br>        <span class="hljs-keyword">return</span> dev_score, dev_loss<br>    <br>    <span class="hljs-comment"># 模型预测阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型设置为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-comment"># 运行模型前向计算，得到预测值</span><br>        logits = self.model(x)<br>        <span class="hljs-keyword">return</span> logits<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_path</span>):<br>        paddle.save(self.model.state_dict(), save_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        model_state_dict = paddle.load(model_path)<br>        self.model.set_state_dict(model_state_dict)<br></code></pre></td></tr></table></figure><hr><h4 id="√-4-5-5-模型训练"><a href="#√-4-5-5-模型训练" class="headerlink" title="[√] 4.5.5 - 模型训练"></a>[√] 4.5.5 - 模型训练</h4><p>实例化RunnerV3类，并传入训练配置，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> opt<br><br>lr = <span class="hljs-number">0.2</span><br><br><span class="hljs-comment"># 定义网络</span><br>model = fnn_model<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数。softmax+交叉熵</span><br>loss_fn = F.cross_entropy<br><br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy(is_logist=<span class="hljs-literal">True</span>)<br><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br></code></pre></td></tr></table></figure><p>使用训练集和验证集进行模型训练，共训练150个epoch。在实验中，保存准确率最高的模型作为最佳模型。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">100</span><br>eval_steps = <span class="hljs-number">50</span><br>runner.train(train_loader, dev_loader, <br>            num_epochs=<span class="hljs-number">150</span>, log_steps=log_steps, eval_steps = eval_steps,<br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>) <br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.09866</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.09026</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.40000</span><br>[Train] epoch: <span class="hljs-number">12</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">100</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.12618</span><br>[Evaluate]  dev score: <span class="hljs-number">0.33333</span>, dev loss: <span class="hljs-number">1.08850</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.08677</span><br>[Train] epoch: <span class="hljs-number">25</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">200</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.08003</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.08576</span><br>[Evaluate]  dev score: <span class="hljs-number">0.26667</span>, dev loss: <span class="hljs-number">1.09782</span><br>[Train] epoch: <span class="hljs-number">37</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">300</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.10121</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.06987</span><br>[Evaluate]  dev score: <span class="hljs-number">0.73333</span>, dev loss: <span class="hljs-number">1.03954</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.40000</span> --&gt; <span class="hljs-number">0.73333</span><br>[Train] epoch: <span class="hljs-number">50</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">400</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.94351</span><br>[Evaluate]  dev score: <span class="hljs-number">0.53333</span>, dev loss: <span class="hljs-number">0.93444</span><br>[Evaluate]  dev score: <span class="hljs-number">0.66667</span>, dev loss: <span class="hljs-number">0.79328</span><br>[Train] epoch: <span class="hljs-number">62</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">500</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.67799</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.64378</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.73333</span> --&gt; <span class="hljs-number">1.00000</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.54416</span><br>[Train] epoch: <span class="hljs-number">75</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">600</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.35588</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80000</span>, dev loss: <span class="hljs-number">0.48563</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.44295</span><br>[Train] epoch: <span class="hljs-number">87</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">700</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.43655</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.41185</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.38773</span><br>[Train] epoch: <span class="hljs-number">100</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">800</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.35982</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80000</span>, dev loss: <span class="hljs-number">0.37680</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.34835</span><br>[Train] epoch: <span class="hljs-number">112</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">900</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.24126</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.32392</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.30578</span><br>[Train] epoch: <span class="hljs-number">125</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">1000</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.27697</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.28536</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.27146</span><br>[Train] epoch: <span class="hljs-number">137</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">1100</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.25255</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.25523</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.24195</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.23231</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure><p>可视化观察训练集损失和训练集loss变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 绘制训练集和验证集的损失变化以及验证集上的准确率变化曲线</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_training_loss_acc</span>(<span class="hljs-params">runner, fig_name, </span><br><span class="hljs-params">    fig_size=(<span class="hljs-params"><span class="hljs-number">16</span>, <span class="hljs-number">6</span></span>), </span><br><span class="hljs-params">    sample_step=<span class="hljs-number">20</span>, </span><br><span class="hljs-params">    loss_legend_loc=<span class="hljs-string">&quot;upper right&quot;</span>, </span><br><span class="hljs-params">    acc_legend_loc=<span class="hljs-string">&quot;lower right&quot;</span>,</span><br><span class="hljs-params">    train_color=<span class="hljs-string">&quot;#8E004D&quot;</span>,</span><br><span class="hljs-params">    dev_color=<span class="hljs-string">&#x27;#E20079&#x27;</span>,</span><br><span class="hljs-params">    fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>,</span><br><span class="hljs-params">    train_linestyle=<span class="hljs-string">&quot;-&quot;</span>,</span><br><span class="hljs-params">    dev_linestyle=<span class="hljs-string">&#x27;--&#x27;</span></span>):<br><br>    plt.figure(figsize=fig_size)<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>    train_items = runner.train_step_losses[::sample_step]<br>    train_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    train_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br><br>    plt.plot(train_steps, train_losses, color=train_color, linestyle=train_linestyle, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(runner.dev_losses)&gt;<span class="hljs-number">0</span>:<br>        dev_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>        dev_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>        plt.plot(dev_steps, dev_losses, color=dev_color, linestyle=dev_linestyle, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=fontsize)<br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=fontsize)<br>    plt.legend(loc=loss_legend_loc, fontsize=fontsize)<br><br>    <span class="hljs-comment"># 绘制评价准确率变化曲线</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(runner.dev_scores)&gt;<span class="hljs-number">0</span>:<br>        plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>        plt.plot(dev_steps, runner.dev_scores,<br>            color=dev_color, linestyle=dev_linestyle, label=<span class="hljs-string">&quot;Dev accuracy&quot;</span>)<br>    <br>        <span class="hljs-comment"># 绘制坐标轴和图例</span><br>        plt.ylabel(<span class="hljs-string">&quot;score&quot;</span>, fontsize=fontsize)<br>        plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=fontsize)<br>        plt.legend(loc=acc_legend_loc, fontsize=fontsize)<br><br>    plt.savefig(fig_name)<br>    plt.show()<br><br>plot_training_loss_acc(runner, <span class="hljs-string">&#x27;fw-loss.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222430247.png" alt="image-20221215175316781"></p><p>从输出结果可以看出准确率随着迭代次数增加逐渐上升，损失函数下降。</p><hr><h4 id="√-4-5-6-模型评价"><a href="#√-4-5-6-模型评价" class="headerlink" title="[√] 4.5.6 - 模型评价"></a>[√] 4.5.6 - 模型评价</h4><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及Loss情况。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] accuracy/loss: <span class="hljs-number">0.8000</span>/<span class="hljs-number">0.6891</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-4-5-7-模型预测"><a href="#√-4-5-7-模型预测" class="headerlink" title="[√] 4.5.7 - 模型预测"></a>[√] 4.5.7 - 模型预测</h4><p>同样地，也可以使用保存好的模型，对测试集中的某一个数据进行模型预测，观察模型效果。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取测试集中第一条数据</span><br>X, label = <span class="hljs-built_in">next</span>(test_loader())<br>logits = runner.predict(X)<br><br>pred_class = paddle.argmax(logits[<span class="hljs-number">0</span>]).numpy()<br>label = label[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].numpy()<br><br><span class="hljs-comment"># 输出真实类别与预测类别</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label, pred_class))<br></code></pre></td></tr></table></figure><figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">The true category <span class="hljs-keyword">is</span> <span class="hljs-comment">[2]</span> and the predicted category <span class="hljs-keyword">is</span> <span class="hljs-comment">[2]</span><br></code></pre></td></tr></table></figure><hr><h3 id="√-4-6-小结"><a href="#√-4-6-小结" class="headerlink" title="[√] 4.6 - 小结"></a>[√] 4.6 - 小结</h3><p>本章介绍前馈神经网络的基本概念、网络结构及代码实现，利用前馈神经网络完成一个分类任务，并通过两个简单的实验，观察前馈神经网络的梯度消失问题和死亡ReLU问题，以及对应的优化策略。 此外，还实践了基于前馈神经网络完成鸢尾花分类任务。</p><hr><h3 id="√-4-7-实验拓展"><a href="#√-4-7-实验拓展" class="headerlink" title="[√] 4.7 - 实验拓展"></a>[√] 4.7 - 实验拓展</h3><p>尝试基于MNIST手写数字识别数据集，设计合适的前馈神经网络进行实验，并取得95%以上的准确率。</p><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第3章 - 线性分类</title>
    <link href="/posts/2180589992/"/>
    <url>/posts/2180589992/</url>
    
    <content type="html"><![CDATA[<h2 id="√-第3章-线性分类"><a href="#√-第3章-线性分类" class="headerlink" title="[√] 第3章 - 线性分类"></a>[√] 第3章 - 线性分类</h2><p>分类是机器学习中最常见的一类任务，其预测标签是一些离散的类别（符号）。根据分类任务的类别数量又可以分为二分类任务和多分类任务。</p><p>线性分类是指利用一个或多个线性函数将样本进行分类。常用的线性分类模型有Logistic回归和Softmax回归。<br>Logistic回归是一种常用的处理二分类问题的线性模型。Softmax回归是Logistic回归在多分类问题上的推广。</p><p>在学习本章内容前，建议您先阅读《神经网络与深度学习》第3章：线性模型的相关内容，关键知识点如 <strong>图3.1</strong> 所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212213737343.png" alt="image-20221212213737343"></p><p>本章内容基于 <strong>《神经网络与深度学习》第3章：线性模型</strong> 相关内容进行设计，主要包含两部分：</p><ul><li><p><strong>模型解读</strong>：介绍两个最常用的线性分类模型Logistic回归和Softmax回归的原理剖析和相应的代码实现。通过理论和代码的结合，加深对线性模型的理解；</p></li><li><p><strong>案例实践</strong>：基于Softmax回归算法完成鸢尾花分类任务。</p></li></ul><hr><h3 id="√-3-1-基于Logistic回归的二分类任务"><a href="#√-3-1-基于Logistic回归的二分类任务" class="headerlink" title="[√] 3.1 - 基于Logistic回归的二分类任务"></a>[√] 3.1 - 基于Logistic回归的二分类任务</h3><p>在本节中，我们实现一个Logistic回归模型，并对一个简单的数据集进行二分类实验。</p><hr><h4 id="√-3-1-1-数据集构建"><a href="#√-3-1-1-数据集构建" class="headerlink" title="[√] 3.1.1 - 数据集构建"></a>[√] 3.1.1 - 数据集构建</h4><p>我们首先构建一个简单的分类任务，并构建训练集、验证集和测试集。<br>本任务的数据来自带噪音的两个弯月形状函数，每个弯月对一个类别。我们采集1000条样本，每个样本包含2个特征。</p><p>数据集的构建函数<code>make_moons</code>的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_moons</span>(<span class="hljs-params">n_samples=<span class="hljs-number">1000</span>, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    生成带噪音的弯月形状数据</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">        - n_samples：数据量大小，数据类型为int</span><br><span class="hljs-string">        - shuffle：是否打乱数据，数据类型为bool</span><br><span class="hljs-string">        - noise：以多大的程度增加噪声，数据类型为None或float，noise为None时表示不增加噪声</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">        - X：特征数据，shape=[n_samples,2]</span><br><span class="hljs-string">        - y：标签数据, shape=[n_samples]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    n_samples_out = n_samples // <span class="hljs-number">2</span><br>    n_samples_in = n_samples - n_samples_out<br><br>    <span class="hljs-comment"># 采集第1类数据，特征为(x,y)</span><br>    <span class="hljs-comment"># 使用&#x27;paddle.linspace&#x27;在0到pi上均匀取n_samples_out个值</span><br>    <span class="hljs-comment"># 使用&#x27;paddle.cos&#x27;计算上述取值的余弦值作为特征1，使用&#x27;paddle.sin&#x27;计算上述取值的正弦值作为特征2</span><br>    outer_circ_x = paddle.cos(paddle.linspace(<span class="hljs-number">0</span>, math.pi, n_samples_out))<br>    outer_circ_y = paddle.sin(paddle.linspace(<span class="hljs-number">0</span>, math.pi, n_samples_out))<br><br>    inner_circ_x = <span class="hljs-number">1</span> - paddle.cos(paddle.linspace(<span class="hljs-number">0</span>, math.pi, n_samples_in))<br>    inner_circ_y = <span class="hljs-number">0.5</span> - paddle.sin(paddle.linspace(<span class="hljs-number">0</span>, math.pi, n_samples_in))<br>    <br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;outer_circ_x.shape:&#x27;</span>, outer_circ_x.shape, <span class="hljs-string">&#x27;outer_circ_y.shape:&#x27;</span>, outer_circ_y.shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;inner_circ_x.shape:&#x27;</span>, inner_circ_x.shape, <span class="hljs-string">&#x27;inner_circ_y.shape:&#x27;</span>, inner_circ_y.shape)<br>    <br>    <span class="hljs-comment"># 使用&#x27;paddle.concat&#x27;将两类数据的特征1和特征2分别延维度0拼接在一起，得到全部特征1和特征2</span><br>    <span class="hljs-comment"># 使用&#x27;paddle.stack&#x27;将两类特征延维度1堆叠在一起</span><br>    X = paddle.stack(<br>        [paddle.concat([outer_circ_x, inner_circ_x]),<br>        paddle.concat([outer_circ_y, inner_circ_y])],<br>        axis=<span class="hljs-number">1</span><br>    )<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;after concat shape:&#x27;</span>, paddle.concat([outer_circ_x, inner_circ_x]).shape)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;X shape:&#x27;</span>, X.shape)<br><br>    <span class="hljs-comment"># 使用&#x27;paddle. zeros&#x27;将第一类数据的标签全部设置为0</span><br>    <span class="hljs-comment"># 使用&#x27;paddle. ones&#x27;将第一类数据的标签全部设置为1</span><br>    y = paddle.concat(<br>        [paddle.zeros(shape=[n_samples_out]), paddle.ones(shape=[n_samples_in])]<br>    )<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y shape:&#x27;</span>, y.shape)<br><br>    <span class="hljs-comment"># 如果shuffle为True，将所有数据打乱</span><br>    <span class="hljs-keyword">if</span> shuffle:<br>        <span class="hljs-comment"># 使用&#x27;paddle.randperm&#x27;生成一个数值在0到X.shape[0]，随机排列的一维Tensor做索引值，用于打乱数据</span><br>        idx = paddle.randperm(X.shape[<span class="hljs-number">0</span>])<br>        X = X[idx]<br>        y = y[idx]<br><br>    <span class="hljs-comment"># 如果noise不为None，则给特征值加入噪声</span><br>    <span class="hljs-keyword">if</span> noise <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        <span class="hljs-comment"># 使用&#x27;paddle.normal&#x27;生成符合正态分布的随机Tensor作为噪声，并加到原始特征上</span><br>        X += paddle.normal(mean=<span class="hljs-number">0.0</span>, std=noise, shape=X.shape)<br><br>    <span class="hljs-keyword">return</span> X, y<br></code></pre></td></tr></table></figure><p>随机采集1000个样本，并进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 采样1000个样本</span><br>n_samples = <span class="hljs-number">1000</span><br>X, y = make_moons(n_samples=n_samples, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">0.5</span>)<br><span class="hljs-comment"># 可视化生产的数据集，不同颜色代表不同类别</span><br>%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>plt.scatter(x=X[:, <span class="hljs-number">0</span>].tolist(), y=X[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=y.tolist())<br>plt.xlim(-<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>plt.ylim(-<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)<br>plt.savefig(<span class="hljs-string">&#x27;linear-dataset-vis.pdf&#x27;</span>)<br>plt.show()<br><br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">outer_circ_x.shape: [<span class="hljs-number">500</span>] outer_circ_y.shape: [<span class="hljs-number">500</span>]<br>inner_circ_x.shape: [<span class="hljs-number">500</span>] inner_circ_y.shape: [<span class="hljs-number">500</span>]<br>after concat shape: [<span class="hljs-number">1000</span>]<br>X shape: [<span class="hljs-number">1000</span>, <span class="hljs-number">2</span>]<br>y shape: [<span class="hljs-number">1000</span>]<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212215525120.png" alt="image-20221212214813390"></p><p>将1000条样本数据拆分成训练集、验证集和测试集，其中训练集640条、验证集160条、测试集200条。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">num_train = <span class="hljs-number">640</span><br>num_dev = <span class="hljs-number">160</span><br>num_test = <span class="hljs-number">200</span><br><br>X_train, y_train = X[:num_train], y[:num_train]<br>X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]<br><br>y_train = y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_dev = y_dev.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_test = y_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><p>这样，我们就完成了Moon1000数据集的构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印X_train和y_train的维度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;X_train shape: &quot;</span>, X_train.shape, <span class="hljs-string">&quot;y_train shape: &quot;</span>, y_train.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train shape:  [<span class="hljs-number">640</span>, <span class="hljs-number">2</span>] y_train shape:  [<span class="hljs-number">640</span>, <span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印一下前5个数据的标签</span><br><span class="hljs-built_in">print</span> (X_train[:<span class="hljs-number">5</span>])<br><span class="hljs-built_in">print</span> (y_train[:<span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">2</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[-<span class="hljs-number">0.83520460</span>,  <span class="hljs-number">0.72044683</span>],<br>        [-<span class="hljs-number">0.41998285</span>,  <span class="hljs-number">0.75988615</span>],<br>        [-<span class="hljs-number">0.19335055</span>,  <span class="hljs-number">0.87097842</span>],<br>        [ <span class="hljs-number">1.60602939</span>, -<span class="hljs-number">0.35923928</span>],<br>        [ <span class="hljs-number">0.56276309</span>, -<span class="hljs-number">0.18897334</span>]])<br>Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>]])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-1-2-模型构建"><a href="#√-3-1-2-模型构建" class="headerlink" title="[√] 3.1.2 - 模型构建"></a>[√] 3.1.2 - 模型构建</h4><p>Logistic回归是一种常用的处理二分类问题的线性模型。与线性回归一样，Logistic回归也会将输入特征与权重做线性叠加。不同之处在于，Logistic回归引入了非线性函数$g:\mathbb{R}^D \rightarrow (0,1)$，预测类别标签的后验概率 $p(y&#x3D;1|\mathbf x)$ ，从而解决连续的线性函数不适合进行分类的问题。</p><p>$$<br>p(y&#x3D;1|\mathbf x) &#x3D; \sigma(\mathbf w^ \mathrm{ T } \mathbf x+b),（3.1）<br>$$</p><p>其中判别函数$\sigma(\cdot)$为Logistic函数，也称为激活函数，作用是将线性函数$f(\mathbf x;\mathbf w,b)$的输出从实数区间“挤压”到（0,1）之间，用来表示概率。Logistic函数定义为：</p><p>$$<br>\sigma(x) &#x3D; \frac{1}{1+\exp(-x)}。（3.2）<br>$$</p><p><strong>Logistic函数</strong></p><p>Logistic函数的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义Logistic函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">logistic</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + paddle.exp(-x))<br><br><span class="hljs-comment"># 在[-10,10]的范围内生成一系列的输入值，用于绘制函数曲线</span><br>x = paddle.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10000</span>)<br>plt.figure()<br>plt.plot(x.tolist(), logistic(x).tolist(), color=<span class="hljs-string">&quot;#E20079&quot;</span>, label=<span class="hljs-string">&quot;Logistic Function&quot;</span>)<br><span class="hljs-comment"># 设置坐标轴</span><br>ax = plt.gca()<br><span class="hljs-comment"># 取消右侧和上侧坐标轴</span><br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br><span class="hljs-comment"># 设置默认的x轴和y轴方向</span><br>ax.xaxis.set_ticks_position(<span class="hljs-string">&#x27;bottom&#x27;</span>) <br>ax.yaxis.set_ticks_position(<span class="hljs-string">&#x27;left&#x27;</span>)<br><span class="hljs-comment"># 设置坐标原点为(0,0)</span><br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br><span class="hljs-comment"># 添加图例</span><br>plt.legend()<br>plt.savefig(<span class="hljs-string">&#x27;linear-logistic.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212214813390.png" alt="image-20221212215525120"></p><p>从输出结果看，当输入在0附近时，Logistic函数近似为线性函数；而当输入值非常大或非常小时，函数会对输入进行抑制。输入越小，则越接近0；输入越大，则越接近1。正因为Logistic函数具有这样的性质，使得其输出可以直接看作为概率分布。</p><p><strong>Logistic回归算子</strong></p><p>&#x3D;&#x3D;Logistic回归模型其实就是线性层与Logistic函数的组合&#x3D;&#x3D;</p><p>通常会将 Logistic回归模型中的权重和偏置初始化为0，同时，为了提高预测样本的效率，我们将$N$个样本归为一组进行成批地预测。<br>$$<br>\hat{\mathbf y} &#x3D; p(\mathbf y|\mathbf x) &#x3D; \sigma(\boldsymbol{X} \boldsymbol{w} + b), (3.3)<br>$$</p><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\hat{\boldsymbol{y}}$为$N$个样本的预测值构成的$N$维向量。</p><p>这里，我们构建一个Logistic回归算子，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> op<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">model_LR</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim</span>):<br>        <span class="hljs-built_in">super</span>(model_LR, self).__init__()<br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 将线性层的权重参数全部初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[input_dim, <span class="hljs-number">1</span>])<br>        <span class="hljs-comment"># self.params[&#x27;w&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, 1])</span><br>        <span class="hljs-comment"># 将线性层的偏置参数初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="hljs-number">1</span>])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs: shape=[N,D], N是样本数量，D为特征维度</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：预测标签为1的概率，shape=[N,1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 线性计算</span><br>        score = paddle.matmul(inputs, self.params[<span class="hljs-string">&#x27;w&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-comment"># Logistic 函数</span><br>        outputs = logistic(score)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure><p><strong>测试一下</strong></p><p>随机生成3条长度为4的数据输入Logistic回归模型，观察输出结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 固定随机种子，保持每次运行结果一致</span><br>paddle.seed(<span class="hljs-number">0</span>)<br><span class="hljs-comment"># 随机生成3条长度为4的数据</span><br>inputs = paddle.randn(shape=[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Input is:&#x27;</span>, inputs)<br><span class="hljs-comment"># 实例化模型</span><br>model = model_LR(<span class="hljs-number">4</span>)<br>outputs = model(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Output is:&#x27;</span>, outputs)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">Input <span class="hljs-keyword">is</span>: Tensor(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[-<span class="hljs-number">0.75711036</span>, -<span class="hljs-number">0.38059190</span>,  <span class="hljs-number">0.10946669</span>,  <span class="hljs-number">1.34467661</span>],<br>        [-<span class="hljs-number">0.84002435</span>, -<span class="hljs-number">1.27341712</span>,  <span class="hljs-number">2.47224617</span>,  <span class="hljs-number">0.14070207</span>],<br>        [ <span class="hljs-number">0.60608417</span>,  <span class="hljs-number">0.23396523</span>,  <span class="hljs-number">1.35604191</span>,  <span class="hljs-number">0.10350471</span>]])<br>Output <span class="hljs-keyword">is</span>: Tensor(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.50000000</span>],<br>        [<span class="hljs-number">0.50000000</span>],<br>        [<span class="hljs-number">0.50000000</span>]])<br></code></pre></td></tr></table></figure><p>从输出结果看，模型最终的输出$g(·)$恒为0.5。这是由于采用全0初始化后，不论输入值的大小为多少，Logistic函数的输入值恒为0，因此输出恒为0.5。</p><hr><h4 id="√-3-1-3-损失函数"><a href="#√-3-1-3-损失函数" class="headerlink" title="[√] 3.1.3 - 损失函数"></a>[√] 3.1.3 - 损失函数</h4><p>&#x3D;&#x3D;线性二分类logistic，通常使用交叉熵作为损失函数。&#x3D;&#x3D;</p><p>在模型训练过程中，需要使用损失函数来量化预测值和真实值之间的差异。<br>给定一个分类任务，$\mathbf y$表示样本$\mathbf x$的标签的真实概率分布，向量$\hat{\mathbf y}&#x3D;p(\mathbf y|\mathbf x)$表示预测的标签概率分布。<br>训练目标是使得$\hat{\mathbf y}$尽可能地接近$\mathbf y$，通常可以使用<strong>交叉熵损失函数</strong>。<br>在给定$\mathbf y$的情况下，如果预测的概率分布$\hat{\mathbf y}$与标签真实的分布$\mathbf y$越接近，则交叉熵越小；如果$p(\mathbf x)$和$\mathbf y$越远，交叉熵就越大。</p><p>对于二分类任务，我们只需要计算$\hat{y}&#x3D;p(y&#x3D;1|\mathbf x)$，用$1-\hat{y}$来表示$p(y&#x3D;0|\mathbf x)$。<br>给定有$N$个训练样本的训练集${(\mathbf x^{(n)},y^{(n)})} ^N_{n&#x3D;1}$，使用交叉熵损失函数，Logistic回归的风险函数计算方式为：</p><p>$$<br>\begin{aligned}<br>\cal R(\mathbf w,b) &amp;&#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N \Big(y^{(n)}\log\hat{y}^{(n)} + (1-y^{(n)})\log(1-\hat{y}^{(n)})\Big)。（3.4）<br>\end{aligned}<br>$$</p><p>向量形式可以表示为：</p><p>$$<br>\begin{aligned}<br>\cal R(\mathbf w,b) &amp;&#x3D; -\frac{1}{N}\Big(\mathbf y^ \mathrm{ T }  \log\hat{\mathbf y} + (1-\mathbf y)^ \mathrm{ T } \log(1-\hat{\mathbf y})\Big),（3.5）<br>\end{aligned}<br>$$</p><p>其中$\mathbf y\in [0,1]^N$为$N$个样本的真实标签构成的$N$维向量，$\hat{\mathbf y}$为$N$个样本标签为1的后验概率构成的$N$维向量。</p><p>二分类任务的交叉熵损失函数的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现交叉熵损失函数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BinaryCrossEntropyLoss</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.predicts = <span class="hljs-literal">None</span><br>        self.labels = <span class="hljs-literal">None</span><br>        self.num = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-keyword">return</span> self.forward(predicts, labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失值：shape=[1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.predicts = predicts<br>        self.labels = labels<br>        self.num = self.predicts.shape[<span class="hljs-number">0</span>]<br>        loss = -<span class="hljs-number">1.</span> / self.num * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) + paddle.matmul((<span class="hljs-number">1</span>-self.labels.t()), paddle.log(<span class="hljs-number">1</span>-self.predicts)))<br>        loss = paddle.squeeze(loss, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> loss<br><br><span class="hljs-comment"># 测试一下</span><br><span class="hljs-comment"># 生成一组长度为3，值为1的标签数据</span><br>labels = paddle.ones(shape=[<span class="hljs-number">3</span>,<span class="hljs-number">1</span>])<br><span class="hljs-comment"># 计算风险函数</span><br>bce_loss = BinaryCrossEntropyLoss()<br><span class="hljs-built_in">print</span>(bce_loss(outputs, labels))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0.69314718</span>])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-1-4-模型优化"><a href="#√-3-1-4-模型优化" class="headerlink" title="[√] 3.1.4 - 模型优化"></a>[√] 3.1.4 - 模型优化</h4><p>不同于线性回归中直接使用最小二乘法即可进行模型参数的求解，Logistic回归需要使用优化算法对模型参数进行有限次地迭代来获取更优的模型，从而尽可能地降低风险函数的值。<br>在机器学习任务中，最简单、常用的优化算法是梯度下降法。</p><p>使用梯度下降法进行模型优化，首先需要初始化参数$\mathbf W$和 $b$，然后不断地计算它们的梯度，并沿梯度的反方向更新参数。</p><hr><h5 id="√-3-1-4-1-梯度计算"><a href="#√-3-1-4-1-梯度计算" class="headerlink" title="[√] 3.1.4.1 - 梯度计算"></a>[√] 3.1.4.1 - 梯度计算</h5><p>在Logistic回归中，风险函数$\cal R(\mathbf w,b)$ 关于参数$\mathbf w$和$b$的偏导数为：</p><p>$$<br>\begin{aligned}<br>\frac{\partial \cal R(\mathbf w,b)}{\partial \mathbf w} &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N \mathbf x^{(n)}(y^{(n)}- \hat{y}^{(n)}) &#x3D; -\frac{1}{N} \mathbf X^ \mathrm{ T }  (\mathbf y-\hat{\mathbf y})<br>\end{aligned}，（3.6）<br>$$</p><p>$$<br>\begin{aligned}<br>\frac{\partial \cal R(\mathbf w,b)}{\partial b} &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N (y^{(n)}- \hat{y}^{(n)}) &#x3D; -\frac{1}{N} \mathbf {sum}(\mathbf y-\hat{\mathbf y})<br>\end{aligned}。（3.7）<br>$$</p><p>通常将偏导数的计算过程定义在Logistic回归算子的<code>backward</code>函数中，代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">model_LR</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim</span>):<br>        <span class="hljs-built_in">super</span>(model_LR, self).__init__()<br>        <span class="hljs-comment"># 存放线性层参数</span><br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 将线性层的权重参数全部初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[input_dim, <span class="hljs-number">1</span>])<br>        <span class="hljs-comment"># self.params[&#x27;w&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, 1])</span><br>        <span class="hljs-comment"># 将线性层的偏置参数初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="hljs-number">1</span>])<br>        <span class="hljs-comment"># 存放参数的梯度</span><br>        self.grads = &#123;&#125;<br>        self.X = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        self.X = inputs<br>        <span class="hljs-comment"># 线性计算</span><br>        score = paddle.matmul(inputs, self.params[<span class="hljs-string">&#x27;w&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-comment"># Logistic 函数</span><br>        self.outputs = logistic(score)<br>        <span class="hljs-keyword">return</span> self.outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        N = labels.shape[<span class="hljs-number">0</span>]<br>        <span class="hljs-comment"># 计算偏导数</span><br>        self.grads[<span class="hljs-string">&#x27;w&#x27;</span>] = -<span class="hljs-number">1</span> / N * paddle.matmul(self.X.t(), (labels - self.outputs))<br>        self.grads[<span class="hljs-string">&#x27;b&#x27;</span>] = -<span class="hljs-number">1</span> / N * paddle.<span class="hljs-built_in">sum</span>(labels - self.outputs)<br></code></pre></td></tr></table></figure><hr><h5 id="√-3-1-4-2-参数更新"><a href="#√-3-1-4-2-参数更新" class="headerlink" title="[√] 3.1.4.2 - 参数更新"></a>[√] 3.1.4.2 - 参数更新</h5><p>在计算参数的梯度之后，我们按照下面公式更新参数：</p><p>$$<br>\mathbf w\leftarrow \mathbf w - \alpha \frac{\partial \cal R(\mathbf w,b)}{\partial \mathbf w}，（3.8）<br>$$<br>$$<br>b\leftarrow b - \alpha \frac{\partial \cal R(\mathbf w,b)}{\partial b}，（3.9）<br>$$</p><p>其中$\alpha$ 为学习率。</p><p>将上面的参数更新过程包装为优化器，首先定义一个优化器基类<code>Optimizer</code>，方便后续所有的优化器调用。在这个基类中，需要初始化优化器的初始学习率<code>init_lr</code>，以及指定优化器需要优化的参数。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> abc <span class="hljs-keyword">import</span> abstractmethod<br><br><span class="hljs-comment"># 优化器基类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, init_lr, model</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        优化器类初始化</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 初始化学习率，用于参数更新的计算</span><br>        self.init_lr = init_lr<br>        <span class="hljs-comment"># 指定优化器需要优化的模型</span><br>        self.model = model<br><br><span class="hljs-meta">    @abstractmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        定义每次迭代如何更新参数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p>然后实现一个梯度下降法的优化器函数<code>SimpleBatchGD</code>来执行参数更新过程。其中<code>step</code>函数从模型的<code>grads</code>属性取出参数的梯度并更新。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">SimpleBatchGD</span>(<span class="hljs-title class_ inherited__">Optimizer</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, init_lr, model</span>):<br>        <span class="hljs-built_in">super</span>(SimpleBatchGD, self).__init__(init_lr=init_lr, model=model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 参数更新</span><br>        <span class="hljs-comment"># 遍历所有参数，按照公式(3.8)和(3.9)更新参数</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(self.model.params, <span class="hljs-built_in">dict</span>):<br>            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> self.model.params.keys():<br>                self.model.params[key] = self.model.params[key] - self.init_lr * self.model.grads[key]<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-1-5-评价指标"><a href="#√-3-1-5-评价指标" class="headerlink" title="[√] 3.1.5 - 评价指标"></a>[√] 3.1.5 - 评价指标</h4><p>在分类任务中，通常使用准确率（Accuracy）作为评价指标。如果模型预测的类别与真实类别一致，则说明模型预测正确。准确率即正确预测的数量与总的预测数量的比值：</p><h1 id="mathcal-A"><a href="#mathcal-A" class="headerlink" title="$$\mathcal{A} "></a>$$<br>\mathcal{A} </h1><p>\frac{1}{N}<br>    \sum_{n&#x3D;1}^N<br>    I<br>        (y^{(n)} &#x3D; \hat{y}^{(n)}),（3.10）<br>$$</p><p>其中$I(·)$是指示函数。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">accuracy</span>(<span class="hljs-params">preds, labels</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">        - preds：预测值，二分类时，shape=[N, 1]，N为样本数量，多分类时，shape=[N, C]，C为类别数量</span><br><span class="hljs-string">        - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">        - 准确率：shape=[1]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 判断是二分类任务还是多分类任务，preds.shape[1]=1时为二分类任务，preds.shape[1]&gt;1时为多分类任务</span><br>    <span class="hljs-keyword">if</span> preds.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>:<br>        <span class="hljs-comment"># 二分类时，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span><br>        <span class="hljs-comment"># 使用&#x27;paddle.cast&#x27;将preds的数据类型转换为float32类型</span><br>        preds = paddle.cast((preds&gt;=<span class="hljs-number">0.5</span>),dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span><br>        preds = paddle.argmax(preds,axis=<span class="hljs-number">1</span>, dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    <span class="hljs-keyword">return</span> paddle.mean(paddle.cast(paddle.equal(preds, labels),dtype=<span class="hljs-string">&#x27;float32&#x27;</span>))<br><br><span class="hljs-comment"># 假设模型的预测值为[[0.],[1.],[1.],[0.]]，真实类别为[[1.],[1.],[0.],[0.]]，计算准确率</span><br>preds = paddle.to_tensor([[<span class="hljs-number">0.</span>],[<span class="hljs-number">1.</span>],[<span class="hljs-number">1.</span>],[<span class="hljs-number">0.</span>]])<br>labels = paddle.to_tensor([[<span class="hljs-number">1.</span>],[<span class="hljs-number">1.</span>],[<span class="hljs-number">0.</span>],[<span class="hljs-number">0.</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;accuracy is:&quot;</span>, accuracy(preds, labels))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">accuracy <span class="hljs-keyword">is</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0.50000000</span>])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-1-6-完善Runner类"><a href="#√-3-1-6-完善Runner类" class="headerlink" title="[√] 3.1.6 - 完善Runner类"></a>[√] 3.1.6 - 完善Runner类</h4><p>基于RunnerV1，本章的RunnerV2类在训练过程中使用梯度下降法进行网络优化，模型训练过程中计算在训练集和验证集上的损失及评估指标并打印，训练过程中保存最优模型。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 用RunnerV2类封装整个训练过程</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV2</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, metric, loss_fn</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric<br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.train_scores = []<br>        self.dev_scores = []<br>        <span class="hljs-comment"># 记录训练过程中的损失函数变化情况</span><br>        self.train_loss = []<br>        self.dev_loss = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_set, dev_set, **kwargs</span>):<br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_epochs = kwargs.get(<span class="hljs-string">&quot;log_epochs&quot;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span><br>        save_path = kwargs.get(<span class="hljs-string">&quot;save_path&quot;</span>, <span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br>        <span class="hljs-comment"># 梯度打印函数，如果没有传入则默认为&quot;None&quot;</span><br>        print_grads = kwargs.get(<span class="hljs-string">&quot;print_grads&quot;</span>, <span class="hljs-literal">None</span>)<br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        best_score = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            X, y = train_set<br>            <span class="hljs-comment"># 获取模型预测</span><br>            logits = self.model(X)<br>            <span class="hljs-comment"># 计算交叉熵损失</span><br>            trn_loss = self.loss_fn(logits, y).item()<br>            self.train_loss.append(trn_loss)<br>            <span class="hljs-comment"># 计算评价指标</span><br>            trn_score = self.metric(logits, y).item()<br>            self.train_scores.append(trn_score)<br>            <span class="hljs-comment"># 计算参数梯度</span><br>            self.model.backward(y)<br>            <span class="hljs-keyword">if</span> print_grads <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># 打印每一层的梯度</span><br>                print_grads(self.model)<br>            <span class="hljs-comment"># 更新模型参数</span><br>            self.optimizer.step()<br>            dev_score, dev_loss = self.evaluate(dev_set)<br>            <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>            <span class="hljs-keyword">if</span> dev_score &gt; best_score:<br>                self.save_model(save_path)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;best accuracy performence has been updated: <span class="hljs-subst">&#123;best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                best_score = dev_score<br>            <span class="hljs-keyword">if</span> epoch % log_epochs == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, loss: <span class="hljs-subst">&#123;trn_loss&#125;</span>, score: <span class="hljs-subst">&#123;trn_score&#125;</span>&quot;</span>)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Dev] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>, loss: <span class="hljs-subst">&#123;dev_loss&#125;</span>, score: <span class="hljs-subst">&#123;dev_score&#125;</span>&quot;</span>)<br>                <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set</span>):<br>        X, y = data_set<br>        <span class="hljs-comment"># 计算模型输出</span><br>        logits = self.model(X)<br>        <span class="hljs-comment"># 计算损失函数</span><br>        loss = self.loss_fn(logits, y).item()<br>        self.dev_loss.append(loss)<br>        <span class="hljs-comment"># 计算评价指标</span><br>        score = self.metric(logits, y).item()<br>        self.dev_scores.append(score)<br>        <span class="hljs-keyword">return</span> score, loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.model(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_path</span>):<br>        paddle.save(self.model.params, save_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        self.model.params = paddle.load(model_path)<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-1-7-模型训练"><a href="#√-3-1-7-模型训练" class="headerlink" title="[√] 3.1.7 - 模型训练"></a>[√] 3.1.7 - 模型训练</h4><p>下面进行Logistic回归模型的训练，使用交叉熵损失函数和梯度下降法进行优化。 使用训练集和验证集进行模型训练，共训练 500个epoch，每隔50个epoch打印出训练集上的指标。 代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 固定随机种子，保持每次运行结果一致</span><br>paddle.seed(<span class="hljs-number">102</span>)<br><br><span class="hljs-comment"># 特征维度</span><br>input_dim = <span class="hljs-number">2</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 实例化模型</span><br>model = model_LR(input_dim=input_dim)<br><span class="hljs-comment"># 指定优化器</span><br>optimizer = SimpleBatchGD(init_lr=lr, model=model)<br><span class="hljs-comment"># 指定损失函数</span><br>loss_fn = BinaryCrossEntropyLoss()<br><span class="hljs-comment"># 指定评价方式</span><br>metric = accuracy<br><br><span class="hljs-comment"># 实例化RunnerV2类，并传入训练配置</span><br>runner = RunnerV2(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">500</span>, log_epochs=<span class="hljs-number">50</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.81250</span><br>[Train] epoch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">0.6931471824645996</span>, score: <span class="hljs-number">0.5140625238418579</span><br>[Dev] epoch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">0.681858479976654</span>, score: <span class="hljs-number">0.8125</span><br>best accuracy performence has been updated: <span class="hljs-number">0.81250</span> --&gt; <span class="hljs-number">0.81875</span><br>best accuracy performence has been updated: <span class="hljs-number">0.81875</span> --&gt; <span class="hljs-number">0.82500</span><br>[Train] epoch: <span class="hljs-number">300</span>, loss: <span class="hljs-number">0.4530242085456848</span>, score: <span class="hljs-number">0.7875000238418579</span><br>[Dev] epoch: <span class="hljs-number">300</span>, loss: <span class="hljs-number">0.374613493680954</span>, score: <span class="hljs-number">0.824999988079071</span><br>best accuracy performence has been updated: <span class="hljs-number">0.82500</span> --&gt; <span class="hljs-number">0.83125</span><br>[Train] epoch: <span class="hljs-number">600</span>, loss: <span class="hljs-number">0.45098423957824707</span>, score: <span class="hljs-number">0.7875000238418579</span><br>[Dev] epoch: <span class="hljs-number">600</span>, loss: <span class="hljs-number">0.36696094274520874</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">900</span>, loss: <span class="hljs-number">0.4508608877658844</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">900</span>, loss: <span class="hljs-number">0.3654332160949707</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.4508519172668457</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.3650430142879486</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">1500</span>, loss: <span class="hljs-number">0.4508512616157532</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">1500</span>, loss: <span class="hljs-number">0.36493727564811707</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">1800</span>, loss: <span class="hljs-number">0.450851172208786</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">1800</span>, loss: <span class="hljs-number">0.3649081885814667</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">2100</span>, loss: <span class="hljs-number">0.4508512020111084</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">2100</span>, loss: <span class="hljs-number">0.3649001121520996</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">2400</span>, loss: <span class="hljs-number">0.4508512020111084</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">2400</span>, loss: <span class="hljs-number">0.3648979365825653</span>, score: <span class="hljs-number">0.831250011920929</span><br>[Train] epoch: <span class="hljs-number">2700</span>, loss: <span class="hljs-number">0.4508512020111084</span>, score: <span class="hljs-number">0.7906249761581421</span><br>[Dev] epoch: <span class="hljs-number">2700</span>, loss: <span class="hljs-number">0.36489737033843994</span>, score: <span class="hljs-number">0.831250011920929</span><br></code></pre></td></tr></table></figure><p>可视化观察训练集与验证集的准确率和损失的变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可视化观察训练集与验证集的指标变化情况</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">runner,fig_name</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>    epochs = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(runner.train_scores))]<br>    <span class="hljs-comment"># 绘制训练损失变化曲线</span><br>    plt.plot(epochs, runner.train_loss, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制评价损失变化曲线</span><br>    plt.plot(epochs, runner.dev_loss, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>)<br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>    <span class="hljs-comment"># 绘制训练准确率变化曲线</span><br>    plt.plot(epochs, runner.train_scores, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train accuracy&quot;</span>)<br>    <span class="hljs-comment"># 绘制评价准确率变化曲线</span><br>    plt.plot(epochs, runner.dev_scores, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev accuracy&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;score&quot;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>)<br>    plt.tight_layout()<br>    plt.savefig(fig_name)<br>    plt.show()<br><br>plot(runner,fig_name=<span class="hljs-string">&#x27;linear-acc.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212231954256.png" alt="image-20221212231954256"></p><p>从输出结果可以看到，在训练集与验证集上，loss得到了收敛，同时准确率指标都达到了较高的水平，训练比较充分。</p><hr><h4 id="√-3-1-8-模型评价"><a href="#√-3-1-8-模型评价" class="headerlink" title="[√] 3.1.8 - 模型评价"></a>[√] 3.1.8 - 模型评价</h4><p>使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率和loss数据。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">score, loss = runner.evaluate([X_test, y_test])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] score/loss: <span class="hljs-number">0.7750</span>/<span class="hljs-number">0.4776</span><br></code></pre></td></tr></table></figure><p>可视化观察拟合的决策边界 $\boldsymbol{X} \boldsymbol{w} + b&#x3D;0$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decision_boundary</span>(<span class="hljs-params">w, b, x1</span>):<br>    w1, w2 = w<br>    x2 = (- w1 * x1 - b) / w2<br>    <span class="hljs-keyword">return</span> x2<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br><span class="hljs-comment"># 绘制原始数据</span><br>plt.scatter(X[:, <span class="hljs-number">0</span>].tolist(), X[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=y.tolist())<br><br>w = model.params[<span class="hljs-string">&#x27;w&#x27;</span>]<br>b = model.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>x1 = paddle.linspace(-<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1000</span>)<br>x2 = decision_boundary(w, b, x1)<br><span class="hljs-comment"># 绘制决策边界</span><br>plt.plot(x1.tolist(), x2.tolist(), color=<span class="hljs-string">&quot;red&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214113635513.png" alt="image-20221212234936572"></p><hr><h3 id="√-3-2-基于Softmax回归的多分类任务"><a href="#√-3-2-基于Softmax回归的多分类任务" class="headerlink" title="[√] 3.2 - 基于Softmax回归的多分类任务"></a>[√] 3.2 - 基于Softmax回归的多分类任务</h3><p>Logistic回归可以有效地解决二分类问题，但在分类任务中，还有一类多分类问题，即类别数$C$大于2 的分类问题。Softmax回归就是Logistic回归在多分类问题上的推广。</p><p>使用Softmax回归模型对一个简单的数据集进行多分类实验。</p><hr><h4 id="√-3-2-1-数据集构建"><a href="#√-3-2-1-数据集构建" class="headerlink" title="[√] 3.2.1 - 数据集构建"></a>[√] 3.2.1 - 数据集构建</h4><p>我们首先构建一个简单的多分类任务，并构建训练集、验证集和测试集。<br>本任务的数据来自3个不同的簇，每个簇对一个类别。我们采集1000条样本，每个样本包含2个特征。</p><p>数据集的构建函数<code>make_multi</code>的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">make_multiclass_classification</span>(<span class="hljs-params">n_samples=<span class="hljs-number">100</span>, n_features=<span class="hljs-number">2</span>, n_classes=<span class="hljs-number">3</span>, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    生成带噪音的多类别数据</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">        - n_samples：数据量大小，数据类型为int</span><br><span class="hljs-string">        - n_features：特征数量，数据类型为int</span><br><span class="hljs-string">        - shuffle：是否打乱数据，数据类型为bool</span><br><span class="hljs-string">        - noise：以多大的程度增加噪声，数据类型为None或float，noise为None时表示不增加噪声</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">        - X：特征数据，shape=[n_samples,2]</span><br><span class="hljs-string">        - y：标签数据, shape=[n_samples,1]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 计算每个类别的样本数量</span><br>    n_samples_per_class = [<span class="hljs-built_in">int</span>(n_samples / n_classes) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_classes)]<br>    <span class="hljs-comment"># print(n_samples_per_class) # [333, 333, 333]</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_samples - <span class="hljs-built_in">sum</span>(n_samples_per_class)):<br>        n_samples_per_class[i % n_classes] += <span class="hljs-number">1</span><br>    <span class="hljs-comment"># print(n_samples_per_class) # [334, 333, 333]</span><br>    <span class="hljs-comment"># 将特征和标签初始化为0</span><br>    X = paddle.zeros([n_samples, n_features])<br>    y = paddle.zeros([n_samples], dtype=<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    <span class="hljs-comment"># 随机生成3个簇中心作为类别中心</span><br>    centroids = paddle.randperm(<span class="hljs-number">2</span> ** n_features)[:n_classes] <span class="hljs-comment"># 生成三个簇的中心</span><br>    centroids_bin = np.unpackbits(centroids.numpy().astype(<span class="hljs-string">&#x27;uint8&#x27;</span>)).reshape((-<span class="hljs-number">1</span>, <span class="hljs-number">8</span>))[:, -n_features:] <span class="hljs-comment"># 将簇中心转为二进制</span><br>    centroids = paddle.to_tensor(centroids_bin, dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <span class="hljs-comment"># 转为tensor</span><br>    <span class="hljs-comment"># 控制簇中心的分离程度</span><br>    centroids = <span class="hljs-number">1.5</span> * centroids - <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 随机生成特征值</span><br>    X[:, :n_features] = paddle.randn(shape=[n_samples, n_features])<br><br>    stop = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 将每个类的特征值控制在簇中心附近</span><br>    <span class="hljs-keyword">for</span> k, centroid <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(centroids):<br>        start, stop = stop, stop + n_samples_per_class[k]<br>        <span class="hljs-comment"># 指定标签值</span><br>        y[start:stop] = k % n_classes<br>        X_k = X[start:stop, :n_features]<br>        <span class="hljs-comment"># 控制每个类别特征值的分散程度</span><br>        A = <span class="hljs-number">2</span> * paddle.rand(shape=[n_features, n_features]) - <span class="hljs-number">1</span><br>        X_k[...] = paddle.matmul(X_k, A)<br>        X_k += centroid<br>        X[start:stop, :n_features] = X_k<br><br>    <span class="hljs-comment"># 如果noise不为None，则给特征加入噪声</span><br>    <span class="hljs-keyword">if</span> noise &gt; <span class="hljs-number">0.0</span>:<br>        <span class="hljs-comment"># 生成noise掩膜，用来指定给那些样本加入噪声</span><br>        noise_mask = paddle.rand([n_samples]) &lt; noise<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(noise_mask)):<br>            <span class="hljs-keyword">if</span> noise_mask[i]:<br>                <span class="hljs-comment"># 给加噪声的样本随机赋标签值</span><br>                y[i] = paddle.randint(n_classes, shape=[<span class="hljs-number">1</span>]).astype(<span class="hljs-string">&#x27;int32&#x27;</span>)<br>    <span class="hljs-comment"># 如果shuffle为True，将所有数据打乱</span><br>    <span class="hljs-keyword">if</span> shuffle:<br>        idx = paddle.randperm(X.shape[<span class="hljs-number">0</span>])<br>        X = X[idx]<br>        y = y[idx]<br><br>    <span class="hljs-keyword">return</span> X, y<br></code></pre></td></tr></table></figure><p>随机采集1000个样本，并进行可视化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 固定随机种子，保持每次运行结果一致</span><br>paddle.seed(<span class="hljs-number">102</span>)<br><span class="hljs-comment"># 采样1000个样本</span><br>n_samples = <span class="hljs-number">1000</span><br>X, y = make_multiclass_classification(n_samples=n_samples, n_features=<span class="hljs-number">2</span>, n_classes=<span class="hljs-number">3</span>, noise=<span class="hljs-number">0.2</span>)<br><br><span class="hljs-comment"># 可视化生产的数据集，不同颜色代表不同类别</span><br>plt.figure(figsize=(<span class="hljs-number">5</span>,<span class="hljs-number">5</span>))<br>plt.scatter(x=X[:, <span class="hljs-number">0</span>].tolist(), y=X[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=y.tolist())<br>plt.savefig(<span class="hljs-string">&#x27;linear-dataset-vis2.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214100148443.png" alt="image-20221214100148443"></p><p>将实验数据拆分成训练集、验证集和测试集。其中训练集640条、验证集160条、测试集200条。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">num_train = <span class="hljs-number">640</span><br>num_dev = <span class="hljs-number">160</span><br>num_test = <span class="hljs-number">200</span><br><br>X_train, y_train = X[:num_train], y[:num_train]<br>X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]<br><br><span class="hljs-comment"># 打印X_train和y_train的维度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;X_train shape: &quot;</span>, X_train.shape, <span class="hljs-string">&quot;y_train shape: &quot;</span>, y_train.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train shape:  [<span class="hljs-number">640</span>, <span class="hljs-number">2</span>] y_train shape:  [<span class="hljs-number">640</span>]<br></code></pre></td></tr></table></figure><p>这样，我们就完成了Multi1000数据集的构建。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印前5个数据的标签</span><br><span class="hljs-built_in">print</span>(y_train[:<span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">5</span>], dtype=int32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-2-2-模型构建"><a href="#√-3-2-2-模型构建" class="headerlink" title="[√] 3.2.2 - 模型构建"></a>[√] 3.2.2 - 模型构建</h4><p>在Softmax回归中，对类别进行预测的方式是预测输入属于每个类别的条件概率。与Logistic 回归不同的是，Softmax回归的输出值个数等于类别数$C$，而每个类别的概率值则通过Softmax函数进行求解。</p><hr><h5 id="√-3-2-2-1-Softmax函数"><a href="#√-3-2-2-1-Softmax函数" class="headerlink" title="[√] 3.2.2.1 Softmax函数"></a>[√] 3.2.2.1 Softmax函数</h5><p>Softmax函数可以将多个标量映射为一个概率分布。对于一个$K$维向量，$\mathbf x&#x3D;[x_1,\cdots,x_K]$，Softmax的计算公式为</p><p>$$<br>\mathrm{softmax}(x_k) &#x3D; \frac{\exp(x_k)}{\sum_{i&#x3D;1}^K \exp(x_i)}。（3.11）<br>$$</p><p>在Softmax函数的计算过程中，要注意<strong>上溢出</strong>和<strong>下溢出</strong>的问题。假设Softmax 函数中所有的$x_k$都是相同大小的数值$a$，理论上，所有的输出都应该为$\frac{1}{k}$。但需要考虑如下两种特殊情况：</p><ul><li>$a$为一个非常大的负数，此时$\exp(a)$ 会发生下溢出现象。计算机在进行数值计算时，当数值过小，会被四舍五入为0。此时，Softmax函数的分母会变为0，导致计算出现问题；</li><li>$a$为一个非常大的正数，此时会导致$\exp(a)$发生上溢出现象，导致计算出现问题。</li></ul><p>&#x3D;&#x3D;通过将softmax公式中的x替换为x-max（x）来避免上溢出和下溢出问题：&#x3D;&#x3D;</p><p>为了解决上溢出和下溢出的问题，在计算Softmax函数时，可以使用$x_k - \max(\mathbf x)$代替$x_k$。 此时，通过减去最大值，$x_k$最大为0，避免了上溢出的问题；同时，分母中至少会包含一个值为1的项，从而也避免了下溢出的问题。</p><p>Softmax函数的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># x为tensor</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">softmax</span>(<span class="hljs-params">X</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">        - X：shape=[N, C]，N为向量数量，C为向量维度</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    x_max = paddle.<span class="hljs-built_in">max</span>(X, axis=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<span class="hljs-comment">#N,1</span><br>    x_exp = paddle.exp(X - x_max)<br>    partition = paddle.<span class="hljs-built_in">sum</span>(x_exp, axis=<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<span class="hljs-comment">#N,1</span><br>    <span class="hljs-keyword">return</span> x_exp / partition<br><br><span class="hljs-comment"># 观察softmax的计算方式</span><br>X = paddle.to_tensor([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])<br>predict = softmax(X)<br><span class="hljs-built_in">print</span>(predict)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.21383820</span>, <span class="hljs-number">0.23632778</span>, <span class="hljs-number">0.26118258</span>, <span class="hljs-number">0.28865141</span>],<br>        [<span class="hljs-number">0.03205860</span>, <span class="hljs-number">0.08714432</span>, <span class="hljs-number">0.23688281</span>, <span class="hljs-number">0.64391422</span>]])<br></code></pre></td></tr></table></figure><p>对于每个c维输入x，预测输出得到一个总和为1的c维概率分布。</p><hr><h5 id="√-3-2-2-2-Softmax回归算子"><a href="#√-3-2-2-2-Softmax回归算子" class="headerlink" title="[√] 3.2.2.2 Softmax回归算子"></a>[√] 3.2.2.2 Softmax回归算子</h5><p>在Softmax回归中，类别标签$y\in{1,2,…,C}$。给定一个样本$\mathbf x$，使用Softmax回归预测的属于类别$c$的条件概率为</p><p>$$<br>\begin{aligned}<br>p(y&#x3D;c|\mathbf x) &amp;&#x3D; \mathrm{softmax}(\mathbf w_c^T \mathbf x+b_c)，（3.12）<br>\end{aligned}<br>$$</p><p>其中$\mathbf w_c$是第 $c$ 类的权重向量，$b_c$是第 $c$ 类的偏置。</p><p>Softmax回归模型其实就是线性函数与Softmax函数的组合。</p><p>将$N$个样本归为一组进行成批地预测。</p><p>$$<br>\hat{\mathbf Y} &#x3D; \mathrm{softmax}(\boldsymbol{X} \boldsymbol{W} + \mathbf b), (3.13)<br>$$</p><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{W}&#x3D;[\mathbf w_1,……,\mathbf w_C]$为$C$个类的权重向量组成的矩阵，$\hat{\mathbf Y}\in \mathbb{R}^{C}$为所有类别的预测条件概率组成的矩阵。</p><p>我们根据公式（3.13）实现Softmax回归算子，代码实现如下：</p><blockquote><p>每个类别有一个权重，因此c个类别就有c组权重，对于一个输入x，x和c组权重相运算，得到c个概率，最大的那个概率对应的类别就是预测的类别。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">model_SR</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim, output_dim</span>):<br>        <span class="hljs-built_in">super</span>(model_SR, self).__init__()<br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 将线性层的权重参数全部初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = paddle.zeros(shape=[input_dim, output_dim])<br>        <span class="hljs-comment"># self.params[&#x27;W&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, output_dim])</span><br>        <span class="hljs-comment"># 将线性层的偏置参数初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[output_dim])<br>        self.outputs = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs: shape=[N,D], N是样本数量，D是特征维度</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：预测值，shape=[N,C]，C是类别数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 线性计算</span><br>        score = paddle.matmul(inputs, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>] <span class="hljs-comment"># 线性函数，得到每个输入对应的c个类别的分数</span><br>        <span class="hljs-comment"># Softmax 函数</span><br>        self.outputs = softmax(score) <span class="hljs-comment"># 使用softmax函数将分数转换为概率</span><br>        <span class="hljs-keyword">return</span> self.outputs<br><br><span class="hljs-comment"># 随机生成1条长度为4的数据</span><br>inputs = paddle.randn(shape=[<span class="hljs-number">1</span>,<span class="hljs-number">4</span>]) <span class="hljs-comment"># 输入一个含有4维的向量</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Input is:&#x27;</span>, inputs)<br><span class="hljs-comment"># 实例化模型，这里令输入长度为4，输出类别数为3</span><br>model = model_SR(input_dim=<span class="hljs-number">4</span>, output_dim=<span class="hljs-number">3</span>)<br>outputs = model(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Output is:&#x27;</span>, outputs)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">Input <span class="hljs-keyword">is</span>: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">4</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[-<span class="hljs-number">0.06042910</span>,  <span class="hljs-number">0.97415614</span>,  <span class="hljs-number">0.28900006</span>,  <span class="hljs-number">0.37233669</span>]])<br>Output <span class="hljs-keyword">is</span>: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.33333334</span>, <span class="hljs-number">0.33333334</span>, <span class="hljs-number">0.33333334</span>]])<br></code></pre></td></tr></table></figure><p>从输出结果可以看出，采用全0初始化后，属于每个类别的条件概率均为$\frac{1}{C}$。这是因为，不论输入值的大小为多少，线性函数$f(\mathbf x;\mathbf W,\mathbf b)$的输出值恒为0。此时，再经过Softmax函数的处理，每个类别的条件概率恒等。</p><hr><h4 id="√-3-2-3-损失函数"><a href="#√-3-2-3-损失函数" class="headerlink" title="[√] 3.2.3 - 损失函数"></a>[√] 3.2.3 - 损失函数</h4><p>&#x3D;&#x3D;Softmax回归同样使用交叉熵损失作为损失函数，并使用梯度下降法对参数进行优化。&#x3D;&#x3D;</p><p>通常使用$C$维的one-hot类型向量$\mathbf y \in {0,1}^C$来表示多分类任务中的类别标签。对于类别$c$，其向量表示为：</p><p>$$<br>\mathbf y &#x3D; [I(1&#x3D;c),I(2&#x3D;c),…,I(C&#x3D;c)]^T，（3.14）<br>$$</p><p>其中$I(·)$是指示函数，即括号内的输入为“真”，$I(·)&#x3D;1$；否则，$I(·)&#x3D;0$。</p><p>给定有$N$个训练样本的训练集${(\mathbf x^{(n)},y^{(n)})} ^N_{n&#x3D;1}$，令$\hat{\mathbf y}^{(n)}&#x3D;\mathrm{softmax}(\mathbf W^ \mathrm{ T } \mathbf x^{(n)}+\mathbf b)$为样本$\mathbf x^{(n)}$在每个类别的后验概率。多分类问题的交叉熵损失函数定义为：</p><p>$$<br>\cal R(\mathbf W,\mathbf b) &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N (\mathbf y^{(n)})^ \mathrm{ T } \log\hat{\mathbf y}^{(n)} &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N \sum_{c&#x3D;1}^C \mathbf y_c^{(n)} \log\hat{\mathbf y}_c^{(n)}.（3.15）<br>$$</p><p>观察上式，$\mathbf y_c^{(n)}$在$c$为真实类别时为1，其余都为0。也就是说，交叉熵损失只关心正确类别的预测概率，因此，上式又可以优化为：</p><p>$$<br>\cal R(\mathbf W,\mathbf b) &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N \log [\hat{\mathbf y}^{(n)}]_{y^{(n)}},（3.16）<br>$$</p><p>其中$y^{(n)}$是第$n$个样本的标签。</p><p>因此，多类交叉熵损失函数的代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MultiCrossEntropyLoss</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.predicts = <span class="hljs-literal">None</span><br>        self.labels = <span class="hljs-literal">None</span><br>        self.num = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-keyword">return</span> self.forward(predicts, labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失值：shape=[1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.predicts = predicts<br>        self.labels = labels<br>        self.num = self.predicts.shape[<span class="hljs-number">0</span>]<br>        loss = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, self.num):<br>            index = self.labels[i]<br>            loss -= paddle.log(self.predicts[i][index]) <span class="hljs-comment"># 预测输入为正确的标签y类的概率的对数，因为y为1，所以此处直接写logy，不用写ylogy</span><br>        <span class="hljs-keyword">return</span> loss / self.num<br><br><span class="hljs-comment"># 测试一下</span><br><span class="hljs-comment"># 假设真实标签为第1类</span><br>labels = paddle.to_tensor([<span class="hljs-number">0</span>])<br><span class="hljs-comment"># 计算风险函数</span><br>mce_loss = MultiCrossEntropyLoss()<br><span class="hljs-built_in">print</span>(mce_loss(outputs, labels))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">1.09861231</span>])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-2-4-模型优化"><a href="#√-3-2-4-模型优化" class="headerlink" title="[√] 3.2.4 - 模型优化"></a>[√] 3.2.4 - 模型优化</h4><p>使用梯度下降算法进行参数学习</p><h5 id="√-3-2-4-1-梯度计算"><a href="#√-3-2-4-1-梯度计算" class="headerlink" title="[√] 3.2.4.1 - 梯度计算"></a>[√] 3.2.4.1 - 梯度计算</h5><p>计算风险函数$\cal R(\mathbf W,\mathbf b)$关于参数$\mathbf W$和$\mathbf b$的偏导数。在Softmax回归中，计算方法为：</p><p>$$<br>\frac{\partial \cal R(\mathbf W,\mathbf b)}{\partial \mathbf W} &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N \mathbf x^{(n)}(y^{(n)}- \hat{ y}^{(n)})^T &#x3D; -\frac{1}{N} \mathbf X^ \mathrm{ T } (\mathbf y- \hat{\mathbf y}),（3.17）<br>$$</p><p>$$<br>\frac{\partial \cal R(\mathbf W,\mathbf b)}{\partial \mathbf b} &#x3D; -\frac{1}{N}\sum_{n&#x3D;1}^N (y^{(n)}- \hat{y}^{(n)})^T &#x3D; -\frac{1}{N} \mathbf 1 (\mathbf y- \hat{\mathbf y}).（3.18）<br>$$</p><p>其中$\mathbf X\in \mathbb{R}^{N\times D}$为$N$个样本组成的矩阵，$\mathbf y\in \mathbb{R}^{N}$为$N$个样本标签组成的向量，$\hat{\mathbf y}\in \mathbb{R}^{N}$为$N$个样本的预测标签组成的向量，$\mathbf{1}$为$N$维的全1向量。</p><p>将上述计算方法定义在模型的<code>backward</code>函数中，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">model_SR</span>(op.Op):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_dim, output_dim</span>):<br>        <span class="hljs-built_in">super</span>(model_SR, self).__init__()<br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 将线性层的权重参数全部初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = paddle.zeros(shape=[input_dim, output_dim])<br>        <span class="hljs-comment"># self.params[&#x27;W&#x27;] = paddle.normal(mean=0, std=0.01, shape=[input_dim, output_dim])</span><br>        <span class="hljs-comment"># 将线性层的偏置参数初始化为0</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[output_dim])<br>        <span class="hljs-comment"># 存放参数的梯度</span><br>        self.grads = &#123;&#125;<br>        self.X = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br>        self.output_dim = output_dim<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        self.X = inputs<br>        <span class="hljs-comment"># 线性计算</span><br>        score = paddle.matmul(self.X, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-comment"># Softmax 函数</span><br>        self.outputs = softmax(score)<br>        <span class="hljs-keyword">return</span> self.outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]，其中N为样本数量</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># 计算偏导数</span><br>        N =labels.shape[<span class="hljs-number">0</span>]<br>        labels = paddle.nn.functional.one_hot(labels, self.output_dim)<br>        self.grads[<span class="hljs-string">&#x27;W&#x27;</span>] = -<span class="hljs-number">1</span> / N * paddle.matmul(self.X.t(), (labels-self.outputs))<br>        self.grads[<span class="hljs-string">&#x27;b&#x27;</span>] = -<span class="hljs-number">1</span> / N * paddle.matmul(paddle.ones(shape=[N]), (labels-self.outputs))<br></code></pre></td></tr></table></figure><hr><h5 id="√-3-2-4-2-参数更新"><a href="#√-3-2-4-2-参数更新" class="headerlink" title="[√] 3.2.4.2 - 参数更新"></a>[√] 3.2.4.2 - 参数更新</h5><p>在计算参数的梯度之后，我们使用3.1.4.2中实现的梯度下降法进行参数更新。</p><hr><h4 id="√-3-2-5-模型训练"><a href="#√-3-2-5-模型训练" class="headerlink" title="[√] 3.2.5 - 模型训练"></a>[√] 3.2.5 - 模型训练</h4><p>实例化RunnerV2类，并传入训练配置。使用训练集和验证集进行模型训练，共训练500个epoch。每隔50个epoch打印训练集上的指标。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 固定随机种子，保持每次运行结果一致</span><br>paddle.seed(<span class="hljs-number">102</span>)<br><br><span class="hljs-comment"># 特征维度</span><br>input_dim = <span class="hljs-number">2</span><br><span class="hljs-comment"># 类别数</span><br>output_dim = <span class="hljs-number">3</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.1</span><br><br><span class="hljs-comment"># 实例化模型</span><br>model = model_SR(input_dim=input_dim, output_dim=output_dim)<br><span class="hljs-comment"># 指定优化器</span><br>optimizer = SimpleBatchGD(init_lr=lr, model=model)<br><span class="hljs-comment"># 指定损失函数</span><br>loss_fn = MultiCrossEntropyLoss()<br><span class="hljs-comment"># 指定评价方式</span><br>metric = accuracy<br><span class="hljs-comment"># 实例化RunnerV2类</span><br>runner = RunnerV2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 模型训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">500</span>, log_eopchs=<span class="hljs-number">50</span>, eval_epochs=<span class="hljs-number">1</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br><span class="hljs-comment"># 可视化观察训练集与验证集的准确率变化情况</span><br>plot(runner,fig_name=<span class="hljs-string">&#x27;linear-acc2.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212234936572.png" alt="image-20221214113635513"></p><hr><h4 id="√-3-2-6-模型评价"><a href="#√-3-2-6-模型评价" class="headerlink" title="[√] 3.2.6 - 模型评价"></a>[√] 3.2.6 - 模型评价</h4><p>使用测试集对训练完成后的最终模型进行评价，观察模型在测试集上的准确率。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">score, loss = runner.evaluate([X_test, y_test])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] score/loss: <span class="hljs-number">0.7400</span>/<span class="hljs-number">0.7366</span><br></code></pre></td></tr></table></figure><p>可视化观察类别划分结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 均匀生成40000个数据点</span><br>x1, x2 = paddle.meshgrid(paddle.linspace(-<span class="hljs-number">3.5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">200</span>), paddle.linspace(-<span class="hljs-number">4.5</span>, <span class="hljs-number">3.5</span>, <span class="hljs-number">200</span>))<br>x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 预测对应类别</span><br>y = runner.predict(x)<br>y = paddle.argmax(y, axis=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 绘制类别区域</span><br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.scatter(x[:,<span class="hljs-number">0</span>].tolist(), x[:,<span class="hljs-number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)<br><br>paddle.seed(<span class="hljs-number">102</span>)<br>n_samples = <span class="hljs-number">1000</span><br>X, y = make_multiclass_classification(n_samples=n_samples, n_features=<span class="hljs-number">2</span>, n_classes=<span class="hljs-number">3</span>, noise=<span class="hljs-number">0.2</span>)<br><br>plt.scatter(X[:, <span class="hljs-number">0</span>].tolist(), X[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=y.tolist())<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214123751267.png" alt="image-20221214113720218"></p><hr><p><strong>拓展</strong></p><p>提前停止是在使用梯度下降法进行模型优化时常用的正则化方法。对于某些拟合能力非常强的机器学习算法，当训练轮数较多时，容易发生过拟合现象，即在训练集上错误率很低，但是在未知数据（或测试集）上错误率很高。为了解决这一问题，通常会在模型优化时，使用验证集上的错误代替期望错误。当验证集上的错误率不在下降时，就停止迭代。</p><p>在3.4.3节的实验中，模型训练过程中会按照提前停止的思想保存验证集上的最优模型。</p><h3 id="√-3-3-实践：基于Softmax回归完成鸢尾花分类任务"><a href="#√-3-3-实践：基于Softmax回归完成鸢尾花分类任务" class="headerlink" title="[√] 3.3 - 实践：基于Softmax回归完成鸢尾花分类任务"></a>[√] 3.3 - 实践：基于Softmax回归完成鸢尾花分类任务</h3><p>在本节，我们用入门深度学习的基础实验之一“鸢尾花分类任务”来进行实践，使用经典学术数据集Iris作为训练数据，实现基于Softmax回归的鸢尾花分类任务。</p><p>实践流程主要包括以下7个步骤：数据处理、模型构建、损失函数定义、优化器构建、模型训练、模型评价和模型预测等，</p><ul><li>数据处理：根据网络接收的数据格式，完成相应的预处理操作，保证模型正常读取；</li><li>模型构建：定义Softmax回归模型类；</li><li>训练配置：训练相关的一些配置，如：优化算法、评价指标等；</li><li>组装Runner类：Runner用于管理模型训练和测试过程；</li><li>模型训练和测试：利用Runner进行模型训练、评价和测试。</li></ul><hr><p><strong>说明：</strong></p><p>使用深度学习进行实践时的操作流程基本一致，后文不再赘述。</p><hr><p>本实践的主要配置如下：</p><ul><li>数据：Iris数据集；</li><li>模型：Softmax回归模型；</li><li>损失函数：交叉熵损失；</li><li>优化器：梯度下降法；</li><li>评价指标：准确率。</li></ul><hr><h4 id="√-3-3-1-数据处理"><a href="#√-3-3-1-数据处理" class="headerlink" title="[√] 3.3.1 - 数据处理"></a>[√] 3.3.1 - 数据处理</h4><h5 id="√-3-3-1-1-数据集介绍"><a href="#√-3-3-1-1-数据集介绍" class="headerlink" title="[√] 3.3.1.1 - 数据集介绍"></a>[√] 3.3.1.1 - 数据集介绍</h5><blockquote><p>输入特征维度：4</p><p>输出类别维度：3</p></blockquote><p><strong>鸢尾花属性类别对应预览</strong></p><p><strong>鸢尾花属性</strong></p><center><table><thead><tr><th align="center">属性1</th><th align="center">属性2</th><th align="center">属性3</th><th align="center">属性4</th></tr></thead><tbody><tr><td align="center">sepal_length</td><td align="center">sepal_width</td><td align="center">petal_length</td><td align="center">petal_width</td></tr><tr><td align="center">花萼长度</td><td align="center">花萼宽度</td><td align="center">花瓣长度</td><td align="center">花瓣宽度</td></tr></tbody></table></center><p><strong>鸢尾花类别</strong></p><center>| 英文名 | 中文名 | 标签 || ------ | ------ | ---- ||        |        |      || :--: | :--: | :--: || ---- | ---- | ---- ||      |      |      ||Setosa Iris      |狗尾草鸢尾   |  0|Versicolour Iris |杂色鸢尾     |  1|Virginica Iris   |<p><strong>鸢尾花属性类别对应预览</strong></p><center><table><thead><tr><th align="center">sepal_length</th><th align="center">sepal_width</th><th align="center">petal_length</th><th align="center">petal_width</th><th align="center">species</th></tr></thead><tbody><tr><td align="center">5.1</td><td align="center">3.5</td><td align="center">1.4</td><td align="center">0.2</td><td align="center">setosa</td></tr><tr><td align="center">4.9</td><td align="center">3</td><td align="center">1.4</td><td align="center">0.2</td><td align="center">setosa</td></tr><tr><td align="center">4.7</td><td align="center">3.2</td><td align="center">1.3</td><td align="center">0.2</td><td align="center">setosa</td></tr><tr><td align="center">…</td><td align="center">…</td><td align="center">…</td><td align="center">…</td><td align="center">…</td></tr></tbody></table></center><hr><h5 id="√-3-3-1-2-数据清洗"><a href="#√-3-3-1-2-数据清洗" class="headerlink" title="[√] 3.3.1.2 - 数据清洗"></a>[√] 3.3.1.2 - 数据清洗</h5><ul><li><strong>缺失值分析</strong></li></ul><p>对数据集中的缺失值或异常值等情况进行分析和处理，保证数据可以被模型正常读取。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> load_iris<br><span class="hljs-keyword">import</span> pandas<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>iris_features = np.array(load_iris().data, dtype=np.float32)<br>iris_labels = np.array(load_iris().target, dtype=np.int32)<br><span class="hljs-built_in">print</span>(pandas.isna(iris_features).<span class="hljs-built_in">sum</span>())<br><span class="hljs-built_in">print</span>(pandas.isna(iris_labels).<span class="hljs-built_in">sum</span>())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-number">0</span><br><span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><p>从输出结果看，鸢尾花数据集中不存在缺失值的情况。</p><ul><li><strong>异常值处理</strong></li></ul><p>通过箱线图直观的显示数据分布，并观测数据中的异常值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment">#可视化工具</span><br><br><span class="hljs-comment"># 箱线图查看异常值分布</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">boxplot</span>(<span class="hljs-params">features</span>):<br>    feature_names = [<span class="hljs-string">&#x27;sepal_length&#x27;</span>, <span class="hljs-string">&#x27;sepal_width&#x27;</span>, <span class="hljs-string">&#x27;petal_length&#x27;</span>, <span class="hljs-string">&#x27;petal_width&#x27;</span>]<br><br>    <span class="hljs-comment"># 连续画几个图片</span><br>    plt.figure(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), dpi=<span class="hljs-number">200</span>)<br>    <span class="hljs-comment"># 子图调整</span><br>    plt.subplots_adjust(wspace=<span class="hljs-number">0.6</span>)<br>    <span class="hljs-comment"># 每个特征画一个箱线图</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">4</span>):<br>        plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, i+<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 画箱线图</span><br>        plt.boxplot(features[:, i], <br>                    showmeans=<span class="hljs-literal">True</span>, <br>                    whiskerprops=&#123;<span class="hljs-string">&quot;color&quot;</span>:<span class="hljs-string">&quot;#E20079&quot;</span>, <span class="hljs-string">&quot;linewidth&quot;</span>:<span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;linestyle&#x27;</span>:<span class="hljs-string">&quot;--&quot;</span>&#125;,<br>                    flierprops=&#123;<span class="hljs-string">&quot;markersize&quot;</span>:<span class="hljs-number">0.4</span>&#125;,<br>                    meanprops=&#123;<span class="hljs-string">&quot;markersize&quot;</span>:<span class="hljs-number">1</span>&#125;)<br>        <span class="hljs-comment"># 图名</span><br>        plt.title(feature_names[i], fontdict=&#123;<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">5</span>&#125;, pad=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># y方向刻度</span><br>        plt.yticks(fontsize=<span class="hljs-number">4</span>, rotation=<span class="hljs-number">90</span>)<br>        plt.tick_params(pad=<span class="hljs-number">0.5</span>)<br>        <span class="hljs-comment"># x方向刻度</span><br>        plt.xticks([])<br>    plt.savefig(<span class="hljs-string">&#x27;ml-vis.pdf&#x27;</span>)<br>    plt.show()<br><br>boxplot(iris_features)<br></code></pre></td></tr></table></figure><p>从输出结果看，数据中基本不存在异常值，所以不需要进行异常值处理。</p><hr><h5 id="√-3-3-1-3-数据读取"><a href="#√-3-3-1-3-数据读取" class="headerlink" title="[√] 3.3.1.3 - 数据读取"></a>[√] 3.3.1.3 - 数据读取</h5><p>本实验中将数据集划分为了三个部分：</p><ul><li>训练集：用于确定模型参数；</li><li>验证集：与训练集独立的样本集合，用于使用提前停止策略选择最优模型；</li><li>测试集：用于估计应用效果（没有在模型中应用过的数据，更贴近模型在真实场景应用的效果）。</li></ul><p>在本实验中，将$80%$的数据用于模型训练，$10%$的数据用于模型验证，$10%$的数据用于模型测试。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> copy<br><span class="hljs-keyword">import</span> paddle <br><br><span class="hljs-comment"># 加载数据集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">shuffle=<span class="hljs-literal">True</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    加载鸢尾花数据</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">        - shuffle：是否打乱数据，数据类型为bool</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">        - X：特征数据，shape=[150,4]</span><br><span class="hljs-string">        - y：标签数据, shape=[150]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># 加载原始数据</span><br>    X = np.array(load_iris().data, dtype=np.float32)<br>    y = np.array(load_iris().target, dtype=np.int32)<br><br>    X = paddle.to_tensor(X)<br>    y = paddle.to_tensor(y)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------start&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(X[:<span class="hljs-number">5</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------end\n&#x27;</span>)<br>    <span class="hljs-comment"># 数据归一化</span><br>    X_min = paddle.<span class="hljs-built_in">min</span>(X, axis=<span class="hljs-number">0</span>)<br>    X_max = paddle.<span class="hljs-built_in">max</span>(X, axis=<span class="hljs-number">0</span>)<br>    X = (X-X_min) / (X_max-X_min)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------start&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(X[:<span class="hljs-number">5</span>])<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------end\n&#x27;</span>)<br>    <span class="hljs-comment"># 如果shuffle为True，随机打乱数据</span><br>    <span class="hljs-keyword">if</span> shuffle:<br>        idx = paddle.randperm(X.shape[<span class="hljs-number">0</span>])<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------start&#x27;</span>)<br>        <span class="hljs-built_in">print</span>(idx)<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;---------------------------end\n&#x27;</span>)<br><br>        X = X[idx]<br>        y = y[idx]<br>    <span class="hljs-keyword">return</span> X, y<br><br><span class="hljs-comment"># 固定随机种子</span><br>paddle.seed(<span class="hljs-number">102</span>)<br><br>num_train = <span class="hljs-number">120</span><br>num_dev = <span class="hljs-number">15</span><br>num_test = <span class="hljs-number">15</span><br><br>X, y = load_data(shuffle=<span class="hljs-literal">True</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;X shape: &quot;</span>, X.shape, <span class="hljs-string">&quot;y shape: &quot;</span>, y.shape)<br>X_train, y_train = X[:num_train], y[:num_train]<br>X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python">---------------------------start<br>Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">4</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">5.09999990</span>, <span class="hljs-number">3.50000000</span>, <span class="hljs-number">1.39999998</span>, <span class="hljs-number">0.20000000</span>],<br>        [<span class="hljs-number">4.90000010</span>, <span class="hljs-number">3.</span>        , <span class="hljs-number">1.39999998</span>, <span class="hljs-number">0.20000000</span>],<br>        [<span class="hljs-number">4.69999981</span>, <span class="hljs-number">3.20000005</span>, <span class="hljs-number">1.29999995</span>, <span class="hljs-number">0.20000000</span>],<br>        [<span class="hljs-number">4.59999990</span>, <span class="hljs-number">3.09999990</span>, <span class="hljs-number">1.50000000</span>, <span class="hljs-number">0.20000000</span>],<br>        [<span class="hljs-number">5.</span>        , <span class="hljs-number">3.59999990</span>, <span class="hljs-number">1.39999998</span>, <span class="hljs-number">0.20000000</span>]])<br>---------------------------end<br><br>---------------------------start<br>Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">4</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.22222215</span>, <span class="hljs-number">0.62500000</span>, <span class="hljs-number">0.06779660</span>, <span class="hljs-number">0.04166666</span>],<br>        [<span class="hljs-number">0.16666664</span>, <span class="hljs-number">0.41666666</span>, <span class="hljs-number">0.06779660</span>, <span class="hljs-number">0.04166666</span>],<br>        [<span class="hljs-number">0.11111101</span>, <span class="hljs-number">0.50000000</span>, <span class="hljs-number">0.05084745</span>, <span class="hljs-number">0.04166666</span>],<br>        [<span class="hljs-number">0.08333325</span>, <span class="hljs-number">0.45833328</span>, <span class="hljs-number">0.08474576</span>, <span class="hljs-number">0.04166666</span>],<br>        [<span class="hljs-number">0.19444440</span>, <span class="hljs-number">0.66666663</span>, <span class="hljs-number">0.06779660</span>, <span class="hljs-number">0.04166666</span>]])<br>---------------------------end<br><br>---------------------------start<br>Tensor(shape=[<span class="hljs-number">150</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">21</span> , <span class="hljs-number">60</span> , <span class="hljs-number">77</span> , <span class="hljs-number">4</span>  , <span class="hljs-number">85</span> , <span class="hljs-number">14</span> , <span class="hljs-number">130</span>, <span class="hljs-number">24</span> , <span class="hljs-number">68</span> , <span class="hljs-number">133</span>, <span class="hljs-number">109</span>, <span class="hljs-number">117</span>, <span class="hljs-number">38</span> , <span class="hljs-number">99</span> ,<br>        <span class="hljs-number">78</span> , <span class="hljs-number">37</span> , <span class="hljs-number">29</span> , <span class="hljs-number">70</span> , <span class="hljs-number">144</span>, <span class="hljs-number">22</span> , <span class="hljs-number">136</span>, <span class="hljs-number">50</span> , <span class="hljs-number">100</span>, <span class="hljs-number">64</span> , <span class="hljs-number">132</span>, <span class="hljs-number">35</span> , <span class="hljs-number">118</span>, <span class="hljs-number">67</span> ,<br>        <span class="hljs-number">81</span> , <span class="hljs-number">89</span> , <span class="hljs-number">72</span> , <span class="hljs-number">48</span> , <span class="hljs-number">52</span> , <span class="hljs-number">94</span> , <span class="hljs-number">79</span> , <span class="hljs-number">44</span> , <span class="hljs-number">23</span> , <span class="hljs-number">123</span>, <span class="hljs-number">34</span> , <span class="hljs-number">116</span>, <span class="hljs-number">53</span> , <span class="hljs-number">6</span>  ,<br>        <span class="hljs-number">41</span> , <span class="hljs-number">1</span>  , <span class="hljs-number">27</span> , <span class="hljs-number">62</span> , <span class="hljs-number">80</span> , <span class="hljs-number">87</span> , <span class="hljs-number">88</span> , <span class="hljs-number">139</span>, <span class="hljs-number">75</span> , <span class="hljs-number">5</span>  , <span class="hljs-number">112</span>, <span class="hljs-number">91</span> , <span class="hljs-number">56</span> , <span class="hljs-number">104</span>,<br>        <span class="hljs-number">142</span>, <span class="hljs-number">31</span> , <span class="hljs-number">47</span> , <span class="hljs-number">127</span>, <span class="hljs-number">101</span>, <span class="hljs-number">9</span>  , <span class="hljs-number">26</span> , <span class="hljs-number">124</span>, <span class="hljs-number">98</span> , <span class="hljs-number">103</span>, <span class="hljs-number">32</span> , <span class="hljs-number">108</span>, <span class="hljs-number">73</span> , <span class="hljs-number">57</span> ,<br>        <span class="hljs-number">30</span> , <span class="hljs-number">114</span>, <span class="hljs-number">128</span>, <span class="hljs-number">120</span>, <span class="hljs-number">149</span>, <span class="hljs-number">134</span>, <span class="hljs-number">71</span> , <span class="hljs-number">43</span> , <span class="hljs-number">45</span> , <span class="hljs-number">46</span> , <span class="hljs-number">55</span> , <span class="hljs-number">146</span>, <span class="hljs-number">90</span> , <span class="hljs-number">12</span> ,<br>        <span class="hljs-number">83</span> , <span class="hljs-number">74</span> , <span class="hljs-number">0</span>  , <span class="hljs-number">33</span> , <span class="hljs-number">82</span> , <span class="hljs-number">115</span>, <span class="hljs-number">65</span> , <span class="hljs-number">15</span> , <span class="hljs-number">140</span>, <span class="hljs-number">20</span> , <span class="hljs-number">69</span> , <span class="hljs-number">11</span> , <span class="hljs-number">131</span>, <span class="hljs-number">107</span>,<br>        <span class="hljs-number">148</span>, <span class="hljs-number">19</span> , <span class="hljs-number">61</span> , <span class="hljs-number">93</span> , <span class="hljs-number">59</span> , <span class="hljs-number">51</span> , <span class="hljs-number">141</span>, <span class="hljs-number">105</span>, <span class="hljs-number">16</span> , <span class="hljs-number">54</span> , <span class="hljs-number">110</span>, <span class="hljs-number">145</span>, <span class="hljs-number">137</span>, <span class="hljs-number">135</span>,<br>        <span class="hljs-number">28</span> , <span class="hljs-number">10</span> , <span class="hljs-number">119</span>, <span class="hljs-number">63</span> , <span class="hljs-number">122</span>, <span class="hljs-number">121</span>, <span class="hljs-number">84</span> , <span class="hljs-number">17</span> , <span class="hljs-number">96</span> , <span class="hljs-number">138</span>, <span class="hljs-number">39</span> , <span class="hljs-number">129</span>, <span class="hljs-number">76</span> , <span class="hljs-number">95</span> ,<br>        <span class="hljs-number">2</span>  , <span class="hljs-number">113</span>, <span class="hljs-number">147</span>, <span class="hljs-number">102</span>, <span class="hljs-number">86</span> , <span class="hljs-number">126</span>, <span class="hljs-number">49</span> , <span class="hljs-number">18</span> , <span class="hljs-number">7</span>  , <span class="hljs-number">3</span>  , <span class="hljs-number">125</span>, <span class="hljs-number">92</span> , <span class="hljs-number">25</span> , <span class="hljs-number">66</span> ,<br>        <span class="hljs-number">40</span> , <span class="hljs-number">58</span> , <span class="hljs-number">36</span> , <span class="hljs-number">106</span>, <span class="hljs-number">111</span>, <span class="hljs-number">97</span> , <span class="hljs-number">42</span> , <span class="hljs-number">143</span>, <span class="hljs-number">13</span> , <span class="hljs-number">8</span>  ])<br>---------------------------end<br><br>X shape:  [<span class="hljs-number">150</span>, <span class="hljs-number">4</span>] y shape:  [<span class="hljs-number">150</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印X_train和y_train的维度</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;X_train shape: &quot;</span>, X_train.shape, <span class="hljs-string">&quot;y_train shape: &quot;</span>, y_train.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X_train shape:  [<span class="hljs-number">120</span>, <span class="hljs-number">4</span>] y_train shape:  [<span class="hljs-number">120</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印前5个数据的标签</span><br><span class="hljs-built_in">print</span>(y_train[:<span class="hljs-number">5</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">5</span>], dtype=int32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-3-2-模型构建"><a href="#√-3-3-2-模型构建" class="headerlink" title="[√] 3.3.2 - 模型构建"></a>[√] 3.3.2 - 模型构建</h4><p>使用Softmax回归模型进行鸢尾花分类实验，将模型的输入维度定义为4，输出维度定义为3。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> op<br><br><span class="hljs-comment"># 输入维度</span><br>input_dim = <span class="hljs-number">4</span><br><span class="hljs-comment"># 类别数</span><br>output_dim = <span class="hljs-number">3</span><br><span class="hljs-comment"># 实例化模型</span><br>model = op.model_SR(input_dim=input_dim, output_dim=output_dim)<br></code></pre></td></tr></table></figure><hr><h4 id="√-3-3-3-模型训练"><a href="#√-3-3-3-模型训练" class="headerlink" title="[√] 3.3.3 - 模型训练"></a>[√] 3.3.3 - 模型训练</h4><p>实例化RunnerV2类，使用训练集和验证集进行模型训练，共训练80个epoch，其中每隔10个epoch打印训练集上的指标，并且保存准确率最高的模型作为最佳模型。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> op, metric, opitimizer, RunnerV2<br><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.2</span><br><br><span class="hljs-comment"># 梯度下降法</span><br>optimizer = opitimizer.SimpleBatchGD(init_lr=lr, model=model)<br><span class="hljs-comment"># 交叉熵损失</span><br>loss_fn = op.MultiCrossEntropyLoss()<br><span class="hljs-comment"># 准确率</span><br>metric = metric.accuracy<br><br><span class="hljs-comment"># 实例化RunnerV2</span><br>runner = RunnerV2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">200</span>, log_epochs=<span class="hljs-number">10</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python">best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.86667</span><br>[Train] epoch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">0.48671841621398926</span>, score: <span class="hljs-number">0.8916666507720947</span><br>[Dev] epoch: <span class="hljs-number">0</span>, loss: <span class="hljs-number">0.511984646320343</span>, score: <span class="hljs-number">0.8666666746139526</span><br>[Train] epoch: <span class="hljs-number">20</span>, loss: <span class="hljs-number">0.4710509479045868</span>, score: <span class="hljs-number">0.8916666507720947</span><br>[Dev] epoch: <span class="hljs-number">20</span>, loss: <span class="hljs-number">0.4989580810070038</span>, score: <span class="hljs-number">0.8666666746139526</span><br>[Train] epoch: <span class="hljs-number">40</span>, loss: <span class="hljs-number">0.45711272954940796</span>, score: <span class="hljs-number">0.8999999761581421</span><br>[Dev] epoch: <span class="hljs-number">40</span>, loss: <span class="hljs-number">0.4874427020549774</span>, score: <span class="hljs-number">0.8666666746139526</span><br>[Train] epoch: <span class="hljs-number">60</span>, loss: <span class="hljs-number">0.4445931017398834</span>, score: <span class="hljs-number">0.8999999761581421</span><br>[Dev] epoch: <span class="hljs-number">60</span>, loss: <span class="hljs-number">0.47714489698410034</span>, score: <span class="hljs-number">0.8666666746139526</span><br>[Train] epoch: <span class="hljs-number">80</span>, loss: <span class="hljs-number">0.43325552344322205</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">80</span>, loss: <span class="hljs-number">0.4678438603878021</span>, score: <span class="hljs-number">0.8666666746139526</span><br>best accuracy performence has been updated: <span class="hljs-number">0.86667</span> --&gt; <span class="hljs-number">0.93333</span><br>[Train] epoch: <span class="hljs-number">100</span>, loss: <span class="hljs-number">0.4229157567024231</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">100</span>, loss: <span class="hljs-number">0.4593701660633087</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">120</span>, loss: <span class="hljs-number">0.41342711448669434</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">120</span>, loss: <span class="hljs-number">0.45159193873405457</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">140</span>, loss: <span class="hljs-number">0.4046727418899536</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">140</span>, loss: <span class="hljs-number">0.4444047808647156</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">160</span>, loss: <span class="hljs-number">0.3965565264225006</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">160</span>, loss: <span class="hljs-number">0.4377250373363495</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">180</span>, loss: <span class="hljs-number">0.38899996876716614</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">180</span>, loss: <span class="hljs-number">0.43148478865623474</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">200</span>, loss: <span class="hljs-number">0.3819374144077301</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">200</span>, loss: <span class="hljs-number">0.42562830448150635</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">220</span>, loss: <span class="hljs-number">0.3753131628036499</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">220</span>, loss: <span class="hljs-number">0.42010951042175293</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">240</span>, loss: <span class="hljs-number">0.369081050157547</span>, score: <span class="hljs-number">0.9166666865348816</span><br>[Dev] epoch: <span class="hljs-number">240</span>, loss: <span class="hljs-number">0.41488999128341675</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">260</span>, loss: <span class="hljs-number">0.36320066452026367</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">260</span>, loss: <span class="hljs-number">0.40993720293045044</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">280</span>, loss: <span class="hljs-number">0.3576377034187317</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">280</span>, loss: <span class="hljs-number">0.4052235782146454</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">300</span>, loss: <span class="hljs-number">0.35236233472824097</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">300</span>, loss: <span class="hljs-number">0.4007255733013153</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">320</span>, loss: <span class="hljs-number">0.34734851121902466</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">320</span>, loss: <span class="hljs-number">0.3964228332042694</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">340</span>, loss: <span class="hljs-number">0.34257346391677856</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">340</span>, loss: <span class="hljs-number">0.3922978341579437</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">360</span>, loss: <span class="hljs-number">0.33801719546318054</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">360</span>, loss: <span class="hljs-number">0.388335257768631</span>, score: <span class="hljs-number">0.9333333373069763</span><br>[Train] epoch: <span class="hljs-number">380</span>, loss: <span class="hljs-number">0.3336619734764099</span>, score: <span class="hljs-number">0.925000011920929</span><br>[Dev] epoch: <span class="hljs-number">380</span>, loss: <span class="hljs-number">0.3845216929912567</span>, score: <span class="hljs-number">0.9333333373069763</span><br></code></pre></td></tr></table></figure><p>可视化观察训练集与验证集的准确率变化情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> plot<br><br>plot(runner,fig_name=<span class="hljs-string">&#x27;linear-acc3.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214113720218.png" alt="image-20221214123751267"></p><hr><h4 id="√-3-3-4-模型评价"><a href="#√-3-3-4-模型评价" class="headerlink" title="[√] 3.3.4 - 模型评价"></a>[√] 3.3.4 - 模型评价</h4><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率情况。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate([X_test, y_test])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] score/loss: <span class="hljs-number">0.9333</span>/<span class="hljs-number">0.4093</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-3-3-5-模型预测"><a href="#√-3-3-5-模型预测" class="headerlink" title="[√] 3.3.5 - 模型预测"></a>[√] 3.3.5 - 模型预测</h4><p>使用保存好的模型，对测试集中的数据进行模型预测，并取出1条数据观察模型效果。代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 预测测试集数据</span><br>logits = runner.predict(X_test)<br><span class="hljs-comment"># 观察其中一条样本的预测结果</span><br>pred = paddle.argmax(logits[<span class="hljs-number">0</span>]).numpy()<br><span class="hljs-comment"># 获取该样本概率最大的类别</span><br>label = y_test[<span class="hljs-number">0</span>].numpy()<br><span class="hljs-comment"># 输出真实类别与预测类别</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label[<span class="hljs-number">0</span>], pred[<span class="hljs-number">0</span>]))<br></code></pre></td></tr></table></figure><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">The <span class="hljs-literal">true</span> category <span class="hljs-keyword">is</span> <span class="hljs-number">0</span> and the predicted category <span class="hljs-keyword">is</span> <span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><hr><h3 id="√-3-4-小结"><a href="#√-3-4-小结" class="headerlink" title="[√] 3.4 - 小结"></a>[√] 3.4 - 小结</h3><p>本节实现了Logistic回归和Softmax回归两种基本的线性分类模型。</p><hr><h3 id="√-3-5-实验拓展"><a href="#√-3-5-实验拓展" class="headerlink" title="[√] 3.5 - 实验拓展"></a>[√] 3.5 - 实验拓展</h3><p>为了加深对机器学习模型的理解，请自己动手完成以下实验：</p><ol><li>尝试调整学习率和训练轮数等超参数，观察是否能够得到更高的精度；</li><li>在解决多分类问题时，还有一个思路是将每个类别的求解问题拆分成一个二分类任务，通过判断是否属于该类别来判断最终结果。请分别尝试两种求解思路，观察哪种能够取得更好的结果；</li><li>尝试使用《神经网络与深度学习》中的其他模型进行鸢尾花识别任务，观察是否能够得到更高的精度。</li></ol><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第2章 - 机器学习概述</title>
    <link href="/posts/2499540575/"/>
    <url>/posts/2499540575/</url>
    
    <content type="html"><![CDATA[<h2 id="√-第二章-机器学习概述"><a href="#√-第二章-机器学习概述" class="headerlink" title="[√] 第二章 机器学习概述"></a>[√] 第二章 机器学习概述</h2><p><strong>机器学习</strong>（Machine Learning，ML）就是让计算机从数据中进行自动学习，得到某种知识（或规律）。作为一门学科，机器学习通常指一类问题以及解决这类问题的方法，即如何从观测数据（样本）中寻找规律，并利用学习到的规律（模型）对未知或无法观测的数据进行预测。</p><p>在学习本章内容前，建议您先阅读《神经网络与深度学习》第 2 章：机器学习概述的相关内容，关键知识点如<strong>图2.1</strong>所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211220940535.png" alt="image-20221211210937929"></p><p>本章内容基于《神经网络与深度学习》第 2 章：机器学习概述 相关内容进行设计，主要包含两部分：</p><ul><li><strong>模型解读</strong>：介绍机器学习实践五要素（数据、模型、学习准则、优化算法、评估指标）的原理剖析和相应的代码实现。通过理论和代码的结合，加深对机器学习的理解；</li><li><strong>案例实践</strong>：基于机器学习线性回归方法，通过数据处理、模型构建、训练配置、组装训练框架Runner、模型训练和模型预测等过程完成波士顿房价预测任务。</li></ul><h3 id="√-2-1-机器学习实践五要素"><a href="#√-2-1-机器学习实践五要素" class="headerlink" title="[√] 2.1 - 机器学习实践五要素"></a>[√] 2.1 - 机器学习实践五要素</h3><p>要通过机器学习来解决一个特定的任务时，我们需要准备5个方面的要素：</p><ol><li>数据集：收集任务相关的数据集用来进行模型训练和测试，可分为训练集、验证集和测试集；</li><li>模型：实现输入到输出的映射，通常为可学习的函数；</li><li>学习准则：模型优化的目标，通常为损失函数和正则化项的加权组合；</li><li>优化算法：根据学习准则优化机器学习模型的参数；</li><li>评价指标：用来评价学习到的机器学习模型的性能．</li></ol><p><strong>图2.2</strong>给出实现一个完整的机器学习系统的主要环节和要素。从流程角度看，实现机器学习系统可以分为两个阶段：训练阶段和评价阶段。训练阶段需要用到训练集、验证集、待学习的模型、损失函数、优化算法，输出学习到的模型；评价阶段也称为测试阶段，需要用到测试集、学习到的模型、评价指标体系，得到模型的性能评价。<strong>图2.2</strong>给出实现一个完整的机器学习系统的主要环节和要素。从流程角度看，实现机器学习系统可以分为两个阶段：训练阶段和评价阶段。训练阶段需要用到训练集、验证集、待学习的模型、损失函数、优化算法，输出学习到的模型；评价阶段也称为测试阶段，需要用到测试集、学习到的模型、评价指标体系，得到模型的性能评价。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211210937929.png" alt="image-20221211213007230"></p><p>在本节中，我们分别对这五个要素进行简单的介绍。</p><hr><p>《神经网络与深度学习》第 2.2 节详细介绍了机器学习的三个基本要素：“模型”、“学习准则”和“优化算法”．在机器学习实践中，“数据”和“评价指标”也非常重要．因此，本书将机器学习在实践中的主要元素归结为五要素．</p><h4 id="√-2-1-1-数据"><a href="#√-2-1-1-数据" class="headerlink" title="[√] 2.1.1 - 数据"></a>[√] 2.1.1 - 数据</h4><p>在实践中，数据的质量会很大程度上影响模型最终的性能，通常数据预处理是完成机器学习实践的第一步，噪音越少、规模越大、覆盖范围越广的数据集往往能够训练出性能更好的模型。数据预处理可分为两个环节：先对收集到的数据进行基本的预处理，如基本的统计、特征归一化和异常值处理等；再将数据划分为训练集、验证集（开发集）和测试集。</p><ul><li><strong>训练集</strong>：用于模型训练时调整模型的参数，在这份数据集上的误差被称为训练误差；</li><li><strong>验证集（开发集）</strong>：对于复杂的模型，常常有一些超参数需要调节，因此需要尝试多种超参数的组合来分别训练多个模型，然后对比它们在验证集上的表现，选择一组相对最好的超参数，最后才使用这组参数下训练的模型在测试集上评估测试误差。</li><li><strong>测试集</strong>：模型在这份数据集上的误差被称为测试误差。训练模型的目的是为了通过从训练数据中找到规律来预测未知数据，因此测试误差是更能反映出模型表现的指标。</li></ul><p>数据划分时要考虑到两个因素：更多的训练数据会降低参数估计的方差，从而得到更可信的模型；而更多的测试数据会降低测试误差的方差，从而得到更可信的测试误差。如果给定的数据集没有做任何划分，我们一般可以大致按照7:3或者8:2的比例划分训练集和测试集，再根据7:3或者8:2的比例从训练集中再次划分出训练集和验证集。</p><p>需要强调的是，测试集只能用来评测模型最终的性能，在整个模型训练过程中不能有测试集的参与。</p><hr><h4 id="√-2-1-2-模型"><a href="#√-2-1-2-模型" class="headerlink" title="[√] 2.1.2 模型"></a>[√] 2.1.2 模型</h4><p>有了数据后，我们可以用数据来训练模型。我们希望能让计算机从一个函数集合 $\mathcal{F} &#x3D; {f_1(\boldsymbol{x}), f_2(\boldsymbol{x}), \cdots }$中<br>自动寻找一个“最优”的函数$f^∗(\boldsymbol{x})$ 来近似每个样本的特征向量 $\boldsymbol{x}$ 和标签 $y$ 之间<br>的真实映射关系，实际上这个函数集合也被称为<strong>假设空间</strong>，在实际问题中，假设空间$\mathcal{F}$通常为一个参数化的函数族<br>$$<br>\mathcal{F}&#x3D;\left{f(\boldsymbol{x} ; \theta) \mid \theta \in \mathbb{R}^{D}\right}, (2.1)<br>$$<br>其中$f(\boldsymbol{x} ; \theta)$是参数为$\theta$的函数，也称为模型，𝐷 为参数的数量。</p><p>常见的假设空间可以分为线性和非线性两种，对应的模型 $f$ 也分别称为线性模型和非线性模型。<strong>线性模型</strong>的假设空间为一个参数化的线性函数族，即：<br>$$<br>f(\boldsymbol{x} ; \theta)&#x3D;\boldsymbol{w}^{\top} \boldsymbol{x}+b, (2.2)<br>$$<br>其中参数$\theta$ 包含了权重向量$\boldsymbol{w}$和偏置$b$。</p><p>线性模型可以由<strong>非线性基函数</strong>$\phi(\boldsymbol{x})$变为<strong>非线性模型</strong>，从而增强模型能力:</p><p>$$<br>f(\boldsymbol{x} ; \theta)&#x3D;\boldsymbol{w}^{\top} \phi(\boldsymbol{x})+b, (2.3)<br>$$<br>其中$\phi(\boldsymbol{x})&#x3D;\left[\phi_{1}(\boldsymbol{x}), \phi_{2}(\boldsymbol{x}), \cdots, \phi_{K}(\boldsymbol{x})\right]^{\top}$为𝐾 个非线性基函数组成的向量，参数 $\theta$ 包含了权重向量$\boldsymbol{w}$和偏置$b$。</p><hr><h4 id="√-2-1-3-学习准则"><a href="#√-2-1-3-学习准则" class="headerlink" title="[√] 2.1.3 学习准则"></a>[√] 2.1.3 学习准则</h4><p>为了衡量一个模型的好坏，我们需要定义一个损失函数$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$。损失函数是一个非负实数函数，用来量化模型预测标签和真实标签之间的差异。常见的损失函数有 0-1 损失、平方损失函数、交叉熵损失函数等。</p><p>机器学习的目标就是找到最优的模型$𝑓(𝒙;\theta^∗)$在真实数据分布上损失函数的期望最小。然而在实际中，我们无法获得真实数据分布，通常会用在训练集上的平均损失替代。</p><p>一个模型在训练集$\mathcal{D}&#x3D;{(\boldsymbol{x}^{(n)},y^{(n)})}_{n&#x3D;1}^N$上的平均损失称为<strong>经验风险</strong>{Empirical Risk}，即:</p><p>$$<br>\mathcal{R}^{emp}<em>\mathcal{D}(\theta)&#x3D;\frac{1}{N}\sum</em>{n&#x3D;1}^{N}\mathcal{L}(y^{(n)},f({x}^{(n)};\theta))。 (2.4)<br>$$</p><p>$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$为损失函数。损失函数是一个非负实数函数，用来量化模型预测和真实标签之间的差异。常见的损失函数有0-1损失、平方损失函数、交叉熵损失函数等。</p><p>在通常情况下，我们可以通过使得<strong>经验风险最小化</strong>来获得具有预测能力的模型。然而，当模型比较复杂或训练数据量比较少时，经验风险最小化获得的模型在测试集上的效果比较差。而模型在测试集上的性能才是我们真正关心的指标．当一个模型在训练集错误率很低，而在测试集上错误率较高时，通常意味着发生了<strong>过拟合</strong>（Overfitting）现象。为了缓解模型的过拟合问题，我们通常会在经验损失上加上一定的正则化项来限制模型能力。</p><p>过拟合通常是由于模型复杂度比较高引起的。在实践中，最常用的正则化方式有对模型的参数进行约束，比如$\ell_1$或者$\ell_2$范数约束。这样，我们就得到了结构风险（Structure Risk）。<br>$$<br>\mathcal{R}^{struct}<em>{\mathcal{D}}(\theta)&#x3D;\mathcal{R}^{emp}</em>{\mathcal{D}}(\theta)+\lambda \ell_p(\theta), (2.5)<br>$$</p><p>其中$\lambda$为正则化系数，$p&#x3D;1$或$2$表示$\ell_1$或者$\ell_2$范数。</p><hr><h4 id="√-2-1-4-优化算法"><a href="#√-2-1-4-优化算法" class="headerlink" title="[√] 2.1.4 优化算法"></a>[√] 2.1.4 优化算法</h4><p>在有了优化目标之后，机器学习问题就转化为优化问题，我们可以利用已知的优化算法来学习最优的参数。当优化函数为凸函数时，我们可以令参数的偏导数等于0来计算最优参数的解析解。当优化函数为非凸函数时，我们可以用一阶的优化算法来进行优化。</p><p>目前机器学习中最常用的优化算法是<strong>梯度下降法</strong>(Gradient Descent Method)。<br>当使用梯度下降法进行参数优化时，还可以利用验证集来<strong>早停法</strong>(Early-Stop)来中止模型的优化过程，避免模型在训练集上过拟合。早停法也是一种常用并且十分有效的正则化方法。</p><hr><h4 id="√-2-1-5-评估指标"><a href="#√-2-1-5-评估指标" class="headerlink" title="[√] 2.1.5 评估指标"></a>[√] 2.1.5 评估指标</h4><p><strong>评估指标</strong>(Metric)用于评价模型效果，即给定一个测试集，用模型对测试集中的每个样本进行预测，并根据预测结果计算评价分数。回归任务的评估指标一般有预测值与真实值的均方差，分类任务的评估指标一般有准确率、召回率、F1值等。</p><p>对于一个机器学习任务，一般会先确定任务类型，再确定任务的评价指标，再根据评价指标来建立模型，选择学习准则。由于评价指标不可微等问题有时候学习准则并不能完全和评价指标一致，我们往往会选择一定的损失函数使得两者尽可能一致。</p><hr><h3 id="√-2-2-实现一个简单的线性回归模型"><a href="#√-2-2-实现一个简单的线性回归模型" class="headerlink" title="[√] 2.2 - 实现一个简单的线性回归模型"></a>[√] 2.2 - 实现一个简单的线性回归模型</h3><p><strong>回归任务</strong>是一类典型的监督机器学习任务，对自变量和因变量之间关系进行建模分析，其预测值通常为一个连续值，比如房屋价格预测、电源票房预测等。<strong>线性回归</strong>(Linear Regression)是指一类利用线性函数来对自变量和因变量之间关系进行建模的回归任务，是机器学习和统计学中最基础和最广泛应用的模型。</p><p>在本节中，我们动手实现一个简单的线性回归模型，并使用最小二乘法来求解参数，以对机器学习任务有更直观的认识。</p><hr><h4 id="√-2-2-1-数据集构建"><a href="#√-2-2-1-数据集构建" class="headerlink" title="[√] 2.2.1 - 数据集构建"></a>[√] 2.2.1 - 数据集构建</h4><p>首先，我们构造一个小的回归数据集。假设输入特征和输出标签的维度都为 1，需要被拟合的函数定义为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 真实函数的参数缺省值为 w=1.2，b=0.5</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">linear_func</span>(<span class="hljs-params">x,w=<span class="hljs-number">1.2</span>,b=<span class="hljs-number">0.5</span></span>):<br>    y = w*x + b<br>    <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>然后，使用<code>paddle.rand()</code>函数来进行随机采样输入特征xx<em>x</em>，并代入上面函数得到输出标签𝑦𝑦<em>y</em>。为了模拟真实环境中样本通常包含噪声的问题，我们采样过程中加入高斯噪声和异常点。</p><p>生成样本数据的函数<code>create_toy_data</code>实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_toy_data</span>(<span class="hljs-params">func, interval, sample_num, noise = <span class="hljs-number">0.0</span>, add_outlier = <span class="hljs-literal">False</span>, outlier_ratio = <span class="hljs-number">0.001</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    根据给定的函数，生成样本</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - func：函数</span><br><span class="hljs-string">       - interval： x的取值范围</span><br><span class="hljs-string">       - sample_num： 样本数目</span><br><span class="hljs-string">       - noise： 噪声均方差</span><br><span class="hljs-string">       - add_outlier：是否生成异常值</span><br><span class="hljs-string">       - outlier_ratio：异常值占比</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - X: 特征数据，shape=[n_samples,1]</span><br><span class="hljs-string">       - y: 标签数据，shape=[n_samples,1]</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># 均匀采样</span><br>    <span class="hljs-comment"># 使用paddle.rand在生成sample_num个随机数</span><br>    X = paddle.rand(shape = [sample_num]) * (interval[<span class="hljs-number">1</span>]-interval[<span class="hljs-number">0</span>]) + interval[<span class="hljs-number">0</span>]<br>    y = func(X)<br><br>    <span class="hljs-comment"># 生成高斯分布的标签噪声</span><br>    <span class="hljs-comment"># 使用paddle.normal生成0均值，noise标准差的数据</span><br>    epsilon = paddle.normal(<span class="hljs-number">0</span>,noise,paddle.to_tensor(y.shape[<span class="hljs-number">0</span>]))<br>    y = y + epsilon<br>    <span class="hljs-keyword">if</span> add_outlier:     <span class="hljs-comment"># 生成额外的异常点</span><br>        outlier_num = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(y)*outlier_ratio)<br>        <span class="hljs-keyword">if</span> outlier_num != <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># 使用paddle.randint生成服从均匀分布的、范围在[0, len(y))的随机Tensor</span><br>            outlier_idx = paddle.randint(<span class="hljs-built_in">len</span>(y),shape = [outlier_num])<br>            y[outlier_idx] = y[outlier_idx] * <span class="hljs-number">5</span><br>    <span class="hljs-keyword">return</span> X, y<br></code></pre></td></tr></table></figure><p>利用上面的生成样本函数，生成 150 个带噪音的样本，其中 100 个训练样本，50 个测试样本，并打印出训练数据的可视化分布。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment"># matplotlib 是 Python 的绘图库</span><br><br>func = linear_func<br>interval = (-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)<br>train_num = <span class="hljs-number">100</span> <span class="hljs-comment"># 训练样本数目</span><br>test_num = <span class="hljs-number">50</span> <span class="hljs-comment"># 测试样本数目</span><br>noise = <span class="hljs-number">2</span><br>X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br>X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br><br>X_train_large, y_train_large = create_toy_data(func=func, interval=interval, sample_num=<span class="hljs-number">5000</span>, noise = noise, add_outlier = <span class="hljs-literal">False</span>)<br><br><span class="hljs-comment"># paddle.linspace返回一个Tensor，Tensor的值为在区间start和stop上均匀间隔的num个值，输出Tensor的长度为num</span><br>X_underlying = paddle.linspace(interval[<span class="hljs-number">0</span>],interval[<span class="hljs-number">1</span>],train_num) <br>y_underlying = linear_func(X_underlying)<br><br><span class="hljs-comment"># 绘制数据</span><br>plt.scatter(X_train, y_train, marker=<span class="hljs-string">&#x27;*&#x27;</span>, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;train data&quot;</span>)<br>plt.scatter(X_test, y_test, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;test data&quot;</span>)<br>plt.plot(X_underlying, y_underlying, c=<span class="hljs-string">&#x27;#000000&#x27;</span>, label=<span class="hljs-string">r&quot;underlying distribution&quot;</span>)<br>plt.legend(fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>) <span class="hljs-comment"># 给图像加图例</span><br>plt.savefig(<span class="hljs-string">&#x27;ml-vis.pdf&#x27;</span>) <span class="hljs-comment"># 保存图像到PDF文件中</span><br>plt.show()<br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212164136707.png" alt="image-20221211220940535" style="zoom: 50%;" /><hr><h4 id="√-2-2-2-模型构建"><a href="#√-2-2-2-模型构建" class="headerlink" title="[√] 2.2.2 - 模型构建"></a>[√] 2.2.2 - 模型构建</h4><p>在线性回归中，自变量为样本的特征向量$\boldsymbol{x}\in \mathbb{R}^D$(每一维对应一个自变量)，因变量是连续值的标签$y\in R$。</p><p>线性模型定义为：<br>$$<br>f(\boldsymbol{x};\boldsymbol{w},b)&#x3D;\boldsymbol{w}^T\boldsymbol{x}+b。 (2.6)<br>$$</p><p>其中权重向量$\boldsymbol{w}\in \mathbb{R}^D$和偏置$b\in \mathbb{R}$都是可学习的参数。</p><hr><p>注意：《神经网络与深度学习》中为了表示的简洁性，使用<strong>增广权重向量</strong>来定义模型。而在本书中，为了和代码实现保持一致，我们使用非增广向量的形式来定义模型。</p><hr><p>在实践中，为了提高预测样本的效率，我们通常会将$N$样本归为一组进行成批地预测，这样可以更好地利用GPU设备的并行计算能力。</p><p>$$<br>\boldsymbol{y} &#x3D;\boldsymbol{X} \boldsymbol{w} + b, (2.7)<br>$$</p><p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{y}\in \mathbb{R}^N$为$N$个预测值组成的列向量。</p><hr><p>注意：在实践中，样本的矩阵$\boldsymbol{X}$是由$N$个$\boldsymbol{x}$的<strong>行向量</strong>组成。而原教材中$\boldsymbol{x}$为列向量，其特征矩阵与本书中的特征矩阵刚好为转置关系。</p><hr><h5 id="√-2-2-2-1-线性算子"><a href="#√-2-2-2-1-线性算子" class="headerlink" title="[√] 2.2.2.1 - 线性算子"></a>[√] 2.2.2.1 - 线性算子</h5><p>实现公式(2.7)中的线性函数非常简单，我们直接利用如下张量运算来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># X: tensor, shape=[N,D]</span><br><span class="hljs-comment"># y_pred: tensor, shape=[N]</span><br><span class="hljs-comment"># w: shape=[D,1]</span><br><span class="hljs-comment"># b: shape=[1]</span><br>y_pred = paddle.matmul(X,w)+b<br></code></pre></td></tr></table></figure><p>使用飞桨构建一个线性回归模型，代码如下：</p><p><strong>说明</strong></p><p>在飞桨框架中，可以直接调用模型的<code>forward()</code>方法进行前向执行，也可以调用<code>__call__()</code>，从而执行在 <code>forward()</code> 当中定义的前向计算逻辑。</p><p>在飞桨框架中，模型一般继承<a href="https://www.paddlepaddle.org.cn/documentation/docs/en/api/paddle/nn/Layer_en.html">nn.Layer</a>，在成员函数<code>forward()</code>中执行模型的前向运算。由于本案例较简单，所以没有继承<code>nn.Layer</code>，但是保留了在<code>forward()</code>函数中执行模型的前向运算的过程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">from</span> nndl.op <span class="hljs-keyword">import</span> Op<br><br>paddle.seed(<span class="hljs-number">10</span>) <span class="hljs-comment">#设置随机种子</span><br><br><span class="hljs-comment"># 线性算子</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - input_size:模型要处理的数据特征向量长度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        self.input_size = input_size<br><br>        <span class="hljs-comment"># 模型参数</span><br>        self.params = &#123;&#125;<br>        self.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.randn(shape=[self.input_size,<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.zeros(shape=[<span class="hljs-number">1</span>],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-comment"># 前向函数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - X: tensor, shape=[N,D]</span><br><span class="hljs-string">           注意这里的X矩阵是由N个x向量的转置拼接成的，与原教材行向量表示方式不一致</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">           - y_pred： tensor, shape=[N]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        N,D = X.shape<br><br>        <span class="hljs-keyword">if</span> self.input_size==<span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> paddle.full(shape=[N,<span class="hljs-number">1</span>], fill_value=self.params[<span class="hljs-string">&#x27;b&#x27;</span>])<br>        <br>        <span class="hljs-keyword">assert</span> D==self.input_size <span class="hljs-comment"># 输入数据维度合法性验证</span><br><br>        <span class="hljs-comment"># 使用paddle.matmul计算两个tensor的乘积</span><br>        y_pred = paddle.matmul(X,self.params[<span class="hljs-string">&#x27;w&#x27;</span>])+self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <br>        <span class="hljs-keyword">return</span> y_pred<br><br><span class="hljs-comment"># 注意这里我们为了和后面章节统一，这里的X矩阵是由N个x向量的转置拼接成的，与原教材行向量表示方式不一致</span><br>input_size = <span class="hljs-number">3</span><br>N = <span class="hljs-number">2</span><br>X = paddle.randn(shape=[N, input_size],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <span class="hljs-comment"># 生成2个维度为3的数据</span><br>model = Linear(input_size)<br>y_pred = model(X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y_pred:&quot;</span>,y_pred) <span class="hljs-comment">#输出结果的个数也是2个</span><br></code></pre></td></tr></table></figure><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs routeros">y_pred: Tensor(shape=[2, 1], <span class="hljs-attribute">dtype</span>=float32, <span class="hljs-attribute">place</span>=CPUPlace, <span class="hljs-attribute">stop_gradient</span>=<span class="hljs-literal">True</span>,<br>       [[0.54838145],<br>        [2.03063798]])<br></code></pre></td></tr></table></figure><h4 id="√-2-2-3-损失函数"><a href="#√-2-2-3-损失函数" class="headerlink" title="[√] 2.2.3 - 损失函数"></a>[√] 2.2.3 - 损失函数</h4><p>回归任务是对<strong>连续值</strong>的预测，希望模型能根据数据的特征输出一个连续值作为预测值。因此回归任务中常用的评估指标是<strong>均方误差</strong>。</p><p>令$\boldsymbol{y}\in \mathbb{R}^N$，$\hat{\boldsymbol{y}}\in \mathbb{R}^N$分别为$N$个样本的真实标签和预测标签，均方误差的定义为：</p><p>$$<br>\mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})&#x3D;\frac{1}{2N}|\boldsymbol{y}-\hat{\boldsymbol{y}}|^2&#x3D;\frac{1}{2N}|\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}|^2, (2.8)<br>$$<br>其中$\boldsymbol{b}$为$N$维向量，所有元素取值都为$b$。</p><p>均方误差的代码实现如下:</p><blockquote><p>注意：代码实现中没有除2。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">mean_squared_error</span>(<span class="hljs-params">y_true, y_pred</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - y_true: tensor，样本真实标签</span><br><span class="hljs-string">       - y_pred: tensor, 样本预测标签</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - error: float，误差值</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">assert</span> y_true.shape[<span class="hljs-number">0</span>] == y_pred.shape[<span class="hljs-number">0</span>]<br>    <br>    <span class="hljs-comment"># paddle.square计算输入的平方值</span><br>    <span class="hljs-comment"># paddle.mean沿 axis 计算 x 的平均值，默认axis是None，则对输入的全部元素计算平均值。</span><br>    error = paddle.mean(paddle.square(y_true - y_pred))<br><br>    <span class="hljs-keyword">return</span> error<br><br><br><span class="hljs-comment"># 构造一个简单的样例进行测试:[N,1], N=2</span><br>y_true= paddle.to_tensor([[-<span class="hljs-number">0.2</span>],[<span class="hljs-number">4.9</span>]],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_pred = paddle.to_tensor([[<span class="hljs-number">1.3</span>],[<span class="hljs-number">2.5</span>]],dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br>error = mean_squared_error(y_true=y_true, y_pred=y_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;error:&quot;</span>,error)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">error: <span class="hljs-number">4.005000114440918</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-2-2-4-模型优化"><a href="#√-2-2-4-模型优化" class="headerlink" title="[√] 2.2.4 - 模型优化"></a>[√] 2.2.4 - 模型优化</h4><p>采用<strong>经验风险最小化</strong>，线性回归可以通过最小二乘法求出参数$\boldsymbol{w}$和$b$的解析解。计算公式(2.8)中均方误差对参数$b$的偏导数，得到<br>$$<br>\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial b} &#x3D; \mathbf{1}^T (\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}), (2.9)<br>$$</p><p>其中$\mathbf{1}$为$N$维的全1向量。<strong>这里为了简单起见省略了均方误差的系数$\frac{1}{N}$，并不影响最后的结果</strong>。</p><p>令上式等于0，得到<br>$$<br>b^* &#x3D;\bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w},(2.10)<br>$$</p><p>其中$\bar{y} &#x3D; \frac{1}{N}\mathbf{1}^T\boldsymbol{y}$为所有标签的平均值，$\bar{\boldsymbol{x}} &#x3D; \frac{1}{N}(\mathbf{1}^T \boldsymbol{X})^T$ 为所有特征向量的平均值。将$b^*$代入公式(2.8)中均方误差对参数$\boldsymbol{w}$的偏导数，得到<br>$$<br>\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial \boldsymbol{w}} &#x3D; (\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T \Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\boldsymbol{w}-(\boldsymbol{y}-\bar{y})\Big).(2.11)<br>$$<br>令上式等于0，得到最优的参数为<br>$$<br>\boldsymbol{w}^*&#x3D;\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.12)<br>$$</p><p>$$<br>b^* &#x3D;  \bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w}^*.(2.13)<br>$$</p><p>若对参数$\boldsymbol{w}$加上$\ell_2$正则化，则最优的$\boldsymbol{w}^*$变为<br>$$<br>\boldsymbol{w}^*&#x3D;\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)+\lambda \boldsymbol{I}\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.14)<br>$$</p><p>其中$\lambda&gt;0$为预先设置的正则化系数，$\boldsymbol{I}\in \mathbb{R}^{D\times D}$为单位矩阵。</p><p>尝试验证公式(2.14)。</p><p>&#x3D;&#x3D;通过最小二乘法优化参数&#x3D;&#x3D;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">optimizer_lsm</span>(<span class="hljs-params">model, X, y, reg_lambda=<span class="hljs-number">0</span></span>):<br>  <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - model: 模型</span><br><span class="hljs-string">       - X: tensor, 特征数据，shape=[N,D]</span><br><span class="hljs-string">       - y: tensor,标签数据，shape=[N]</span><br><span class="hljs-string">       - reg_lambda: float, 正则化系数，默认为0</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - model: 优化好的模型</span><br><span class="hljs-string">   &#x27;&#x27;&#x27;</span><br><br>  N, D = X.shape<br><br>  <span class="hljs-comment"># 对输入特征数据所有特征向量求平均</span><br>  x_bar_tran = paddle.mean(X,axis=<span class="hljs-number">0</span>).T <br>  <br>  <span class="hljs-comment"># 求标签的均值,shape=[1]</span><br>  y_bar = paddle.mean(y)<br>  <br>  <span class="hljs-comment"># paddle.subtract通过广播的方式实现矩阵减向量</span><br>  x_sub = paddle.subtract(X,x_bar_tran)<br><br>  <span class="hljs-comment"># 使用paddle.all判断输入tensor是否全0</span><br>  <span class="hljs-keyword">if</span> paddle.<span class="hljs-built_in">all</span>(x_sub==<span class="hljs-number">0</span>):<br>    model.params[<span class="hljs-string">&#x27;b&#x27;</span>] = y_bar<br>    model.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.zeros(shape=[D])<br>    <span class="hljs-keyword">return</span> model<br>  <br>  <span class="hljs-comment"># paddle.inverse求方阵的逆</span><br>  tmp = paddle.inverse(paddle.matmul(x_sub.T,x_sub)+<br>          reg_lambda*paddle.eye(num_rows = (D)))<br><br>  w = paddle.matmul(paddle.matmul(tmp,x_sub.T),(y-y_bar))<br>  <br>  b = y_bar-paddle.matmul(x_bar_tran,w)<br>  <br>  model.params[<span class="hljs-string">&#x27;b&#x27;</span>] = b<br>  model.params[<span class="hljs-string">&#x27;w&#x27;</span>] = paddle.squeeze(w,axis=-<span class="hljs-number">1</span>)<br><br>  <span class="hljs-keyword">return</span> model<br><br></code></pre></td></tr></table></figure><hr><h4 id="√-2-2-5-模型训练"><a href="#√-2-2-5-模型训练" class="headerlink" title="[√] 2.2.5 - 模型训练"></a>[√] 2.2.5 - 模型训练</h4><p>在准备了数据、模型、损失函数和参数学习的实现之后，我们开始模型的训练。</p><p>在回归任务中，模型的评价指标和损失函数一致，都为均方误差。</p><p>通过上文实现的线性回归类来拟合训练数据，并输出模型在训练集上的损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">input_size = <span class="hljs-number">1</span><br>model = Linear(input_size)<br>model = optimizer_lsm(model,X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]),y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w_pred:&quot;</span>,model.params[<span class="hljs-string">&#x27;w&#x27;</span>].item(), <span class="hljs-string">&quot;b_pred: &quot;</span>, model.params[<span class="hljs-string">&#x27;b&#x27;</span>].item())<br><br>y_train_pred = model(X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>train_error = mean_squared_error(y_true=y_train, y_pred=y_train_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train error: &quot;</span>,train_error)<br></code></pre></td></tr></table></figure><p># 真实函数的参数缺省值为 w&#x3D;1.2，b&#x3D;0.5</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w_pred: <span class="hljs-number">1.200244426727295</span> b_pred:  <span class="hljs-number">0.41967445611953735</span><br>train error:  <span class="hljs-number">3.8776581287384033</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model_large = Linear(input_size)<br>model_large = optimizer_lsm(model_large,X_train_large.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]),y_train_large.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;w_pred large:&quot;</span>,model_large.params[<span class="hljs-string">&#x27;w&#x27;</span>].item(), <span class="hljs-string">&quot;b_pred large: &quot;</span>, model_large.params[<span class="hljs-string">&#x27;b&#x27;</span>].item())<br><br>y_train_pred_large = model_large(X_train_large.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>train_error_large = mean_squared_error(y_true=y_train_large, y_pred=y_train_pred_large).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;train error large: &quot;</span>,train_error_large)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w_pred large: <span class="hljs-number">1.1942962408065796</span> b_pred large:  <span class="hljs-number">0.5079830884933472</span><br>train error large:  <span class="hljs-number">4.086254119873047</span><br></code></pre></td></tr></table></figure><p>从输出结果看，预测结果与真实值$\boldsymbol{w}&#x3D;1.2$，$b&#x3D;0.5$有一定的差距。</p><hr><h4 id="√-2-2-6-模型评估"><a href="#√-2-2-6-模型评估" class="headerlink" title="[√] 2.2.6 - 模型评估"></a>[√] 2.2.6 - 模型评估</h4><p>下面用训练好的模型预测一下测试集的标签，并计算在测试集上的损失。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">y_test_pred = model(X_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>test_error = mean_squared_error(y_true=y_test, y_pred=y_test_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test error: &quot;</span>,test_error)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test error:  <span class="hljs-number">4.075982570648193</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">y_test_pred_large = model_large(X_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])).squeeze()<br>test_error_large = mean_squared_error(y_true=y_test, y_pred=y_test_pred_large).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;test error large: &quot;</span>,test_error_large)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test error large:  <span class="hljs-number">4.081584453582764</span><br></code></pre></td></tr></table></figure><p>动手练习:</p><p>为了加深对机器学习模型的理解，请自己动手完成以下实验：</p><p>（1） 调整训练数据的样本数量，由 100 调整到 5000，观察对模型性能的影响。</p><p>（2） 调整正则化系数，观察对模型性能的影响。</p><p>（1）训练集数量5000，测试集数量1000</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w_pred: <span class="hljs-number">1.2003157138824463</span> b_pred:  <span class="hljs-number">0.5225465893745422</span><br>train error:  <span class="hljs-number">4.16726541519165</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test error:  <span class="hljs-number">3.9572794437408447</span><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">test error large:  <span class="hljs-number">3.96787428855896</span><br></code></pre></td></tr></table></figure><p>模型的错误率有所下降</p><hr><h3 id="√-2-3-多项式回归"><a href="#√-2-3-多项式回归" class="headerlink" title="[√] 2.3 - 多项式回归"></a>[√] 2.3 - 多项式回归</h3><p>多项式回归是回归任务的一种形式，其中自变量和因变量之间的关系是$M$次多项式的一种线性回归形式，即：<br>$$<br>f(\boldsymbol{x};\boldsymbol{w})&#x3D;w_1x+w_2x^2+…+w_Mx^M+b&#x3D;\boldsymbol{w}^T\phi(x)+b, （2.10）<br>$$<br>其中$M$为多项式的阶数，$\boldsymbol{w}&#x3D;[w_1,…,w_M]^T$为多项式的系数，$\phi(x)&#x3D;[x,x^2,\cdots,x^M]^T$为多项式基函数，将原始特征$x$映射为$M$维的向量。当$M&#x3D;0$时，$f(\boldsymbol{x};\boldsymbol{w})&#x3D;b$。</p><p>公式（2.10）展示的是特征维度为1的多项式表达，当特征维度大于1时，存在不同特征之间交互的情况，这是线性回归无法实现。公式（2.11）展示的是当特征维度为2，多项式阶数为2时的多项式回归：</p><p>$$<br>f(\boldsymbol{x};\boldsymbol{w})&#x3D;w_1x_1+w_2x_2+w_3x_1^2+w_4x_1x_2+w_5x_2^2+b, （2.11）<br>$$</p><p>当自变量和因变量之间并不是线性关系时，我们可以定义非线性基函数对特征进行变换，从而可以使得线性回归算法实现非线性的曲线拟合。</p><p>接下来我们基于特征维度为1的自变量介绍多项式回归实验。</p><hr><h4 id="√-2-3-1-数据集构建"><a href="#√-2-3-1-数据集构建" class="headerlink" title="[√] 2.3.1 - 数据集构建"></a>[√] 2.3.1 - 数据集构建</h4><p>假设我们要拟合的非线性函数为一个缩放后的sin函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># sin函数: sin(2 * pi * x)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sin</span>(<span class="hljs-params">x</span>):<br>    y = paddle.sin(<span class="hljs-number">2</span> * math.pi * x)<br>    <span class="hljs-keyword">return</span> y<br></code></pre></td></tr></table></figure><p>这里仍然使用前面定义的<code>create_toy_data</code>函数来构建训练和测试数据，其中训练数样本 15 个，测试样本 10 个，高斯噪声标准差为 0.1，自变量范围为 (0,1)。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 生成数据</span><br>func = sin<br>interval = (<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)<br>train_num = <span class="hljs-number">15</span><br>test_num = <span class="hljs-number">10</span><br>noise = <span class="hljs-number">0.1</span> <span class="hljs-comment">#0.5 </span><br>X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise = noise)<br>X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise = noise)<br><br>X_underlying = paddle.linspace(interval[<span class="hljs-number">0</span>],interval[<span class="hljs-number">1</span>],num=<span class="hljs-number">100</span>)<br>y_underlying = sin(X_underlying)<br><br><span class="hljs-comment"># 绘制图像</span><br>plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = (<span class="hljs-number">8.0</span>, <span class="hljs-number">6.0</span>)<br>plt.scatter(X_train, y_train, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;train data&quot;</span>)<br><span class="hljs-comment">#plt.scatter(X_test, y_test, facecolor=&quot;none&quot;, edgecolor=&quot;r&quot;, s=50, label=&quot;test data&quot;)</span><br>plt.plot(X_underlying, y_underlying, c=<span class="hljs-string">&#x27;#000000&#x27;</span>, label=<span class="hljs-string">r&quot;$\sin(2\pi x)$&quot;</span>)<br>plt.legend(fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;ml-vis2.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165459211.png" alt="image-20221212164136707" style="zoom:33%;" /><hr><h4 id="√-2-3-2-模型构建"><a href="#√-2-3-2-模型构建" class="headerlink" title="[√] 2.3.2 - 模型构建"></a>[√] 2.3.2 - 模型构建</h4><p>通过多项式的定义可以看出，多项式回归和线性回归一样，同样学习参数$\boldsymbol{w}$，只不过需要对输入特征$\phi(x)$根据多项式阶数进行变换。因此，我们可以套用求解线性回归参数的方法来求解多项式回归参数。</p><p>首先，我们实现<strong>多项式基函数</strong><code>polynomial_basis_function</code>对原始特征$x$进行转换。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 多项式转换</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">polynomial_basis_function</span>(<span class="hljs-params">x, degree = <span class="hljs-number">2</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    输入：</span><br><span class="hljs-string">       - x: tensor, 输入的数据，shape=[N,1]</span><br><span class="hljs-string">       - degree: int, 多项式的阶数</span><br><span class="hljs-string">       example Input: [[2], [3], [4]], degree=2</span><br><span class="hljs-string">       example Output: [[2^1, 2^2], [3^1, 3^2], [4^1, 4^2]]</span><br><span class="hljs-string">       注意：本案例中,在degree&gt;=1时不生成全为1的一列数据；degree为0时生成形状与输入相同，全1的Tensor</span><br><span class="hljs-string">    输出：</span><br><span class="hljs-string">       - x_result： tensor</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <br>    <span class="hljs-keyword">if</span> degree==<span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">return</span> paddle.ones(shape = x.shape,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) <br><br>    x_tmp = x<br>    x_result = x_tmp<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, degree+<span class="hljs-number">1</span>):<br>        x_tmp = paddle.multiply(x_tmp,x) <span class="hljs-comment"># 逐元素相乘</span><br>        x_result = paddle.concat((x_result,x_tmp),axis=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">return</span> x_result<br><br><span class="hljs-comment"># 简单测试</span><br>data = [[<span class="hljs-number">2</span>], [<span class="hljs-number">3</span>], [<span class="hljs-number">4</span>]]<br>X = paddle.to_tensor(data = data,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>degree = <span class="hljs-number">3</span><br>transformed_X = polynomial_basis_function(X,degree=degree)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;转换前：&quot;</span>,X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;阶数为&quot;</span>,degree,<span class="hljs-string">&quot;转换后：&quot;</span>,transformed_X)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">转换前： Tensor(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">2.</span>],<br>        [<span class="hljs-number">3.</span>],<br>        [<span class="hljs-number">4.</span>]])<br>阶数为 <span class="hljs-number">3</span> 转换后： Tensor(shape=[<span class="hljs-number">3</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">2.</span> , <span class="hljs-number">4.</span> , <span class="hljs-number">8.</span> ],<br>        [<span class="hljs-number">3.</span> , <span class="hljs-number">9.</span> , <span class="hljs-number">27.</span>],<br>        [<span class="hljs-number">4.</span> , <span class="hljs-number">16.</span>, <span class="hljs-number">64.</span>]])<br></code></pre></td></tr></table></figure><hr><h4 id="√-2-3-3-模型训练"><a href="#√-2-3-3-模型训练" class="headerlink" title="[√] 2.3.3 - 模型训练"></a>[√] 2.3.3 - 模型训练</h4><p>对于多项式回归，我们可以同样使用前面线性回归中定义的<code>LinearRegression</code>算子、训练函数<code>train</code>、均方误差函数<code>mean_squared_error</code>。拟合训练数据的目标是最小化损失函数，同线性回归一样，也可以通过矩阵运算直接求出$\boldsymbol{w}$的值。</p><p>我们设定不同的多项式阶，$M$的取值分别为0、1、3、8，之前构造的训练集上进行训练，观察样本数据对$\sin$曲线的拟合结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python">plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = (<span class="hljs-number">12.0</span>, <span class="hljs-number">8.0</span>)<br><br><span class="hljs-keyword">for</span> i, degree <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>]): <span class="hljs-comment"># []中为多项式的阶数</span><br>    model = Linear(degree)<br>    X_train_transformed = polynomial_basis_function(X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), degree)<br>    X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), degree)<br>    <br>    model = optimizer_lsm(model,X_train_transformed,y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])) <span class="hljs-comment">#拟合得到参数</span><br><br>    y_underlying_pred = model(X_underlying_transformed).squeeze()<br><br>    <span class="hljs-built_in">print</span>(model.params)<br>    <br>    <span class="hljs-comment"># 绘制图像</span><br>    plt.subplot(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, i + <span class="hljs-number">1</span>)<br>    plt.scatter(X_train, y_train, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;train data&quot;</span>)<br>    plt.plot(X_underlying, y_underlying, c=<span class="hljs-string">&#x27;#000000&#x27;</span>, label=<span class="hljs-string">r&quot;$\sin(2\pi x)$&quot;</span>)<br>    plt.plot(X_underlying, y_underlying_pred, c=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, label=<span class="hljs-string">&quot;predicted function&quot;</span>)<br>    plt.ylim(-<span class="hljs-number">2</span>, <span class="hljs-number">1.5</span>)<br>    plt.annotate(<span class="hljs-string">&quot;M=&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(degree), xy=(<span class="hljs-number">0.95</span>, -<span class="hljs-number">1.4</span>))<br><br><span class="hljs-comment">#plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)</span><br>plt.legend(loc=<span class="hljs-string">&#x27;lower left&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;ml-vis3.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">&#123;<span class="hljs-string">&#x27;w&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0.</span>]), <span class="hljs-string">&#x27;b&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0.06668118</span>])&#125;<br>&#123;<span class="hljs-string">&#x27;w&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [-<span class="hljs-number">1.34165502</span>]), <span class="hljs-string">&#x27;b&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0.74084926</span>])&#125;<br>&#123;<span class="hljs-string">&#x27;w&#x27;</span>: Tensor(shape=[<span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [ <span class="hljs-number">11.50885582</span>, -<span class="hljs-number">33.70772171</span>,  <span class="hljs-number">22.45395279</span>]), <span class="hljs-string">&#x27;b&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [-<span class="hljs-number">0.17524421</span>])&#125;<br>&#123;<span class="hljs-string">&#x27;w&#x27;</span>: Tensor(shape=[<span class="hljs-number">8</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [ <span class="hljs-number">6.39199781</span>  ,  <span class="hljs-number">38.72201538</span> , -<span class="hljs-number">377.10943604</span>,  <span class="hljs-number">912.26202393</span>,<br>        -<span class="hljs-number">1013.89459229</span>,  <span class="hljs-number">374.96646118</span>,  <span class="hljs-number">198.60220337</span>, -<span class="hljs-number">150.03273010</span>]), <span class="hljs-string">&#x27;b&#x27;</span>: Tensor(shape=[<span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">3.29901624</span>])&#125;<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165722946.png" alt="image-20221212165145316"></p><p>观察可视化结果，红色的曲线表示不同阶多项式分布拟合数据的结果：</p><ul><li>当 $M&#x3D;0$ 或 $M&#x3D;1$ 时，拟合曲线较简单，模型欠拟合；</li><li>当 $M&#x3D;8$ 时，拟合曲线较复杂，模型过拟合；</li><li>当 $M&#x3D;3$ 时，模型拟合最为合理。</li></ul><hr><h4 id="√-2-3-4-模型评估"><a href="#√-2-3-4-模型评估" class="headerlink" title="[√] 2.3.4 - 模型评估"></a>[√] 2.3.4 - 模型评估</h4><p>下面通过均方误差来衡量训练误差、测试误差以及在没有噪音的加入下<code>sin</code>函数值与多项式回归值之间的误差，更加真实地反映拟合结果。多项式分布阶数从0到8进行遍历。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练误差和测试误差</span><br>training_errors = []<br>test_errors = []<br>distribution_errors = []<br><br><span class="hljs-comment"># 遍历多项式阶数</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">9</span>):<br>    model = Linear(i)<br><br>    X_train_transformed = polynomial_basis_function(X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), i) <br>    X_test_transformed = polynomial_basis_function(X_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), i) <br>    X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), i)<br>    <br>    optimizer_lsm(model,X_train_transformed,y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br>    <br>    y_train_pred = model(X_train_transformed).squeeze()<br>    y_test_pred = model(X_test_transformed).squeeze()<br>    y_underlying_pred = model(X_underlying_transformed).squeeze()<br><br>    train_mse = mean_squared_error(y_true=y_train, y_pred=y_train_pred).item()<br>    training_errors.append(train_mse)<br><br>    test_mse = mean_squared_error(y_true=y_test, y_pred=y_test_pred).item()<br>    test_errors.append(test_mse)<br><br>    <span class="hljs-comment">#distribution_mse = mean_squared_error(y_true=y_underlying, y_pred=y_underlying_pred).item()</span><br>    <span class="hljs-comment">#distribution_errors.append(distribution_mse)</span><br><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;train errors: \n&quot;</span>,training_errors)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;test errors: \n&quot;</span>,test_errors)<br><span class="hljs-comment">#print (&quot;distribution errors: \n&quot;, distribution_errors)</span><br><br><span class="hljs-comment"># 绘制图片</span><br>plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = (<span class="hljs-number">8.0</span>, <span class="hljs-number">6.0</span>)<br>plt.plot(training_errors, <span class="hljs-string">&#x27;-.&#x27;</span>, mfc=<span class="hljs-string">&quot;none&quot;</span>, mec=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, ms=<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, label=<span class="hljs-string">&quot;Training&quot;</span>)<br>plt.plot(test_errors, <span class="hljs-string">&#x27;--&#x27;</span>, mfc=<span class="hljs-string">&quot;none&quot;</span>, mec=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, ms=<span class="hljs-number">10</span>, c=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, label=<span class="hljs-string">&quot;Test&quot;</span>)<br><span class="hljs-comment">#plt.plot(distribution_errors, &#x27;-&#x27;, mfc=&quot;none&quot;, mec=&quot;#3D3D3F&quot;, ms=10, c=&quot;#3D3D3F&quot;, label=&quot;Distribution&quot;)</span><br>plt.legend(fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;degree&quot;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;MSE&quot;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;ml-mse-error.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">train errors: <br> [<span class="hljs-number">0.4672910273075104</span>, <span class="hljs-number">0.2713072597980499</span>, <span class="hljs-number">0.25924286246299744</span>, <span class="hljs-number">0.007656396366655827</span>, <span class="hljs-number">0.007648141589015722</span>, <span class="hljs-number">0.004982923157513142</span>, <span class="hljs-number">0.055855292826890945</span>, <span class="hljs-number">0.01697038859128952</span>, <span class="hljs-number">13.178251266479492</span>]<br>test errors: <br> [<span class="hljs-number">0.48431509733200073</span>, <span class="hljs-number">0.20496585965156555</span>, <span class="hljs-number">0.22155332565307617</span>, <span class="hljs-number">0.02112610451877117</span>, <span class="hljs-number">0.021505268290638924</span>, <span class="hljs-number">0.009555519558489323</span>, <span class="hljs-number">0.07138563692569733</span>, <span class="hljs-number">0.014405457302927971</span>, <span class="hljs-number">8.89521312713623</span>]<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211213007230.png" alt="image-20221212165459211"></p><p>观察可视化结果：</p><ul><li>当阶数较低的时候，模型的表示能力有限，训练误差和测试误差都很高，代表模型欠拟合；</li><li>当阶数较高的时候，模型表示能力强，但将训练数据中的噪声也作为特征进行学习，一般情况下训练误差继续降低而测试误差显著升高，代表模型过拟合。</li></ul><blockquote><p>此处多项式阶数大于等于5时，训练误差并没有下降，尤其是在多项式阶数为7时，训练误差变得非常大，请思考原因？提示：请从幂函数特性角度思考。</p></blockquote><p>对于模型过拟合的情况，可以引入正则化方法，通过向误差函数中添加一个<strong>惩罚项</strong>来避免系数倾向于较大的取值。下面加入$\mathcal{l_{2}}$正则化项，查看拟合结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python">degree = <span class="hljs-number">8</span> <span class="hljs-comment"># 多项式阶数</span><br>reg_lambda = <span class="hljs-number">0.0001</span> <span class="hljs-comment"># 正则化系数</span><br><br>X_train_transformed = polynomial_basis_function(X_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), degree)<br>X_test_transformed = polynomial_basis_function(X_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), degree)<br>X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]), degree)<br><br>model = Linear(degree) <br><br>optimizer_lsm(model,X_train_transformed,y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]))<br><br>y_test_pred=model(X_test_transformed).squeeze()<br>y_underlying_pred=model(X_underlying_transformed).squeeze()<br><br>model_reg = Linear(degree) <br><br>optimizer_lsm(model_reg,X_train_transformed,y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>]),reg_lambda=reg_lambda)<br><br>y_test_pred_reg=model_reg(X_test_transformed).squeeze()<br>y_underlying_pred_reg=model_reg(X_underlying_transformed).squeeze()<br><br>mse = mean_squared_error(y_true = y_test, y_pred = y_test_pred).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mse:&quot;</span>,mse)<br>mes_reg = mean_squared_error(y_true = y_test, y_pred = y_test_pred_reg).item()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;mse_with_l2_reg:&quot;</span>,mes_reg)<br><br><span class="hljs-comment"># 绘制图像</span><br>plt.scatter(X_train, y_train, facecolor=<span class="hljs-string">&quot;none&quot;</span>, edgecolor=<span class="hljs-string">&quot;#e4007f&quot;</span>, s=<span class="hljs-number">50</span>, label=<span class="hljs-string">&quot;train data&quot;</span>)<br>plt.plot(X_underlying, y_underlying, c=<span class="hljs-string">&#x27;#000000&#x27;</span>, label=<span class="hljs-string">r&quot;$\sin(2\pi x)$&quot;</span>)<br>plt.plot(X_underlying, y_underlying_pred, c=<span class="hljs-string">&#x27;#e4007f&#x27;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>, label=<span class="hljs-string">&quot;$deg. = 8$&quot;</span>)<br>plt.plot(X_underlying, y_underlying_pred_reg, c=<span class="hljs-string">&#x27;#f19ec2&#x27;</span>, linestyle=<span class="hljs-string">&quot;-.&quot;</span>, label=<span class="hljs-string">&quot;$deg. = 8, \ell_2 reg$&quot;</span>)<br>plt.ylim(-<span class="hljs-number">1.5</span>, <span class="hljs-number">1.5</span>)<br>plt.annotate(<span class="hljs-string">&quot;lambda=&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(reg_lambda), xy=(<span class="hljs-number">0.82</span>, -<span class="hljs-number">1.4</span>))<br>plt.legend(fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;ml-vis4.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">mse: <span class="hljs-number">8.89521312713623</span><br>mse_with_l2_reg: <span class="hljs-number">0.009005023166537285</span><br></code></pre></td></tr></table></figure><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173347233.png" alt="image-20221212165722946" style="zoom: 50%;" /><p>观察可视化结果，其中黄色曲线为加入$\mathcal{l_{2}}$正则后多项式分布拟合结果，红色曲线为未加入$\mathcal{l_{2}}$正则的拟合结果，黄色曲线的拟合效果明显好于红色曲线。</p><hr><h3 id="√-2-4-Runner类介绍"><a href="#√-2-4-Runner类介绍" class="headerlink" title="[√] 2.4 - Runner类介绍"></a>[√] 2.4 - Runner类介绍</h3><p>通过上面的实践，我们可以看到，在一个任务上应用机器学习方法的流程基本上包括：数据集构建、模型构建、损失函数定义、优化器、模型训练、模型评价、模型预测等环节。</p><p>为了更方便地将上述环节规范化，我们将机器学习模型的基本要素封装成一个<strong>Runner</strong>类。除上述提到的要素外，再加上模型保存、模型加载等功能。</p><p><strong>Runner</strong>类的成员函数定义如下：</p><ul><li>__init__函数：实例化<strong>Runner</strong>类时默认调用，需要传入模型、损失函数、优化器和评价指标等；</li><li>train函数：完成模型训练，指定模型训练需要的训练集和验证集；</li><li>evaluate函数：通过对训练好的模型进行评价，在验证集或测试集上查看模型训练效果；</li><li>predict函数：选取一条数据对训练好的模型进行预测；</li><li>save_model函数：模型在训练过程和训练结束后需要进行保存；</li><li>load_model函数：调用加载之前保存的模型。</li></ul><p><code>Runner</code>类的框架定义如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Runner</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, loss_fn, metric</span>):<br>        self.model = model         <span class="hljs-comment"># 模型</span><br>        self.optimizer = optimizer <span class="hljs-comment"># 优化器</span><br>        self.loss_fn = loss_fn     <span class="hljs-comment"># 损失函数   </span><br>        self.metric = metric       <span class="hljs-comment"># 评估指标</span><br><br>    <span class="hljs-comment"># 模型训练</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_dataset, dev_dataset=<span class="hljs-literal">None</span>, **kwargs</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-comment"># 模型评价</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set, **kwargs</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-comment"># 模型预测</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x, **kwargs</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-comment"># 模型保存</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_path</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-comment"># 模型加载</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        <span class="hljs-keyword">pass</span><br></code></pre></td></tr></table></figure><p><strong>Runner</strong>类的流程如<strong>图2.8</strong>所示，可以分为 4 个阶段：</p><ol><li>初始化阶段：传入模型、损失函数、优化器和评价指标。</li><li>模型训练阶段：基于训练集调用<code>train()</code>函数训练模型，基于验证集通过<code>evaluate()</code>函数验证模型。通过<code>save_model()</code>函数保存模型。</li><li>模型评价阶段：基于测试集通过<code>evaluate()</code>函数得到指标性能。</li><li>模型预测阶段：给定样本，通过<code>predict()</code>函数得到该样本标签。</li></ol><center><img src="https://ai-studio-static-online.cdn.bcebos.com/fdb656daadb349a78560fa464b0de5fa5d63423fc2234adfac48e6ff020a6d60" width=700 ></img></center><center>图2.8 Runner类</center><hr><h3 id="√-2-5-基于线性回归的波士顿房价预测"><a href="#√-2-5-基于线性回归的波士顿房价预测" class="headerlink" title="[√] 2.5 - 基于线性回归的波士顿房价预测"></a>[√] 2.5 - 基于线性回归的波士顿房价预测</h3><p>在本节中，我们使用线性回归来对马萨诸塞州波士顿郊区的房屋进行预测。实验流程主要包含如下5个步骤：</p><ul><li>数据处理：包括数据清洗（缺失值和异常值处理）、数据集划分，以便数据可以被模型正常读取，并具有良好的泛化性;</li><li>模型构建：定义线性回归模型类；</li><li>训练配置:训练相关的一些配置，如：优化算法、评价指标等；</li><li>组装训练框架Runner:<code>Runner</code>用于管理模型训练和测试过程；</li><li>模型训练和测试:利用<code>Runner</code>进行模型训练和测试。</li></ul><hr><h4 id="√-2-5-1-数据处理"><a href="#√-2-5-1-数据处理" class="headerlink" title="[√] 2.5.1 - 数据处理"></a>[√] 2.5.1 - 数据处理</h4><h5 id="√-2-5-1-1-数据集介绍"><a href="#√-2-5-1-1-数据集介绍" class="headerlink" title="[√] 2.5.1.1 - 数据集介绍"></a>[√] 2.5.1.1 - 数据集介绍</h5><p>本实验使用波士顿房价预测数据集，共506条样本数据，每条样本包含了12种可能影响房价的因素和该类房屋价格的中位数，各字段含义如<strong>表2.1</strong>所示：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173220360.png" alt="image-20221212173220360"></p><p>预览前5条数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <span class="hljs-comment"># 开源数据分析和操作工具</span><br><br><span class="hljs-comment"># 利用pandas加载波士顿房价的数据集</span><br>data=pd.read_csv(<span class="hljs-string">&quot;/home/aistudio/work/boston_house_prices.csv&quot;</span>)<br><span class="hljs-comment"># 预览前5行数据</span><br>data.head()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173523825.png" alt="image-20221212173347233"></p><hr><h5 id="√-2-5-1-2-数据清洗"><a href="#√-2-5-1-2-数据清洗" class="headerlink" title="[√] 2.5.1.2 - 数据清洗"></a>[√] 2.5.1.2 - 数据清洗</h5><p>对数据集中的缺失值或异常值等情况进行分析和处理，保证数据可以被模型正常读取。</p><ul><li><strong>缺失值分析</strong></li></ul><p>通过<code>isna()</code>方法判断数据中各元素是否缺失，然后通过<code>sum()</code>方法统计每个字段缺失情况，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 查看各字段缺失值统计情况</span><br>data.isna().<span class="hljs-built_in">sum</span>()<br></code></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165145316.png" alt="image-20221212173523825"></p><p>可以看出不存在缺失数据的情况</p><ul><li><strong>异常值处理</strong></li></ul><p>通过箱线图直观的显示数据分布，并观测数据中的异常值。箱线图一般由五个统计值组成：最大值、上四分位、中位数、下四分位和最小值。一般来说，观测到的数据大于最大估计值或者小于最小估计值则判断为异常值，其中<br>$$<br>最大估计值 &#x3D; 上四分位 + 1.5 * (上四分位 - 下四分位)\<br>最小估计值&#x3D;下四分位 - 1.5 * (上四分位 - 下四分位)<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment"># 可视化工具</span><br><br><span class="hljs-comment"># 箱线图查看异常值分布</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">boxplot</span>(<span class="hljs-params">data, fig_name</span>):<br>    <span class="hljs-comment"># 绘制每个属性的箱线图</span><br>    data_col = <span class="hljs-built_in">list</span>(data.columns)<br>    <br>    <span class="hljs-comment"># 连续画几个图片</span><br>    plt.figure(figsize=(<span class="hljs-number">5</span>, <span class="hljs-number">5</span>), dpi=<span class="hljs-number">300</span>)<br>    <span class="hljs-comment"># 子图调整</span><br>    plt.subplots_adjust(wspace=<span class="hljs-number">0.6</span>)<br>    <span class="hljs-comment"># 每个特征画一个箱线图</span><br>    <span class="hljs-keyword">for</span> i, col_name <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(data_col):<br>        plt.subplot(<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, i+<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 画箱线图</span><br>        plt.boxplot(data[col_name], <br>                    showmeans=<span class="hljs-literal">True</span>, <br>                    meanprops=&#123;<span class="hljs-string">&quot;markersize&quot;</span>:<span class="hljs-number">1</span>,<span class="hljs-string">&quot;marker&quot;</span>:<span class="hljs-string">&quot;D&quot;</span>,<span class="hljs-string">&quot;markeredgecolor&quot;</span>:<span class="hljs-string">&quot;#C54680&quot;</span>&#125;, <span class="hljs-comment"># 均值的属性</span><br>                    medianprops=&#123;<span class="hljs-string">&quot;color&quot;</span>:<span class="hljs-string">&quot;#946279&quot;</span>&#125;, <span class="hljs-comment"># 中位数线的属性</span><br>                    whiskerprops=&#123;<span class="hljs-string">&quot;color&quot;</span>:<span class="hljs-string">&quot;#8E004D&quot;</span>, <span class="hljs-string">&quot;linewidth&quot;</span>:<span class="hljs-number">0.4</span>, <span class="hljs-string">&#x27;linestyle&#x27;</span>:<span class="hljs-string">&quot;--&quot;</span>&#125;,<br>                    flierprops=&#123;<span class="hljs-string">&quot;markersize&quot;</span>:<span class="hljs-number">0.4</span>&#125;,<br>                    ) <br>        <span class="hljs-comment"># 图名</span><br>        plt.title(col_name, fontdict=&#123;<span class="hljs-string">&quot;size&quot;</span>:<span class="hljs-number">5</span>&#125;, pad=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># y方向刻度</span><br>        plt.yticks(fontsize=<span class="hljs-number">4</span>, rotation=<span class="hljs-number">90</span>)<br>        plt.tick_params(pad=<span class="hljs-number">0.5</span>)<br>        <span class="hljs-comment"># x方向刻度</span><br>        plt.xticks([])<br>    plt.savefig(fig_name)<br>    plt.show()<br><br>boxplot(data, <span class="hljs-string">&#x27;ml-vis5.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p>箱线图的具体含义：</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173925680.png" alt="image-20221212173925680"></p><p>从输出结果看，数据中存在较多的异常值（图中上下边缘以外的空心小圆圈）。</p><p>使用四分位值筛选出箱线图中分布的异常值，并将这些数据视为噪声，其将被临界值取代，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 四分位处理异常值</span><br>num_features=data.select_dtypes(exclude=[<span class="hljs-string">&#x27;object&#x27;</span>,<span class="hljs-string">&#x27;bool&#x27;</span>]).columns.tolist()<br><br><span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> num_features:<br>    <span class="hljs-keyword">if</span> feature ==<span class="hljs-string">&#x27;CHAS&#x27;</span>:<br>        <span class="hljs-keyword">continue</span><br>    <br>    Q1  = data[feature].quantile(q=<span class="hljs-number">0.25</span>) <span class="hljs-comment"># 下四分位</span><br>    Q3  = data[feature].quantile(q=<span class="hljs-number">0.75</span>) <span class="hljs-comment"># 上四分位</span><br>    <br>    IQR = Q3-Q1 <br>    top = Q3+<span class="hljs-number">1.5</span>*IQR <span class="hljs-comment"># 最大估计值</span><br>    bot = Q1-<span class="hljs-number">1.5</span>*IQR <span class="hljs-comment"># 最小估计值</span><br>    values=data[feature].values<br>    values[values &gt; top] = top <span class="hljs-comment"># 临界值取代噪声</span><br>    values[values &lt; bot] = bot <span class="hljs-comment"># 临界值取代噪声</span><br>    data[feature] = values.astype(data[feature].dtypes)<br><br><span class="hljs-comment"># 再次查看箱线图，异常值已被临界值替换（数据量较多或本身异常值较少时，箱线图展示会不容易体现出来）</span><br>boxplot(data, <span class="hljs-string">&#x27;ml-vis6.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure><p>从输出结果看，经过异常值处理后，箱线图中异常值得到了改善。</p><hr><h5 id="√-2-5-1-3-数据集划分"><a href="#√-2-5-1-3-数据集划分" class="headerlink" title="[√] 2.5.1.3 - 数据集划分"></a>[√] 2.5.1.3 - 数据集划分</h5><p>由于本实验比较简单，将数据集划分为两份：训练集和测试集，不包括验证集。</p><p>具体代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br>paddle.seed(<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># 划分训练集和测试集</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_test_split</span>(<span class="hljs-params">X, y, train_percent=<span class="hljs-number">0.8</span></span>):<br>    n = <span class="hljs-built_in">len</span>(X)<br>    shuffled_indices = paddle.randperm(n) <span class="hljs-comment"># 返回一个数值在0到n-1、随机排列的1-D Tensor</span><br>    train_set_size = <span class="hljs-built_in">int</span>(n*train_percent)<br>    train_indices = shuffled_indices[:train_set_size]<br>    test_indices = shuffled_indices[train_set_size:]<br><br>    X = X.values<br>    y = y.values<br><br>    X_train=X[train_indices]<br>    y_train = y[train_indices]<br>    <br>    X_test = X[test_indices]<br>    y_test = y[test_indices]<br><br>    <span class="hljs-keyword">return</span> X_train, X_test, y_train, y_test <br><br><br>X = data.drop([<span class="hljs-string">&#x27;MEDV&#x27;</span>], axis=<span class="hljs-number">1</span>)<br>y = data[<span class="hljs-string">&#x27;MEDV&#x27;</span>]<br><br>X_train, X_test, y_train, y_test = train_test_split(X,y)<span class="hljs-comment"># X_train每一行是个样本，shape[N,D]</span><br><br></code></pre></td></tr></table></figure><hr><h5 id="√-2-5-1-4-特征工程"><a href="#√-2-5-1-4-特征工程" class="headerlink" title="[√] 2.5.1.4 - 特征工程"></a>[√] 2.5.1.4 - 特征工程</h5><p>为了消除纲量对数据特征之间影响，在模型训练前，需要对特征数据进行归一化处理，将数据缩放到[0, 1]区间内，使得不同特征之间具有可比性。</p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br>X_train = paddle.to_tensor(X_train,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>X_test = paddle.to_tensor(X_test,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_train = paddle.to_tensor(y_train,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>y_test = paddle.to_tensor(y_test,dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><br>X_min = paddle.<span class="hljs-built_in">min</span>(X_train,axis=<span class="hljs-number">0</span>)<br>X_max = paddle.<span class="hljs-built_in">max</span>(X_train,axis=<span class="hljs-number">0</span>)<br><br>X_train = (X_train-X_min)/(X_max-X_min)<br><br>X_test  = (X_test-X_min)/(X_max-X_min)<br><br><span class="hljs-comment"># 训练集构造</span><br>train_dataset=(X_train,y_train)<br><span class="hljs-comment"># 测试集构造</span><br>test_dataset=(X_test,y_test)<br></code></pre></td></tr></table></figure><hr><h4 id="√-2-5-2-模型构建"><a href="#√-2-5-2-模型构建" class="headerlink" title="[√] 2.5.2 - 模型构建"></a>[√] 2.5.2 - 模型构建</h4><p>实例化一个线性回归模型，特征维度为 12:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.op <span class="hljs-keyword">import</span> Linear<br><br><span class="hljs-comment"># 模型实例化</span><br>input_size = <span class="hljs-number">12</span><br>model=Linear(input_size)<br></code></pre></td></tr></table></figure><hr><h4 id="√-2-5-3-完善Runner类"><a href="#√-2-5-3-完善Runner类" class="headerlink" title="[√] 2.5.3 - 完善Runner类"></a>[√] 2.5.3 - 完善Runner类</h4><p>模型定义好后，围绕模型需要配置损失函数、优化器、评估、测试等信息，以及模型相关的一些其他信息（如模型存储路径等）。</p><p>在本章中使用的<strong>Runner</strong>类为V1版本。其中训练过程通过直接求解解析解的方式得到模型参数，没有模型优化及计算损失函数过程，模型训练结束后保存模型参数。</p><p>训练配置中定义:</p><ul><li>训练环境，如GPU还是CPU，本案例不涉及；</li><li>优化器，本案例不涉及；</li><li>损失函数，本案例通过平方损失函数得到模型参数的解析解；</li><li>评估指标，本案例利用MSE评估模型效果。</li></ul><p>在测试集上使用MSE对模型性能进行评估。本案例利用飞桨框架提供的<a href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/MSELoss_cn.html">MSELoss API</a>实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br>mse_loss = nn.MSELoss()<br></code></pre></td></tr></table></figure><p>具体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> nndl.opitimizer <span class="hljs-keyword">import</span> optimizer_lsm<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Runner</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, loss_fn, metric</span>):<br>        <span class="hljs-comment"># 优化器和损失函数为None,不再关注</span><br><br>        <span class="hljs-comment"># 模型</span><br>        self.model=model<br>        <span class="hljs-comment"># 评估指标</span><br>        self.metric = metric<br>        <span class="hljs-comment"># 优化器</span><br>        self.optimizer = optimizer<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self,dataset,reg_lambda,model_dir</span>):<br>        X,y = dataset<br>        self.optimizer(self.model,X,y,reg_lambda)<br><br>        <span class="hljs-comment"># 保存模型</span><br>        self.save_model(model_dir)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, dataset, **kwargs</span>):<br>        X,y = dataset<br><br>        y_pred = self.model(X)<br>        result = self.metric(y_pred, y)<br><br>        <span class="hljs-keyword">return</span> result<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X, **kwargs</span>):<br>        <span class="hljs-keyword">return</span> self.model(X)<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, model_dir</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(model_dir):<br>            os.makedirs(model_dir)<br>        <br>        params_saved_path = os.path.join(model_dir,<span class="hljs-string">&#x27;params.pdtensor&#x27;</span>)<br>        paddle.save(model.params,params_saved_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_dir</span>):<br>        params_saved_path = os.path.join(model_dir,<span class="hljs-string">&#x27;params.pdtensor&#x27;</span>)<br>        self.model.params=paddle.load(params_saved_path)<br><br>optimizer = optimizer_lsm<br><br><span class="hljs-comment"># 实例化Runner</span><br>runner = Runner(model, optimizer=optimizer,loss_fn=<span class="hljs-literal">None</span>, metric=mse_loss)<br></code></pre></td></tr></table></figure><hr><h4 id="√-2-5-4-模型训练"><a href="#√-2-5-4-模型训练" class="headerlink" title="[√] 2.5.4 - 模型训练"></a>[√] 2.5.4 - 模型训练</h4><p>在组装完成<code>Runner</code>之后，我们将开始进行模型训练、评估和测试。首先，我们先实例化<code>Runner</code>，然后开始进行装配训练环境，接下来就可以开始训练了，相关代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型保存文件夹</span><br>saved_dir = <span class="hljs-string">&#x27;/home/aistudio/work/models&#x27;</span><br><br><span class="hljs-comment"># 启动训练</span><br>runner.train(train_dataset,reg_lambda=<span class="hljs-number">0</span>,model_dir=saved_dir)<br></code></pre></td></tr></table></figure><p>打印出训练得到的权重：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">columns_list = data.columns.to_list()<br>weights = runner.model.params[<span class="hljs-string">&#x27;w&#x27;</span>].tolist()<br>b = runner.model.params[<span class="hljs-string">&#x27;b&#x27;</span>].item()<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(weights)):<br>    <span class="hljs-built_in">print</span>(columns_list[i],<span class="hljs-string">&quot;weight:&quot;</span>,weights[i])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;b:&quot;</span>,b)<br><br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">CRIM weight: -<span class="hljs-number">6.7268967628479</span><br>ZN weight: <span class="hljs-number">1.28081214427948</span><br>INDUS weight: -<span class="hljs-number">0.4696650803089142</span><br>CHAS weight: <span class="hljs-number">2.235346794128418</span><br>NOX weight: -<span class="hljs-number">7.0105814933776855</span><br>RM weight: <span class="hljs-number">9.76220417022705</span><br>AGE weight: -<span class="hljs-number">0.8556219339370728</span><br>DIS weight: -<span class="hljs-number">9.265738487243652</span><br>RAD weight: <span class="hljs-number">7.973038673400879</span><br>TAX weight: -<span class="hljs-number">4.365403175354004</span><br>PTRATIO weight: -<span class="hljs-number">7.105883598327637</span><br>LSTAT weight: -<span class="hljs-number">13.165120124816895</span><br>b: <span class="hljs-number">32.1007522583008</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-2-5-5-模型测试"><a href="#√-2-5-5-模型测试" class="headerlink" title="[√] 2.5.5 - 模型测试"></a>[√] 2.5.5 - 模型测试</h4><p>加载训练好的模型参数，在测试集上得到模型的MSE指标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载模型权重</span><br>runner.load_model(saved_dir)<br><br>mse = runner.evaluate(test_dataset)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;MSE:&#x27;</span>, mse.item())<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">MSE</span>: <span class="hljs-number">12</span>.<span class="hljs-number">345974922180176</span><br></code></pre></td></tr></table></figure><hr><h4 id="√-2-5-6-模型预测"><a href="#√-2-5-6-模型预测" class="headerlink" title="[√] 2.5.6 - 模型预测"></a>[√] 2.5.6 - 模型预测</h4><p>使用<code>Runner</code>中<code>load_model</code>函数加载保存好的模型，使用<code>predict</code>进行模型预测，代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">runner.load_model(saved_dir)<br>pred = runner.predict(X_test[:<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;真实房价：&quot;</span>,y_test[:<span class="hljs-number">1</span>].item())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;预测的房价：&quot;</span>,pred.item())<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs">真实房价： 33.099998474121094<br>预测的房价： 33.04654312133789<br></code></pre></td></tr></table></figure><hr>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第1章 - 实践基础1</title>
    <link href="/posts/2304768005/"/>
    <url>/posts/2304768005/</url>
    
    <content type="html"><![CDATA[<h2 id="√-课节1-实践基础"><a href="#√-课节1-实践基础" class="headerlink" title="[√] 课节1: 实践基础"></a>[√] 课节1: 实践基础</h2><h3 id="1-1-视频-直播回放——邱锡鹏老师"><a href="#1-1-视频-直播回放——邱锡鹏老师" class="headerlink" title="1.1 - [视频] 直播回放——邱锡鹏老师"></a>1.1 - [视频] 直播回放——邱锡鹏老师</h3><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209164937541.png" alt="image-20221209164905028"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209164905028.png" alt="image-20221209164937541"></p><p>模型需要自己构建，其中线性模型可以看做浅层学习，非线性函数可以看成深层学习或深度学习</p><p>学习准则可以定义为一些损失函数来评判学习的好坏</p><p>有了学习准则之后可以通过一些优化算法来优化，让其在学习准则下达到最优</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209165732312.png" alt="image-20221209165332527"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209165332527.png" alt="image-20221209165554684"></p><p>深度学习 &#x3D; 表示学习 + 预测</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209165820537.png" alt="image-20221209165732312"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209170339498.png" alt="image-20221209165820537"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209165554684.png" alt="image-20221209170102382"></p><p>通过偏导数，在机器学习中，对参数稍微调整一下，看一下这个调整对最终结果的影响是多少，从而确定这个参数是否重要已经应该怎样调整这个参数。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209170617176.png" alt="image-20221209170339498"></p><p>深度学习是一类机器学习问题，主要解决贡献度分配问题，此处的贡献度分配就是指的神经网络模型中的权重参数w。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209170913255.png" alt="image-20221209170519833"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171050706.png" alt="image-20221209170617176"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171719637.png" alt="image-20221209170913255"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209170102382.png" alt="image-20221209171050706"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171517443.png" alt="image-20221209171152826"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172155193.png" alt="image-20221209171517443"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172120885.png" alt="image-20221209171719637"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172243261.png" alt="image-20221209171807581"></p><blockquote><p>数据的表示形式：张量</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171807581.png" alt="image-20221209171932632"></p><blockquote><p>模型的基本单位：算子</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172413995.png" alt="image-20221209172120885"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172334081.png" alt="image-20221209172155193"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171932632.png" alt="image-20221209172243261"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209184904577.png" alt="image-20221209172334081"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209190201310.png" alt="image-20221209172402042"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172402042.png" alt="image-20221209172413995"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209203902367.png" alt="image-20221209172451113"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209204026855.png" alt="image-20221209184636787"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209205003669.png" alt="image-20221209184730941"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210309555.png" alt="image-20221209184904577"></p><hr><h3 id="1-2-视频-直播回放——程军老师"><a href="#1-2-视频-直播回放——程军老师" class="headerlink" title="1.2 - [视频] 直播回放——程军老师"></a>1.2 - [视频] 直播回放——程军老师</h3><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209204701684.png" alt="image-20221209190201310"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209211448984.png" alt="image-20221209190403077"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209211723963.png" alt="image-20221209190439105"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209204211326.png"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209213426769.png" alt="image-20221209190912554"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209211809857.png" alt="image-20221209191043045"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211203408158.png" alt="image-20221209191142578"></p><p>resnet最常用的版本是resnet50，resnet后续的一些改进的版本有一些是对步长做优化。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209213957385.png" alt="image-20221209203557361"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211203606088.png" alt="image-20221209203902367"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/a7fe6d71b8754f5a80bfbd44639508a64024972f4af14c609e4bebf0f2e0d21c.png" alt="image-20221209204026855"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/085004d14449417392350eec7404219a803444b4e49b4bbc963bdae4f59d2fcb.png" alt="image-20221209204211326"></p><p>输出通道：等于卷积核的个数，也等于输出特征图的深度。</p><blockquote><p>多通道卷积层算子的代码</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/cd5e251e1b9c4f37888af85b4ee9386ffc65fca2d4f4473a8f6adc14e1e3238d.png" alt="image-20221209204457431"></p><blockquote><p>汇聚层算子</p></blockquote><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209170519833.png" alt="image-20221209204701684"></p><p>汇聚层 &#x3D; 池化层？</p><p>平均池化、最大池化</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209171152826.png" alt="image-20221209205003669"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209184636787.png" alt="image-20221209205908083"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209190439105.png" alt="image-20221209210046272"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209172451113.png" alt="image-20221209210241288"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209190912554.png" alt="image-20221209210309555"></p><p>transformer的核心</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209190625140.png" alt="image-20221209210516027"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209191043045.png" alt="image-20221209210809368"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209203557361.png" alt="image-20221209210857085"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209204457431.png" alt="image-20221209211255275"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209191142578.png" alt="image-20221209211448984"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209190403077.png" alt="image-20221209211723963"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209205908083.png" alt="image-20221209211809857"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210241288.png" alt="image-20221209212008942"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210046272.png" alt="image-20221209213426769"></p><p>自注意力机制中，一个元素是和所有的元素做运算的，这也是transformer为什么能够取得很好的效果的原因。</p><p>通过这种方式，不同于RNN的地方在于，不同的时间序列能够做信息的传递、融合；还有一个好处是这里面的运算是可以并行的</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210809368.png" alt="image-20221209213544884"></p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210857085.png" alt="image-20221209213957385"></p><hr><p>##↓↓↓############################↓↓↓</p><h3 id="1-3-项目-第1章：实践基础"><a href="#1-3-项目-第1章：实践基础" class="headerlink" title="1.3 - [项目] 第1章：实践基础"></a>1.3 - [项目] 第1章：实践基础</h3><p>在讲解本书主要内容之前，本章先对实践环节的基础知识进行介绍，主要介绍以下内容：</p><ul><li>张量（Tensor）：深度学习中表示和存储数据的主要形式。在动手实践机器学习之前，需要熟悉张量的概念、性质和运算规则，以及了解飞桨中张量的各种API。</li><li>算子（Operator）：构建神经网络模型的基础组件。每个算子有前向和反向计算过程，前向计算对应一个数学函数，而反向计算对应这个数学函数的梯度计算。有了算子，我们就可以很方便地通过算子来搭建复杂的神经网络模型，而不需要手工计算梯度。</li></ul><p>此外，本章还汇总了在本书中自定义的一些算术、数据集以及轻量级训练框架Runner类。</p><h4 id="D（√）1-1-如何运行本书的代码"><a href="#D（√）1-1-如何运行本书的代码" class="headerlink" title="D（√）1.1 如何运行本书的代码"></a>D（√）1.1 如何运行本书的代码</h4><p>本书中代码有两种运行方式：本地运行AI Studio运行。下面我们分别介绍两种运行方式的环境准备及操作方法。</p><h5 id="（√）1-1-1-本地运行"><a href="#（√）1-1-1-本地运行" class="headerlink" title="（√）1.1.1 本地运行"></a>（√）1.1.1 本地运行</h5><h5 id="（√）1-1-2-AI-studio运行√"><a href="#（√）1-1-2-AI-studio运行√" class="headerlink" title="（√）1.1.2 AI studio运行√"></a>（√）1.1.2 AI studio运行√</h5><p>AI Studio是基于飞桨的人工智能学习与实训社区，提供免费的算力支持，本书的内容在AI Studio上提供配套的BML Codelab项目。BML Codelab是面向个人和企业开发者的AI开发工具，基于Jupyter提供了在线的交互式开发环境。</p><p>BML Codelab目前默认使用飞桨2.2.2版本，无须额外安装。如图1.1所示，通过选择“启动环境”→\rightarrow→“基础版”即可在CPU环境下运行，选择“至尊版GPU”即可在32G RAM的Tesla V100上运行代码，至尊版GPU每天有8小时的免费使用时间。</p><h4 id="D（√）1-2-张量"><a href="#D（√）1-2-张量" class="headerlink" title="D（√）1.2 张量"></a>D（√）1.2 张量</h4><p>在深度学习框架中，数据经常用张量(Tensor)的形式来存储。张量是矩阵的扩展与延伸，可以认为是高阶的矩阵。1阶张量为向量，2阶张量为矩阵。如果你对</p><p>Numpy熟悉，那么张量是类似于Numpy的多维数组(ndarray)的概念，可以具有任意多的维度。</p><p>张量中元素的类型可以是布尔型数据、整数、浮点数或者复数，<strong>但同一张量中所有元素的数据类型均相同</strong>。因此我们可以给张量定义一个数据类型(dtype)来表</p><p>示其元素的类型。</p><h5 id="E（√）1-2-1-创建张量"><a href="#E（√）1-2-1-创建张量" class="headerlink" title="E（√）1.2.1 创建张量"></a>E（√）1.2.1 创建张量</h5><h6 id="F（√）1-2-1-1-指定数据创建张量"><a href="#F（√）1-2-1-1-指定数据创建张量" class="headerlink" title="F（√）1.2.1.1 指定数据创建张量"></a>F（√）1.2.1.1 指定数据创建张量</h6><p>（1）指定数据，创建一维张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 导入PaddlePaddle</span><br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-comment"># 创建一维Tensor</span><br>ndim_1_Tensor = paddle.to_tensor([<span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>])<br><span class="hljs-built_in">print</span>(ndim_1_Tensor)<br></code></pre></td></tr></table></figure><p>（2）指定数据，创建二维矩阵</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建二维Tensor</span><br>ndim_2_Tensor = paddle.to_tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>],<br>                                  [<span class="hljs-number">4.0</span>, <span class="hljs-number">5.0</span>, <span class="hljs-number">6.0</span>]])<br><span class="hljs-built_in">print</span>(ndim_2_Tensor)<br></code></pre></td></tr></table></figure><p>（3）指定数据，创建多维张量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建多维Tensor</span><br>ndim_3_Tensor = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>                                   [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]],<br>                                  [[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>                                   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]]])<br><span class="hljs-built_in">print</span>(ndim_3_Tensor)<br></code></pre></td></tr></table></figure><p>注意：张量在任何一个维度上的元素数量必须相等</p><h6 id="F（√）1-2-1-2-指定形状创建"><a href="#F（√）1-2-1-2-指定形状创建" class="headerlink" title="F（√）1.2.1.2 指定形状创建"></a>F（√）1.2.1.2 指定形状创建</h6><p>如果要创建一个指定形状、元素数据相同的张量，可以使用paddle.zeros、paddle.ones、paddle.full等API。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">m, n = <span class="hljs-number">2</span>, <span class="hljs-number">3</span><br><br><span class="hljs-comment"># 使用paddle.zeros创建数据全为0，形状为[m, n]的Tensor</span><br>zeros_Tensor = paddle.zeros([m, n])<br><br><span class="hljs-comment"># 使用paddle.ones创建数据全为1，形状为[m, n]的Tensor</span><br>ones_Tensor = paddle.ones([m, n])<br><br><span class="hljs-comment"># 使用paddle.full创建数据全为指定值，形状为[m, n]的Tensor，这里我们指定数据为10</span><br>full_Tensor = paddle.full([m, n], <span class="hljs-number">10</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;zeros Tensor: &#x27;</span>, zeros_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;ones Tensor: &#x27;</span>, ones_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;full Tensor: &#x27;</span>, full_Tensor)<br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-1-3-指定区间创建"><a href="#F（√）1-2-1-3-指定区间创建" class="headerlink" title="F（√）1.2.1.3 指定区间创建"></a>F（√）1.2.1.3 指定区间创建</h6><p>如果要在指定区间内创建张量，可以使用<code>paddle.arange</code>、<code>paddle.linspace</code>等API。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用paddle.arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor</span><br>arange_Tensor = paddle.arange(start=<span class="hljs-number">1</span>, end=<span class="hljs-number">5</span>, step=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 使用paddle.linspace创建以元素个数num均匀分隔数值区间[start, stop]的Tensor</span><br>linspace_Tensor = paddle.linspace(start=<span class="hljs-number">1</span>, stop=<span class="hljs-number">5</span>, num=<span class="hljs-number">5</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;arange Tensor: &#x27;</span>, arange_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;linspace Tensor: &#x27;</span>, linspace_Tensor)<br></code></pre></td></tr></table></figure><h5 id="E（√）1-2-2-张量的属性"><a href="#E（√）1-2-2-张量的属性" class="headerlink" title="E（√）1.2.2 张量的属性"></a>E（√）1.2.2 张量的属性</h5><h6 id="F（√）1-2-2-1-张量的形状"><a href="#F（√）1-2-2-1-张量的形状" class="headerlink" title="F（√）1.2.2.1 张量的形状"></a>F（√）1.2.2.1 张量的形状</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">ndim_4_Tensor = paddle.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Number of dimensions:&quot;</span>, ndim_4_Tensor.ndim)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Shape of Tensor:&quot;</span>, ndim_4_Tensor.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Elements number along axis 0 of Tensor:&quot;</span>, ndim_4_Tensor.shape[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Elements number along the last axis of Tensor:&quot;</span>, ndim_4_Tensor.shape[-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Number of elements in Tensor: &#x27;</span>, ndim_4_Tensor.size)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">Number of dimensions: <span class="hljs-number">4</span><br>Shape of Tensor: [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]<br>Elements number along axis <span class="hljs-number">0</span> of Tensor: <span class="hljs-number">2</span><br>Elements number along the last axis of Tensor: <span class="hljs-number">5</span><br>Number of elements <span class="hljs-keyword">in</span> Tensor:  <span class="hljs-number">120</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-2-2-形状的改变"><a href="#F（√）1-2-2-2-形状的改变" class="headerlink" title="F（√）1.2.2.2 形状的改变"></a>F（√）1.2.2.2 形状的改变</h6><blockquote><p>总结：</p><p>使用reshape方法和unsqueeze方法改变张量的shape</p></blockquote><p>除了查看张量的形状外，重新设置张量的在实际编程中也具有重要意义，飞桨提供了<code>paddle.reshape</code>接口来改变张量的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义一个shape为[3,2,5]的三维Tensor</span><br>ndim_3_Tensor = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>],<br>                                   [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>]],<br>                                  [[<span class="hljs-number">11</span>, <span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>],<br>                                   [<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>]],<br>                                  [[<span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>, <span class="hljs-number">25</span>],<br>                                   [<span class="hljs-number">26</span>, <span class="hljs-number">27</span>, <span class="hljs-number">28</span>, <span class="hljs-number">29</span>, <span class="hljs-number">30</span>]]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;the shape of ndim_3_Tensor:&quot;</span>, ndim_3_Tensor.shape)<br><br><span class="hljs-comment"># paddle.reshape 可以保持在输入数据不变的情况下，改变数据形状。这里我们设置reshape为[2,5,3]</span><br>reshape_Tensor = paddle.reshape(ndim_3_Tensor, [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;After reshape:&quot;</span>, reshape_Tensor)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">the shape of ndim_3_Tensor: [<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>]<br>After reshape: Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">3</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[[<span class="hljs-number">1</span> , <span class="hljs-number">2</span> , <span class="hljs-number">3</span> ],<br>         [<span class="hljs-number">4</span> , <span class="hljs-number">5</span> , <span class="hljs-number">6</span> ],<br>         [<span class="hljs-number">7</span> , <span class="hljs-number">8</span> , <span class="hljs-number">9</span> ],<br>         [<span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span>],<br>         [<span class="hljs-number">13</span>, <span class="hljs-number">14</span>, <span class="hljs-number">15</span>]],<br><br>        [[<span class="hljs-number">16</span>, <span class="hljs-number">17</span>, <span class="hljs-number">18</span>],<br>         [<span class="hljs-number">19</span>, <span class="hljs-number">20</span>, <span class="hljs-number">21</span>],<br>         [<span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">24</span>],<br>         [<span class="hljs-number">25</span>, <span class="hljs-number">26</span>, <span class="hljs-number">27</span>],<br>         [<span class="hljs-number">28</span>, <span class="hljs-number">29</span>, <span class="hljs-number">30</span>]]])<br></code></pre></td></tr></table></figure><p>从输出结果看，将张量从[3, 2, 5]的形状reshape为[2, 5, 3]的形状时，张量内的数据不会发生改变，元素顺序也没有发生改变，只有数据形状发生了改变。</p><p><strong>笔记</strong></p><p>使用reshape时存在一些技巧，比如：</p><ul><li>-1表示这个维度的值是从张量的元素总数和剩余维度推断出来的。因此，有且只有一个维度可以被设置为-1。</li><li>0表示实际的维数是从张量的对应维数中复制出来的，因此shape中0所对应的索引值不能超过张量的总维度。</li></ul><p>分别对上文定义的ndim_3_Tensor进行reshape为[-1]和reshape为[0, 5, 2]两种操作，观察新张量的形状。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">new_Tensor1 = ndim_3_Tensor.reshape([-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 1 shape: &#x27;</span>, new_Tensor1.shape)<br>new_Tensor2 = ndim_3_Tensor.reshape([<span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 2 shape: &#x27;</span>, new_Tensor2.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">new Tensor <span class="hljs-number">1</span> shape:  [<span class="hljs-number">30</span>]<br>new Tensor <span class="hljs-number">2</span> shape:  [<span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>]<br></code></pre></td></tr></table></figure><p>从输出结果看，第一行代码中的第一个reshape操作将张量<code>reshape</code>为元素数量为30的一维向量；第四行代码中的第二个<code>reshape</code>操作中，0对应的维度的元素个数与原张量在该维度上的元素个数相同。</p><p>除使用<code>paddle.reshape</code>进行张量形状的改变外，还可以通过<code>paddle.unsqueeze</code>将张量中的一个或多个维度中插入尺寸为1的维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">ones_Tensor = paddle.ones([<span class="hljs-number">5</span>, <span class="hljs-number">10</span>])<br>new_Tensor1 = paddle.unsqueeze(ones_Tensor, axis=<span class="hljs-number">0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 1 shape: &#x27;</span>, new_Tensor1.shape)<br>new_Tensor2 = paddle.unsqueeze(ones_Tensor, axis=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;new Tensor 2 shape: &#x27;</span>, new_Tensor2.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">new Tensor <span class="hljs-number">1</span> shape:  [<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>]<br>new Tensor <span class="hljs-number">2</span> shape:  [<span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">10</span>]<br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-2-3-张量的数据类型"><a href="#F（√）1-2-2-3-张量的数据类型" class="headerlink" title="F（√）1.2.2.3 张量的数据类型"></a>F（√）1.2.2.3 张量的数据类型</h6><p>飞桨中可以通过<code>Tensor.dtype</code>来查看张量的数据类型，类型支持bool、float16、float32、float64、uint8、int8、int16、int32、int64和复数类型数据。</p><p>1）通过Python元素创建的张量，可以通过dtype来指定数据类型，如果未指定：</p><ul><li>对于Python整型数据，则会创建int64型张量。</li><li>对于Python浮点型数据，默认会创建float32型张量。</li></ul><p>2）通过Numpy数组创建的张量，则与其原来的数据类型保持相同。通过<code>paddle.to_tensor()</code>函数可以将Numpy数组转化为张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用paddle.to_tensor通过已知数据来创建一个Tensor</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor dtype from Python integers:&quot;</span>, paddle.to_tensor(<span class="hljs-number">1</span>).dtype)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor dtype from Python floating point:&quot;</span>, paddle.to_tensor(<span class="hljs-number">1.0</span>).dtype)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor dtype <span class="hljs-keyword">from</span> Python integers: paddle.int64<br>Tensor dtype <span class="hljs-keyword">from</span> Python floating point: paddle.float32<br></code></pre></td></tr></table></figure><p>如果想改变张量的数据类型，可以通过调用<code>paddle.cast</code>API来实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义dtype为float32的Tensor</span><br>float32_Tensor = paddle.to_tensor(<span class="hljs-number">1.0</span>)<br><span class="hljs-comment"># paddle.cast可以将输入数据的数据类型转换为指定的dtype并输出。支持输出和输入数据类型相同。</span><br>int64_Tensor = paddle.cast(float32_Tensor, dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor after cast to int64:&quot;</span>, int64_Tensor.dtype)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor after cast to int64: paddle.int64<br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-2-4-张量的设备位置"><a href="#F（√）1-2-2-4-张量的设备位置" class="headerlink" title="F（√）1.2.2.4 张量的设备位置"></a>F（√）1.2.2.4 张量的设备位置</h6><p>初始化张量时可以通过place来指定其分配的设备位置，可支持的设备位置有三种：CPU、GPU和固定内存。</p><p>固定内存也称为不可分页内存或锁页内存，它与GPU之间具有更高的读写效率，并且支持异步传输，这对网络整体性能会有进一步提升，但它的缺点是分配空间过多时可能会降低主机系统的性能，因为它减少了用于存储虚拟内存数据的可分页内存。当未指定设备位置时，张量默认设备位置和安装的飞桨版本一致，如安装了GPU版本的飞桨，则设备位置默认为GPU。</p><p>如下代码分别创建了CPU、GPU和固定内存上的张量，并通过<code>Tensor.place</code>查看张量所在的设备位置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 创建CPU上的Tensor</span><br>cpu_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CPUPlace())<br><span class="hljs-comment"># 通过Tensor.place查看张量所在设备位置</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;cpu Tensor: &#x27;</span>, cpu_Tensor.place)<br><span class="hljs-comment"># 创建GPU上的Tensor</span><br>gpu_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CUDAPlace(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;gpu Tensor: &#x27;</span>, gpu_Tensor.place)<br><span class="hljs-comment"># 创建固定内存上的Tensor</span><br>pin_memory_Tensor = paddle.to_tensor(<span class="hljs-number">1</span>, place=paddle.CUDAPinnedPlace())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;pin memory Tensor: &#x27;</span>, pin_memory_Tensor.place)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">cpu Tensor:  CPUPlace<br>gpu Tensor:  CUDAPlace(<span class="hljs-number">0</span>)<br>pin memory Tensor:  CUDAPinnedPlace<br></code></pre></td></tr></table></figure><h5 id="E（√）1-2-3-张量与Numpy数组转换"><a href="#E（√）1-2-3-张量与Numpy数组转换" class="headerlink" title="E（√）1.2.3 张量与Numpy数组转换"></a>E（√）1.2.3 张量与Numpy数组转换</h5><p>张量和Numpy数组可以相互转换。第1.2.2.3节中我们了解到paddle.to_tensor()函数可以将Numpy数组转化为张量，也可以通过<code>Tensor.numpy()</code>函数将张量转化为Numpy数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">ndim_1_Tensor = paddle.to_tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>])<br><span class="hljs-comment"># 将当前 Tensor 转化为 numpy.ndarray</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Tensor to convert: &#x27;</span>, ndim_1_Tensor.numpy())<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(ndim_1_Tensor.numpy()))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor to convert:  [<span class="hljs-number">1.</span> <span class="hljs-number">2.</span>]<br>&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;numpy.ndarray&#x27;</span>&gt;<br></code></pre></td></tr></table></figure><h5 id="E（√）1-2-4-张量的访问"><a href="#E（√）1-2-4-张量的访问" class="headerlink" title="E（√）1.2.4 张量的访问"></a>E（√）1.2.4 张量的访问</h5><h6 id="F（√）1-2-4-1-索引和切片"><a href="#F（√）1-2-4-1-索引和切片" class="headerlink" title="F（√）1.2.4.1 索引和切片"></a>F（√）1.2.4.1 索引和切片</h6><p>我们可以通过索引或切片方便地访问或修改张量。飞桨使用标准的Python索引规则与Numpy索引规则，具有以下特点：</p><ul><li>基于0−n0-n0−<em>n</em>的下标进行索引，如果下标为负数，则从尾部开始计算。</li><li>通过冒号“:”分隔切片参数start:stop:step来进行切片操作，也就是访问start到stop范围内的部分元素并生成一个新的序列。其中start为切片的起始位置，stop为切片的截止位置，step是切片的步长，这三个参数均可缺省。</li></ul><h6 id="F（√）1-2-4-2-访问张量"><a href="#F（√）1-2-4-2-访问张量" class="headerlink" title="F（√）1.2.4.2 访问张量"></a>F（√）1.2.4.2 访问张量</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义1个一维Tensor</span><br>ndim_1_Tensor = paddle.to_tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Origin Tensor:&quot;</span>, ndim_1_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First element:&quot;</span>, ndim_1_Tensor[<span class="hljs-number">0</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Last element:&quot;</span>, ndim_1_Tensor[-<span class="hljs-number">1</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;All element:&quot;</span>, ndim_1_Tensor[:])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Before 3:&quot;</span>, ndim_1_Tensor[:<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Interval of 3:&quot;</span>, ndim_1_Tensor[::<span class="hljs-number">3</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Reverse:&quot;</span>, ndim_1_Tensor[::-<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python">Origin Tensor: Tensor(shape=[<span class="hljs-number">9</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br>First element: Tensor(shape=[<span class="hljs-number">1</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>])<br>Last element: Tensor(shape=[<span class="hljs-number">1</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">8</span>])<br>All element: Tensor(shape=[<span class="hljs-number">9</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])<br>Before <span class="hljs-number">3</span>: Tensor(shape=[<span class="hljs-number">3</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>])<br>Interval of <span class="hljs-number">3</span>: Tensor(shape=[<span class="hljs-number">3</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>])<br>Reverse: Tensor(shape=[<span class="hljs-number">9</span>], dtype=int64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [<span class="hljs-number">8</span>, <span class="hljs-number">7</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure><p>针对二维及以上维度的张量，在多个维度上进行索引或切片。索引或切片的第一个值对应第0维，第二个值对应第1维，以此类推，如果某个维度上未指定索引，则默认为“:”。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义1个二维Tensor</span><br>ndim_2_Tensor = paddle.to_tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],<br>                                  [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>],<br>                                  [<span class="hljs-number">8</span>, <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Origin Tensor:&quot;</span>, ndim_2_Tensor)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>])<span class="hljs-comment"># 第一行</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>, :])<span class="hljs-comment"># 第一行</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First column:&quot;</span>, ndim_2_Tensor[:, <span class="hljs-number">0</span>]) <span class="hljs-comment"># 第一列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Last column:&quot;</span>, ndim_2_Tensor[:, -<span class="hljs-number">1</span>]) <span class="hljs-comment"># 最后一列</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;All element:&quot;</span>, ndim_2_Tensor[:]) <span class="hljs-comment"># 全部</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;First row and second column:&quot;</span>, ndim_2_Tensor[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>]) <span class="hljs-comment"># 第一行，第二列</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-4-3-修改张量"><a href="#F（√）1-2-4-3-修改张量" class="headerlink" title="F（√）1.2.4.3 修改张量"></a>F（√）1.2.4.3 修改张量</h6><p>与访问张量类似，可以在单个或多个轴上通过索引或切片操作来修改张量。</p><blockquote><p><strong>提醒</strong><br>慎重通过索引或切片操作来修改张量，此操作仅会原地修改该张量的数值，且原值不会被保存。如果被修改的张量参与梯度计算，将仅会使用修改后的数值，这可能会给梯度计算引入风险。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义1个二维Tensor</span><br>ndim_2_Tensor = paddle.ones([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Origin Tensor: &#x27;</span>, ndim_2_Tensor)<br><span class="hljs-comment"># 修改第1维为0</span><br>ndim_2_Tensor[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)<br><span class="hljs-comment"># 修改第1维为2.1</span><br>ndim_2_Tensor[<span class="hljs-number">0</span>:<span class="hljs-number">1</span>] = <span class="hljs-number">2.1</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)<br><span class="hljs-comment"># 修改全部Tensor</span><br>ndim_2_Tensor[...] = <span class="hljs-number">3</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;change Tensor: &#x27;</span>, ndim_2_Tensor)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">Origin Tensor:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>change Tensor:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br>change Tensor:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">2.09999990</span>, <span class="hljs-number">2.09999990</span>, <span class="hljs-number">2.09999990</span>],<br>        [<span class="hljs-number">1.</span>        , <span class="hljs-number">1.</span>        , <span class="hljs-number">1.</span>        ]])<br>change Tensor:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],<br>        [<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>]])<br></code></pre></td></tr></table></figure><h5 id="E（√）1-2-5-张量的运算"><a href="#E（√）1-2-5-张量的运算" class="headerlink" title="E（√）1.2.5 张量的运算"></a>E（√）1.2.5 张量的运算</h5><p>张量支持包括基础数学运算、逻辑运算、矩阵运算等100余种运算操作，以加法为例，有如下两种实现方式：<br>1）使用飞桨API <code>paddle.add(x,y)</code>。<br>2）使用张量类成员函数<code>x.add(y)</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义两个Tensor</span><br>x = paddle.to_tensor([[<span class="hljs-number">1.1</span>, <span class="hljs-number">2.2</span>], [<span class="hljs-number">3.3</span>, <span class="hljs-number">4.4</span>]], dtype=<span class="hljs-string">&quot;float64&quot;</span>)<br>y = paddle.to_tensor([[<span class="hljs-number">5.5</span>, <span class="hljs-number">6.6</span>], [<span class="hljs-number">7.7</span>, <span class="hljs-number">8.8</span>]], dtype=<span class="hljs-string">&quot;float64&quot;</span>)<br><span class="hljs-comment"># 第一种调用方法，paddle.add逐元素相加算子，并将各个位置的输出元素保存到返回结果中</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Method 1: &#x27;</span>, paddle.add(x, y))<br><span class="hljs-comment"># 第二种调用方法</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Method 2: &#x27;</span>, x.add(y))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">Method <span class="hljs-number">1</span>:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">6.60000000</span> , <span class="hljs-number">8.80000000</span> ],<br>        [<span class="hljs-number">11.</span>        , <span class="hljs-number">13.20000000</span>]])<br>Method <span class="hljs-number">2</span>:  Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>], dtype=float64, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">6.60000000</span> , <span class="hljs-number">8.80000000</span> ],<br>        [<span class="hljs-number">11.</span>        , <span class="hljs-number">13.20000000</span>]])<br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-5-1-数学运算"><a href="#F（√）1-2-5-1-数学运算" class="headerlink" title="F（√）1.2.5.1 数学运算"></a>F（√）1.2.5.1 数学运算</h6><p>张量类的基础数学函数运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">x.<span class="hljs-built_in">abs</span>()                       <span class="hljs-comment"># 逐元素取绝对值</span><br>x.ceil()                      <span class="hljs-comment"># 逐元素向上取整</span><br>x.floor()                     <span class="hljs-comment"># 逐元素向下取整</span><br>x.<span class="hljs-built_in">round</span>()                     <span class="hljs-comment"># 逐元素四舍五入</span><br>x.exp()                       <span class="hljs-comment"># 逐元素计算自然常数为底的指数</span><br>x.log()                       <span class="hljs-comment"># 逐元素计算x的自然对数</span><br>x.reciprocal()                <span class="hljs-comment"># 逐元素求倒数</span><br>x.square()                    <span class="hljs-comment"># 逐元素计算平方</span><br>x.sqrt()                      <span class="hljs-comment"># 逐元素计算平方根</span><br>x.sin()                       <span class="hljs-comment"># 逐元素计算正弦</span><br>x.cos()                       <span class="hljs-comment"># 逐元素计算余弦</span><br>x.add(y)                      <span class="hljs-comment"># 逐元素加</span><br>x.subtract(y)                 <span class="hljs-comment"># 逐元素减</span><br>x.multiply(y)                 <span class="hljs-comment"># 逐元素乘（积）</span><br>x.divide(y)                   <span class="hljs-comment"># 逐元素除</span><br>x.mod(y)                      <span class="hljs-comment"># 逐元素除并取余</span><br>x.<span class="hljs-built_in">pow</span>(y)                      <span class="hljs-comment"># 逐元素幂</span><br>x.<span class="hljs-built_in">max</span>()                       <span class="hljs-comment"># 指定维度上元素最大值，默认为全部维度</span><br>x.<span class="hljs-built_in">min</span>()                       <span class="hljs-comment"># 指定维度上元素最小值，默认为全部维度</span><br>x.prod()                      <span class="hljs-comment"># 指定维度上元素累乘，默认为全部维度</span><br>x.<span class="hljs-built_in">sum</span>()                       <span class="hljs-comment"># 指定维度上元素的和，默认为全部维度</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-5-2-逻辑运算"><a href="#F（√）1-2-5-2-逻辑运算" class="headerlink" title="F（√）1.2.5.2 逻辑运算"></a>F（√）1.2.5.2 逻辑运算</h6><p>张量类的逻辑运算函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">x.isfinite()                  <span class="hljs-comment"># 判断Tensor中元素是否是有限的数字，即不包括inf与nan</span><br>x.equal_all(y)                <span class="hljs-comment"># 判断两个Tensor的全部元素是否相等，并返回形状为[1]的布尔类Tensor</span><br>x.equal(y)                    <span class="hljs-comment"># 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor</span><br>x.not_equal(y)                <span class="hljs-comment"># 判断两个Tensor的每个元素是否不相等</span><br>x.less_than(y)                <span class="hljs-comment"># 判断Tensor x的元素是否小于Tensor y的对应元素</span><br>x.less_equal(y)               <span class="hljs-comment"># 判断Tensor x的元素是否小于或等于Tensor y的对应元素</span><br>x.greater_than(y)             <span class="hljs-comment"># 判断Tensor x的元素是否大于Tensor y的对应元素</span><br>x.greater_equal(y)            <span class="hljs-comment"># 判断Tensor x的元素是否大于或等于Tensor y的对应元素</span><br>x.allclose(y)                 <span class="hljs-comment"># 判断两个Tensor的全部元素是否接近</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-2-5-3-矩阵运算"><a href="#F（√）1-2-5-3-矩阵运算" class="headerlink" title="F（√）1.2.5.3 矩阵运算"></a>F（√）1.2.5.3 矩阵运算</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">x.t()                         <span class="hljs-comment"># 矩阵转置</span><br>x.transpose([<span class="hljs-number">1</span>, <span class="hljs-number">0</span>])           <span class="hljs-comment"># 交换第 0 维与第 1 维的顺序</span><br>x.norm(<span class="hljs-string">&#x27;fro&#x27;</span>)                 <span class="hljs-comment"># 矩阵的弗罗贝尼乌斯范数</span><br>x.dist(y, p=<span class="hljs-number">2</span>)                <span class="hljs-comment"># 矩阵（x-y）的2范数</span><br>x.matmul(y)                   <span class="hljs-comment"># 矩阵乘法</span><br></code></pre></td></tr></table></figure><p>有些矩阵运算中也支持大于两维的张量，比如matmul函数，对最后两个维度进行矩阵乘。比如x是形状为[j,k,n,m]的张量，另一个y是[j,k,m,p]的张量，则x.matmul(y)输出的张量形状为[j,k,n,p]。</p><h6 id="F（√）1-2-5-4-广播机制"><a href="#F（√）1-2-5-4-广播机制" class="headerlink" title="F（√）1.2.5.4 广播机制"></a>F（√）1.2.5.4 广播机制</h6><p>飞桨的一些API在计算时支持广播(Broadcasting)机制，允许在一些运算时使用不同形状的张量。通常来讲，如果有一个形状较小和一个形状较大的张量，会希望多次使用较小的张量来对较大的张量执行某些操作，看起来像是形状较小的张量首先被扩展到和较大的张量形状一致，然后再做运算。</p><p><strong>广播机制的条件</strong></p><p>飞桨的广播机制主要遵循如下规则（参考Numpy广播机制）：</p><p>1）每个张量至少为一维张量。</p><p>2）从后往前比较张量的形状，当前维度的大小要么相等，要么其中一个等于1，要么其中一个不存在。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 当两个Tensor的形状一致时，可以广播</span><br>x = paddle.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>y = paddle.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>))<br>z = x + y<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;broadcasting with two same shape tensor: &#x27;</span>, z.shape)<br><br>x = paddle.ones((<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>))<br>y = paddle.ones((<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>))<br><span class="hljs-comment"># 从后往前依次比较：</span><br><span class="hljs-comment"># 第一次：y的维度大小是1</span><br><span class="hljs-comment"># 第二次：x的维度大小是1</span><br><span class="hljs-comment"># 第三次：x和y的维度大小相等，都为3</span><br><span class="hljs-comment"># 第四次：y的维度不存在</span><br><span class="hljs-comment"># 所以x和y是可以广播的</span><br>z = x + y<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;broadcasting with two different shape tensor:&#x27;</span>, z.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">broadcasting <span class="hljs-keyword">with</span> two same shape tensor:  [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]<br>broadcasting <span class="hljs-keyword">with</span> two different shape tensor: [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><p><strong>广播机制的计算规则</strong></p><p>现在我们知道在什么情况下两个张量是可以广播的。两个张量进行广播后的结果张量的形状计算规则如下：</p><p>1）如果两个张量shape的长度不一致，那么需要在较小长度的shape前添加1，直到两个张量的形状长度相等。</p><p>2） 保证两个张量形状相等之后，每个维度上的结果维度就是当前维度上较大的那个。</p><p>以张量x和y进行广播为例，x的shape为[2, 3, 1，5]，张量y的shape为[3，4，1]。首先张量y的形状长度较小，因此要将该张量形状补齐为[1, 3, 4, 1]，再对两个张量的每一维进行比较。从第一维看，x在一维上的大小为2，y为1，因此，结果张量在第一维的大小为2。以此类推，对每一维进行比较，得到结果张量的形状为[2, 3, 4, 5]。</p><p>由于矩阵乘法函数paddle.matmul在深度学习中使用非常多，这里需要特别说明一下它的广播规则：</p><p>1）如果两个张量均为一维，则获得点积结果。</p><p>2） 如果两个张量都是二维的，则获得矩阵与矩阵的乘积。</p><p>3） 如果张量x是一维，y是二维，则将x的shape转换为[1, D]，与y进行矩阵相乘后再删除前置尺寸。</p><p>4） 如果张量x是二维，y是一维，则获得矩阵与向量的乘积。</p><p>5） 如果两个张量都是N维张量（N &gt; 2），则根据广播规则广播非矩阵维度（除最后两个维度外其余维度）。比如：如果输入x是形状为[j,1,n,m]的张量，另一个y是[k,m,p]的张量，则输出张量的形状为[j,k,n,p]。</p><p><strong>矩阵乘法示例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x = paddle.ones([<span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">2</span>])<br>y = paddle.ones([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>])<br>z = paddle.matmul(x, y)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;After matmul: &#x27;</span>, z.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">After matmul:  [<span class="hljs-number">10</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]<br></code></pre></td></tr></table></figure><p><strong>笔记</strong><br>飞桨的API有原位（inplace）操作和非原位操作之分。原位操作即在原张量上保存操作结果，非原位操作则不会修改原张量，而是返回一个新的张量来表示运算结果。在飞桨框架V2.1及之后版本，部分API有对应的原位操作版本，在API后加上’_‘表示，如：<code>x.add(y)</code>是非原位操作，<code>x.add_(y)</code>为原位操作。</p><h4 id="D（√）1-3-算子"><a href="#D（√）1-3-算子" class="headerlink" title="D（√）1.3 算子"></a>D（√）1.3 算子</h4><p>一个复杂的机器学习模型（比如神经网络）可以看作一个复合函数，输入是数据特征，输出是标签的值或概率。简单起见，假设一个由$L$个函数复合的神经网络定义为：<br>$$<br>y&#x3D;f_L(\cdots f_2(f_1(x))),<br>$$</p><p>其中$f_l(\cdot)$可以为带参数的函数，也可以为不带参数的函数，$x$为输入特征，$y$为某种损失。<br>我们将从$x$到$y$的计算看作一个前向计算过程。而神经网络的参数学习需要计算损失关于所有参数的偏导数（即梯度）。假设函数$f_l(\cdot)$包含参数$\theta_l$，根据链式法则，</p><p>$$<br>\begin{aligned}<br>\frac{\partial y}{\partial \theta_l} &amp;&#x3D; {\frac{\partial f_l}{\partial \theta_l}} \frac{\partial y}{\partial f_l} \<br>&amp;&#x3D; \frac{\partial f_l}{\partial \theta_l} \frac{\partial f_{l+1}}{\partial f_l} \cdots \frac{\partial f_L}{\partial f_{L-1}} .<br>\end{aligned}<br>$$</p><p>在实践中，一种比较高效的计算$y$关于每个函数$f_l$的偏导数的方式是利用递归进行反向计算。令$\delta_l\triangleq \frac{\partial y}{\partial f_l}$，则有<br>$$<br>\delta_{l-1} &#x3D; \frac{\partial f_l}{\partial f_{l-1}} \delta_{l}.<br>$$</p><p>如果将函数$f_l(\cdot)$称为前向函数，则$\delta_{l-1}$的计算称为函数$f(x)$的反向函数。</p><p>如果我们实现每个基础函数的前向函数和反向函数，就可以非常方便地通过这些基础函数组合出复杂函数，并通过链式法则反向计算复杂函数的偏导数。<br>在深度学习框架中，这些基本函数的实现称为算子(Operator，Op)。有了算子，就可以像搭积木一样构建复杂的模型。</p><h5 id="E（√）1-3-1-算子定义"><a href="#E（√）1-3-1-算子定义" class="headerlink" title="E（√）1.3.1 算子定义"></a>E（√）1.3.1 算子定义</h5><p>算子是构建复杂机器学习模型的基础组件，包含一个函数f(x)f(x)<em>f</em>(<em>x</em>)的前向函数和反向函数。为了可以更便捷地进行算子组合，本书中定义算子Op}的接口如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Op</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">pass</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-keyword">return</span> self.forward(inputs)<br><br>    <span class="hljs-comment"># 前向函数</span><br>    <span class="hljs-comment"># 输入：张量inputs</span><br>    <span class="hljs-comment"># 输出：张量outputs</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># return outputs</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br><br>    <span class="hljs-comment"># 反向函数</span><br>    <span class="hljs-comment"># 输入：最终输出对outputs的梯度outputs_grads</span><br>    <span class="hljs-comment"># 输出：最终输出对inputs的梯度inputs_grads</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, outputs_grads</span>):<br>        <span class="hljs-comment"># return inputs_grads</span><br>        <span class="hljs-keyword">raise</span> NotImplementedError<br></code></pre></td></tr></table></figure><p>在上面的接口中，<code>forward</code>是自定义Op的前向函数，必须被子类重写，它的参数为输入对象，参数的类型和数量任意；<code>backward</code>是自定义Op的反向函数，必须被子类重写，它的参数为<code>forward</code>输出张量的梯度<code>outputs_grads</code>，它的输出为<code>forward</code>输入张量的梯度<code>inputs_grads</code>。</p><h6 id="F（√）1-3-1-1-加法算子"><a href="#F（√）1-3-1-1-加法算子" class="headerlink" title="F（√）1.3.1.1 加法算子"></a>F（√）1.3.1.1 加法算子</h6><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">add</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(add, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-keyword">return</span> self.forward(x, y)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y</span>):<br>        self.x = x<br>        self.y = y<br>        outputs = x + y<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads_x = grads * <span class="hljs-number">1</span><br>        grads_y = grads * <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> grads_x, grads_y<br></code></pre></td></tr></table></figure><p>定义x&#x3D;1、y&#x3D;4，根据反向计算，得到x、y的梯度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">x = <span class="hljs-number">1</span><br>y = <span class="hljs-number">4</span><br>add_op = add()<br>z = add_op(x, y)<br>grads_x, grads_y = add_op.backward(grads=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;x&#x27;s grad is: &quot;</span>, grads_x)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;y&#x27;s grad is: &quot;</span>, grads_y)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">x<span class="hljs-string">&#x27;s grad is:  1</span><br><span class="hljs-string">y&#x27;</span>s grad <span class="hljs-keyword">is</span>:  <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-3-1-2-乘法算子"><a href="#F（√）1-3-1-2-乘法算子" class="headerlink" title="F（√）1.3.1.2 乘法算子"></a>F（√）1.3.1.2 乘法算子</h6><p>乘法算子的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">multiply</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(multiply, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, x, y</span>):<br>        <span class="hljs-keyword">return</span> self.forward(x, y)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, y</span>):<br>        self.x = x<br>        self.y = y<br>        outputs = x * y<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads_x = grads * self.y<br>        grads_y = grads * self.x<br>        <span class="hljs-keyword">return</span> grads_x, grads_y<br></code></pre></td></tr></table></figure><h6 id="F（√）1-3-1-3-指数算子"><a href="#F（√）1-3-1-3-指数算子" class="headerlink" title="F（√）1.3.1.3 指数算子"></a>F（√）1.3.1.3 指数算子</h6><p>指数算子的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">exponential</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(exponential, self).__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        self.x = x<br>        outputs = math.exp(x)<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        grads = grads * math.exp(self.x)<br>        <span class="hljs-keyword">return</span> grads<br></code></pre></td></tr></table></figure><p>分别指定a、b、c、d的值，通过实例化算子，调用加法、乘法和指数运算算子，计算得到y。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">a, b, c, d = <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span><br><span class="hljs-comment"># 实例化算子</span><br>multiply_op = multiply()<br>add_op = add()<br>exp_op = exponential()<br>y = exp_op(add_op(multiply_op(a, b), multiply_op(c, d)))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;y: &#x27;</span>, y)<br></code></pre></td></tr></table></figure><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">22026</span>.<span class="hljs-number">465794806718</span><br></code></pre></td></tr></table></figure><h5 id="E（√）1-3-2-自动微分机制"><a href="#E（√）1-3-2-自动微分机制" class="headerlink" title="E（√）1.3.2 自动微分机制"></a>E（√）1.3.2 自动微分机制</h5><p>目前大部分深度学习平台都支持自动微分（Automatic Differentiation），即根据<code>forward()</code>函数来自动构建<code>backward()</code>函数。</p><p><strong>笔记</strong></p><p>自动微分的原理是将所有的数值计算都分解为基本的原子操作，并构建\mykey{计算图}{Computational Graph}。计算图上每个节点都是一个原子操作，保留前向和反向的计算结果，很方便通过链式法则来计算梯度。自动微分的详细介绍可以参考《神经网络与深度学习》第4.5节。</p><p>飞桨的自动微分是通过<code>trace</code>的方式，记录各种算子和张量的前向计算，并自动创建相应的反向函数和反向变量，来实现反向梯度的计算。</p><p>在飞桨中，可以通<code>过paddle.grad()</code>API或张量类成员函数x.grad来查看张量的梯度。</p><p>下面用一个比较简单的例子来了解整个过程。定义两个张量a和b，并用<code>stop_gradient</code>属性用来设置是否传递梯度。将a的<code>stop_gradient</code>属性设为False，会自动为a创建一个反向张量，将b的<code>stop_gradient</code>属性设为True，即不会为b创建反向张量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义张量a，stop_gradient=False代表进行梯度传导</span><br>a = paddle.to_tensor(<span class="hljs-number">2.0</span>, stop_gradient=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 定义张量b，stop_gradient=True代表不进行梯度传导</span><br>b = paddle.to_tensor(<span class="hljs-number">5.0</span>, stop_gradient=<span class="hljs-literal">True</span>)<br>c = a * b<br><span class="hljs-comment"># 自动计算反向梯度</span><br>c.backward()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor a&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(a.grad))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor b&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(b.grad))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Tensor c&#x27;s grad is: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(c.grad))<br></code></pre></td></tr></table></figure><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs jboss-cli">Tensor a&#x27;s grad is: Tensor<span class="hljs-params">(<span class="hljs-attr">shape</span>=[1], <span class="hljs-attr">dtype</span>=float32, <span class="hljs-attr">place</span>=CPUPlace, <span class="hljs-attr">stop_gradient</span>=False,</span><br><span class="hljs-params">       [5.])</span> <span class="hljs-comment"># a的梯度是b</span><br>Tensor b&#x27;s grad is: None <span class="hljs-comment"># b关闭了梯度，因此没有梯度</span><br>Tensor c&#x27;s grad is: Tensor<span class="hljs-params">(<span class="hljs-attr">shape</span>=[1], <span class="hljs-attr">dtype</span>=float32, <span class="hljs-attr">place</span>=CPUPlace, <span class="hljs-attr">stop_gradient</span>=False,</span><br><span class="hljs-params">       [1.])</span> <span class="hljs-comment"># c的梯度是1</span><br></code></pre></td></tr></table></figure><h6 id="F（√）1-3-2-1-前向执行"><a href="#F（√）1-3-2-1-前向执行" class="headerlink" title="F（√）1.3.2.1 前向执行"></a>F（√）1.3.2.1 前向执行</h6><p>在上面代码中，第7行<code>c.backward()</code>被执行前，会为每个张量和算子创建相应的反向张量和反向函数。</p><p>当创建张量或执行算子的前向计算时，会自动创建反向张量或反向算子。这里以上面代码中乘法为例来进行说明。</p><ol><li>当创建张量a时，由于其属性<code>stop_gradient=False</code>，因此会自动为a创建一个反向张量，也就是图1.8中的a_grad。由于a不依赖其它张量或算子，a_grad的<code>grad_op</code>为None。</li><li>当创建张量b时，由于其属性<code>stop_gradient=True</code>，因此不会为b创建一个反向张量。</li><li>执行乘法c&#x3D;a×bc&#x3D;a\times b<em>c</em>&#x3D;<em>a</em>×<em>b</em> 时，×\times×是一个前向算子Mul，为其构建反向算子MulBackward。由于Mul的输入是a和b，输出是c，对应反向算子<code>MulBackward</code>的输入是张量c的反向张量<code>c_grad</code>，输出是a和b的反向张量。如果输入定义<code>stop_gradient=True</code>，反向张量即为None。在此例子中就是a_grad和None。</li><li>反向算子MulBackward中的<code>grad_pending_ops</code>用于在自动构建反向网络时，明确该反向算子的下一个可执行的反向算子。可以理解为在反向计算中，该算子衔接的下一个反向算子。</li><li>当c通过乘法算子Mul被创建后，c会创建一个反向张量c_grad，它的<code>grad_op</code>为该乘法算子的反向算子，即MulBackward。</li></ol><p>由于此时还没有进行反向计算，因此这些反向张量和反向算子中的具体数值为空(data &#x3D; None)。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209184730941.png" alt="image-20221211203408158"></p><h6 id="F（√）1-3-2-2-反向执行"><a href="#F（√）1-3-2-2-反向执行" class="headerlink" title="F（√）1.3.2.2 反向执行"></a>F（√）1.3.2.2 反向执行</h6><p>调用<code>backward()</code>后，执行计算图上的反向过程，即通过链式法则自动计算每个张量或算子的微分，计算过程如图1.9所示。经过自动反向梯度计算，获得c_grad和a_grad的值。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209211255275.png" alt="image-20221211203606088"></p><h5 id="E（√）1-3-3-预定义的算子"><a href="#E（√）1-3-3-预定义的算子" class="headerlink" title="E（√）1.3.3 预定义的算子"></a>E（√）1.3.3 预定义的算子</h5><p>从零开始构建各种复杂的算子和模型是一个很复杂的过程，在开发的过程中也难以避免地会出现很多冗余代码，因此飞桨提供了基础算子和中间算子，可以便捷地实现复杂模型。</p><p>在深度学习中，大多数模型都是以各种神经网络为主，由一系列层(Layer)组成，层是模型的基础逻辑执行单元。飞桨提供了<code>paddle.nn.Layer</code>类来方便快速地实现自己的层和模型。模型和层都可以基于<code>paddle.nn.Layer</code>扩充实现，模型只是一种特殊的层。</p><p>当我们实现的算子继承<code>paddle.nn.Layer</code>类时，就不用再定义<code>backward</code>函数。飞桨的自动微分机制可以自动完成反向传播过程，让我们只关注模型构建的前向过程，不必再进行烦琐的梯度求导。</p><h5 id="E（√）1-3-4-本书中实现的算子"><a href="#E（√）1-3-4-本书中实现的算子" class="headerlink" title="E（√）1.3.4 本书中实现的算子"></a>E（√）1.3.4 本书中实现的算子</h5><p>更深入的理解深度学习的模型和算法，在本书中，我们也手动实现自己的算子库：nndl，并基于自己的算子库来构建机器学习模型。本书中的自定义算子分为两类：一类是继承在第1.3.1节中定义Op类，这些算子是为了进行更好的展示模型的实现细节，需要自己动手计算并实现其反向函数；另一类是继承飞桨的paddle.nn.Layer类，更方便地搭建复杂算子，并和飞桨预定义算子混合使用。</p><p>本书中实现的算子见表1.1所示，其中Model_开头为完整的模型。</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209212008942.png" alt="img"></p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209210516027.png" alt="img" style="zoom: 50%;" /><p>E（√）1.3.6 本书中实现的优化器</p><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221209213544884.png" alt="img"></p><h4 id="D（√）1-4-本书中实现的DataSet类"><a href="#D（√）1-4-本书中实现的DataSet类" class="headerlink" title="D（√）1.4 本书中实现的DataSet类"></a>D（√）1.4 本书中实现的DataSet类</h4><p>为了更好地实践，本书在模型解读部分主要使用简单任务和数据集，在案例实践部分主要使用公开的实际案例数据集。下面介绍我们用到的数据集以及对应构建的DataSet类。</p><h5 id="E（√）1-4-1-数据集"><a href="#E（√）1-4-1-数据集" class="headerlink" title="E（√）1.4.1 数据集"></a>E（√）1.4.1 数据集</h5><p>本书中使用的数据集如下：</p><ol><li>线性回归数据集ToyLinear150：在第2.2.1.2节中构建，用于简单的线性回归任务。ToyLinear150数据集包含150条带噪音的样本数据，其中训练集100条、测试集50条，由在第2.2.1.2节中create_toy_data函数构建。</li><li>非线性回归数据集ToySin25：在第2.3.1节中构建，用于简单的多项式回归任务.ToySin25数据集包含25条样本数据，其中训练集15条、测试集10条。ToySin25数据集同样使用在第2.2.1.1节中create_toy_data函数进行构建。</li><li>波士顿房价预测数据集：波士顿房价预测数据集共506条样本数据，每条样本包含了12种可能影响房价的因素和该类房屋价格的中位数。该数据集在第2.5节中使用。</li><li>二分类数据集Moon1000：在第3.1中构建，二分类数据集Moon1000数据是从两个带噪音 的弯月形状数据分布中采样得到，每个样本包含2个特征，其中训练集640条、验证集160条、测试集200条。该数据集在本书第3.1节和第4.2节中使用。数据集构建函数make_moons在第7.4.2.3节和第7.6节中使用。</li><li>三分类数据集Multi1000：在第3.2.1节中构建三分类数据集集Multi1000，其中训练集640条、验证集160条、测试集200条。该数据集来自三个不同的簇，每个簇对应一个类别。</li><li>鸢尾花数据集：鸢尾花数据集包含了3种鸢尾花类别(Setosa、Versicolour、Virginica)，每种类别有50个样本，共计150个样本。每个样本中包含了4个属性：花萼长度、花萼宽度、花瓣长度以及花瓣宽度。该数据集在第3.3节和第4.5节使用。</li><li>MNIST数据集：MNIST手写数字识别数据集是计算机视觉领域的经典入门数据集，包含了训练集60 000条、测试集10 000条。MNIST数据集在第5.3.1节和第7.2节中使用。</li><li>CIFAR-10 数据集：CIFAR-10数据集是计算机视觉领域的经典数据集，包含了10种不同的类别、共 60 000 张图像，其中每个类别的图像都是6 000 张，图像大小均为32×32像素。CIFAR-10数据集在第5.5节中使用.</li><li>IMDB电影评论数据集：IMDB电影评论数据集是一份关于电影评论的经典二分类数据集。IMDB按照评分的高低筛选出了积极评论和消极评论，如果评分≥7，则认为是积极评论；如果评分≤4，则认为是消极评论。数据集包含训练集和测试集数据，数量各为25 000 条，每条数据都是一段用户关于某个电影的真实评价，以及观众对这个电影的情感倾向。IMDB数据集在第6.4节、第8.1节和第8.2节中使用.</li><li>数字求和数据集DigitSum：在第6.1.1节中构建，包含用于数字求和任务的不同长度的数据集。数字求和任务的输入是一串数字，前两个位置的数字为0-9，其余数字随机生成(主要为0)，预测目标是输入序列中前两个数字的加和，用来测试模型的对序列数据的记忆能力。</li><li>LCQMC通用领域问题匹配数据集：LCQMC数据集是百度知道领域的中文问题匹配数据集，目的是为了解决在中文领域大规模问题匹配数据集的缺失。该数据集从百度知道不同领域的用户问题中抽取构建数据。LCQMC数据集共包含训练集238 766条、验证集8 802条和测试集12 500 条。LCQMC数据集在第8.3节中使用。</li></ol><h5 id="E（√）1-4-2-Dataset类"><a href="#E（√）1-4-2-Dataset类" class="headerlink" title="E（√）1.4.2 Dataset类"></a>E（√）1.4.2 Dataset类</h5><p>为了更好地支持使用随机梯度下降进行参数学习，我们构建了DataSet类，以便可以更好地进行数据迭代。</p><p>本书中构建的DataSet类见表1.3。关于Dataset类的具体介绍见第4.5.1.1节。</p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/5bc27443b5ab4df99c7578f75ea973e84cab41d1caf949bea84af0b814325621.png" alt="img" style="zoom:33%;" /><h4 id="D（√）1-5-本书中使用的Runner类"><a href="#D（√）1-5-本书中使用的Runner类" class="headerlink" title="D（√）1.5 本书中使用的Runner类"></a>D（√）1.5 本书中使用的<code>Runner</code>类</h4><p>在一个任务上应用机器学习方法的流程基本上包括：数据集构建、模型构建、损失函数定义、优化器定义、评价指标定义、模型训练、模型评价和模型预测等环节。为了将上述环节规范化，我们将机器学习模型的基本要素封装成一个Runner类，使得我们可以更方便进行机器学习实践。除上述提到的要素外，Runner类还包括模型保存、模型加载等功能。Runner类的具体介绍可参见第2节. 这里我们对本书中用到的三个版本的Runner类进行汇总，说明每一个版本Runner类的构成方式.</p><ol><li>RunnerV1：在第2节中实现，用于线性回归模型的训练，其中训练过程通过直接求解解析解的方式得到模型参数，没有模型优化及计算损失函数过程，模型训练结束后保存模型参数。</li><li>RunnerV2：在第3.1.6节中实现。RunnerV2主要增加的功能为：</li></ol><ul><li>在训练过程引入梯度下降法进行模型优化；</li><li>模型训练过程中计算在训练集和验证集上的损失及评价指标并打印，训练过程中保存最优模型。我们在第4.3.2节和第4.3.2节分别对RunnerV2进行了完善，加入自定义日志输出、模型阶段控制等功能。</li></ul><ol start="3"><li>RunnerV3：在第4.5.4节中实现。RunnerV3主要增加三个功能：使用随机梯度下降法进行参数优化；训练过程使用DataLoader加载批量数据；模型加载与保存中，模型参数使用state_dict方法获取，使用state_dict加载。</li></ol><h4 id="D（√）1-6-小结"><a href="#D（√）1-6-小结" class="headerlink" title="D（√）1.6 小结"></a>D（√）1.6 小结</h4>]]></content>
    
    
    <categories>
      
      <category>深度学习技术栈</category>
      
      <category>深度学习</category>
      
      <category>分支导航</category>
      
      <category>实践学习</category>
      
      <category>神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2022年11月11日牛客测试题题目整理</title>
    <link href="/posts/1896225260/"/>
    <url>/posts/1896225260/</url>
    
    <content type="html"><![CDATA[<h2 id="1-依赖注入的方式有哪些？（√）"><a href="#1-依赖注入的方式有哪些？（√）" class="headerlink" title="1.依赖注入的方式有哪些？（√）"></a>1.依赖注入的方式有哪些？（√）</h2><blockquote><p>  <a href="https://pdai.tech/md/spring/spring-x-framework-ioc.html#%E4%BE%9D%E8%B5%96%E6%B3%A8%E5%85%A5%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F">参考：依赖注入的三种方式 - Java 全栈知识体系 - pdai(√)</a></p></blockquote><p>什么是依赖注入：依赖指的是bean需要的参数值，比如bean中的简单类型属性、引用类型属性。给bean中的属性传递值的过程叫做依赖注入。</p><p>常见的依赖注入的方式有三种，分别是set方法注入、构造方法注入、基于注解的注入</p><h3 id="（1）参数注入方式一：set方法注入"><a href="#（1）参数注入方式一：set方法注入" class="headerlink" title="（1）参数注入方式一：set方法注入"></a>（1）参数注入方式一：set方法注入</h3><p>[xml配置方式 + set方法]进行依赖注入：</p><p>在xml配置文件中，通过set进行注入：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans</span></span><br><span class="hljs-string"><span class="hljs-tag"> http://www.springframework.org/schema/beans/spring-beans.xsd&quot;</span>&gt;</span><br>    <br>    <span class="hljs-comment">&lt;!-- services --&gt;</span><br>    <span class="hljs-comment">&lt;!-- 要被注入的对象 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;userService&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;tech.pdai.springframework.service.UserServiceImpl&quot;</span>&gt;</span><br>        <span class="hljs-comment">&lt;!-- 注入的引用类型对象/属性 --&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">property</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;userDao&quot;</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;userDao&quot;</span>/&gt;</span><br>        <span class="hljs-comment">&lt;!-- additional collaborators and configuration for this bean go here --&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- more bean definitions for services go here --&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span><br><br></code></pre></td></tr></table></figure><blockquote><p>  使用property + value进行setter注入简单类型属性<br>  使用property + ref进行setter注入引用类型属性</p></blockquote><p>对应的，UserServiceImpl 类中需要为 userDao 属性添加set方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment"> * <span class="hljs-doctag">@author</span> pdai</span><br><span class="hljs-comment"> */</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserServiceImpl</span> &#123;<br>    <span class="hljs-comment">// 属性声明</span><br>    <span class="hljs-keyword">private</span> UserDaoImpl userDao;<br>    <span class="hljs-comment">// 无参构造</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">UserServiceImpl</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br>    <span class="hljs-comment">// 属性的set方法</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUserDao</span><span class="hljs-params">(UserDaoImpl userDao)</span> &#123;<br>        <span class="hljs-built_in">this</span>.userDao = userDao;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>注解配置方式 + set方式 进行依赖注入</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserServiceImpl</span> &#123;<br>    <span class="hljs-comment">// 属性声明</span><br>    <span class="hljs-keyword">private</span> UserDaoImpl userDao;<br>    <span class="hljs-comment">// [注解配置方式 + set方法进行]依赖注入</span><br>    <span class="hljs-meta">@Autowired</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setUserDao</span><span class="hljs-params">(UserDaoImpl userDao)</span> &#123;<br>        <span class="hljs-built_in">this</span>.userDao = userDao;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="（2）参数注入方式二：构造方法注入"><a href="#（2）参数注入方式二：构造方法注入" class="headerlink" title="（2）参数注入方式二：构造方法注入"></a>（2）参数注入方式二：构造方法注入</h3><p>[xml配置方式 + 构造方法]进行依赖注入：</p><p>在xml配置文件中，通过<code>&lt;constructor-arg&gt;</code>进行构造方法方式的属性注入</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans</span></span><br><span class="hljs-string"><span class="hljs-tag"> http://www.springframework.org/schema/beans/spring-beans.xsd&quot;</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- services --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;userService&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;tech.pdai.springframework.service.UserServiceImpl&quot;</span>&gt;</span><br>        <span class="hljs-comment">&lt;!-- 通过构造方法进行属性注入 --&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">constructor-arg</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;userDao&quot;</span> <span class="hljs-attr">ref</span>=<span class="hljs-string">&quot;userDao&quot;</span>/&gt;</span><br>        <span class="hljs-comment">&lt;!-- additional collaborators and configuration for this bean go here --&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">bean</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- more bean definitions for services go here --&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span><br><br></code></pre></td></tr></table></figure><p>在被注入的类的定义中，添有参的构造方法用于为属性输入值：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserServiceImpl</span> &#123;<br>    <span class="hljs-comment">// 声明属性</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> UserDaoImpl userDao;<br>    <span class="hljs-comment">// 有参构造方法进行属性注入</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">UserServiceImpl</span><span class="hljs-params">(UserDaoImpl userDaoImpl)</span> &#123;<br>        <span class="hljs-built_in">this</span>.userDao = userDaoImpl;<br>    &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><p>[注解配置方式 + 构造方法]进行依赖注入：</p><p>注解的配置方式，省去了进行xml文件配置的繁琐</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"> <span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserServiceImpl</span> &#123;<br>    <span class="hljs-comment">// 属性声明</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> UserDaoImpl userDao;<br>    <span class="hljs-comment">// 注解 + 构造方法 = 构造方法进行依赖注入</span><br>    <span class="hljs-meta">@Autowired</span> <span class="hljs-comment">// 这里@Autowired也可以省略</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">UserServiceImpl</span><span class="hljs-params">(<span class="hljs-keyword">final</span> UserDaoImpl userDaoImpl)</span> &#123;<br>        <span class="hljs-built_in">this</span>.userDao = userDaoImpl;<br>    &#125;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="（3）参数注入方式三：注解的方式进行注入"><a href="#（3）参数注入方式三：注解的方式进行注入" class="headerlink" title="（3）参数注入方式三：注解的方式进行注入"></a>（3）参数注入方式三：注解的方式进行注入</h3><p>set的方式进行依赖注入，是在 xml配置方式中 或者 注解配置方式 中通过set方法为类的属性进行传值</p><p>构造方法的方式进行依赖注入，是在 xml配置方式中 或者 注解配置方式 中通过构造方法为类的属性进行传值</p><p>而注解的方式进行依赖注入，则是直接在类中的对应属性上面打上注解，方便。</p><p>以@Autowired（自动注入）注解注入为例，修饰符有三个属性：Constructor，byType，byName。默认按照byType注入。</p><ul><li><strong>constructor</strong>：通过构造方法进行自动注入，spring会匹配与构造方法参数类型一致的bean进行注入，如果有一个多参数的构造方法，一个只有一个参数的构造方法，在容器中查找到多个匹配多参数构造方法的bean，那么spring会优先将bean注入到多参数的构造方法中。</li><li><strong>byName</strong>：被注入bean的id名必须与set方法后半截匹配，并且id名称的第一个单词首字母必须小写，这一点与手动set注入有点不同。</li><li><strong>byType</strong>：查找所有的set方法，将符合符合参数类型的bean注入。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserServiceImpl</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    <span class="hljs-keyword">private</span> UserDaoImpl userDao;<br><br>&#125;<br><br></code></pre></td></tr></table></figure><h2 id="2-spring容器的bean是线程安全的吗（√）"><a href="#2-spring容器的bean是线程安全的吗（√）" class="headerlink" title="2.spring容器的bean是线程安全的吗（√）"></a>2.spring容器的bean是线程安全的吗（√）</h2><blockquote><p><a href="https://cloud.tencent.com/developer/article/1743283">面试：Spring 中的bean 是线程安全的吗？ - 腾讯云开发者社区 - JAVA日知录（√）</a></p><p><a href="https://tobebetterjavaer.com/sidebar/sanfene/spring.html#_14-spring-%E4%B8%AD%E7%9A%84%E5%8D%95%E4%BE%8B-bean-%E4%BC%9A%E5%AD%98%E5%9C%A8%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%E5%90%97">14.Spring 中的单例 Bean 会存在线程安全问题吗？ - 面渣逆袭 - 三分恶（√）</a></p></blockquote><p>首先结论在这：Spring 中的单例 Bean不是线程安全的。</p><p>因为单例 Bean，是全局只有一个 Bean，所有线程共享。如果说单例 Bean，是一个无状态的，也就是线程中的操作不会对 Bean 中的成员变量执行查询以外的操作，那么这个单例 Bean 是线程安全的。比如 Spring mvc 的 Controller、Service、Dao 等，这些 Bean 大多是无状态的，只关注于方法本身。</p><p>假如这个 Bean 是有状态的，也就是会对 Bean 中的成员变量进行写操作，那么可能就存在线程安全的问题。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211131055092.png" alt="image-20221113105533020" style="zoom:50%;" /><hr><p>结论：Spring容器中的bean不是线程安全的</p><hr><p>spring中没有对bean提供线程安全的策略，但是bean具体是否安全，要根据bean的具体的scope来研究</p><hr><p>spring的bean的作用域有5种：</p><p>1、singleton:单例，默认作用域</p><p>2、prototype:原型，每次创建一个新对象</p><p>3、request:请求，每次Http请求创建一个新对象，适用于WebApplicationContext环境下</p><p>4、session:会话，同一个会话共享一个实例，不同会话使用不用的实例</p><p>5、global-session:全局会话，所有会话共享一个实例</p><hr><p>对于原型Bean,每次创建一个新对象，也就是线程之间并不存在Bean共享，自然是不会有线程安全的问题。</p><p>对于单例Bean,所有线程都共享一个单例实例Bean,因此是存在资源的竞争。</p><ul><li>如果单例Bean,是一个无状态Bean，也就是线程中的操作不会对Bean的成员执行「查询」以外的操作，那么这个单例Bean是线程安全的。比如Spring mvc 的 Controller、Service、Dao等，这些Bean大多是无状态的，只关注于方法本身。</li><li>线程不安全主要是因为有写操作，因此如果bean是无状态的，只涉及到读操作，那么就不会有线程安全问题</li></ul><hr><p>spring单例，为什么controller、service和dao确能保证线程安全？</p><p>因为这几个bean是无状态的，不会保存数据，因此不存在线程安全问题。</p><p>如果自己需要的bean是有状态的，那么就需要开发人员自己动手进行线程安全的保证。其中一个最简单的办法就是将单例bean改为原型bean，这样每次请求bean都会创建一个新的bean，因此就可以保证线程安全。</p><p>controller、service和dao层本身并不是线程安全的，只是如果只是调用里面的方法，而且多线程调用一个实例的方法，会在内存中复制变量，这是自己的线程的工作内存，是安全的。</p><p>所以其实任何无状态单例都是线程安全的。Spring的根本就是通过大量这种单例构建起系统，以事务脚本的方式提供服务。</p><hr><p>首先问@Controller @Service是不是线程安全的？</p><p>答：默认配置下不是的。为啥呢？因为默认情况下@Controller没有加上@Scope，没有加@Scope就是默认值singleton，单例的。意思就是系统只会初始化一次Controller容器，所以每次请求的都是同一个Controller容器，当然是非线程安全的。</p><hr><p>分析各种情况是否是线程安全的？</p><p>（1）单例模式 + 简单类型属性：不是线程安全的</p><p>（2）单例模式 + ThreadLocal：线程安全</p><p>（3）原型模式 + 简单类型属性：线程安全</p><p>（4）原型模式 + 引用类型属性：不是线程安全的</p><p>（5）原型模式 + 静态变量：不是线程安全的</p><hr><p>小结：</p><p>1.在 @Controller&#x2F;@Service 等容器中，默认情况下，scope值是单例-singleton的，也是线程不安全的。</p><p>2.尽量不要在@Controller&#x2F;@Service 等容器中定义静态变量，不论是单例(singleton)还是多实例(prototype)他都是线程不安全的。</p><p>3.默认注入的Bean对象，在不设置scope的时候他也是线程不安全的。</p><p>4.一定要定义变量的话，用ThreadLocal来封装，这个是线程安全的。</p><hr><blockquote><h3 id="子问题：单例-Bean-线程安全问题怎么解决呢？"><a href="#子问题：单例-Bean-线程安全问题怎么解决呢？" class="headerlink" title="子问题：单例 Bean 线程安全问题怎么解决呢？"></a>子问题：单例 Bean 线程安全问题怎么解决呢？</h3><p>  常见的有这么些解决办法：</p><p>  （1）将 Bean 定义为多例</p><p>  这样每一个线程请求过来都会创建一个新的 Bean，但是这样容器就不好管理 Bean，不能这么办。</p><p>  （2）在 Bean 对象中尽量避免定义可变的成员变量</p><p>  削足适履了属于是，也不能这么干。</p><p>  （3）将 Bean 中的成员变量保存在 ThreadLocal 中 ⭐</p><p>  我们知道 ThredLoca 能保证多线程下变量的隔离，可以在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 里，这是推荐的一种方式</p></blockquote><h2 id="3-用了哪些注解以及自动装配的原理（√）"><a href="#3-用了哪些注解以及自动装配的原理（√）" class="headerlink" title="3.用了哪些注解以及自动装配的原理（√）"></a>3.用了哪些注解以及自动装配的原理（√）</h2><h2 id="3-1-自动装配的原理（√）"><a href="#3-1-自动装配的原理（√）" class="headerlink" title="3.1 - 自动装配的原理（√）"></a>3.1 - 自动装配的原理（√）</h2><blockquote><p>  【参考】</p><p>  黑马程序员 - P15 Spring-13-自动装配（√）</p></blockquote><h3 id="3-1-1-自动装配的步骤"><a href="#3-1-1-自动装配的步骤" class="headerlink" title="3.1.1 自动装配的步骤"></a>3.1.1 自动装配的步骤</h3><p>（1）被装配的类为被装配的属性提供set方法</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">BookServiceImpl</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">BookService</span>&#123;<br>    <span class="hljs-keyword">private</span> BookDao bookDao;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setBookDao</span><span class="hljs-params">(BookDao bookDao)</span> &#123;<br>        <span class="hljs-built_in">this</span>.bookDao = bookDao;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">save</span><span class="hljs-params">()</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;book service save ...&quot;</span>);<br>        bookDao.save();<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>（2）实现自动装配的配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span><br><span class="hljs-tag">       <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd&quot;</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;com.itheima.dao.impl.BookDaoImpl&quot;</span>/&gt;</span><br>    <span class="hljs-comment">&lt;!--autowire属性：开启自动装配，通常使用按类型装配--&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;bookService&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;com.itheima.service.impl.BookServiceImpl&quot;</span> <span class="hljs-attr">autowire</span>=<span class="hljs-string">&quot;byType&quot;</span>/&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="3-1-2-细节-注意事项"><a href="#3-1-2-细节-注意事项" class="headerlink" title="3.1.2 细节 注意事项"></a>3.1.2 细节 注意事项</h3><p>自动装配内部其实是调用的被注入属性值的类的set方法，因此set方法不能省略，如果省略则无法完成自动装配。</p><p>被注入的对象必须要被Spring的IOC容器管理，即需要将被注入的类作为bean注册到容器中</p><p>按照类型在Spring的IOC容器中如果找到多个对象，会报<code>NoUniqueBeanDefinitionException</code></p><p>按照类型装配，连被注入的bean的名字都可以不起，即省去bean定义的id属性：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!--省去bean的名字--&gt;</span><br><span class="hljs-comment">&lt;!--&lt;bean id=&quot;bookDao&quot; class=&quot;com.itheima.dao.impl.BookDaoImpl&quot;/&gt;--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">bean</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;com.itheima.dao.impl.BookDaoImpl&quot;</span>/&gt;</span><br></code></pre></td></tr></table></figure><h3 id="3-1-3-自动装配-注意事项"><a href="#3-1-3-自动装配-注意事项" class="headerlink" title="3.1.3 自动装配 注意事项"></a>3.1.3 自动装配 注意事项</h3><ol><li>自动装配用于引用类型依赖注入，不能对简单类型进行操作</li><li>使用按类型装配时（byType）必须保障容器中相同类型的bean唯一，推荐使用</li><li>使用按名称装配时（byName）必须保障容器中具有指定名称的bean，因变量名与配置耦合，不推荐使用</li><li>自动装配优先级低于setter注入与构造器注入，同时出现时自动装配配置失效</li></ol><h2 id="3-2-Spring常用的注解有哪些-（√）"><a href="#3-2-Spring常用的注解有哪些-（√）" class="headerlink" title="3.2 - Spring常用的注解有哪些?（√）"></a>3.2 - Spring常用的注解有哪些?（√）</h2><blockquote><p>  【参考】</p><p>  <a href="https://tobebetterjavaer.com/sidebar/sanfene/spring.html#_3-spring-%E6%9C%89%E5%93%AA%E4%BA%9B%E5%B8%B8%E7%94%A8%E6%B3%A8%E8%A7%A3%E5%91%A2">3.Spring 有哪些常用注解呢？ - 面渣逆袭 - 三分恶（√）</a></p></blockquote><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211131520454.png" alt="image-20221113152023394" style="zoom:67%;" /><p><strong>简略的介绍：</strong></p><p>（1）Web：</p><p>&#x3D;&#x3D;@Controller：&#x3D;&#x3D;组合注解（组合了@Component 注解），应用在 MVC 层（控制层）</p><p>&#x3D;&#x3D;@RestController：&#x3D;&#x3D;该注解为一个组合注解，相当于@Controller 和@ResponseBody 的组合，注解在类上，意味着，该 Controller 的所有方法都默认加上了@ResponseBody。</p><p>&#x3D;&#x3D;@ResponseBody：&#x3D;&#x3D;支持将返回值放在 response 内，而不是一个页面，通常用户返回 json 数据。</p><p>&#x3D;&#x3D;@RequestMapping：&#x3D;&#x3D;用于映射 Web 请求，包括访问路径和参数。如果是 Restful 风格接口，还可以根据请求类型使用不同的注解：</p><ul><li>&#x3D;&#x3D;@GetMapping&#x3D;&#x3D;</li><li>&#x3D;&#x3D;@PostMapping&#x3D;&#x3D;</li><li>&#x3D;&#x3D;@PutMapping&#x3D;&#x3D;</li><li>&#x3D;&#x3D;@DeleteMapping&#x3D;&#x3D;</li></ul><p>&#x3D;&#x3D;@RequestBody：&#x3D;&#x3D;允许 request 的参数在 request 体中，而不是在直接连接在地址后面。</p><p>&#x3D;&#x3D;@PathVariable：&#x3D;&#x3D;用于接收路径参数，比如 @RequestMapping(“&#x2F;hello&#x2F;{name}”)申明的路径，将注解放在参数中前，即可获取该值，通常作为 Restful 的接口实现方法。</p><p>（2）容器：</p><p>&#x3D;&#x3D;@Component：&#x3D;&#x3D;表示一个带注释的类是一个“组件”，成为 Spring 管理的 Bean。当使用基于注解的配置和类路径扫描时，这些类被视为自动检测的候选对象。同时@Component 还是一个元注解。</p><p>&#x3D;&#x3D;@Service：&#x3D;&#x3D;组合注解（组合了@Component 注解），应用在 service 层（业务逻辑层）。</p><p>&#x3D;&#x3D;@Repository：&#x3D;&#x3D;组合注解（组合了@Component 注解），应用在 dao 层（数据访问层）。</p><p>&#x3D;&#x3D;@Bean：&#x3D;&#x3D;注解在方法上，声明当前方法的返回值为一个 Bean。返回的 Bean 对应的类中可以定义 init()方法和 destroy()方法，然后在@Bean(initMethod&#x3D;”init”,destroyMethod&#x3D;”destroy”)定义，在构造之后执行 init，在销毁之前执行 destroy。</p><p>&#x3D;&#x3D;@Autowired：&#x3D;&#x3D;Spring 提供的工具（由 Spring 的依赖注入工具（BeanPostProcessor、BeanFactoryPostProcessor）自动注入）。</p><p>&#x3D;&#x3D;@Qualifier：&#x3D;&#x3D;该注解通常跟 @Autowired 一起使用，当想对注入的过程做更多的控制，@Qualifier 可帮助配置，比如两个以上相同类型的 Bean 时 Spring 无法抉择，用到此注解</p><p>&#x3D;&#x3D;@Value：&#x3D;&#x3D;可用在字段，构造器参数跟方法参数，指定一个默认值，支持 #{} 跟 ${} 两个方式。一般将 SpringbBoot 中的 application.properties 配置的属性值赋值给变量。</p><p>&#x3D;&#x3D;@Scope:&#x3D;&#x3D;定义我们采用什么模式去创建 Bean（方法上，得有@Bean） 其设置类型包括：Singleton 、Prototype、Request 、 Session、GlobalSession。</p><p>&#x3D;&#x3D;@Configuration：&#x3D;&#x3D;声明当前类是一个配置类（相当于一个 Spring 配置的 xml 文件）</p><p>（3）AOP：</p><p>&#x3D;&#x3D;@Aspect&#x3D;&#x3D;:声明一个切面（类上） 使用@After、@Before、@Around 定义建言（advice），可直接将拦截规则（切点）作为参数。</p><p>&#x3D;&#x3D;@After&#x3D;&#x3D;：在方法执行之后执行（方法上）。<br>&#x3D;&#x3D;@Before&#x3D;&#x3D;:在方法执行之前执行（方法上）。<br>&#x3D;&#x3D;@Around&#x3D;&#x3D; :在方法执行之前与之后执行（方法上）。<br>&#x3D;&#x3D;PointCut&#x3D;&#x3D;:声明切点 在 java 配置类中使用@EnableAspectJAutoProxy 注解开启 Spring 对 AspectJ 代理的支持（类上）。</p><p>（4）事务：</p><p>&#x3D;&#x3D;@Transactional&#x3D;&#x3D;：在要开启事务的方法上使用@Transactional 注解，即可声明式开启事务。</p><h2 id="4-queue线程安全的集合了解吗（整理每个集合线程对应安全的集合）（√）"><a href="#4-queue线程安全的集合了解吗（整理每个集合线程对应安全的集合）（√）" class="headerlink" title="4.queue线程安全的集合了解吗（整理每个集合线程对应安全的集合）（√）"></a>4.queue线程安全的集合了解吗（整理每个集合线程对应安全的集合）（√）</h2><p>Java 提供的线程安全的 Queue 可以分为阻塞队列和非阻塞队列，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue。</p><h2 id="4-1-队列线程安全版本之阻塞队列"><a href="#4-1-队列线程安全版本之阻塞队列" class="headerlink" title="4.1 - 队列线程安全版本之阻塞队列"></a>4.1 - 队列线程安全版本之阻塞队列</h2><blockquote><p>  【参考】</p><p>  <a href="https://javaguide.cn/java/concurrent/java-concurrent-collections.html#concurrentlinkedqueue">Java 常见并发容器总结 - JavaGuide（√）</a></p></blockquote><h3 id="1-BlockingQueue-简介"><a href="#1-BlockingQueue-简介" class="headerlink" title="1 - BlockingQueue 简介"></a>1 - BlockingQueue 简介</h3><p>队列的高性能非阻塞版本是：<code>ConcurrentLinkedQueue</code></p><p>队列的阻塞版本是：<code>BlockingQueue</code></p><hr><p>阻塞队列（<code>BlockingQueue</code>）被广泛使用在“生产者-消费者”问题中，其原因是 <code>BlockingQueue</code> 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止。</p><hr><p>BlockingQueue 是一个接口，继承自 Queue 接口；同时 Queue 接口继承自 Collection 接口。</p><p>下面是BlockingQueue的相关实现类：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211131805123.png" alt="image-20221113180502066"></p><p>常见的 BlockingQueue 有 ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue。</p><h3 id="2-ArrayBlockingQueue"><a href="#2-ArrayBlockingQueue" class="headerlink" title="2 - ArrayBlockingQueue"></a>2 - ArrayBlockingQueue</h3><blockquote><p>  随手记：</p><p>  ArrayBlockingQueue 是有界队列实现类</p><p>  LinkedBlockingQueue 是无界队列实现类</p></blockquote><p>ArrayBlockingQueue 是 BlockingQueue 接口的有界队列实现类，底层采用数组来实现。</p><hr><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ArrayBlockingQueue</span>&lt;E&gt;<br><span class="hljs-keyword">extends</span> <span class="hljs-title class_">AbstractQueue</span>&lt;E&gt;<br><span class="hljs-keyword">implements</span> <span class="hljs-title class_">BlockingQueue</span>&lt;E&gt;, Serializable&#123;&#125;<br></code></pre></td></tr></table></figure><hr><p><code>ArrayBlockingQueue</code> 一旦创建，容量不能改变。容量是有限的，有界的。</p><hr><blockquote><p>  使用 ReentrantLock 进行并发控制</p></blockquote><p>其并发控制采用可重入锁 <code>ReentrantLock</code> ，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。</p><p>当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。</p><hr><blockquote><p>  ReentrantLock 默认是非公平的，因为绝对的公平性会降低吞吐量。因此 ArrayBlockingQueue 默认也是非公平的阻塞队列。即最先阻塞的队列元素，不一定最先获得锁。</p></blockquote><p><code>ArrayBlockingQueue</code> 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 <code>ArrayBlockingQueue</code>。而非公平性则是指访问 <code>ArrayBlockingQueue</code> 的顺序不是遵守严格的时间顺序，有可能存在，当 <code>ArrayBlockingQueue</code> 可以被访问时，长时间阻塞的线程依然无法访问到 <code>ArrayBlockingQueue</code>。如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 <code>ArrayBlockingQueue</code>，可采用如下代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 在创建该对象的时候，指定队列的长度和执行公平锁。</span><br><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> ArrayBlockingQueue&lt;Integer&gt; blockingQueue = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayBlockingQueue</span>&lt;Integer&gt;(<span class="hljs-number">10</span>,<span class="hljs-literal">true</span>);<br></code></pre></td></tr></table></figure><h3 id="3-LinkedBlockingQueue"><a href="#3-LinkedBlockingQueue" class="headerlink" title="3 - LinkedBlockingQueue"></a>3 - LinkedBlockingQueue</h3><h3 id="数据结构："><a href="#数据结构：" class="headerlink" title="数据结构："></a>数据结构：</h3><blockquote><p>  【辨析】</p><p>  ArrayBlockingQueue 的底层是数组，是有界的。</p><p>  LinkedBlockingQueue 的底层是单向链表，可以是有界的，也可以是无界的。</p><p>  LinkedBlockingQueue 与 ArrayBlockingQueue 相比起来具有更高的吞吐量</p></blockquote><p>单向链表实现的阻塞队列，可以是有界的，也可以是无界的。</p><p>有界指的是：为了防止 LinkedBlockingQueue 容量迅速增，损耗大量内存。通常在创建 LinkedBlockingQueue 对象时，会指定其大小。</p><p>无界指的是：如果未指定，容量等于 Integer.MAX_VALUE，那么就是无界的。</p><p>相关的构造方法代码：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment">    *某种意义上的无界队列</span><br><span class="hljs-comment">    */</span><br>   <span class="hljs-keyword">public</span> <span class="hljs-title function_">LinkedBlockingQueue</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-built_in">this</span>(Integer.MAX_VALUE);<span class="hljs-comment">// 调用的是有界队列，将容量指定为 Integer.MAX_VALUE</span><br>   &#125;<br><br>   <span class="hljs-comment">/**</span><br><span class="hljs-comment">    *有界队列</span><br><span class="hljs-comment">    */</span><br>   <span class="hljs-keyword">public</span> <span class="hljs-title function_">LinkedBlockingQueue</span><span class="hljs-params">(<span class="hljs-type">int</span> capacity)</span> &#123;<br>       <span class="hljs-keyword">if</span> (capacity &lt;= <span class="hljs-number">0</span>) <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">IllegalArgumentException</span>();<br>       <span class="hljs-built_in">this</span>.capacity = capacity;<br>       last = head = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Node</span>&lt;E&gt;(<span class="hljs-literal">null</span>);<br>   &#125;<br></code></pre></td></tr></table></figure><h3 id="4-PriorityBlockingQueue"><a href="#4-PriorityBlockingQueue" class="headerlink" title="4 - PriorityBlockingQueue"></a>4 - PriorityBlockingQueue</h3><blockquote><p>  ArrayBlockingQueue 是有界阻塞队列，LinkedBlockingQueue 和 PriorityBlockingQueue 是无界阻塞队列</p></blockquote><p>PriorityBlockingQueue 是一个支持优先级的无界阻塞队列。</p><hr><p>默认采用自然顺序进行排序，也可以自定义排序规则</p><hr><p>PriorityBlockingQueue 并发控制采用的是可重入锁 ReentrantLock</p><hr><p>该队列为无界队列，PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容</p><hr><p>简单地说，它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block，因为它是无界队列（take 方法在队列为空的时候会阻塞）。</p><h2 id="4-2-队列线程安全版本之非阻塞队列"><a href="#4-2-队列线程安全版本之非阻塞队列" class="headerlink" title="4.2 - 队列线程安全版本之非阻塞队列"></a>4.2 - 队列线程安全版本之非阻塞队列</h2><p>Java 提供的线程安全的 Queue 可以分为阻塞队列和非阻塞队列，其中阻塞队列的典型例子是 BlockingQueue，非阻塞队列的典型例子是 ConcurrentLinkedQueue，在实际应用中要根据实际需要选用阻塞队列或者非阻塞队列。 阻塞队列可以通过加锁来实现，非阻塞队列可以通过 CAS 操作实现。</p><p>从名字可以看出，ConcurrentLinkedQueue这个队列使用链表作为其数据结构．ConcurrentLinkedQueue 应该算是在高并发环境中性能最好的队列了。它之所有能有很好的性能，是因为其内部复杂的实现。</p><p>ConcurrentLinkedQueue 内部代码我们就不分析了，大家知道 ConcurrentLinkedQueue 主要使用 CAS 非阻塞算法来实现线程安全就好了。</p><p>ConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue 来替代。</p><h2 id="4-3-List的线程安全的版本-CopyOnWriteArrayList"><a href="#4-3-List的线程安全的版本-CopyOnWriteArrayList" class="headerlink" title="4.3 - List的线程安全的版本 CopyOnWriteArrayList"></a>4.3 - List的线程安全的版本 CopyOnWriteArrayList</h2><h3 id="1-CopyOnWriteArrayList-简介"><a href="#1-CopyOnWriteArrayList-简介" class="headerlink" title="1 - CopyOnWriteArrayList 简介"></a>1 - CopyOnWriteArrayList 简介</h3><p>&#x3D;&#x3D;类的声明：&#x3D;&#x3D;</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">CopyOnWriteArrayList</span>&lt;E&gt;<br><span class="hljs-keyword">extends</span> <span class="hljs-title class_">Object</span><br><span class="hljs-keyword">implements</span> <span class="hljs-title class_">List</span>&lt;E&gt;, RandomAccess, Cloneable, Serializable<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;思想：&#x3D;&#x3D;</p><p>在多数的场景中，读操作远远多于写操作，并且读操作不会修改数据，因此如果无论读或者写都加锁，那么就太浪费资源和性能了。因此应该允许并发进行读操作，这是线程安全的。</p><p>这种思想和 ReentrantReadWriteLock 读写锁的思想非常类似，也就是读读共享、写写互斥、读写互斥、写读互斥。但是JDK 中提供的 CopyOnWriteArrayList 类比相比于在读写锁的思想又更进一步。</p><p>为了将读取的性能发挥到极致，CopyOnWriteArrayList 读取是完全不用加锁的，并且更厉害的是：写入也不会阻塞读取操作。只有写入和写入之间需要进行同步等待。这样一来，读操作的性能就会大幅度提升。</p><p>ReentrantReadWriteLock 无论在写写、读写、写读的时候，都需要加锁；但是 CopyOnWriteArrayList 只需要在写写的时候进行阻塞，其它的时候完全不用阻塞。</p><h3 id="2-CopyOnWriteArrayList-是如何做到只在-写-写-的时候，阻塞？"><a href="#2-CopyOnWriteArrayList-是如何做到只在-写-写-的时候，阻塞？" class="headerlink" title="2 - CopyOnWriteArrayList 是如何做到只在 写-写 的时候，阻塞？"></a>2 - CopyOnWriteArrayList 是如何做到只在 写-写 的时候，阻塞？</h3><p>CopyOnWriteArrayList 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，我并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。</p><p>但是在写写的时候，因此两个线程都要修改数据，那么这个时候就要排队一个一个来了，不然就会产生线程安全问题。</p><p>从 CopyOnWriteArrayList 的名字就能看出 CopyOnWriteArrayList 是满足 CopyOnWrite 的。所谓 CopyOnWrite 也就是说：在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存，原来的内存就可以被回收掉了。</p><h3 id="3-CopyOnWriteArrayList-读取和写入源码简单分析"><a href="#3-CopyOnWriteArrayList-读取和写入源码简单分析" class="headerlink" title="3 - CopyOnWriteArrayList 读取和写入源码简单分析"></a>3 - CopyOnWriteArrayList 读取和写入源码简单分析</h3><h4 id="3-1-读取操作的实现"><a href="#3-1-读取操作的实现" class="headerlink" title="3.1 - 读取操作的实现"></a>3.1 - 读取操作的实现</h4><p>读取操作没有任何同步控制和锁操作，理由就是内部数组 array 不会发生修改，只会被另外一个 array 替换，因此可以保证数据安全。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/** The array, accessed only via getArray/setArray. */</span><br><span class="hljs-keyword">private</span> <span class="hljs-keyword">transient</span> <span class="hljs-keyword">volatile</span> Object[] array;<br><span class="hljs-keyword">public</span> E <span class="hljs-title function_">get</span><span class="hljs-params">(<span class="hljs-type">int</span> index)</span> &#123;<br>    <span class="hljs-keyword">return</span> get(getArray(), index);<br>&#125;<br><span class="hljs-meta">@SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="hljs-keyword">private</span> E <span class="hljs-title function_">get</span><span class="hljs-params">(Object[] a, <span class="hljs-type">int</span> index)</span> &#123;<br>    <span class="hljs-keyword">return</span> (E) a[index];<br>&#125;<br><span class="hljs-keyword">final</span> Object[] getArray() &#123;<br>    <span class="hljs-keyword">return</span> array;<br>&#125;<br></code></pre></td></tr></table></figure><h4 id="3-2-写入操作的实现"><a href="#3-2-写入操作的实现" class="headerlink" title="3.2 - 写入操作的实现"></a>3.2 - 写入操作的实现</h4><p>CopyOnWriteArrayList 写入操作 add()方法在添加集合的时候加了锁，保证了同步，避免了多线程写的时候会 copy 出多个副本出来。</p><p>add方法：加锁，然后拷贝出一个新的数组，新的数组的长度是原来的长度 + 1，然后在新数组的末尾添加元素，并将指针指向新的数组。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/**</span><br><span class="hljs-comment">     * Appends the specified element to the end of this list.</span><br><span class="hljs-comment">     *</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@param</span> e element to be appended to this list</span><br><span class="hljs-comment">     * <span class="hljs-doctag">@return</span> &#123;<span class="hljs-doctag">@code</span> true&#125; (as specified by &#123;<span class="hljs-doctag">@link</span> Collection#add&#125;)</span><br><span class="hljs-comment">     */</span><br><span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">add</span><span class="hljs-params">(E e)</span> &#123;<br>    <span class="hljs-keyword">final</span> <span class="hljs-type">ReentrantLock</span> <span class="hljs-variable">lock</span> <span class="hljs-operator">=</span> <span class="hljs-built_in">this</span>.lock;<br>    lock.lock();<span class="hljs-comment">//加锁</span><br>    <span class="hljs-keyword">try</span> &#123;<br>        Object[] elements = getArray();<br>        <span class="hljs-type">int</span> <span class="hljs-variable">len</span> <span class="hljs-operator">=</span> elements.length;<br>        Object[] newElements = Arrays.copyOf(elements, len + <span class="hljs-number">1</span>);<span class="hljs-comment">//拷贝新数组</span><br>        newElements[len] = e;<br>        setArray(newElements);<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>    &#125; <span class="hljs-keyword">finally</span> &#123;<br>        lock.unlock();<span class="hljs-comment">//释放锁</span><br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="4-4-ConcurrentHashMap"><a href="#4-4-ConcurrentHashMap" class="headerlink" title="4.4 - ConcurrentHashMap"></a>4.4 - ConcurrentHashMap</h2><p>&#x3D;&#x3D;直接加锁存在严重的性能问题&#x3D;&#x3D;</p><p>我们知道 <code>HashMap</code> 不是线程安全的，在并发场景下如果要保证一种可行的方式是使用 <code>Collections.synchronizedMap()</code> 方法来包装我们的 <code>HashMap</code>。但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来不可忽视的性能问题。</p><p>&#x3D;&#x3D;HashMap 的线程安全版本&#x3D;&#x3D;</p><p>所以就有了 HashMap 的线程安全版本—— ConcurrentHashMap 的诞生。</p><p>在 ConcurrentHashMap 中，无论是读操作还是写操作都能保证很高的性能：在进行读操作时(几乎)不需要加锁，而在写操作时通过锁分段技术只对所操作的段加锁而不影响客户端对其它段的访问。</p><p>读操作的时候几乎不需要加锁，在写操作的时候，使用分段锁技术，细粒度地只对操作的位置进行加锁，其它的问题仍然可以继续访问。</p><h2 id="5-newFixedThreadPool梳理（√）"><a href="#5-newFixedThreadPool梳理（√）" class="headerlink" title="5.newFixedThreadPool梳理（√）"></a>5.newFixedThreadPool梳理（√）</h2><p>（1）newFixedThreadPool</p><p>固定的n个核心线程，多余的任务去阻塞队列排队，阻塞队列是LinkedBlockingQueue，是无限的队列，有内存溢出风险。</p><p>（2）SingleThreadExecutor（只有一个核心线程）</p><p>当 newFixedThreadPool 的核心线程数量为 1 的时候，为 SingleThreadExecutor 。</p><p>（3）newCachedThreadPool</p><p>线程线程数量为0，任务先放到阻塞队列中，如果有空闲的最大线程，则复用，没有的话则创建。可以创建无限个线程。阻塞队列中不存储任务。</p><p>使用了SynchronousBlockingQueue作为任务队列，不存储元素，吞吐量高于 LinkedBlockingQueue 。</p>]]></content>
    
    
    <categories>
      
      <category>求职招聘</category>
      
      <category>面试展开</category>
      
    </categories>
    
    
    <tags>
      
      <tag>复盘</tag>
      
      <tag>面试</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022年11月10日牛客测试题题目整理</title>
    <link href="/posts/2145513789/"/>
    <url>/posts/2145513789/</url>
    
    <content type="html"><![CDATA[<h3 id="001-一个类具体是怎么加载的？"><a href="#001-一个类具体是怎么加载的？" class="headerlink" title="001 - 一个类具体是怎么加载的？"></a>001 - 一个类具体是怎么加载的？</h3><blockquote><p>  参考：</p><ul><li><p><a href="https://blog.csdn.net/qweqwruio/article/details/81359906">类加载过程是怎样的？ - CSDN - wiseph</a></p></li><li><p>深入理解JVM</p></li></ul></blockquote><p>一种三个阶段，加载、链接、初始化。加载是将静态的字节码加载到JVM中，链接是将静态的数据和运行中的JVM信息关联起来，初始化是对静态变量和静态代码块等内容赋初始值（用户定义的初始值）。</p><p>1.加载阶段</p><p>将来自于class文件、jar文件、网络数据源等地方的字节码文件加载到JVM里面，并在JVM中映射为JVM认可的class对象</p><p>2.链接阶段</p><p>（1）验证</p><p>验证字节码是否合法、是否安全，以防破坏JVM的安全运行。</p><p>（2）准备</p><p>将类或接口中的静态变量设置初始值，此处的重点是给静态变量分配内存空间，赋予默认的初始值，而不是用户定义的显示初始值。</p><p>比如静态变量<code>static int a = 1;</code>，此处会初始化为 int 类型的初始值 0，而不是初始化为 1。</p><p>（3）解析</p><p>将<code>常量池</code>中的 符号引用 替换为 直接引用。</p><p>符号引用是字面量，在JVM中对应的对象可以还未存在。直接引用是地址值，即指针、偏移量或句柄，指向的对象在内存中必须已经存在。</p><p>举例：中国的首都是字面量，北京市是直接引用。</p><p>解析主要对 类或接口，字段，类方法，接口方法，方法类型等进行解析。</p><p>3.初始化阶段</p><blockquote><p>  准备阶段主要对静态变量分配空间，初始化阶段主要对类的静态变量显示赋初始值和执行静态代码块的逻辑。</p></blockquote><p>即静态变量赋初始值、执行静态代码块的逻辑。</p><hr><h5 id="知识扩展："><a href="#知识扩展：" class="headerlink" title="知识扩展："></a>知识扩展：</h5><ol><li>Java 8之前的类加载器</li></ol><p>(1)启动类加载器（Bootstrap Class-Loader），加载jre&#x2F;lib下面的jar文件，如rt.jar.</p><p>(2)扩展类加载器（Extension or Ext Class-Loader），负责加载我们放到jre&#x2F;lib&#x2F;ext目录下面的jar包，这就是所谓的extension机制。该目录也可以通过设置“java.ext.dirs”来覆盖。</p><p>(3)应用类加载器（Application or App Class-Loader），就是加载我们最熟悉的classpath的内容。这里有一个容易混淆的概念，系统（System）类加载器，通常来说，其默认就是JDK內建的应用类加载器。</p><blockquote><p>  自定义类加载器:</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">java -Djava.system.class.loader=com.yourcorp.YourClassLoader HelloWorld<br></code></pre></td></tr></table></figure><p>  如果我们指定了这个参数，JDK內建的应用类加载器就会成为定制加载器的父亲，这种方式通常用在类似需要改变双亲委派模式的场景。</p></blockquote><p>参考图：</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211101430487.png" alt="image-20221110143031413" style="zoom:67%;" /><p>2.双亲委派机制</p><p>当类加载器（Class-Loader）试图加载某个类型的时候，除非父加载器找不到相应的类型，否则尽量将这个任务代理给当前加载器的父加载器去做。</p><p>参考上面这个结构图就很容易理解了。试想，如果不同类加载器都自己加载需要的某个类型，那么就会出现多次重复加载，完全是种浪费。</p><p>通常类加载器机制有三个基本特征：</p><ul><li>双亲委派模型。但不是所有类加载都遵守这个模型，有的时候，启动类加载器所加载的类型，是可能要加载用户代码的。比如JDK内部的ServiceProvider&#x2F;ServiceLoader机制，用户可以在标准API框架上，提供自己的实现，JDK也需要提供些默认的参考实现。例如，Java中JNDI、JDBC、文件系统、Cipher等很多方面，都是利用的这种机制，这种情况就不会用双亲委派模型去加载，而是利用所谓的上下文加载器。</li><li>可见性。子加载器可以访问父加载器加载的类型，但是反过来是不允许的。不然，因为缺少必要的隔离，我们就没有办法利用类加载器去实现容器的逻辑。</li><li>单一性。由于父加载器的类型对于子加载器是可见的，所以父加载器中加载过的类型，就不会在子加载器中重复加载。但是注意，类加载器“邻居”间，同一类型仍然可以被加载多次，因为互相不可见。</li></ul><h3 id="002-一个JVM程序有多少个类加载器？"><a href="#002-一个JVM程序有多少个类加载器？" class="headerlink" title="002 - 一个JVM程序有多少个类加载器？"></a>002 - 一个JVM程序有多少个类加载器？</h3><blockquote><p>  参考：</p><ul><li><a href="https://segmentfault.com/q/1010000014745626">JAVA为什么要有多个类加载器，1个不行吗 - segmentfault - 近光176</a></li></ul></blockquote><blockquote><p>  【前言】</p><ul><li>类加载的作用是通过类名获取二进制字节流</li><li>主要分为四种类加载器：启动类-&gt;扩展类-&gt;应用类-&gt;自定义类</li><li>双亲委派的好处：越基础的类交给越高级的类加载器</li><li>问题：只有一个加载器来加载全部的类不行吗</li></ul></blockquote><p>JVM需要不同的类加载器，而不是使用同一个类加载器，本质上是对类有不同的需求所导致的。不同的场景下使用不同的类加载器，目的是更加的灵活。</p><p>在明确 目的的情况下， 专用代码 比 通用代码 更简单，也更有效。</p><p>使用不同的类加载器，方便在不同的路径下加载不同的类。更加的灵活，也方便对类进行管理。</p><p>举例两个场景：</p><p>（1）在JVM中运行不同的程序，每个程序依赖同一个 x 类，但是依赖的该类的版本不同。有的需要版本高的 x 类，有的需要版本低的 x 类。因此使用不同的类加载器可以加载不同的类。</p><p>（2）Java具有面向切面进行功能增强的特性。怎样实现修改一个类进行特性的功能增强，而不对其它的类库产生影响呢？一个方面的方式就是对每个类库使用独立的类加载器。</p><h3 id="003-mysql的性能瓶颈在哪里，怎么排查"><a href="#003-mysql的性能瓶颈在哪里，怎么排查" class="headerlink" title="003 - mysql的性能瓶颈在哪里，怎么排查"></a>003 - mysql的性能瓶颈在哪里，怎么排查</h3><blockquote><p>  【参考】</p><ul><li><a href="https://developer.aliyun.com/article/177931">优化系列 | 实例解析MySQL性能瓶颈排查定位 - 阿里云开发者社区 - 晚来风急</a></li></ul><p>  【大纲】</p><ul><li>OS层面的检查<ul><li>检查总体的负载情况</li><li>检查哪个进程的负载高</li></ul></li><li>MySQL层面的检查<ul><li>查看查看慢查询日志，来找到效率低的SQL语句，想办法对这个语句进行优化。</li></ul></li></ul></blockquote><p>1.OS层面的检查</p><p>OS层面的检查的目的是需要检查服务器上哪些进程的负载高。<strong>通常服务器上容易成为性能瓶颈的是磁盘的IO动作。</strong></p><p>第一步：查看整体的负载。</p><p>整体负载高的话，那么每个进程都慢。可以通过执行指令<code>w</code>或者<code>sar -q 1</code>来查看负载。</p><p>执行上述命令之后，得到结果中，字段<code>load average</code>表示当前的CPU有多少任务在排队等待。load 数值超过 5 的话，负载就挺高了。</p><blockquote><p>  子问题1：引起 CPU 负载高的可能的原因？</p><ul><li>某些进程消耗更多的 CPU 资源，比如需要响应大量的请求（进程的事）</li><li>物理内存不足，需要频繁的进行 swap（内存的事）</li><li>磁盘的 IO 比较慢，导致 CPU 一直等待IO（磁盘的事）</li><li>发生严重的中断，比如因为网络原因。（其它的事）</li></ul></blockquote><p>第二步：查看具体是哪个进程的负载高。</p><p>使用<code>top</code>命令可以查看每个进程的资源占用情况。</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211101613719.png" alt="image-20221110161326648" style="zoom:67%;" /><p>如上图，</p><p>整体来看，通过 us 和 wa 的数值过高，可以推测当前的性能瓶颈可能是用户进行消耗 CPU 以及磁盘 IO等待 消耗CPU。</p><p>分开来看，看到下面每行中，第一、二行的进程， 字段 CPU 的数值很高，可以推出这两个进程是性能瓶颈。</p><p>查看到上面的 wa 数值高，推测是 IO 消耗大，因此这时可以执行 iotop 命令，查看哪些进程的磁盘 IO 消耗最大。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs bash">[yejr@imysql.com:~ ]<span class="hljs-comment"># iotop</span><br>Total DISK READ: 60.38 M/s | Total DISK WRITE: 640.34 K/s<br>  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND<br>16397 be/4 mysql       8.92 M/s    0.00 B/s  0.00 % 94.77 % mysqld --basedir=/usr/local/m~og_3320/mysql.sock --port=3320<br> 7295 be/4 mysql      10.98 M/s    0.00 B/s  0.00 % 93.59 % mysqld --basedir=/usr/local/m~og_3320/mysql.sock --port=3320<br>14295 be/4 mysql      10.50 M/s    0.00 B/s  0.00 % 93.57 % mysqld --basedir=/usr/local/m~og_3320/mysql.sock --port=3320<br>14288 be/4 mysql      14.30 M/s    0.00 B/s  0.00 % 91.86 % mysqld --basedir=/usr/local/m~og_3320/mysql.sock --port=3320<br>14292 be/4 mysql      14.37 M/s    0.00 B/s  0.00 % 91.23 % mysqld --basedir=/usr/local/m~og_3320/mysql.sock --port=3320<br></code></pre></td></tr></table></figure><p>可以看到，端口号是3320的实例消耗的磁盘I&#x2F;O资源比较多，那就看看这个实例里都有什么查询在跑吧。</p><p>2.MySQL层面的检查</p><p>首先看下当前都有哪些查询在运行（横版查看）：<code>mysqladmin pr|grep -v Sleep</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs bash">[yejr@imysql.com(db)]&gt; mysqladmin <span class="hljs-built_in">pr</span>|grep -v Sleep<br>+----+----+----------+----+-------+-----+--------------+-----------------------------------------------------------------------------------------------+<br>| Id |User| Host     | db |Command|Time | State        | Info                                                                                          |<br>+----+----+----------+----+-------+-----+--------------+-----------------------------------------------------------------------------------------------+<br>| 25 | x | 10.x:8519 | db | Query | 68  | Sending data | select max(Fvideoid) from (select Fvideoid from t <span class="hljs-built_in">where</span> Fvideoid&gt;404612 order by Fvideoid) t1 |<br>| 26 | x | 10.x:8520 | db | Query | 65  | Sending data | select max(Fvideoid) from (select Fvideoid from t <span class="hljs-built_in">where</span> Fvideoid&gt;484915 order by Fvideoid) t1 |<br>| 28 | x | 10.x:8522 | db | Query | 130 | Sending data | select max(Fvideoid) from (select Fvideoid from t <span class="hljs-built_in">where</span> Fvideoid&gt;404641 order by Fvideoid) t1 |<br>| 27 | x | 10.x:8521 | db | Query | 167 | Sending data | select max(Fvideoid) from (select Fvideoid from t <span class="hljs-built_in">where</span> Fvideoid&gt;324157 order by Fvideoid) t1 |<br>| 36 | x | 10.x:8727 | db | Query | 174 | Sending data | select max(Fvideoid) from (select Fvideoid from t <span class="hljs-built_in">where</span> Fvideoid&gt;324346 order by Fvideoid) t1 |<br>+----+----+----------+----+-------+-----+--------------+-----------------------------------------------------------------------------------------------+<br></code></pre></td></tr></table></figure><p>通过上面的结果可以看到还有不少的慢查询在进行。通过 slow query log 也能发现慢的 SQL 语句。</p><p>慢查询语句为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mysql">select max(Fvideoid) from (select Fvideoid from t where Fvideoid&gt;404612 order by Fvideoid) t1<br></code></pre></td></tr></table></figure><p>这种方式先正排序，然后查询最大。查询效率非常低，因为只需要求最大值，却需要全表扫描。</p><p>优化方式为：查询之后倒排，然后取第一条。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mysql">select Fvideoid from t where Fvideoid&gt;404612 order by Fvideoid desc limit 1;<br></code></pre></td></tr></table></figure><p>3.小结</p><p>在实际的生产环境中，可能导致产生性能瓶颈的原因通常有：</p><p><strong>（1）IO瓶颈：</strong></p><ul><li>一次请求读写的数据量太大，比如一条SQL可能要读取几万行数据，这种情况最好减少一次读写的量</li></ul><p><strong>（2）没有合适的索引：</strong></p><ul><li>没有建立合适的索引帮助进行过滤、排序、分组等。这个时候可以添加索引或者SQL改写</li></ul><p><strong>（3）并发量压垮服务器：</strong></p><ul><li>瞬间请求量太大，导致服务器雪崩</li></ul><p><strong>（4）服务器自适应节能策略导致反应不及时</strong></p><p>服务器自身的节能策略发现负载较低时会让CPU降频，当发现负载升高时再自动升频，但通常不是那么及时，结果导致CPU性能不足，抗不过突发的请求；</p><h3 id="004-http常见的状态码"><a href="#004-http常见的状态码" class="headerlink" title="004 - http常见的状态码"></a>004 - http常见的状态码</h3><blockquote><ul><li>http常见的状态码要背一下，不要只知道大概。比如每类记3个</li><li>(方法)尝试不要根据数字记状态，否则容易记混，根据状态记数字。</li><li>2022年11月11日默写：<ul><li>100继续，101根据客户端要求升级协议</li><li>200ok，201成功创建，202成功接收，204成功但无返回内容</li><li>301永久重定向，302临时重定向，304未改变</li><li>400语法错误，401未授权，403丑拒，404未找到</li><li>500服务器内部错误，501内支持，502网关处出错，503服务器不可用</li></ul></li></ul></blockquote><h5 id="信息响应-100–199"><a href="#信息响应-100–199" class="headerlink" title="信息响应(100–199)"></a>信息响应(100–199)</h5><ul><li>继续 - 100</li><li>服务器根据客户端的请求切换协议 - 101</li></ul><h5 id="成功响应-200–299-（缺3）"><a href="#成功响应-200–299-（缺3）" class="headerlink" title="成功响应(200–299)（缺3）"></a>成功响应(200–299)（缺3）</h5><ul><li>请求成功 - 200</li><li>已创建。成功请求并创建了新的资源 - 201</li><li>已接受。已经接受请求，但未处理完成 - 202</li><li>无内容。服务器成功处理，但未返回内容 - 204</li></ul><h5 id="重定向-300–399-（缺0、3）"><a href="#重定向-300–399-（缺0、3）" class="headerlink" title="重定向(300–399)（缺0、3）"></a>重定向(300–399)（缺0、3）</h5><ul><li>资源被永久转移到其它的URL - 301</li><li>临时移动 - 302</li><li>未修改 - 304</li></ul><h5 id="客户端错误-400–499-（缺2）"><a href="#客户端错误-400–499-（缺2）" class="headerlink" title="客户端错误(400–499)（缺2）"></a>客户端错误(400–499)（缺2）</h5><ul><li>请求的语法错误 - 400（我不李姐） - Bad Request</li><li><code>未</code>授权 - 401 - Unauthorized（农行广研面试问题）</li><li>禁止 - 403 - Forbidden</li><li>请求的资源不存在 - 404 - not found</li></ul><h5 id="服务器错误-500–599"><a href="#服务器错误-500–599" class="headerlink" title="服务器错误 (500–599)"></a>服务器错误 (500–599)</h5><ul><li>内部服务器错误 - 500 - Internal Server Error</li><li>功能<code>未</code>实现 - 501 - Not Implemented</li><li>（网关处出问题）网关从服务器收到了一个无效的响应 - 502 - Bad Gateway</li><li>（服务器处出问题）服务不可用，比如正在维护 - 503 - Service Unavailable</li></ul><h3 id="005-Java是值传递还是引用传递"><a href="#005-Java是值传递还是引用传递" class="headerlink" title="005 - Java是值传递还是引用传递"></a>005 - Java是值传递还是引用传递</h3><p>值传递，值传递，值传递</p>]]></content>
    
    
    <categories>
      
      <category>求职招聘</category>
      
      <category>面试展开</category>
      
    </categories>
    
    
    <tags>
      
      <tag>复盘</tag>
      
      <tag>面试</tag>
      
      <tag>农业银行研发中心</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2022年11月8日牛客刷题测试题目整理</title>
    <link href="/posts/2971606116/"/>
    <url>/posts/2971606116/</url>
    
    <content type="html"><![CDATA[<h1 id="牛客刷题测试题目整理"><a href="#牛客刷题测试题目整理" class="headerlink" title="牛客刷题测试题目整理"></a>牛客刷题测试题目整理</h1><h3 id="001-下列关于反射和泛型的联系说法正确的是：（√）"><a href="#001-下列关于反射和泛型的联系说法正确的是：（√）" class="headerlink" title="001 - 下列关于反射和泛型的联系说法正确的是：（√）"></a>001 - 下列关于反射和泛型的联系说法正确的是：（√）</h3><blockquote><p>  【参考】</p><p>  <a href="https://blog.csdn.net/cpcpcp123/article/details/115141681">面试：Java中的泛型会被类型擦除，那为什么在运行期仍然可以使用反射获取到具体的泛型类型</a></p></blockquote><hr><blockquote><p>  A - 错误，泛型只在编译期间有效，但是运行的时候也可以获取泛型类型信息</p><p>  B - 错误，可以通过反射绕过泛型检查，但是运行期间泛型有时也会用到</p><p>  D - 正确，反射机制可以获取到泛型的具体类型</p></blockquote><p>A - 泛型只在编译期间有效，无法在运行时获取泛型的具体类型</p><p>B - 可以通过反射绕过泛型检查，因为运行期泛型根本没有用</p><p>C - 其它选项说法都不对</p><p>D - 反射机制可以获取到泛型的具体类型</p><hr><p>正确答案是D。</p><p>编译的时候，Java中的泛型会被类型擦除。</p><p>但是在运行期间仍然可以通过反射获取到具体的泛型类型。</p><p>泛型是在java5出现的语法糖，用于在编译期间进行类型检查。源代码编译之后成为class字节码文件的时候擦除泛型的原因是为了兼容之前的代码。</p><p>但是有的时候，还需要或许泛型的信息。因此就引入了 Signature 属性。</p><p>Signature属性的出现，Java泛型擦除法所谓的擦除，只是对方法的Code属性中的字节码进行擦除，实际上元数据中还是保留了泛型信息，这也是我们能通过反射手段获取参数化类型的根本依据。</p><p>众所周知，java是在Java5的时候引入的泛型，为了支持泛型，JVM的class文件也做了相应的修改，其中最重要的就是新增了Signature属性表，java编译为字节码后，其申明侧泛型信息都存储在Signature中，通过反射获取的泛型信息都来源于这里。</p><p>而Signature属性表可以被class文件，字段表，方法表携带，这就使得：类声明，字段声明，方法声明中的泛型信息得以保留。</p><hr><h3 id="002-下列关于反射的说法错误的是：（√）"><a href="#002-下列关于反射的说法错误的是：（√）" class="headerlink" title="002 - 下列关于反射的说法错误的是：（√）"></a>002 - 下列关于反射的说法错误的是：（√）</h3><p>A - 通过new对象实现反射机制</p><p>B - 通过方法名和参数类型实现反射机制</p><p>C - 通过路径实现反射机制</p><p>D - 通过类名实现反射机制</p><hr><p>A - 错误，通过new来创建对象的方式是正射。通过反射创建对象的方式叫反射。</p><h3 id="003-关于如何使用FileWriter向文件中写入内容（√）"><a href="#003-关于如何使用FileWriter向文件中写入内容（√）" class="headerlink" title="003 - 关于如何使用FileWriter向文件中写入内容（√）"></a>003 - 关于如何使用FileWriter向文件中写入内容（√）</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.io.File;<br><span class="hljs-keyword">import</span> java.io.FileWriter;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.io.Writer;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-comment">//使用FileWriter向文本文件中写信息</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">str</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;Hello World&quot;</span>;<br>        <span class="hljs-comment">//1.创建流</span><br>        <span class="hljs-type">Writer</span> <span class="hljs-variable">fw</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>        <span class="hljs-keyword">try</span> &#123;<br>            <span class="hljs-comment">/*创建txt文件*/</span><br>            <span class="hljs-type">File</span> <span class="hljs-variable">file</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">File</span>(<span class="hljs-string">&quot;D:\\hello.txt&quot;</span>);<br>            <span class="hljs-keyword">if</span> (!file.exists()) &#123;<br>                file.createNewFile();<br>            &#125;<br>            fw = <span class="hljs-keyword">new</span> <span class="hljs-title class_">FileWriter</span>(<span class="hljs-string">&quot;D:\\hello.txt&quot;</span>);<span class="hljs-comment">//1</span><br>            <span class="hljs-comment">//2.写入信息</span><br>            fw.write(str);<br>            <span class="hljs-comment">// 3.刷新缓冲区，即写入内容</span><br>            fw.flush();<br>            <span class="hljs-keyword">if</span> (fw != <span class="hljs-literal">null</span>) &#123;<br>                <span class="hljs-comment">// 4.关闭流,关闭缓冲流时，也会刷新一次缓冲区</span><br>                fw.close();<br><br>            &#125;<br>        &#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>            e.printStackTrace();<br>        &#125;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><h3 id="004-下面哪个流是面向字符的输入流？（√）"><a href="#004-下面哪个流是面向字符的输入流？（√）" class="headerlink" title="004 - 下面哪个流是面向字符的输入流？（√）"></a>004 - 下面哪个流是面向字符的输入流？（√）</h3><p>A.BufferedWriter </p><p>字符输出流</p><p>B.FilelnputStream </p><p>字节输入流</p><p>C.ObjectInputStream </p><p>字节输入流</p><p>D.ImputStream Reader</p><p>字符输入流</p><p>正确答案是D</p><h3 id="005-以下说法错误的有：BCD（√）"><a href="#005-以下说法错误的有：BCD（√）" class="headerlink" title="005 - 以下说法错误的有：BCD（√）"></a>005 - 以下说法错误的有：BCD（√）</h3><p>A - 数组是一种对象</p><p>B - 数组属于一种原生类</p><p>C - int number&#x3D;[]&#x3D;{31,23,33,43,35,63}</p><p>D - 数组的大小可以任意改变</p><hr><p>解答：</p><p>A：数组是一种对象，正确</p><p>数组能够被Object接收，一切能够被Object接收的均为对象</p><p>数组能够调用Object的方法，因此数组的最顶层父类也是Object，因此数组是一种对象</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">int</span>[] a = <span class="hljs-keyword">new</span> <span class="hljs-title class_">int</span>[<span class="hljs-number">4</span>];<br><span class="hljs-comment">//a.length;  //对属性的引用不能当成语句</span><br><span class="hljs-type">int</span> <span class="hljs-variable">len</span> <span class="hljs-operator">=</span> a.length;  <span class="hljs-comment">//数组中保存一个字段, 表示数组的长度</span><br><br><span class="hljs-comment">//以下方法说明数组可以调用方法,java中的数组是对象.这些方法是Object中的方法,所以可以肯定,数组的最顶层父类也是Object</span><br>a.clone();<br>a.toString(); <br></code></pre></td></tr></table></figure><p>B：错误，数组不是原生类。</p><p>原生类只有8种，分别是int double boolean float byte short long char ；</p><p>C：语法错误</p><p>D：错误，数组的大小一开始就已经确定了 int[]test&#x3D;new test[2];</p><h3 id="006-不输入单元测试的内容是：（√）"><a href="#006-不输入单元测试的内容是：（√）" class="headerlink" title="006 - 不输入单元测试的内容是：（√）"></a>006 - 不输入单元测试的内容是：（√）</h3><p>A - 边界条件测试</p><p>B - 局部数据结构测试</p><p>C - 独立路径测试</p><p>D - 用户界面的测试</p><hr><p>答案选择D，用户界面测试不属于单元测试的内容</p><p>单元测试的内容：</p><ul><li>模块接口测试</li><li>局部数据结构测试</li><li>边界条件测试</li><li>独立路径测试</li></ul><h3 id="007-使用反射机制获取一个类的属性，下列关于getField0方法说法正确的是（√）"><a href="#007-使用反射机制获取一个类的属性，下列关于getField0方法说法正确的是（√）" class="headerlink" title="007 - 使用反射机制获取一个类的属性，下列关于getField0方法说法正确的是（√）"></a>007 - 使用反射机制获取一个类的属性，下列关于getField0方法说法正确的是（√）</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211141602317.png" alt="image-20221114160206258"></p><p>A，getField()方法，需要指定一个String类型的参数来指定要获取的属性名</p><p>B, 该方法不能获取私有属性，能够获取公有属性以及超类的属性</p><p>C，该方法不止能够获取公有属性，还能获取超类的属性</p><p>D，该方法不能获取私有属性，getDeclaredField（）才能获取私有属性</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211141605135.png" alt="image-20221114160505090" style="zoom:67%;" /><h3 id="008-现有3个变量boolean-a，boolean-b，int-c，请问一下哪个表达式合法？（）"><a href="#008-现有3个变量boolean-a，boolean-b，int-c，请问一下哪个表达式合法？（）" class="headerlink" title="008 - 现有3个变量boolean a，boolean b，int c，请问一下哪个表达式合法？（）"></a>008 - 现有3个变量boolean a，boolean b，int c，请问一下哪个表达式合法？（）</h3><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211141711018.png" alt="image-20221114171141933" style="zoom:50%;" /><p>A，按位或 | 的两边，可以是布尔类型，也可以是 int 等整数类型（合法）</p><p>B，逻辑或，短路或 || 的两边，只能是布尔类型（合法）</p><p>C，安慰异或 ^ 的两边ke可以是布尔类型，也可以是 int 等整数类型。（a^b）得到布尔类型，布尔类型不能和 int 类型进行异或（不合法）</p><p>D，按位与 &amp; 的两边可以是布尔类型，也可以是 int 等整数类型，但是不能int和布尔不能同时运算，两边必须同是整数类型或者同时布尔类型（不合法）</p><p>E，逻辑与 &amp;&amp; 的两边，只能是布尔类型（不合法）</p><h3 id="009-下面对JAVA反射机制的描述错误的是？A（√）"><a href="#009-下面对JAVA反射机制的描述错误的是？A（√）" class="headerlink" title="009 - 下面对JAVA反射机制的描述错误的是？A（√）"></a>009 - 下面对JAVA反射机制的描述错误的是？A（√）</h3><p>A - 能通过反射破坏枚举类型实现的单例模式</p><p>B - 通过反射能调用类的私有构造函数</p><p>C - 反射机制是通过调用该类加载进jvm后生成的Class对象来实现的</p><p>D - 反射调用能获得保留到运行时的注解信息</p><hr><p>B，反射能够调用类的私有构造函数。（正确）</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.lang.reflect.Constructor;<br><span class="hljs-keyword">import</span> java.lang.reflect.InvocationTargetException;<br><span class="hljs-keyword">import</span> java.lang.reflect.Method;<br> <br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Test</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-comment">//get Constructor</span><br>        <span class="hljs-type">Class</span> <span class="hljs-variable">clazz</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;T&quot;</span>);<br>        <span class="hljs-type">Constructor</span> <span class="hljs-variable">cons</span> <span class="hljs-operator">=</span> clazz.getDeclaredConstructor(<span class="hljs-literal">null</span>);<br>         <br>        <span class="hljs-comment">//set accessble to access private constructor</span><br>        cons.setAccessible(<span class="hljs-literal">true</span>);<br>        cons.newInstance(<span class="hljs-literal">null</span>);<br>    &#125;<br>&#125;<br> <br> <br><span class="hljs-keyword">class</span> <span class="hljs-title class_">T</span> &#123;<br>    <span class="hljs-keyword">private</span> <span class="hljs-title function_">T</span><span class="hljs-params">()</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;init T&quot;</span>);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><hr><p>C，反射机制是通过调用该类加载进jvm后生成的Class对象来实现的（正确）</p><hr><p>D，反射能拿到注解信息、泛型信息等。注解的原理就是基于反射。（正确）</p><hr><p>A，反射不能破坏枚举方式的单例。除枚举方式外, 其他方法都会通过反射的方式破坏单例。</p><h3 id="010-从文件中读取第10个字节，存到变量C中-√"><a href="#010-从文件中读取第10个字节，存到变量C中-√" class="headerlink" title="010 - 从文件中读取第10个字节，存到变量C中(√)"></a>010 - 从文件中读取第10个字节，存到变量C中(√)</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">FileInputStream</span> <span class="hljs-variable">in</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">FileInputStream</span>(<span class="hljs-string">&quot;a.txt&quot;</span>);<br>in.skip(<span class="hljs-number">9</span>);<span class="hljs-comment">// skip(long n)方法，跳过文件 n 个字节数</span><br><span class="hljs-type">int</span> <span class="hljs-variable">b</span> <span class="hljs-operator">=</span> in.read;<br></code></pre></td></tr></table></figure><h3 id="011-Java基本数据类型的默认初始值？（√）"><a href="#011-Java基本数据类型的默认初始值？（√）" class="headerlink" title="011 - Java基本数据类型的默认初始值？（√）"></a>011 - Java基本数据类型的默认初始值？（√）</h3><p>1、整数类型（byte、short、int、long）的默认值是0</p><p>2、float类型的默认值是0.0f</p><p>3、double类型的默认值是0.0d</p><p>4、char类型的默认值是&#x2F;u0000</p><p>5、boolean类型的默认值false</p><p>6、引用类型的变量是默认值为 null。 </p><p>7、数组引用类型的变量的默认值为 null</p><h3 id="012-枚举类型不支持-public-和-protected-修饰符的构造方法，因此构造函数一定要是-private-或-friendly-的。（√）"><a href="#012-枚举类型不支持-public-和-protected-修饰符的构造方法，因此构造函数一定要是-private-或-friendly-的。（√）" class="headerlink" title="012 - 枚举类型不支持 public 和 protected 修饰符的构造方法，因此构造函数一定要是 private 或 friendly 的。（√）"></a>012 - 枚举类型不支持 public 和 protected 修饰符的构造方法，因此构造函数一定要是 private 或 friendly 的。（√）</h3><h3 id="013-servlet的声明周期不包括：A（√）"><a href="#013-servlet的声明周期不包括：A（√）" class="headerlink" title="013 - servlet的声明周期不包括：A（√）"></a>013 - servlet的声明周期不包括：A（√）</h3><p>A - 开始</p><p>B - 请求处理</p><p>C - 销毁</p><p>D - 初始化</p><p>解析：</p><p>servlet的声明周期包含：初始化、处理请求、销毁</p><h3 id="014-方法重载体现了面向对象的多态性（√）"><a href="#014-方法重载体现了面向对象的多态性（√）" class="headerlink" title="014 - 方法重载体现了面向对象的多态性（√）"></a>014 - 方法重载体现了面向对象的多态性（√）</h3><h3 id="015-对数组进行初始化的知识点：（√）"><a href="#015-对数组进行初始化的知识点：（√）" class="headerlink" title="015 - 对数组进行初始化的知识点：（√）"></a>015 - 对数组进行初始化的知识点：（√）</h3><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211151218809.png" alt="image-20221115121836724" style="zoom:50%;" /> <p>对于数组大小的定义，不能放在左边。因此ABD三个选项都不对。</p><h3 id="016-反射可以运行时修改属性的值，但是不能修改方法（√）"><a href="#016-反射可以运行时修改属性的值，但是不能修改方法（√）" class="headerlink" title="016 - 反射可以运行时修改属性的值，但是不能修改方法（√）"></a>016 - 反射可以运行时修改属性的值，但是不能修改方法（√）</h3><h3 id="017-为了提高读写性能，可以采用什么流？（√）"><a href="#017-为了提高读写性能，可以采用什么流？（√）" class="headerlink" title="017 - 为了提高读写性能，可以采用什么流？（√）"></a>017 - 为了提高读写性能，可以采用什么流？（√）</h3><p>A.InputStream B.DatalnputStream C.OutputStream D.BufferedInputStream</p><hr><p>使用 BufferedInputStream，BufferedOutputStream 能够提高读写性能。因此本题选D</p><h3 id="018-servlet接口中有哪些方法？（√）"><a href="#018-servlet接口中有哪些方法？（√）" class="headerlink" title="018 - servlet接口中有哪些方法？（√）"></a>018 - servlet接口中有哪些方法？（√）</h3><p>Servlet接口定义了5种方法：</p><p>init()</p><p>service()</p><p>destroy()</p><p>getServletConfig()</p><p>getServletInfo()</p><h3 id="019-web请求的方法不包括？（√）"><a href="#019-web请求的方法不包括？（√）" class="headerlink" title="019 - web请求的方法不包括？（√）"></a>019 - web请求的方法不包括？（√）</h3><p>get、push、post、delete</p><hr><p>web中没有push这个请求方式，8种请求方式分别是：</p><p>get、post、head</p><p>put、delete、options、trace、connect</p><h3 id="020-下面的Java赋值语句正确的是？CD（√）"><a href="#020-下面的Java赋值语句正确的是？CD（√）" class="headerlink" title="020 - 下面的Java赋值语句正确的是？CD（√）"></a>020 - 下面的Java赋值语句正确的是？CD（√）</h3><p>A - float f &#x3D; 45.0;</p><p>B - Double d &#x3D; 100;</p><p>C - String s &#x3D; “hello, world\0”;</p><p>D - Integer i &#x3D; 1000;</p><hr><p>解答：</p><p>A不对，正确应该是:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">float</span> <span class="hljs-variable">f</span> <span class="hljs-operator">=</span> <span class="hljs-number">45</span>;<br><span class="hljs-type">float</span> <span class="hljs-variable">f</span> <span class="hljs-operator">=</span> <span class="hljs-number">45f</span>;<br><span class="hljs-type">float</span> <span class="hljs-variable">f</span> <span class="hljs-operator">=</span> <span class="hljs-number">45.0f</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100d</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0d</span>;<br></code></pre></td></tr></table></figure><p>即float类型的 45.1f 的 f 不能省略</p><p>B不对，因为int不能直接转成包装类型Double，正确应该是</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0</span>;<br><span class="hljs-type">Double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0d</span>;<br><br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100d</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0</span>;<br><span class="hljs-type">double</span> <span class="hljs-variable">d</span> <span class="hljs-operator">=</span> <span class="hljs-number">100.0d</span>;<br></code></pre></td></tr></table></figure><p>C 正确</p><p>D 正确</p><h3 id="021-在视图上不能完成的操作是C（√）"><a href="#021-在视图上不能完成的操作是C（√）" class="headerlink" title="021 - 在视图上不能完成的操作是C（√）"></a>021 - 在视图上不能完成的操作是C（√）</h3><p>A - 更新视图</p><p>B - 查询</p><p>C - 在视图上定义新的表</p><p>D - 在视图上定义新的视图</p><blockquote><p>  A：通过视图进行查询没有任何限制，通过它们进行数据修改时的限制也很少。对视图的操作和普通的表一样。—&gt; 定义出来的视图，可以进行修改。</p><p>  B：对视图的操作和普通的表一样。—&gt; 可以对视图进行查询操作。</p><p>  C：定义视图的筛选可以来自当前或其它数据库的一个或多个表，或者其它视图。—&gt; 视图可以来自于表或者视图。但是表不能来自于视图。</p><p>  D：可以在视图上定音新的视图。</p><p>  可以对视图进行增删改查操作。</p></blockquote><h3 id="022-如下哪（个）些特性功能不用于数据的查询优化？（√）"><a href="#022-如下哪（个）些特性功能不用于数据的查询优化？（√）" class="headerlink" title="022 - 如下哪（个）些特性功能不用于数据的查询优化？（√）"></a>022 - 如下哪（个）些特性功能不用于数据的查询优化？（√）</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">并行查询<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">索引<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">视图<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">分区<br></code></pre></td></tr></table></figure><p>选C。视图并不在数据库中以存储的数据值集形式存在，而仅仅是一个给用户展示的逻辑虚表，其在数据库中底层还是以完整的数据存储。视图不能优化数据的查询。目的是为了保证数据的安全和使得复杂的查询易于理解和使用。</p><p>数据的查询优化策略：</p><p>避免全表扫描，采用分区的形式，找到指定区域来避免全表查询。所以D正确。</p><p>建立索引，是数据信息进行排序的数据结构，目的是提高查询效率。所以B正确。</p><p>并行查询是从操作系统和CPU上给予多线程多核的操作，也属于提高查询效率的一种。所以A正确。</p><h3 id="023-以-MySQL-5-7-或更低版本为准的数据库中，如何正确选择和使用合适的数据类（√）"><a href="#023-以-MySQL-5-7-或更低版本为准的数据库中，如何正确选择和使用合适的数据类（√）" class="headerlink" title="023 - 以 MySQL 5.7 或更低版本为准的数据库中，如何正确选择和使用合适的数据类（√）"></a>023 - 以 MySQL 5.7 或更低版本为准的数据库中，如何正确选择和使用合适的数据类（√）</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">更小通常更好<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">尽量用最简单的数据类型<br></code></pre></td></tr></table></figure><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 1c">尽量不使用 <span class="hljs-literal">Null</span> 作为字段值<br></code></pre></td></tr></table></figure><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">字符串较长时优先用 <span class="hljs-built_in">Text</span> 数据类型<br></code></pre></td></tr></table></figure><p>正确答案：ABC</p><p>D选项，字符串较长时应采用varchar, 灵活可变长度。使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。尽量避免使用TEXT&#x2F;BLOB类型，查询时会使用临时表，导致严重的性能开销。</p><h3 id="024-给名字是zhangsan的用户分配对数据库studb中的stuinfo表的查询和插入数据权限的语句是-（√）"><a href="#024-给名字是zhangsan的用户分配对数据库studb中的stuinfo表的查询和插入数据权限的语句是-（√）" class="headerlink" title="024 - 给名字是zhangsan的用户分配对数据库studb中的stuinfo表的查询和插入数据权限的语句是:（√）"></a>024 - 给名字是zhangsan的用户分配对数据库studb中的stuinfo表的查询和插入数据权限的语句是:（√）</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">grant</span> <span class="hljs-keyword">select</span>, <span class="hljs-keyword">insert</span> <span class="hljs-keyword">on</span> studb.stuinfo <span class="hljs-keyword">to</span> <span class="hljs-string">&#x27;zhangsan&#x27;</span>@<span class="hljs-string">&#x27;localhost&#x27;</span>;<br></code></pre></td></tr></table></figure><h3 id="025-servlet的生命周期的先后顺序是？（√）"><a href="#025-servlet的生命周期的先后顺序是？（√）" class="headerlink" title="025 - servlet的生命周期的先后顺序是？（√）"></a>025 - servlet的生命周期的先后顺序是？（√）</h3><p>类加载 -&gt; 实例化 -&gt; 初始化 -&gt; 服务 -&gt; 销毁</p><h3 id="026-下面哪个选项不是-Spring-为简化-Java-开发采用的策略？答案：D（√）"><a href="#026-下面哪个选项不是-Spring-为简化-Java-开发采用的策略？答案：D（√）" class="headerlink" title="026 - 下面哪个选项不是 Spring 为简化 Java 开发采用的策略？答案：D（√）"></a>026 - 下面哪个选项不是 Spring 为简化 Java 开发采用的策略？答案：D（√）</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">基于POJO的轻量级和最小侵入性编程<br></code></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">通过依赖注入和面向接口实现松耦合 -&gt; IOC<br></code></pre></td></tr></table></figure><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">通过切面和模板减少样板式代码 -&gt; AOP<br></code></pre></td></tr></table></figure><figure class="highlight actionscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs actionscript">通过自定义类加载器实现<span class="hljs-keyword">class</span>动态加载<br></code></pre></td></tr></table></figure><p><strong>Spring框架为了简化开发的4大策略:</strong>  </p><p>  1.采用轻量级PoJo（Plain Ordinary Java Object – Java普通对象），最小侵入式编程。  </p><p>  2.依赖注入（DI）和面向接口编程实现松耦合。  </p><p>  3.基于切面和惯例进行声明式编程。  </p><p>  4.通过切面和模板减少样板式代码</p><h3 id="027-下列关于Spring事务管理的描述中，错误的是D（√）"><a href="#027-下列关于Spring事务管理的描述中，错误的是D（√）" class="headerlink" title="027 - 下列关于Spring事务管理的描述中，错误的是D（√）"></a>027 - 下列关于Spring事务管理的描述中，错误的是D（√）</h3><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">Spring提供了声明式事务、编程式事务两种事务管理方案。<br></code></pre></td></tr></table></figure><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs lasso">声明式事务，只需通过<span class="hljs-built_in">XML</span>或注解进行配置，即可实现对事务的管理。<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs sql">编程式事务，需要通过TransactionTemplate组件执行<span class="hljs-keyword">SQL</span>，达到管理事务的目的。<br></code></pre></td></tr></table></figure><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs">声明式事务优于编程式事务，应该一律采用声明式事务。<br></code></pre></td></tr></table></figure><p>两种方式，具体选择哪个要看具体的场景。</p><p>编程式事务的使用场景举例：在有些场景下，我们需要获取事务的状态，是执行成功了还是失败回滚了，那么使用声明式事务就不够用了，需要编程式事务。</p><p>事务控制的粒度：</p><ul><li><p>声明式事务:通过xml或注解方式进行事务的配置。最小只能配置到方法。 </p></li><li><p>编程式事务:代码中根据业务逻辑进行事务实现。可配置方法或代码块，粒度更小。</p></li></ul><h3 id="028-以下不属于mybatis的基本工作流程的是：C（√）"><a href="#028-以下不属于mybatis的基本工作流程的是：C（√）" class="headerlink" title="028 - 以下不属于mybatis的基本工作流程的是：C（√）"></a>028 - 以下不属于mybatis的基本工作流程的是：C（√）</h3><ul><li>创建SqlSessionFactory</li><li>使用SqlSessionFactory创建SqlSession</li><li>使用SqlSessionFactory执行数据库操作</li><li>调用session.commit()提交事务</li></ul><hr><p>应该是使用SqlSession执行数据库操作。</p><p>整个流程为：</p><ul><li>创建SqlSessionFactory（数据库连接池）</li><li>使用SqlSessionFactory创建SqlSession（单个连接）</li><li>使用SqlSession执行SQL语句（单个语句）</li><li>使用SqlSession提交事务：session.commit（）</li><li>关闭连接：SqlSession.close()</li></ul><h3 id="029-Spring容器装配的Bean默认作用域为：（√）"><a href="#029-Spring容器装配的Bean默认作用域为：（√）" class="headerlink" title="029 - Spring容器装配的Bean默认作用域为：（√）"></a>029 - Spring容器装配的Bean默认作用域为：（√）</h3><p>singleton 是Spring 容器默认的作用域，当一个Bean 的作用域为singleton 时，Spring 容器中只会存在一个共享的Bean 实例，并且所有对Bean 的请求，只要id 与该Bean 定义相匹配，就只会返回Bean 的同一个实例</p><h3 id="030-下列步骤中，不是创建进程所必须的步骤的是：（√）"><a href="#030-下列步骤中，不是创建进程所必须的步骤的是：（√）" class="headerlink" title="030 - 下列步骤中，不是创建进程所必须的步骤的是：（√）"></a>030 - 下列步骤中，不是创建进程所必须的步骤的是：（√）</h3><p>A - 为进程分配CPU</p><p>B - 建立一个PCB</p><p>C - 为进程分配内存</p><p>D - 将PCB加入就绪队列</p><hr><p>答案选A，创建进程之后，不需要立即为进程分配CPU。因为进程可以在就绪队列中等待。</p><p>创建进程所必须的四步是：</p><p>1、申请PCB</p><p>2、申请资源</p><p>3、初始化PCB</p><p>4、将PCB加入就绪队列</p><h3 id="031-静态优先级算法，优先级在进程创建时确定，之后不再改变。"><a href="#031-静态优先级算法，优先级在进程创建时确定，之后不再改变。" class="headerlink" title="031 - 静态优先级算法，优先级在进程创建时确定，之后不再改变。"></a>031 - 静态优先级算法，优先级在进程创建时确定，之后不再改变。</h3><h3 id="032-bash中，需要将脚本demo-sh的标准输出和标准错误输出重定向至文件demo-log，以下哪些用法是正确的（）"><a href="#032-bash中，需要将脚本demo-sh的标准输出和标准错误输出重定向至文件demo-log，以下哪些用法是正确的（）" class="headerlink" title="032 - bash中，需要将脚本demo.sh的标准输出和标准错误输出重定向至文件demo.log，以下哪些用法是正确的（）"></a>032 - bash中，需要将脚本demo.sh的标准输出和标准错误输出重定向至文件demo.log，以下哪些用法是正确的（）</h3><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">bash <span class="hljs-built_in">demo</span>.sh &amp;&gt;<span class="hljs-built_in">demo</span>.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">bash <span class="hljs-built_in">demo</span>.sh &gt;&amp; <span class="hljs-built_in">demo</span>.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">bash <span class="hljs-built_in">demo</span>.sh &gt;<span class="hljs-built_in">demo</span>.<span class="hljs-built_in">log</span> <span class="hljs-number">2</span>&gt;&amp;<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs maxima">bash <span class="hljs-built_in">demo</span>.sh <span class="hljs-number">2</span>&gt;<span class="hljs-built_in">demo</span>.<span class="hljs-built_in">log</span> <span class="hljs-number">1</span>&gt;<span class="hljs-built_in">demo</span>.<span class="hljs-built_in">log</span><br></code></pre></td></tr></table></figure><p>正确答案是A、B、C</p><h3 id="033-以下哪一项不是性能测试需要关注的指标：（√）"><a href="#033-以下哪一项不是性能测试需要关注的指标：（√）" class="headerlink" title="033 - 以下哪一项不是性能测试需要关注的指标：（√）"></a>033 - 以下哪一项不是性能测试需要关注的指标：（√）</h3><p>A 系统吞吐量</p><p>B 操作响应时间</p><p>C 系统的并发数</p><p>D 业务逻辑正确性</p><p>正确答案选D。性能测试，侧重点在性能，先不关注正确性。</p><hr><p>性能测试有哪些指标需要测试？</p><p>1、并发用户：并发一般分两种情况。一种是严格意义的并发，即所有的用户在同一时刻做同一件事情或者操作，这种操作一般指做同一类型的业务。另外一种并发是广义范围的并发，这种并发与前一种并发的区别是，尽管多个用户对系统发生了请求或者进行了操作，但是这些请求或者操作可以是相同的，也可以是不同的。</p><p>2、并发用户数量：在同一时刻与服务器进行交互的在线用户数量。</p><p>3、请求响应时间：指的是客户端发出请求到得到响应的整个过程的时间。</p><p>4、 吞吐量：指的是在一次性能测试过程中网络上传输的数据量的总和。吞吐量&#x2F;传输时间，就是吞吐率。</p><p>5、吞吐率：单位时间内网络上传输的数据量，也可以指单位时间内处理的客户端请求数量。它是衡量网络性能的重要指标。通常情况下，吞吐率用“请求数&#x2F;秒”。</p><p>6、TPS：每秒钟系统能够处理的交易或者事物的数量。它是衡量系统处理能力的重要指标。</p><p>7、点击率：每秒钟用户向Web服务器提交的HTTP请求数。这个指标是Web应用特有的一个指标：Web应用是“请求-响应”模式，用户发出一次申请，服务器就要处理一次，所以点击是Web应用能够处理的交易的最小单位，如果把每次点击定义为一个交易，点击率和TPS就是一个概念。容易看出，点击率越大，对服务器的压力也越大。点击率只是一个性能参考指标，重要的是分析点击是产生的影响。</p><p>8、资源利用率：指的是对不同系统资源的使用程度，例如服务器的CPU利用率，磁盘利用率等。资源利用率是分析系统性能指标进而改善性能的主要依据。</p><h3 id="034-下列哪个git命令不是合并代码用的：（√）"><a href="#034-下列哪个git命令不是合并代码用的：（√）" class="headerlink" title="034 - 下列哪个git命令不是合并代码用的：（√）"></a>034 - 下列哪个git命令不是合并代码用的：（√）</h3><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">git pull <span class="hljs-comment">--rebase</span><br></code></pre></td></tr></table></figure><figure class="highlight cos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cos">git <span class="hljs-keyword">merge</span><br></code></pre></td></tr></table></figure><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">git cherry-pick</span><br></code></pre></td></tr></table></figure><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ebnf"><span class="hljs-attribute">git blame</span><br></code></pre></td></tr></table></figure><p>正确选项：D</p><h3 id="035-瀑布模型的优缺点（√）"><a href="#035-瀑布模型的优缺点（√）" class="headerlink" title="035 - 瀑布模型的优缺点（√）"></a>035 - 瀑布模型的优缺点（√）</h3><p>优点：</p><p>为项目提供了按阶段划分的检查点（告知了检查哪些地方）</p><p>当前阶段完成之后，您只需要去关注后续阶段（只需要专注于当前阶段的开发）</p><p>可在迭代模型中应用瀑布模型</p><p>缺点：</p><p>在项目各个阶段之间极少有反馈（阶段之间沟通少）</p><p>只有在项目生命周期的后期才能看到结果（最后才知道结果，这个时候出错了可能就来不及了）</p><p>通过过多的强制完成日期和里程碑来跟踪各个项目阶段</p><h3 id="036-实施-DevOps-的前提条件是什么？（√）"><a href="#036-实施-DevOps-的前提条件是什么？（√）" class="headerlink" title="036 -  实施 DevOps 的前提条件是什么？（√）"></a>036 -  实施 DevOps 的前提条件是什么？（√）</h3><p>团队成员之间的适当沟通。<br>至少一个版本控制软件。<br>自动化测试。<br>自动化部署。</p><h3 id="037-下面不属于创建型模式的有（√）"><a href="#037-下面不属于创建型模式的有（√）" class="headerlink" title="037 - 下面不属于创建型模式的有（√）"></a>037 - 下面不属于创建型模式的有（√）</h3><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">抽象工厂模式（ Abstract Factory ）<span class="hljs-regexp">//</span> 创建型模式<br></code></pre></td></tr></table></figure><figure class="highlight oxygene"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs oxygene">工厂方法模式（Factory <span class="hljs-keyword">Method</span>）// 创建型模式<br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">适配器模式 （Adapter）<span class="hljs-regexp">//</span> 结构型模式<br></code></pre></td></tr></table></figure><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk">单例模式（Singleton）<span class="hljs-regexp">//</span> 创建型模式<br></code></pre></td></tr></table></figure><h3 id="038-下列不属于行为模式的是（√）"><a href="#038-下列不属于行为模式的是（√）" class="headerlink" title="038 - 下列不属于行为模式的是（√）"></a>038 - 下列不属于行为模式的是（√）</h3><p>工厂模式 &#x2F;&#x2F; 创建型模式</p><p>策略模式 &#x2F;&#x2F; 行为模式</p><p>观察者模式 &#x2F;&#x2F; 行为模式</p><p>备忘录模式 &#x2F;&#x2F; 行为模式</p><h3 id="039-当我们想将抽象部分和实现部分分离时，使它们可以独立变化，可以使用（√）"><a href="#039-当我们想将抽象部分和实现部分分离时，使它们可以独立变化，可以使用（√）" class="headerlink" title="039 - 当我们想将抽象部分和实现部分分离时，使它们可以独立变化，可以使用（√）"></a>039 - 当我们想将抽象部分和实现部分分离时，使它们可以独立变化，可以使用（√）</h3><p>桥接模式(Bridge pattern): 使用桥接模式通过将实现和抽象放在两个不同的类层次中而使它们可以独立改变</p><p>设计模式分为三种类型：<br> （1）创建型模式：单例模式、抽象工厂模式、建造者模式、工厂模式、原型模式。<br> （2）结构型模式：适配器模式、桥接模式、装饰模式、组合模式、外观模式、享元模式、代理模式。<br> （3）行为型模式：模版方法模式、命令模式、迭代器模式、观察者模式、中介者模式、备忘录模式、解释器模式、状态模式、策略模式、职责链模式、访问者模式。</p><h3 id="040-以下哪些不是容器化相对于虚拟化的优势？（√）"><a href="#040-以下哪些不是容器化相对于虚拟化的优势？（√）" class="headerlink" title="040 - 以下哪些不是容器化相对于虚拟化的优势？（√）"></a>040 - 以下哪些不是容器化相对于虚拟化的优势？（√）</h3><p>A - 容器是轻量级的（√）</p><p>B - 容器提供实时配置和可扩展性（√）</p><p>C - 容器完全使用沙箱机制（×）</p><p>D - 容器具有更好的资源利用率（√）</p><h3 id="041-以下哪些框架不能用于服务发现B（√）"><a href="#041-以下哪些框架不能用于服务发现B（√）" class="headerlink" title="041 - 以下哪些框架不能用于服务发现B（√）"></a>041 - 以下哪些框架不能用于服务发现B（√）</h3><ul><li><pre><code class="hljs">Zookeeper<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>+   ```<br>    Nginx（Nginx可以作为收口服务器，反向代理，动静分离，负载均衡）<br></code></pre></td></tr></table></figure></code></pre></li><li><p>&#96;&#96;&#96;<br>etcd</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>+   ```<br>    Consul<br></code></pre></td></tr></table></figure></li></ul><blockquote><p>  用于服务发现的框架&#x2F;软件有哪些？</p><p>  Zookeeper:分布式应用程序协调服务</p><p>  Consul:实现分布式系统的服务发现与配置，支持健康检查。（consult，商议）</p><p>  ETCD：分布式存储，用于服务发现、共享配置以及一致性保障。Etcd 是 CoreOS 基于 Raft 协议开发的分布式键值对存储 (key-value peer store) ，设计用来可靠而快速的保存关键数据并提供访问。</p><p>  DNS：DNS可以用于实现服务发现</p></blockquote><h3 id="042-以下不属于敏捷开发工具集的是A（√）"><a href="#042-以下不属于敏捷开发工具集的是A（√）" class="headerlink" title="042 - 以下不属于敏捷开发工具集的是A（√）"></a>042 - 以下不属于敏捷开发工具集的是A（√）</h3><ul><li>聊天：跟相关人员聊天</li><li>站会：三个问题，简洁有效的小团队沟通方式</li><li>看板：直观反应工作进度，反映流程遵守情况，反映流程缺陷</li><li>用户故事：站在用户的角度讲需求</li></ul><h3 id="043-关于索引下面哪些描述是正确的：（B、D、E、F）（√）"><a href="#043-关于索引下面哪些描述是正确的：（B、D、E、F）（√）" class="headerlink" title="043 - 关于索引下面哪些描述是正确的：（B、D、E、F）（√）"></a>043 - 关于索引下面哪些描述是正确的：（B、D、E、F）（√）</h3><ul><li><pre><code class="hljs"> 索引是为了提高查询效率的，通过建立索引查询效率会得到提高<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>+   ```<br>     索引对数据插入的效率有一定的影响<br></code></pre></td></tr></table></figure></code></pre></li><li><pre><code class="hljs"> 唯一索引是一种特殊的索引，表中的行的物理顺序与索引顺序一致，且不允许两行数据在索引列上有相同的值<figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>+   ```<br>     每个表都必须具有一个主键索引<br></code></pre></td></tr></table></figure></code></pre></li><li><p>&#96;&#96;&#96;<br> 对于数据重复度高，值范围有限的列如果建索引建议使用位图索引</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey"><br>+   ```<br>     可以在多个列上建立联合索引<br></code></pre></td></tr></table></figure></li></ul><p>A选项 并不是建立了索引就会提高索引查找速度</p><p>C选项 唯一索引的列的物理顺序 并不和索引顺序一致，只有聚集索引列的物理顺序和逻辑顺 序 一致，一个表也只能有一个聚集索引</p>]]></content>
    
    
    <categories>
      
      <category>求职招聘</category>
      
      <category>笔试展开</category>
      
    </categories>
    
    
    <tags>
      
      <tag>复盘</tag>
      
      <tag>计算机基础知识</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>设计模式题目整理</title>
    <link href="/posts/3208215547/"/>
    <url>/posts/3208215547/</url>
    
    <content type="html"><![CDATA[<h3 id="001-对象间存在一对多关系，当一个对象被修改时，则会自动通知它的依赖对象，采用以下哪种设计模式最好？"><a href="#001-对象间存在一对多关系，当一个对象被修改时，则会自动通知它的依赖对象，采用以下哪种设计模式最好？" class="headerlink" title="001 - 对象间存在一对多关系，当一个对象被修改时，则会自动通知它的依赖对象，采用以下哪种设计模式最好？"></a>001 - 对象间存在一对多关系，当一个对象被修改时，则会自动通知它的依赖对象，采用以下哪种设计模式最好？</h3><p>建造者模式</p><p>观察者模式</p><p>策略模式</p><p>代理模式</p><p>正确答案：B，观察者模式</p><hr><p>观察者模式：对象间存在一对多关系，如果一个对象被修改时，会自动通知它的依赖对象。</p><p>一个目标物件管理所有相依于它的观察者物件，并且在它本身的状态改变时主动发出通知。这通常透过呼叫各观察者所提供的方法来实现。</p><p>此种模式通常被用来实现事件处理系统。 观察者设计模式定义了对象间的一种一对多的组合关系，以便一个对象的状态发生变化时，所有依赖于它的对象都得到通知并自动刷新。</p><hr><p>建造者模式：使用对各简单的对象一步一步构建出一个复杂对象</p><p>将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示</p><p>建造者 搭房子（一步一步复杂化）</p><hr><p>代理模式：指一个类别可以作为其他东西的接口</p><p>类别可以作为其它东西的接口。代理者可以作任何东西的接口：网上连接、存储器中的大对象、文件或其它昂贵或无法复制的资源</p><hr><p>策略模式：将每一个算法封装起来，使得每个算法可以相互替代，使得算法本身和使用算法的客户端分割开来相互独立</p><p>针对一组算法， 将每一个算法封装到同一个接口的独立的实现类中，  使得它们可以互换</p>]]></content>
    
    
    <categories>
      
      <category>方法论</category>
      
      <category>设计模式</category>
      
      <category>笔记</category>
      
      <category>0 - 知识点收集</category>
      
    </categories>
    
    
    <tags>
      
      <tag>题目整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>敏捷开发 - 敏捷软件开发理论及流程</title>
    <link href="/posts/4181991940/"/>
    <url>/posts/4181991940/</url>
    
    <content type="html"><![CDATA[<h2 id="0-知识积累"><a href="#0-知识积累" class="headerlink" title="0 - 知识积累"></a>0 - 知识积累</h2><h3 id="0-1-敏捷开发的工具集有哪些？"><a href="#0-1-敏捷开发的工具集有哪些？" class="headerlink" title="0.1 - 敏捷开发的工具集有哪些？"></a>0.1 - 敏捷开发的工具集有哪些？</h3><p><strong>首先，敏捷开发是一种过程控制论，通俗的说，就是一种做事情的方法。</strong></p><ol><li>它适用于软件，因为软件是软的，可以改。要是硬件，改起来就没那么方便了</li><li>它适用于客户不知道自己要啥的情况，其实，这样的客户占绝大多数。因为客户不知道要啥，所以你需要不断帮客户弄明白他到底想要啥。。。换句话说，你需要和客户沟通，合作，倾听反馈，持续改进。。。</li><li>它适用于竞争激烈的市场，这样的情况下，赶在竞争对手前交付一个不完美但至少能用的产品非常重要。</li><li>它适用于快速变化的市场，你在埋头造一辆汽车的时候，客户已经想开飞机满天飞了，这就需要你能一步步的把汽车改成飞机，还能按时交付。</li><li>它适用于在一个地方办公的小团队，一般10个人以内。这样能使敏捷中主要的沟通方式“Face to Face” 是可行的。</li></ol><p><strong>其次，敏捷开发是一套工具集，里面有形形色色的工具，你可以不搞敏捷，但可以用那么一两个来提高工作效率</strong></p><p>比如：</p><p>1.站会：三个问题，简洁有效的小团队沟通方式</p><p>2.看板：直观反映工作进度，反映流程遵守情况，反映流程缺陷</p><p>3.演示，计划，反思会：适合于小团队的协作和优化反馈方式</p><p>4.用户故事：站在用户的角度讲需求</p><blockquote><p>  习题 - 以下不属于敏捷开发工具集的是（A）</p><ul><li>聊天：跟相关人员聊天</li><li>站会：三个问题，简洁有效的小团队沟通方式</li><li>看板：直观反应工作进度，反映流程遵守情况，反映流程缺陷</li><li>用户故事：站在用户的角度讲需求</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>方法论</category>
      
      <category>开发流程</category>
      
      <category>笔记</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>敏捷开发</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>DevOps笔记</title>
    <link href="/posts/843030040/"/>
    <url>/posts/843030040/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  摘自文章：</p><p>  参考文章：</p><p>  <a href="https://cloud.tencent.com/developer/article/1510975">2019 DevOps 必备面试题——容器化和虚拟化 - 腾讯云 - CODING（√）</a></p></blockquote><h2 id="1-容器化和虚拟化-Docker"><a href="#1-容器化和虚拟化-Docker" class="headerlink" title="1 - 容器化和虚拟化 - Docker"></a>1 - 容器化和虚拟化 - Docker</h2><h3 id="1-1-什么是容器？"><a href="#1-1-什么是容器？" class="headerlink" title="1.1 - 什么是容器？"></a>1.1 - 什么是容器？</h3><hr><p>容器提供了从开发人员的笔记本电脑到测试环境、从类生产环境到生产环境一致的运行环境。</p><p>接下来给出容器的定义，容器由一个完整的运行环境组成：将一个应用程序，以及它所有的依赖项、库和其他二进制文件，以及运行它所需的配置文件打包到一起。将应用平台及其依赖项容器化，可以消除操作系统版本和底层基础架构间的差异。</p><h3 id="1-2-容器化相比虚拟化有哪些优势？"><a href="#1-2-容器化相比虚拟化有哪些优势？" class="headerlink" title="1.2 - 容器化相比虚拟化有哪些优势？"></a>1.2 - 容器化相比虚拟化有哪些优势？</h3><hr><p>以下是容器化相对于虚拟化的优势：</p><ul><li>容器提供实时资源调配和可拓展性，虚拟机提供的资源调配速度较慢（虚拟机的速度慢）</li><li>与虚拟机相比，容器是轻量级的（容器更轻量，不需要虚拟机）</li><li>与容器相比，虚拟机的性能有限（虚拟机配置受限）</li><li>与虚拟机相比，容器具有更好的资源利用率（虚拟机受固定的资源分配限制）</li></ul><h3 id="1-3-容器（在我们的例子中指-Docker）与管理程序虚拟化（vSphere）有何不同？有哪些好处？"><a href="#1-3-容器（在我们的例子中指-Docker）与管理程序虚拟化（vSphere）有何不同？有哪些好处？" class="headerlink" title="1.3 - 容器（在我们的例子中指 Docker）与管理程序虚拟化（vSphere）有何不同？有哪些好处？"></a>1.3 - 容器（在我们的例子中指 Docker）与管理程序虚拟化（vSphere）有何不同？有哪些好处？</h3><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211181617804.png" alt="image-20221118161726717"></p><h3 id="1-4-什么是-Docker-镜像？"><a href="#1-4-什么是-Docker-镜像？" class="headerlink" title="1.4 - 什么是 Docker 镜像？"></a>1.4 - 什么是 Docker 镜像？</h3><hr><p>Docker 镜像是 Docker 容器的来源，换句话说，Docker 镜像用于创建容器。使用 build 命令创建镜像，当开始运行时，它们将生成一个容器。镜像存储在 Docker 注册表中，如 registry.hub.docker.com，因为它们可能变得非常大，镜像被设计成由其他镜像层组成，允许在通过网络传输镜像时发送最小的数据量。</p><h3 id="1-5-什么是Docker容器？"><a href="#1-5-什么是Docker容器？" class="headerlink" title="1.5 - 什么是Docker容器？"></a>1.5 - 什么是Docker容器？</h3><hr><p>这是一个非常重要的问题，所以请确保回答不偏离主题，我建议你遵循以下格式：</p><p>Docker 容器包括应用程序及其所有依赖项，但与其他容器共享内核，在主机操作系统的用户空间中作为独立进程运行。Docker 容器不绑定于任何特定的基础架构：它们可以在任何计算机、任何基础架构和任何云上运行。</p><p>现在解释如何创建 Docker 容器：可以通过创建 Docker 镜像然后运行它来创建 Docker 容器，也可以使用 Dockerhub 上已存在的 Docker 镜像。</p><p>Docker 容器基本上是 Docker 镜像的运行时实例。</p><h3 id="1-6-什么是-Docker-Hub？"><a href="#1-6-什么是-Docker-Hub？" class="headerlink" title="1.6 - 什么是 Docker Hub？"></a>1.6 - 什么是 Docker Hub？</h3><hr><p>Docker Hub 是一个基于云的注册表服务，允许你链接到代码仓库，构建并测试你的镜像，存储手动推送的镜像以及指向 Docker 云的链接，以便你可以将镜像部署到主机。它为整个开发流程中的容器镜像发现、分发和变更管理、用户和团队协作以及工作流自动化提供了集中的资源管理。</p><h3 id="1-7-Docker-与其他容器技术有何不同？"><a href="#1-7-Docker-与其他容器技术有何不同？" class="headerlink" title="1.7 - Docker 与其他容器技术有何不同？"></a>1.7 - Docker 与其他容器技术有何不同？</h3><hr><p>据我所知，你的答案应该包括以下几点：</p><p>Docker 容器易于在云上部署。与其他技术相比，它可以在相同硬件上运行更多应用程序，使开发人员可以轻松地快速创建可立即运行的容器化应用程序，并使管理和部署应用程序变得更加容易。你甚至可以与你的应用程序共享容器。</p><p>你也可以补充更多要点，但要确保上述内容在你的回答中。</p><h3 id="1-8-什么是-Docker-Swarm？"><a href="#1-8-什么是-Docker-Swarm？" class="headerlink" title="1.8 - 什么是 Docker Swarm？"></a>1.8 - 什么是 Docker Swarm？</h3><hr><p>它是 Docker 的本地集群，它将 Docker 主机池转换为单个虚拟 Docker 主机。Docker Swarm 提供标准的 Docker API，任何已经与 Docker 守护进程通信的工具都可以使用 Swarm 透明地扩展到多个主机。</p><p>我还建议你提及一些支持的工具：</p><p>Dokku<br>Docker Compose<br>Docker Machine<br>Jenkins</p><h3 id="1-9-Dockerfile-的用途是什么？"><a href="#1-9-Dockerfile-的用途是什么？" class="headerlink" title="1.9 - Dockerfile 的用途是什么？"></a>1.9 - Dockerfile 的用途是什么？</h3><hr><p>Docker 可以通过从 Dockerfile 中读取指令来自动构建镜像。</p><p>Dockerfile 是一个配置文件，其中包含用户可以在命令行上调用以组合镜像的所有命令。使用 Docker 构建用户可以创建一个连续执行多个命令行指令的自动构建。</p><h3 id="1-10-你在过去的职位中是如何使用-Docker-的？"><a href="#1-10-你在过去的职位中是如何使用-Docker-的？" class="headerlink" title="1.10 - 你在过去的职位中是如何使用 Docker 的？"></a>1.10 - 你在过去的职位中是如何使用 Docker 的？</h3><hr><p>这里需要解释你如何使用 Docker 来帮助进行快速部署，以及如何编写 Docker 脚本，并配合其他工具如 Puppet、Chef 或 Jenkins 等来使用 Docker。如果你在 Docker 方面缺乏以往的实践经验，但是在类似的领域中有过使用其他工具的经验，请诚实地告知并解释相同的内容。在这种情况下，如果您可以将其他工具与 Docker 在功能方面进行比较，这是有意义的。</p><h3 id="1-11-如何创建-Docker-容器？"><a href="#1-11-如何创建-Docker-容器？" class="headerlink" title="1.11 - 如何创建 Docker 容器？"></a>1.11 - 如何创建 Docker 容器？</h3><hr><p>关于这个我建议你给出一个直观的回答——可以使用以下命令使用 Docker 镜像创建 Docker 容器：  </p><p><strong>docker run -t -i <image name> <command name></strong>  </p><p>此命令将创建并启动容器。</p><p>你还应该提到，如果要检查主机上所有状态为正在运行的容器的列表，需要使用以下命令：  </p><p><strong>docker ps -a</strong></p><h3 id="1-12-如何停止并重新启动-Docker-容器？"><a href="#1-12-如何停止并重新启动-Docker-容器？" class="headerlink" title="1.12 - 如何停止并重新启动 Docker 容器？"></a>1.12 - 如何停止并重新启动 Docker 容器？</h3><hr><p>要停止 Docker 容器，可以使用以下命令：  </p><p>docker stop <container ID></p><p>现在重新启动 Docker 容器，可以使用以下命令：  </p><p>docker restart <container ID></p><h3 id="1-13-Docker-容器可以扩展到什么程度？"><a href="#1-13-Docker-容器可以扩展到什么程度？" class="headerlink" title="1.13 - Docker 容器可以扩展到什么程度？"></a>1.13 - Docker 容器可以扩展到什么程度？</h3><hr><p>像 Google 和 Twitter 这样的大型网络部署，以及像 Heroku 和 DotCloud 这样的平台供应商都运行在容器技术上，规模达到几十万甚至数百万个容器并行运行。</p><h3 id="1-14-Docker-在什么平台上运行？"><a href="#1-14-Docker-在什么平台上运行？" class="headerlink" title="1.14 - Docker 在什么平台上运行？"></a>1.14 - Docker 在什么平台上运行？</h3><hr><p>首先，我会说 Docker 通常在 Linux 和云平台上运行，然后我会提到以下 Linux 供应商：</p><ul><li>Ubuntu 12.04、13.04 等</li><li>Fedora 19&#x2F;20+</li><li>RHEL 6.5+</li><li>CentOS 6+</li><li>Gentoo</li><li>ArchLinux</li><li>openSUSE 12.3+</li><li>CRUX 3.0+</li></ul><h3 id="1-15-当-Docker-容器退出时，我会丢失数据吗？"><a href="#1-15-当-Docker-容器退出时，我会丢失数据吗？" class="headerlink" title="1.15 - 当 Docker 容器退出时，我会丢失数据吗？"></a>1.15 - 当 Docker 容器退出时，我会丢失数据吗？</h3><hr><p>你可以这样回答：当 Docker 容器退出时，我不会丢失我的数据。在你明确删除容器前，应用程序写入磁盘的任何数据都将保留在其容器中。即使在容器停止后，容器的文件系统仍然存在。</p>]]></content>
    
    
    <categories>
      
      <category>方法论</category>
      
      <category>开发流程</category>
      
      <category>笔记</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>DevOps</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>如何减小主从复制从库的延迟时间</title>
    <link href="/posts/4069224514/"/>
    <url>/posts/4069224514/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  参考：</p><p>  <a href="https://www.csdn.net/tags/MtTaMg4sMjc5ODIwLWJsb2cO0O0O.html">以下哪个不能有效减少从库延迟时间（√）</a></p><p>  <a href="https://blog.csdn.net/a13568hki/article/details/104003596">mysql如何减少主从复制延迟？（√）</a></p><p>  <a href="https://www.jianshu.com/p/04c8c4f62cd5">MySQL 主从延迟问题思路（√）</a></p></blockquote><h3 id="一、题目：以下哪个不能有效减小从库延迟时间"><a href="#一、题目：以下哪个不能有效减小从库延迟时间" class="headerlink" title="一、题目：以下哪个不能有效减小从库延迟时间"></a>一、题目：以下哪个不能有效减小从库延迟时间</h3><p>A.主库进行update操作时where后条件没有索引，添加索引。（添加索引，减轻主库的压力，因此能够及时的将数据传给从库）</p><p>B.主库有大事物，增加缓存，异步写入数据库，减少直接对db的大量写入（主库添加缓存，缓解直接写入DB的压力）</p><p>C.主库并发更新写入频繁，从库设置innodb_flush_log_at_trx_commit&#x3D;1及sync_binlog&#x3D;1</p><p>D.数据库中存在大量myisam表，修改表结构为innodb存储引擎的表（myisam不支持行级锁，并发会锁住整个表，效率低）</p><p>正确答案:C</p><hr><h3 id="二、MySQL主从复制延迟较大，主要从以下几个方面来考虑"><a href="#二、MySQL主从复制延迟较大，主要从以下几个方面来考虑" class="headerlink" title="二、MySQL主从复制延迟较大，主要从以下几个方面来考虑"></a>二、MySQL主从复制延迟较大，主要从以下几个方面来考虑</h3><p><strong>1 - 从库的问题</strong></p><p>1.1 - 从库硬件比主库差，导致复制延迟。主库写binlog日志到文件的时候，是顺序写入到磁盘，顺序写入速度是很快，避免了磁盘随机寻址。从库的同步线程(Slave_IO_Running)，将binlog在slave上执行的时候，实际上是随机的，速度肯定要慢点。</p><p>解决方案是：从库配置比主库更好的配置。</p><ul><li>从库使用高性能主机。包括cpu强悍、内存加大。避免使用虚拟云主机，使用物理主机，这样提升了i&#x2F;o方面性。</li><li>从库使用SSD磁盘。机械硬盘是靠磁头旋转到指定位置来读数据、写数据。转来转去的，我们叫做i&#x2F;o。磁盘i&#x2F;o存在速度瓶颈。固态硬盘是一个电子设备，电子设备不需要机械旋转，读写固态硬盘上任意位置的数据，速度都是一样的。</li></ul><p>1.2 - 从库的负载大，从库的读请求太频繁，来不及复制主库的数据。</p><p>解决方案是：使用多台slave来分摊读请求，再从这些slave中取一台专用的服务器。只作为备份用，不进行其他任何操作。</p><p><strong>2 - 主从之间的问题</strong></p><p>2.1 - 传输的条件差主从复制单线程，如果主库写并发太大，来不及传送到从库就会导致延迟。</p><p>解决方案是：更高版本的mysql可以支持多线程复制</p><p>2.2 - 网络延迟原因</p><p>解决方案是：通常配置以上2个参数可以减少网络问题导致的主从数据同步延迟</p><p><code>–slave-net-timeout=seconds</code> 单位为秒 默认设置为 3600秒，参数含义是：当slave从主数据库读取log数据失败后，等待多久重新建立连接并获取数据</p><p><code>–master-connect-retry=seconds</code> 单位为秒 默认设置为 60秒，参数含义是：当重新建立主从连接时，如果连接建立失败，间隔多久后重试</p><p><strong>3 - 主库的问题</strong></p><p>3.1 - 主库的负载大：主库读写压力大，导致复制延迟。当主库的TPS并发较高时，产生的DDL(修改类的sql语句)数量，超过了slave机器sql线程所能承受的能力，那么延时就会产生了。</p><p>解决方案1是：架构的前端要加buffer及缓存层，通过缓存层来缓解直接进行磁盘IO的压力</p><p>解决方案2是：</p><p>主库是写，对数据安全性较高，因此刷盘策略可以设置为sync_binlog&#x3D;1，innodb_flush_log_at_trx_commit &#x3D; 1</p><p>而从库是读，则不需要这么高的数据安全，完全可以将 sync_binlog 设置为 0 或者关闭binlog</p><p>innodb_flushlog也可以设置为0来提高sql的执行效率</p><p>3.2 - 主库的负载大：慢SQL语句过多（慢SQL导致主库的压力过大，来不及传送到从库，就会导致延迟）</p><p>解决方案是：优化慢SQL</p><hr><blockquote><p>  参考：</p><p>  <a href="https://support.huaweicloud.com/bestpractice-rds/rds_02_0010.html">innodb_flush_log_at_trx_commit和sync_binlog参数详解（√）</a></p></blockquote><h3 id="三、innodb-flush-log-at-trx-commit-和-sync-binlog-参数解析"><a href="#三、innodb-flush-log-at-trx-commit-和-sync-binlog-参数解析" class="headerlink" title="三、innodb_flush_log_at_trx_commit 和 sync_binlog 参数解析"></a>三、innodb_flush_log_at_trx_commit 和 sync_binlog 参数解析</h3><p>“innodb_flush_log_at_trx_commit”和“sync_binlog”两个参数是控制RDS for MySQL磁盘写入策略以及数据安全性的关键参数。当两个参数为不同值时，在性能，安全角度下会产生不同的影响。</p><hr><p><strong>innodb_flush_log_at_trx_commit 参数解析：</strong></p><ul><li>0：每一秒，（1）将日志缓冲区的数据写入到日志文件，（2）将日志文件刷到磁盘；该模式不受事务提交的影响</li><li>1：每次事务提交，（1）将日志缓冲区的数据写入到日志文件，（2）将日志文件刷到磁盘；该模式不受时间的影响</li><li>2：每次事务提交，（1）将日志缓冲区的数据写入到日志文件，每一秒，将日志文件刷到磁盘；</li></ul><p>注：</p><ul><li>日志缓冲区在mysql进程的用户空间，日志文件在内核空间的缓冲区，刷盘则是将内核中的缓冲区数据持久化到磁盘中。</li><li>MySQL宕机，不影响内核空间和磁盘中的数据</li><li>操作系统宕机，不影响磁盘中的数据</li></ul><p>说明：</p><ul><li>刷盘这一步最消耗时间，因此刷盘越频繁，越慢。一个事务的时间一般远小于 1 秒。</li><li>当设置为0，该模式速度最快，因为将数据写入内核和刷盘是定时的，不受频繁事务提交的影响。但不太安全，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。</li><li>当设置为1，该模式是最安全的，但也是最慢的一种方式。在mysqld服务崩溃或者服务器主机宕机的情况下，日志缓存区只有可能丢失最多一个语句或者一个事务；</li><li>当设置为2，该模式速度较快，较取值为0情况下更安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失；</li></ul><p>说明：</p><ul><li>在主从复制的场景下，主机负责写入，因此对数据安全的保证要求较高，因此一般将主机的 innodb_flush_log_at_trx_commit 参数设置为1，而不会将从机的该参数设置为1。</li></ul><hr><p><strong>sync_binlog&#x3D;1 or N 参数解析：</strong></p><p>默认情况下，并不是每次写入时都将binlog日志文件与磁盘同步。因此如果操作系统或服务器崩溃，有可能binlog中最后的语句丢失。</p><p>为了防止这种情况，你可以使用<strong>“sync_binlog”</strong>全局变量（1是最安全的值，但也是最慢的），使binlog在每N次binlog日志文件写入后与磁盘同步。</p><hr><p><strong>推荐配置组合：</strong></p><table><thead><tr><th align="center">innodb_flush_log_at_trx_commit</th><th align="center">sync_binlog</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">1</td><td align="center">适合数据安全性要求非常高，而且磁盘写入能力足够支持业务。</td></tr><tr><td align="center">1</td><td align="center">0</td><td align="center">适合数据安全性要求高，磁盘写入能力支持业务不足，允许备库落后或无复制。</td></tr><tr><td align="center">2</td><td align="center">0&#x2F;N(0&lt;N&lt;100)</td><td align="center">适合数据安全性要求低，允许丢失一点事务日志，允许复制延迟。</td></tr><tr><td align="center">0</td><td align="center">0</td><td align="center">磁盘写能力有限，无复制或允许复制延迟较长。</td></tr></tbody></table><p>“innodb_flush_log_at_trx_commit”和“sync_binlog”两个参数设置为1的时候，安全性最高，写入性能最差。在mysqld服务崩溃或者服务器主机宕机的情况下，日志缓存区只有可能丢失最多一个语句或者一个事务。但是会导致频繁的磁盘写入操作，因此该模式也是最慢的一种方式。</p>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
      <category>mysql</category>
      
      <category>笔记</category>
      
      <category>主从复制和读写分离</category>
      
    </categories>
    
    
    <tags>
      
      <tag>主从复制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SQL中DQL，DML，DDL，DCL，TCL的区别</title>
    <link href="/posts/410845037/"/>
    <url>/posts/410845037/</url>
    
    <content type="html"><![CDATA[<h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><p>SQL语言根据操作性质一共分为5大类</p><p>DQL：select（记录的操作）</p><p>DML：insert、delete、update（记录的操作）</p><p>DDL：create、alter、drop、truncate（表的操作）</p><p>DCL：grant、revoke（权限的操作）</p><p>TCL：commint、rollback、savepoint、set transaction（事务的操作）</p><h3 id="1-数据查询语言，DQL（Query）"><a href="#1-数据查询语言，DQL（Query）" class="headerlink" title="1 - 数据查询语言，DQL（Query）"></a>1 - 数据查询语言，DQL（Query）</h3><h4 id="1-1-作用"><a href="#1-1-作用" class="headerlink" title="1.1 - 作用"></a>1.1 - 作用</h4><p>从数据库&#x2F;表中查找字段的值</p><h4 id="1-2-主要命令-select（查）"><a href="#1-2-主要命令-select（查）" class="headerlink" title="1.2 - 主要命令 - select（查）"></a>1.2 - 主要命令 - select（查）</h4><blockquote><p>  select 语法：</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs sql"><span class="hljs-keyword">SELECT</span>         select_list<br><br>[ <span class="hljs-keyword">INTO</span>             new_table ]<br><br><span class="hljs-keyword">FROM</span>             table_source<br><br>[ <span class="hljs-keyword">WHERE</span>        search_condition ]<br><br>[ GROUPBY   group_by_expression ]<br><br>[ <span class="hljs-keyword">HAVING</span>        search_condition ]<br><br>[ ORDERBY    order_expression [ <span class="hljs-keyword">ASC</span> <span class="hljs-operator">|</span> <span class="hljs-keyword">DESC</span> ] ]<br></code></pre></td></tr></table></figure><hr><h3 id="2-数据操纵语言，DML（manipulation）"><a href="#2-数据操纵语言，DML（manipulation）" class="headerlink" title="2 - 数据操纵语言，DML（manipulation）"></a>2 - 数据操纵语言，DML（manipulation）</h3><h4 id="2-1-作用"><a href="#2-1-作用" class="headerlink" title="2.1 - 作用"></a>2.1 - 作用</h4><p>对数据库的数据进行相关操作（对表中的记录进行操作）</p><h4 id="2-2-主要命令-insert、delete、update（增删改）"><a href="#2-2-主要命令-insert、delete、update（增删改）" class="headerlink" title="2.2 - 主要命令 - insert、delete、update（增删改）"></a>2.2 - 主要命令 - insert、delete、update（增删改）</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">insert</span>语法<br><span class="hljs-keyword">INSERT</span>      <span class="hljs-keyword">INTO</span>      表名（列<span class="hljs-number">1</span>，列<span class="hljs-number">2</span>，...）     <span class="hljs-keyword">VALUES</span>    （值<span class="hljs-number">1</span>，值<span class="hljs-number">1</span>，...）<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">delete</span>语法<br> <span class="hljs-keyword">DELETE</span>    <span class="hljs-keyword">FROM</span>    表名      <span class="hljs-keyword">WHERE</span>       列名 <span class="hljs-operator">=</span> 值<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">update</span>语法<br> <span class="hljs-keyword">UPDATE</span>    表名     <span class="hljs-keyword">SET</span>     列名 <span class="hljs-operator">=</span> 新值       <span class="hljs-keyword">WHERE</span>       列名称 <span class="hljs-operator">=</span> 某值<br></code></pre></td></tr></table></figure><hr><h3 id="3-数据定义语言，DDL（definition）"><a href="#3-数据定义语言，DDL（definition）" class="headerlink" title="3 - 数据定义语言，DDL（definition）"></a>3 - 数据定义语言，DDL（definition）</h3><h4 id="3-1-作用"><a href="#3-1-作用" class="headerlink" title="3.1 - 作用"></a>3.1 - 作用</h4><p>主要是对表进行操作</p><h4 id="3-2-主要命令-create、alter、drop、truncate（建立表、修改表（增加列、更改列、删除列）、删除表）"><a href="#3-2-主要命令-create、alter、drop、truncate（建立表、修改表（增加列、更改列、删除列）、删除表）" class="headerlink" title="3.2 - 主要命令 - create、alter、drop、truncate（建立表、修改表（增加列、更改列、删除列）、删除表）"></a>3.2 - 主要命令 - create、alter、drop、truncate（建立表、修改表（增加列、更改列、删除列）、删除表）</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">create</span>语法<br><span class="hljs-keyword">CREATE</span>       <span class="hljs-keyword">table</span>         表名<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">alter</span>语法<br><br><span class="hljs-keyword">ALTER</span>       <span class="hljs-keyword">table</span>       表名<br><br><span class="hljs-keyword">ADD</span>           (test_id    number)           <span class="hljs-comment">--增加列</span><br><br>#<span class="hljs-comment">----------------------------------------------------------------------------</span><br><br><span class="hljs-keyword">ALTER</span>       <span class="hljs-keyword">table</span>       表名<br><br>MODIFY     (test_id    number)          <span class="hljs-comment">--更改列</span><br><br>#<span class="hljs-comment">----------------------------------------------------------------------------</span><br><br><span class="hljs-keyword">ALTER</span>       <span class="hljs-keyword">table</span>       表名<br><br><span class="hljs-keyword">DELETE</span>     (test_id  )                         <span class="hljs-comment">--删除列</span><br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">drop</span>语法<br><span class="hljs-keyword">DROP</span>             <span class="hljs-keyword">table</span>         表名<br></code></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs sql">#<span class="hljs-keyword">truncate</span>语法<br><span class="hljs-keyword">TRUNCATE</span>     <span class="hljs-keyword">table</span>        表名<br></code></pre></td></tr></table></figure><hr><h3 id="4-数据控制语言，DCL（Control）"><a href="#4-数据控制语言，DCL（Control）" class="headerlink" title="4 - 数据控制语言，DCL（Control）"></a>4 - 数据控制语言，DCL（Control）</h3><h4 id="4-1-作用"><a href="#4-1-作用" class="headerlink" title="4.1 - 作用"></a>4.1 - 作用</h4><p>DCL用来设置或更改数据库用户或角色权限</p><h4 id="4-2-主要命令-grant、revoke（授权和撤销）"><a href="#4-2-主要命令-grant、revoke（授权和撤销）" class="headerlink" title="4.2 - 主要命令 - grant、revoke（授权和撤销）"></a>4.2 - 主要命令 - grant、revoke（授权和撤销）</h4><p>注意： 在默认状态下，只有sysadmin,dbcreator,db_owner或db_securityadmin等人员才有权力执行DCL</p><hr><h3 id="5-事务控制语言，TCL（Transaction-Control）"><a href="#5-事务控制语言，TCL（Transaction-Control）" class="headerlink" title="5 - 事务控制语言，TCL（Transaction Control）"></a>5 - 事务控制语言，TCL（Transaction Control）</h3><h4 id="5-1-作用"><a href="#5-1-作用" class="headerlink" title="5.1 - 作用"></a>5.1 - 作用</h4><p>用于数据库事务的相关操作</p><h4 id="5-2-主要命令-commit、rollback、savepoint、set-transaction（提交、回滚、设置保存点、设置事务选项）"><a href="#5-2-主要命令-commit、rollback、savepoint、set-transaction（提交、回滚、设置保存点、设置事务选项）" class="headerlink" title="5.2 - 主要命令 - commit、rollback、savepoint、set transaction（提交、回滚、设置保存点、设置事务选项）"></a>5.2 - 主要命令 - commit、rollback、savepoint、set transaction（提交、回滚、设置保存点、设置事务选项）</h4><h2 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h2><p>下列四组SQL命令，全部属于数据定义语句的命令是（A）。</p><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">CREATE</span>，<span class="hljs-keyword">DROP</span>，<span class="hljs-keyword">ALTER</span><br></code></pre></td></tr></table></figure><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">CREATE</span>，<span class="hljs-keyword">DROP</span>，<span class="hljs-keyword">UPDATE</span><br></code></pre></td></tr></table></figure><figure class="highlight n1ql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs n1ql"><span class="hljs-keyword">CREATE</span>，<span class="hljs-keyword">DROP</span>，<span class="hljs-keyword">GRANT</span><br></code></pre></td></tr></table></figure><figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs gauss"><span class="hljs-keyword">CREATE</span>，<span class="hljs-built_in">DROP</span>，<span class="hljs-built_in">SELECT</span><br></code></pre></td></tr></table></figure><p>下列哪些属于DQL语句的命令(BCD)</p><figure class="highlight arcade"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs arcade">A、<span class="hljs-built_in">INSERT</span><br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-tag">B</span>、WHERE<br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">C、<span class="hljs-selector-tag">FROM</span><br></code></pre></td></tr></table></figure><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ada">D、<span class="hljs-keyword">SELECT</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>数据库</category>
      
      <category>mysql</category>
      
      <category>笔记</category>
      
      <category>SQL语法</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>解决一直困扰的坚果云在资源管理器中左边栏显示问题</title>
    <link href="/posts/4124768445/"/>
    <url>/posts/4124768445/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  参考：<a href="https://loesspie.com/2021/01/19/win10-remove-jianguoyun/">https://loesspie.com/2021/01/19/win10-remove-jianguoyun/</a></p><p>  日期：2022年11月8日alec实测有效</p></blockquote><p>其实一直在用坚果云，但仅仅是自动同步一些文件和文件夹，并不需要在我的资源管理器里有那么强的存在感<br>同时现在用 库 功能比较多，汇总各类相同属性&#x2F;用途的文件或文件夹不要太方便。所以它在那儿占着位置看着碍眼了<br>但网上一堆方法都无效，只能自己上，搞定之后记录下</p><p>首先是通过全局搜索坚果云，找到了N个条目，然后尝试修改其名称（默认项），如改为坚果云云</p><p>当找到正确的条目之后，打开资源管理器，会发现左边栏会变成坚果云云，然后将此项的ispinned从1改成0，会发现资源管理器左栏的图标已经消失</p><p>消失之后，再全局搜索坚果云云，将其改成坚果云.</p><p>如此，便整洁许多</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211081724039.png" alt="image-20221108172445976"></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>windows</category>
      
      <category>inbox</category>
      
    </categories>
    
    
    <tags>
      
      <tag>坚果云</tag>
      
      <tag>资源管理器</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>typora + GitHub + jsDelivr + PicGo</title>
    <link href="/posts/3859538395/"/>
    <url>/posts/3859538395/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考文章：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/168729465">PicGo图床与Typora（PicGo+Typora+GitHub的完整设置）（√）</a></li><li><a href="https://www.cnblogs.com/sitoi/p/11848816.html">GitHub + jsDelivr + PicGo + Imagine 打造稳定快速、高效免费图床（√）</a></li></ul></blockquote><p>1.github创建仓库</p><p>github创建仓库</p><p>2.获取github账号token</p><ul><li>点击头像，选中头像列表中的Settings</li><li>进入Settings,点击Developer Settings</li><li>点击Personal access tokens 过后再点击 Generate new token</li><li>在Note中取一个名字，选中repo这个框过后直接点击完成（Generate token）</li><li>生成token，记住这个令牌一定要复制保存，如果没有保存的删除重新来一遍</li></ul><p>3.下载picgo</p><ul><li>github官网下载picgo</li></ul><p>4.配置picgo图床</p><ul><li>点击左边图床设计，选择GitHub图床，具体配置如下</li><li>设定仓库名，填写：<strong>GitHub名&#x2F;库名</strong></li><li>分支，<strong>默认填master</strong></li><li>设定Token，<strong>刚才保存的token令牌</strong></li><li>指定存储路径，<strong>默认填img&#x2F;</strong></li><li>点击确定和设为默认图床</li></ul><p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221221205516662.png" alt="image-20221221205516662"></p><p>5.设置picgo</p><ul><li>打开<code>开机自启</code></li><li>打开<code>时间戳重命名</code></li><li>关闭<code>上传后自动复制url</code></li></ul>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>typora</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>9 - fluid添加看板娘</title>
    <link href="/posts/2499736958/"/>
    <url>/posts/2499736958/</url>
    
    <content type="html"><![CDATA[<p>（1）</p><p>下载 <a href="https://github.com/stevenjoezhang/live2d-widget">张书樵大神的项目</a>，解压到本地博客目录的 <code>themes/fluid/source/alec_diy/</code> 下，修改文件夹名为 <code>live2d-widget</code>，修改项目中的 <code>autoload.js</code> 文件，如下将：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">const</span> live2d_path = <span class="hljs-string">&#x27;https://cdn.jsdelivr.net/gh/stevenjoezhang/live2d-widget/&#x27;</span><br></code></pre></td></tr></table></figure><p>改为</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-keyword">const</span> live2d_path = <span class="hljs-string">&#x27;/alec_diy/live2d-widget/&#x27;</span><br></code></pre></td></tr></table></figure><p>（2）在主题配置文件的<code>custom_js</code>和<code>custom_css</code>中加入：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">custom_js:</span><br>  <span class="hljs-comment"># live2d的js文件（2）</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">/alec_diy/live2d-widget/autoload.js</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js</span><br><br><span class="hljs-attr">custom_css:</span><br>  <span class="hljs-comment"># live2d的css文件（1）</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">//cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css</span><br></code></pre></td></tr></table></figure><p>也可以将上面两个依赖文件下载到本地然后再引入</p><p>（3）在主题配置文件中,新增如下内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">live2d: enable:</span> <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><p>（4）想修改看板娘大小、位置、格式、文本内容等，可查看并修改 <code>waifu-tips.js</code> 、 <code>waifu-tips.json</code> 和 <code>waifu.css</code></p><p>（5）效果展示：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212251419659.png" alt="image-20221225141923800"></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>8 - fluid添加音乐页面和音乐播放器</title>
    <link href="/posts/3564283204/"/>
    <url>/posts/3564283204/</url>
    
    <content type="html"><![CDATA[<h2 id="√-添加悬浮音乐播放器"><a href="#√-添加悬浮音乐播放器" class="headerlink" title="[√] 添加悬浮音乐播放器"></a>[√] 添加悬浮音乐播放器</h2><hr><p>在fluid的主题配置文件中，提供了自定义html的位置，因此直接在主题配置文件中添加html代码</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">custom_html:</span> <span class="hljs-string">&#x27;</span><br><span class="hljs-string"> &lt;!--音乐--&gt;</span><br><span class="hljs-string">  &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css&quot;&gt;</span><br><span class="hljs-string">  &lt;script src=&quot;https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="hljs-string">  &lt;script src=&quot;https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="hljs-string">  &lt;div id=&quot;player&quot; class=&quot;aplayer aplayer-withlist aplayer-fixed&quot; data-id=&quot;3025663508&quot; data-server=&quot;netease&quot; data-type=&quot;playlist&quot; data-order=&quot;random&quot; data-fixed=&quot;true&quot; data-listfolded=&quot;true&quot; data-theme=&quot;#2D8CF0&quot;&gt;&lt;/div&gt;</span><br><span class="hljs-string">&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="√-添加音乐页面"><a href="#√-添加音乐页面" class="headerlink" title="[√] 添加音乐页面"></a>[√] 添加音乐页面</h2><hr><p>（1）在主题配置文件中的<code>nemu</code>处，添加音乐配置：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;音乐&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/playlist/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-music&quot;</span> &#125;<br></code></pre></td></tr></table></figure><p>具体为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">menu:</span><br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;home&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-home-fill&quot;</span> &#125;<br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;archive&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/archives/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-archive-fill&quot;</span> &#125;<br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;category&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/categories/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-category-fill&quot;</span> &#125;<br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;tag&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/tags/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-tags-fill&quot;</span> &#125;<br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;音乐&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/playlist/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-music&quot;</span> &#125;<br>  <span class="hljs-bullet">-</span> &#123; <span class="hljs-attr">key:</span> <span class="hljs-string">&quot;about&quot;</span>, <span class="hljs-attr">link:</span> <span class="hljs-string">&quot;/about/&quot;</span>, <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;iconfont icon-user-fill&quot;</span> &#125;<br></code></pre></td></tr></table></figure><p>（2）使用命令创建音乐界面，比如命名为playlist</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shel">hexo new page playlist<br></code></pre></td></tr></table></figure><p>（3）打开网站根目录source\playlist\index.md根据<a href="https://github.com/MoePlayer/hexo-tag-aplayer">hexo-tag-aplayer</a>文档书写即可</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs markdown">示例：<br>&#123;% meting &quot;7729098320&quot; &quot;netease&quot; &quot;playlist&quot; %&#125;<br><br>&#123;% meting &quot;2305794885&quot; &quot;netease&quot; &quot;playlist&quot; %&#125;<br><br>&#123;% meting &quot;7724497259&quot; &quot;tencent&quot; &quot;playlist&quot; &quot;theme:#3F51B5&quot; &quot;mutex:true&quot; &quot;preload:auto&quot; %&#125;<br></code></pre></td></tr></table></figure><h2 id="√-参考"><a href="#√-参考" class="headerlink" title="[√] 参考"></a>[√] 参考</h2><hr><p><a href="https://cloud.tencent.com/developer/article/1953317%EF%BC%88%E2%88%9A%EF%BC%89">https://cloud.tencent.com/developer/article/1953317（√）</a></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>7 - fluid主题添加关于页面</title>
    <link href="/posts/295724089/"/>
    <url>/posts/295724089/</url>
    
    <content type="html"><![CDATA[<h2 id="√-添加关于页面步骤"><a href="#√-添加关于页面步骤" class="headerlink" title="[√] 添加关于页面步骤"></a>[√] 添加关于页面步骤</h2><hr><p>（1）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hexo new page &quot;about&quot;<br></code></pre></td></tr></table></figure><p>（2）修改主题配置文件，将<code>menu</code>中<code>about</code>前的注释去掉。</p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>6 - fluid主题无侵入式方式添加页面焦点监控文字</title>
    <link href="/posts/3644508848/"/>
    <url>/posts/3644508848/</url>
    
    <content type="html"><![CDATA[<h2 id="√-步骤"><a href="#√-步骤" class="headerlink" title="[√] 步骤"></a>[√] 步骤</h2><hr><h4 id="√-添加自定义monitortext-ejs文件"><a href="#√-添加自定义monitortext-ejs文件" class="headerlink" title="[√] 添加自定义monitortext.ejs文件"></a>[√] 添加自定义monitortext.ejs文件</h4><hr><p>在<code>blog\source\_inject\</code>文件夹中，新增文件<code>monitortext.ejs</code>，用于向页面中添加焦点监控代码，其内容为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;% if(theme.fun_features.monitortext.enable) &#123; %&gt;<br>&lt;script type=&quot;text/javascript&quot;&gt;<br>  /*窗口监视*/<br>  var originalTitle = document.title;<br>  window.onblur = function()&#123;document.title = &quot;&lt;%- theme.fun_features.monitortext.text %&gt;&quot;&#125;;<br>  window.onfocus = function()&#123;document.title = originalTitle&#125;;<br>&lt;/script&gt;<br>  &lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><h4 id="√-注入上述代码"><a href="#√-注入上述代码" class="headerlink" title="[√] 注入上述代码"></a>[√] 注入上述代码</h4><hr><p>在文件<code>blog\scripts\page.js</code>中，添加以下代码：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// 添加页面焦点监控文字</span><br>injects.<span class="hljs-property">bodyBegin</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;monitortext&#x27;</span>, <span class="hljs-string">&quot;source/_inject/monitortext.ejs&quot;</span>);<br></code></pre></td></tr></table></figure><p>快照为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs js">hexo.<span class="hljs-property">extend</span>.<span class="hljs-property">filter</span>.<span class="hljs-title function_">register</span>(<span class="hljs-string">&#x27;theme_inject&#x27;</span>, <span class="hljs-keyword">function</span>(<span class="hljs-params">injects</span>) &#123;<br>    injects.<span class="hljs-property">bodyBegin</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;default&#x27;</span>, <span class="hljs-string">&quot;source/_inject/bodyBegin.ejs&quot;</span>);<br>    injects.<span class="hljs-property">header</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;video-banner&#x27;</span>, <span class="hljs-string">&#x27;source/_inject/header.ejs&#x27;</span>, &#123; <span class="hljs-attr">key</span>: <span class="hljs-string">&#x27;value&#x27;</span> &#125;, -<span class="hljs-number">1</span>);<br>    <span class="hljs-comment">// 添加页面焦点监控文字(here)</span><br>    injects.<span class="hljs-property">bodyBegin</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;monitortext&#x27;</span>, <span class="hljs-string">&quot;source/_inject/monitortext.ejs&quot;</span>);<br>  &#125;);<br></code></pre></td></tr></table></figure><h4 id="√-编辑主题配置文件"><a href="#√-编辑主题配置文件" class="headerlink" title="[√] 编辑主题配置文件"></a>[√] 编辑主题配置文件</h4><hr><ul><li>编辑fluid 主题配置文件，在 <code>fun_features</code> 项下添加</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">monitortext:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">text:</span> <span class="hljs-string">失去焦点显示该文字</span><br></code></pre></td></tr></table></figure><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212241646975.png" alt="image-20221224113744839" style="zoom: 80%;" />]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>5 - fluid无需代码改动的基本+美化设置</title>
    <link href="/posts/135347359/"/>
    <url>/posts/135347359/</url>
    
    <content type="html"><![CDATA[<h2 id="√-主题配置文件"><a href="#√-主题配置文件" class="headerlink" title="[√] 主题配置文件"></a>[√] 主题配置文件</h2><hr><h4 id="√-修改浏览器标签的图标"><a href="#√-修改浏览器标签的图标" class="headerlink" title="[√] 修改浏览器标签的图标"></a>[√] 修改浏览器标签的图标</h4><hr><p>（1）在<code>blog/themes/fluid/source/img/</code>中，添加自己喜欢的图片，并命名为<code>photo.png</code></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212241646455.png" alt="image-20221223112505185"></p><p>（2）修改主题配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">#---------------------------</span><br><span class="hljs-comment"># 全局</span><br><span class="hljs-comment"># Global</span><br><span class="hljs-comment">#---------------------------</span><br><br><span class="hljs-comment"># 用于浏览器标签的图标</span><br><span class="hljs-comment"># Icon for browser tab</span><br><span class="hljs-attr">favicon:</span> <span class="hljs-string">/img/photo.png</span><br><br><span class="hljs-comment"># 用于苹果设备的图标</span><br><span class="hljs-comment"># Icon for Apple touch</span><br><span class="hljs-attr">apple_touch_icon:</span> <span class="hljs-string">/img/photo.png</span><br></code></pre></td></tr></table></figure><p>（3）效果展示：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212241646456.png" alt="image-20221223112704543"></p><h4 id="√-副标题美化"><a href="#√-副标题美化" class="headerlink" title="[√] 副标题美化"></a>[√] 副标题美化</h4><hr><p>修改打字机效果速度：80</p><p>指定页面使用打字机效果</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 一些好玩的功能</span><br><span class="hljs-comment"># Some fun features</span><br><span class="hljs-attr">fun_features:</span><br>  <span class="hljs-comment"># 为 subtitle 添加打字机效果</span><br>  <span class="hljs-comment"># Typing animation for subtitle</span><br>  <span class="hljs-attr">typing:</span><br>    <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br><br>    <span class="hljs-comment"># 打印速度，数字越大越慢</span><br>    <span class="hljs-comment"># Typing speed, the larger the number, the slower</span><br>    <span class="hljs-attr">typeSpeed:</span> <span class="hljs-number">80</span><br><br>    <span class="hljs-comment"># 游标字符</span><br>    <span class="hljs-comment"># Cursor character</span><br>    <span class="hljs-attr">cursorChar:</span> <span class="hljs-string">&quot;_&quot;</span><br><br>    <span class="hljs-comment"># 是否循环播放效果</span><br>    <span class="hljs-comment"># If true, loop animation</span><br>    <span class="hljs-attr">loop:</span> <span class="hljs-literal">false</span><br><br>    <span class="hljs-comment"># 在指定页面开启，不填则在所有页面开启</span><br>    <span class="hljs-comment"># Enable in specified page, all pages by default</span><br>    <span class="hljs-comment"># Options: home | post | tag | category | about | links | page | 404</span><br>    <span class="hljs-attr">scope:</span> [<span class="hljs-string">home</span>]<br></code></pre></td></tr></table></figure><h4 id="√-为文章内容中的当前标题添加锚图标"><a href="#√-为文章内容中的当前标题添加锚图标" class="headerlink" title="[√] 为文章内容中的当前标题添加锚图标"></a>[√] 为文章内容中的当前标题添加锚图标</h4><hr><p>（1）设置</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 为文章内容中的标题添加锚图标</span><br><span class="hljs-comment"># Add an anchor icon to the title on the post page</span><br><span class="hljs-attr">anchorjs:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">element:</span> <span class="hljs-string">h1,h2,h3,h4,h5,h6</span><br>  <span class="hljs-comment"># Options: left | right</span><br>  <span class="hljs-attr">placement:</span> <span class="hljs-string">left</span><br>  <span class="hljs-comment"># Options: hover | always | touch</span><br>  <span class="hljs-attr">visible:</span> <span class="hljs-string">hover</span><br>  <span class="hljs-comment"># Options: § | # | ❡</span><br>  <span class="hljs-attr">icon:</span> <span class="hljs-string">&quot;-&gt;&quot;</span><br><br></code></pre></td></tr></table></figure><p>（2）效果展示</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212241646457.png" alt="image-20221223220959466"></p><h4 id="√-加载进度条调整宽度和颜色"><a href="#√-加载进度条调整宽度和颜色" class="headerlink" title="[√] 加载进度条调整宽度和颜色"></a>[√] 加载进度条调整宽度和颜色</h4><hr><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 加载进度条</span><br><span class="hljs-comment"># Progress bar when loading</span><br><span class="hljs-attr">progressbar:</span><br>  <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br>  <span class="hljs-attr">height_px:</span> <span class="hljs-number">5</span><br>  <span class="hljs-attr">color:</span> <span class="hljs-string">&quot;#29d&quot;</span><br>  <span class="hljs-comment"># See: https://github.com/rstacruz/nprogress</span><br>  <span class="hljs-attr">options:</span> &#123; <span class="hljs-attr">showSpinner:</span> <span class="hljs-literal">false</span>, <span class="hljs-attr">trickleSpeed:</span> <span class="hljs-number">100</span> &#125;<br><br></code></pre></td></tr></table></figure><p>效果展示</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212241646458.png" alt="image-20221223221218379"></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4 - fluid页脚增加网站运行时间</title>
    <link href="/posts/2108047675/"/>
    <url>/posts/2108047675/</url>
    
    <content type="html"><![CDATA[<p>Fluid 1.8.4 版本支持自定义页脚内容了，本文记录页脚添加网站运行时间的方法。</p><h2 id="√-步骤"><a href="#√-步骤" class="headerlink" title="[√] 步骤"></a>[√] 步骤</h2><hr><h4 id="√-添加js文件"><a href="#√-添加js文件" class="headerlink" title="[√] 添加js文件"></a>[√] 添加js文件</h4><hr><p>（1）js文件内容：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs js">!(<span class="hljs-keyword">function</span>(<span class="hljs-params"></span>) &#123;<br>  <span class="hljs-comment">/** 计时起始时间，自行修改 **/</span><br>  <span class="hljs-keyword">var</span> start = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>(<span class="hljs-string">&quot;2020/01/01 00:00:00&quot;</span>);<br><br>  <span class="hljs-keyword">function</span> <span class="hljs-title function_">update</span>(<span class="hljs-params"></span>) &#123;<br>    <span class="hljs-keyword">var</span> now = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Date</span>();<br>    now.<span class="hljs-title function_">setTime</span>(now.<span class="hljs-title function_">getTime</span>()+<span class="hljs-number">250</span>);<br>    days = (now - start) / <span class="hljs-number">1000</span> / <span class="hljs-number">60</span> / <span class="hljs-number">60</span> / <span class="hljs-number">24</span>;<br>    dnum = <span class="hljs-title class_">Math</span>.<span class="hljs-title function_">floor</span>(days);<br>    hours = (now - start) / <span class="hljs-number">1000</span> / <span class="hljs-number">60</span> / <span class="hljs-number">60</span> - (<span class="hljs-number">24</span> * dnum);<br>    hnum = <span class="hljs-title class_">Math</span>.<span class="hljs-title function_">floor</span>(hours);<br>    <span class="hljs-keyword">if</span>(<span class="hljs-title class_">String</span>(hnum).<span class="hljs-property">length</span> === <span class="hljs-number">1</span> )&#123;<br>      hnum = <span class="hljs-string">&quot;0&quot;</span> + hnum;<br>    &#125;<br>    minutes = (now - start) / <span class="hljs-number">1000</span> /<span class="hljs-number">60</span> - (<span class="hljs-number">24</span> * <span class="hljs-number">60</span> * dnum) - (<span class="hljs-number">60</span> * hnum);<br>    mnum = <span class="hljs-title class_">Math</span>.<span class="hljs-title function_">floor</span>(minutes);<br>    <span class="hljs-keyword">if</span>(<span class="hljs-title class_">String</span>(mnum).<span class="hljs-property">length</span> === <span class="hljs-number">1</span> )&#123;<br>      mnum = <span class="hljs-string">&quot;0&quot;</span> + mnum;<br>    &#125;<br>    seconds = (now - start) / <span class="hljs-number">1000</span> - (<span class="hljs-number">24</span> * <span class="hljs-number">60</span> * <span class="hljs-number">60</span> * dnum) - (<span class="hljs-number">60</span> * <span class="hljs-number">60</span> * hnum) - (<span class="hljs-number">60</span> * mnum);<br>    snum = <span class="hljs-title class_">Math</span>.<span class="hljs-title function_">round</span>(seconds);<br>    <span class="hljs-keyword">if</span>(<span class="hljs-title class_">String</span>(snum).<span class="hljs-property">length</span> === <span class="hljs-number">1</span> )&#123;<br>      snum = <span class="hljs-string">&quot;0&quot;</span> + snum;<br>    &#125;<br>    <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&quot;timeDate&quot;</span>).<span class="hljs-property">innerHTML</span> = <span class="hljs-string">&quot;本站安全运行&amp;nbsp&quot;</span>+dnum+<span class="hljs-string">&quot;&amp;nbsp天&quot;</span>;<br>    <span class="hljs-variable language_">document</span>.<span class="hljs-title function_">getElementById</span>(<span class="hljs-string">&quot;times&quot;</span>).<span class="hljs-property">innerHTML</span> = hnum + <span class="hljs-string">&quot;&amp;nbsp小时&amp;nbsp&quot;</span> + mnum + <span class="hljs-string">&quot;&amp;nbsp分&amp;nbsp&quot;</span> + snum + <span class="hljs-string">&quot;&amp;nbsp秒&quot;</span>;<br>  &#125;<br><br>  <span class="hljs-title function_">update</span>();<br>  <span class="hljs-built_in">setInterval</span>(update, <span class="hljs-number">1000</span>);<br>&#125;)();<br></code></pre></td></tr></table></figure><p>在调用该js代码之后，会执行每1秒循环调用<code>update()</code>这个函数，</p><p>在这个函数中，比如执行如下语句：</p><p><code>document.getElementById(&quot;timeDate&quot;).innerHTML = &quot;本站安全运行&amp;nbsp&quot;+dnum+&quot;&amp;nbsp天&quot;;</code></p><p>通过执行这个语句，可以例如将页面内容<code>&lt;span id=&quot;timeDate&quot;&gt;载入天数...&lt;/span&gt;</code>替换，替换为计算出来的天数内容。</p><p>（2）将<code>var start = new Date(&quot;2020/01/01 00:00:00&quot;);</code>一行修改为自己的网站开始时间。</p><p>（3）js文件存放位置</p><p>需要说明的是，如果将这个js 文件直接放在 hexo 目录的source 文件夹中，会报错无法渲染站点，此处有两种解决方案</p><p>方案1：</p><ul><li>在主题 themes -&gt; fluid -&gt; source -&gt; js 文件夹中添加文件 duration.js</li></ul><p>方案2：</p><ul><li><p>在hexo站点 hexo -&gt; source -&gt; alec_js 文件夹中添加文件 duration.js</p></li><li><p>同时在站点配置文件 hexo&#x2F;_config.yml 中为 alec_js 文件夹添加跳过渲染的选项：</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">skip_render:</span><br>    <span class="hljs-bullet">-</span> <span class="hljs-string">alec_js/**</span><br></code></pre></td></tr></table></figure><h4 id="√-修改主题配置文件"><a href="#√-修改主题配置文件" class="headerlink" title="[√] 修改主题配置文件"></a>[√] 修改主题配置文件</h4><hr><p>在主题配置中的 footer: content 添加div：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">&lt;div</span> <span class="hljs-string">style=&quot;font-size:</span> <span class="hljs-number">0.</span><span class="hljs-string">85rem&quot;&gt;</span><br>  <span class="hljs-string">&lt;span</span> <span class="hljs-string">id=&quot;timeDate&quot;&gt;载入天数...&lt;/span&gt;</span><br>  <span class="hljs-string">&lt;span</span> <span class="hljs-string">id=&quot;times&quot;&gt;载入时分秒...&lt;/span&gt;</span><br>  <span class="hljs-string">&lt;script</span> <span class="hljs-string">src=&quot;/vvd_js/duration.js&quot;&gt;&lt;/script&gt;</span><br><span class="hljs-string">&lt;/div&gt;</span><br></code></pre></td></tr></table></figure><p>具体为：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">footer:</span><br>  <span class="hljs-attr">content:</span> <span class="hljs-string">&#x27;</span><br><span class="hljs-string">    &lt;a href=&quot;https://hexo.io&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span&gt;Hexo&lt;/span&gt;&lt;/a&gt;</span><br><span class="hljs-string">    &lt;i class=&quot;iconfont icon-love&quot;&gt;&lt;/i&gt;</span><br><span class="hljs-string">    &lt;a href=&quot;https://github.com/fluid-dev/hexo-theme-fluid&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span&gt;Fluid&lt;/span&gt;&lt;/a&gt;</span><br><span class="hljs-string">    &lt;div style=&quot;font-size: 0.85rem&quot;&gt;</span><br><span class="hljs-string">      &lt;span id=&quot;timeDate&quot;&gt;载入天数...&lt;/span&gt;</span><br><span class="hljs-string">      &lt;span id=&quot;times&quot;&gt;载入时分秒...&lt;/span&gt;</span><br><span class="hljs-string">      &lt;script src=&quot;/vvd_js/duration.js&quot;&gt;&lt;/script&gt;</span><br><span class="hljs-string">    &lt;/div&gt;</span><br><span class="hljs-string">  &#x27;</span><br></code></pre></td></tr></table></figure><h2 id="√-效果展示"><a href="#√-效果展示" class="headerlink" title="[√] 效果展示"></a>[√] 效果展示</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137981.png" alt="image-20221223013654140"></p><h2 id="√-参考"><a href="#√-参考" class="headerlink" title="[√] 参考"></a>[√] 参考</h2><hr><p><a href="https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-run-how-long/fluid-run-how-long/%EF%BC%88%E2%88%9A%EF%BC%89">https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-run-how-long/fluid-run-how-long/（√）</a></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3 - fluid主题自定义CSS样式：debug footer字体颜色问题</title>
    <link href="/posts/2526870413/"/>
    <url>/posts/2526870413/</url>
    
    <content type="html"><![CDATA[<h2 id="√-相关知识"><a href="#√-相关知识" class="headerlink" title="[√] 相关知识"></a>[√] 相关知识</h2><hr><h4 id="√-div和span的区别是什么"><a href="#√-div和span的区别是什么" class="headerlink" title="[√] div和span的区别是什么"></a>[√] div和span的区别是什么</h4><hr><p>在html页面布局时，我们经常会使用到div标签和span标签，那么div标签和span标签之间有什么区别？</p><p><strong>div</strong></p><ul><li>占一行：<ul><li>div标签是块级元素，每个div标签都会从新行开始显示，占据一行</li></ul></li><li>嵌套：<ul><li>div标签内可以添加其他的标签元素（行内元素、块级元素都行），比如：span标签，p标签，也可以是div标签</li></ul></li><li>支持自定义CSS样式：<ul><li>div标签可以通过css样式来设置自身的宽度（也可省略，当没有使用css自定义宽度时，div标签的宽度为其的容器的100%）、高度，且还可以设置标签之间的距离（外边距和内边距）</li></ul></li></ul><p><strong>span</strong></p><ul><li>行内元素：<ul><li>span标签是行内元素，会在一行显示；span标签元素会和其他标签元素会在一行显示（块级元素除外），不会另起一行显示</li><li>span标签内只能添加行内元素的标签或文本，span标签里只能容纳文本或者是其他的行内元素，不能容纳块级元素。</li></ul></li><li>不支持CSS，只能在标签内定义格式：<ul><li>span标签的宽度、高度都无法通过css样式设置，它的宽高受其本身内容（文字、图片）控制，随着内容的宽高改变而改变；span标签无法控制外边距和内边距，虽然可以设置左右的外边距和内边距，但上下的外边距和内边距无法设置。</li></ul></li></ul><p>总结，div标签可以单独的自定义CSS样式，且独占一行，支持嵌套，span只能在标签内通过如<code>&lt;span style=&quot;color: #DDD;&quot;  id=&quot;hitokoto&quot;&gt;&lt;/span&gt;</code>的方式改变样式，且为行内元素，不能嵌套。</p><h4 id="√-无侵入式自定义CSS样式"><a href="#√-无侵入式自定义CSS样式" class="headerlink" title="[√] 无侵入式自定义CSS样式"></a>[√] 无侵入式自定义CSS样式</h4><hr><p>hexo中经常需要修改网页中的样式，为了无侵入地修改CSS样式可以使用 Fluid 自定义 CSS样式的功能，本文记录使用方法。</p><p>方法为：</p><ul><li>创建<code>blog/source/css/</code>路径，并在这个路径创建自定义名称如<code>alec_custom.css</code>文件</li><li>主题配置文件中加入该文件相对路径：</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">custom_css:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">/css/custom.css</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">//at.alicdn.com/t/font_1736178_ijqayz9ro8k.css</span><br></code></pre></td></tr></table></figure><ul><li>在该css文件中自定义css样式即可</li><li>具体实现步骤可以往下看</li></ul><h2 id="√-debug修改footer字体颜色步骤"><a href="#√-debug修改footer字体颜色步骤" class="headerlink" title="[√] debug修改footer字体颜色步骤"></a>[√] debug修改footer字体颜色步骤</h2><hr><p>在上一文中在页表<code>footer</code>添加了<code>一言</code>之后，出现了如下切换黑夜、日间模式时字体颜色不统一问题：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137471.png" alt="image-20221222225634604"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137472.png" alt="image-20221222225657666"></p><p>因此想要通过自定义CSS样式无侵入式的解决这个问题。</p><p>首先在网页页面，通过<code>ctrl + shift + c</code>快捷键或者<code>F12</code>快捷键，定位到想要修改样式的地方，</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137473.png" alt="image-20221223010222198"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137474.png" alt="image-20221223010315398"></p><p>如图所示，发现 footer 此处的总的div是<code>class=&quot;footer-inner&quot;</code>，因此可以在自定义的CSS文件中，统一修改颜色：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137475.png" alt="image-20221223010451794"></p><p>在修改了颜色之后，发现上述问题解决，但是出现了小bug，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230107284.png" alt="image-20221223010708930"></p><p>即仍有两个地方颜色未变，查看源代码：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs html">#---------------------------<br># 页脚<br># Footer<br>#---------------------------<br>footer:<br>  # 页脚第一行文字的 HTML，建议保留 Fluid 的链接，用于向更多人推广本主题<br>  # HTML of the first line of the footer, it is recommended to keep the Fluid link to promote this theme to more people<br>  content: &#x27;<br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://hexo.io&quot;</span> <span class="hljs-attr">target</span>=<span class="hljs-string">&quot;_blank&quot;</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;nofollow noopener&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">span</span>&gt;</span>Hexo<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">i</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;iconfont icon-love&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">i</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://github.com/fluid-dev/hexo-theme-fluid&quot;</span> <span class="hljs-attr">target</span>=<span class="hljs-string">&quot;_blank&quot;</span> <span class="hljs-attr">rel</span>=<span class="hljs-string">&quot;nofollow noopener&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">span</span>&gt;</span>Fluid<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br>  &#x27;<br></code></pre></td></tr></table></figure><p>发现<code>hexo</code>和<code>fluid</code>文字是通过<code>span</code>定义的，查阅资料知道，<code>span</code>标签，不支持自定义CSS文件修改样式。因此在此处直接在<code>span</code>标签上修改样式，改动为为：</p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">span</span>&gt;</span>Hexo<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">span</span>&gt;</span>Fluid<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br># ↓<br><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;color: #DDD;&quot;</span>&gt;</span>Hexo<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;color: #DDD;&quot;</span>&gt;</span>Fluid<span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><br></code></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212230137476.png" alt="image-20221223011414766"></p><p>最终，问题得以解决！</p><blockquote><p>补充：</p><p>若自定义CSS失效，可以在代码中直接侵入式修改：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;div class=&quot;footer-inner&quot; style=&quot;font-size: 0.85rem;color:#DDD&quot;&gt;<br></code></pre></td></tr></table></figure><p>或者：</p><p># 注意，此处的CSS文件，或者只能同时放在blog中的source或者主题中的source，不能分开放</p></blockquote><h2 id="√-参考"><a href="#√-参考" class="headerlink" title="[√] 参考"></a>[√] 参考</h2><hr><p><a href="https://m.php.cn/article/413753.html%EF%BC%88%E2%88%9A%EF%BC%89">https://m.php.cn/article/413753.html（√）</a></p><p><a href="https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-custom-css/fluid-custom-css/%EF%BC%88%E2%88%9A%EF%BC%89">https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-custom-css/fluid-custom-css/（√）</a></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2 - fluid主题添加&#39;一言&#39;</title>
    <link href="/posts/151223124/"/>
    <url>/posts/151223124/</url>
    
    <content type="html"><![CDATA[<h2 id="√-介绍"><a href="#√-介绍" class="headerlink" title="[√] 介绍"></a>[√] 介绍</h2><hr><ul><li>一言网（<a href="http://hitokoto.cn/">hitokoto.cn</a>）创立于 2016 年，隶属于萌创团队，网站主要提供一句话服务。</li><li>‘一言’ 的初衷——动漫也好、小说也好、网络也好，不论在哪里，我们总会看到有那么一两个句子能穿透你的心。我们把这些句子汇聚起来，形成一言网络，以传递更多的感动。</li><li>简单来说，一言指的就是一句话，可以是动漫中的台词，也可以是网络上的各种小段子。 或是感动，或是开心，有或是单纯的回忆。来到这里，留下你所喜欢的那一句句话，与大家分享，这就是一言存在的目的。</li></ul><p>官网链接：<a href="https://developer.hitokoto.cn/">https://developer.hitokoto.cn/</a></p><h2 id="√-实践"><a href="#√-实践" class="headerlink" title="[√] 实践"></a>[√] 实践</h2><hr><h4 id="√-在首页添加随机slogan"><a href="#√-在首页添加随机slogan" class="headerlink" title="[√] 在首页添加随机slogan"></a>[√] 在首页添加随机slogan</h4><hr><p>在 Fluid 主题配置文件修改 <code>index/slogan</code> 配置的 <code>url</code> 和 <code>keys</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># 首页副标题的独立设置</span><br><span class="hljs-comment"># Independent config of home page subtitle</span><br><span class="hljs-attr">slogan:</span><br><span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># 为空则按 hexo config.subtitle 显示</span><br><span class="hljs-comment"># If empty, text based on `subtitle` in hexo config</span><br><span class="hljs-attr">text:</span> <span class="hljs-string">&quot;要走起来，你才知道方向。&quot;</span><br><br><span class="hljs-comment"># 通过 API 接口作为首页副标题的内容，必须返回的是 JSON 格式，如果请求失败则按 text 字段显示，该功能必须先开启 typing 打字机功能</span><br><span class="hljs-comment"># Subtitle of the homepage through the API, must be returned a JSON. If the request fails, it will be displayed in `text` value. This feature must first enable the typing animation</span><br><span class="hljs-attr">api:</span><br><span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br><br><span class="hljs-comment"># 请求地址</span><br><span class="hljs-comment"># Request url</span><br><span class="hljs-attr">url:</span> <span class="hljs-string">&quot;https://v1.hitokoto.cn/&quot;</span><br><br><span class="hljs-comment"># 请求方法</span><br><span class="hljs-comment"># Request method</span><br><span class="hljs-comment"># Available: GET | POST | PUT</span><br><span class="hljs-attr">method:</span> <span class="hljs-string">&quot;GET&quot;</span><br><br><span class="hljs-comment"># 请求头</span><br><span class="hljs-comment"># Request headers</span><br><span class="hljs-attr">headers:</span> &#123;&#125;<br><br><span class="hljs-comment"># 从请求结果获取字符串的取值字段，最终必须是一个字符串，例如返回结果为 &#123;&quot;data&quot;: &#123;&quot;author&quot;: &quot;fluid&quot;, &quot;content&quot;: &quot;An elegant theme&quot;&#125;&#125;, 则取值字段为 [&#x27;data&#x27;, &#x27;content&#x27;]；如果返回是列表则自动选择第一项</span><br><span class="hljs-comment"># The value field of the string obtained from the response. For example, the response content is &#123;&quot;data&quot;: &#123;&quot;author&quot;: &quot;fluid&quot;, &quot;content&quot;: &quot;An elegant theme&quot;&#125;&#125;, the expected `keys: [&#x27;data&#x27;,&#x27;content&#x27;]`; if the return is a list, the first item is automatically selected</span><br><span class="hljs-attr">keys:</span> [<span class="hljs-string">&#x27;hitokoto&#x27;</span>]<br></code></pre></td></tr></table></figure><h4 id="√-在footer添加slogan"><a href="#√-在footer添加slogan" class="headerlink" title="[√] 在footer添加slogan"></a>[√] 在footer添加slogan</h4><hr><p>（1）修改主题配置文件</p><p>嵌入代码为：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs xml"> <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;statistics&quot;</span>&gt;</span><br> <span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">href</span>=<span class="hljs-string">&quot;https://developer.hitokoto.cn/&quot;</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;hitokoto_text&quot;</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">span</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;color: #DDD;&quot;</span>  <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;hitokoto&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">span</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">script</span> <span class="hljs-attr">src</span>=<span class="hljs-string">&quot;https://v1.hitokoto.cn/?encode=js&amp;select=%23hitokoto&quot;</span> <span class="hljs-attr">defer</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">script</span>&gt;</span><br> <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br></code></pre></td></tr></table></figure><p>在主题配置文件中，<code>footer</code>处添加<code>content2</code>：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">#---------------------------</span><br><span class="hljs-comment"># 页脚</span><br><span class="hljs-comment"># Footer</span><br><span class="hljs-comment">#---------------------------</span><br><span class="hljs-attr">footer:</span><br>  <span class="hljs-comment"># 页脚第一行文字的 HTML，建议保留 Fluid 的链接，用于向更多人推广本主题</span><br>  <span class="hljs-comment"># HTML of the first line of the footer, it is recommended to keep the Fluid link to promote this theme to more people</span><br>  <span class="hljs-attr">content:</span> <span class="hljs-string">&#x27;</span><br><span class="hljs-string">    &lt;a href=&quot;https://hexo.io&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span&gt;Hexo&lt;/span&gt;&lt;/a&gt;</span><br><span class="hljs-string">    &lt;i class=&quot;iconfont icon-love&quot;&gt;&lt;/i&gt;</span><br><span class="hljs-string">    &lt;a href=&quot;https://github.com/fluid-dev/hexo-theme-fluid&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span&gt;Fluid&lt;/span&gt;&lt;/a&gt;</span><br><span class="hljs-string">  &#x27;</span><br>  <span class="hljs-attr">content2:</span> <span class="hljs-string">&#x27; &lt;div class=&quot;statistics&quot;&gt;</span><br><span class="hljs-string"> &lt;a href=&quot;https://developer.hitokoto.cn/&quot; id=&quot;hitokoto_text&quot;&gt;&lt;span style=&quot;color: #DDD;&quot;  id=&quot;hitokoto&quot;&gt;&lt;/span&gt;&lt;/a&gt;</span><br><span class="hljs-string">&lt;script src=&quot;https://v1.hitokoto.cn/?encode=js&amp;select=%23hitokoto&quot; defer&gt;&lt;/script&gt;</span><br><span class="hljs-string"> &lt;/div&gt;&#x27;</span><br><br></code></pre></td></tr></table></figure><p>（2）修改<code>footer.ejs</code>文件（或者也可以在<code>bolg/themes/fluid/layout/_partials/footer/statistics.ejs</code>中添加这个代码）</p><p>要添加的代码为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;% if (theme.footer.content2) &#123; %&gt;<br>  &lt;div class=&quot;footer-content&quot;&gt;<br>    &lt;%- theme.footer.content2 %&gt;<br>  &lt;/div&gt;<br>&lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>在<code>bolg/themes/fluid/layout/_partials/footer.ejs</code>中添加上述自定义的div块,</p><p>同时统一此footer处的字体大小：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;div class=&quot;footer-inner&quot; style=&quot;font-size: 0.85rem;&quot;&gt;<br></code></pre></td></tr></table></figure><p>如图所示：</p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212222217085.png" alt="image-20221222221711339" style="zoom:67%;" /><h2 id="√-效果图展示"><a href="#√-效果图展示" class="headerlink" title="[√] 效果图展示"></a>[√] 效果图展示</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212222222688.png" alt="image-20221222222210016"></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212222222689.png" alt="image-20221222221903010"></p><h2 id="√-参考资料"><a href="#√-参考资料" class="headerlink" title="[√] 参考资料"></a>[√] 参考资料</h2><hr><blockquote><p><a href="https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-yiyan/yiyan/">https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-yiyan/yiyan/</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>10 - fluid添加樱花飘落效果和鼠标点击效果</title>
    <link href="/posts/703315339/"/>
    <url>/posts/703315339/</url>
    
    <content type="html"><![CDATA[<h2 id="√-步骤"><a href="#√-步骤" class="headerlink" title="[√] 步骤"></a>[√] 步骤</h2><hr><p>（1）创建路径，并在<code>blog_test\themes\fluid\source\alec_diy\mouse_click\</code>路径下载鼠标点击效果</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212251454939.png" alt="image-20221225145308788"></p><p>（2）修改主题配置文件，添加</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">custom_js:</span><br>  <span class="hljs-comment">#############</span><br>  <span class="hljs-comment">#鼠标点击特效#</span><br>  <span class="hljs-comment">#############</span><br>  <span class="hljs-comment"># （1）鼠标移动星星特效</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">/alec_diy/mouse_click/star.js</span><br>  <span class="hljs-comment"># （2）鼠标点击爱心特效</span><br>  <span class="hljs-comment"># - /alec_diy/mouse_click/love.js</span><br>  <span class="hljs-comment"># （3）鼠标点击文字特效</span><br>  <span class="hljs-comment"># - /alec_diy/mouse_click/dianjichuzi.js</span><br>  <br>  <span class="hljs-comment">#############</span><br>  <span class="hljs-comment">#满屏飘落特效#</span><br>  <span class="hljs-comment">#############  </span><br>  <span class="hljs-comment"># （1）樱花飘落</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-string">//cdn.jsdelivr.net/gh/bynotes/texiao/source/js/yinghua.js</span><br>  <br></code></pre></td></tr></table></figure><blockquote><p>tips:</p><ul><li>站点下的资源文件和主题下的资源文件同时在custom中的时候，有冲突，或者可能主题配置文件中的自定义js文件，无法搜索到站点下的source文件</li></ul></blockquote>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1 - fluid主题无侵入式方式添加视频背景</title>
    <link href="/posts/4083129976/"/>
    <url>/posts/4083129976/</url>
    
    <content type="html"><![CDATA[<hr><h2 id="1-fluid主题无侵入式方式添加视频背景"><a href="#1-fluid主题无侵入式方式添加视频背景" class="headerlink" title="1 - fluid主题无侵入式方式添加视频背景"></a>1 - fluid主题无侵入式方式添加视频背景</h2><blockquote><p>参考：<a href="https://www.zywvvd.com/notes/hexo/theme/fluid/fluid-inject/fluid-inject/">Fluid -20- 使用 Fluid 注入功能实现背景视频（√）</a></p></blockquote><blockquote><p>通过代码注入的方式修改主题，可以实现无侵入式的修改。hexo中开发者开发的各种主题，会不断的优化改进、迭代新的版本。侵入式的方式在<code>hexo博客目录/themes/fluid/</code>文件夹中修改源代码，当主题升级之后，可能就会出现兼容性问题。因此推荐使用代码注入的方式实现无侵入式的修改。</p></blockquote><h2 id="1-代码注入介绍"><a href="#1-代码注入介绍" class="headerlink" title="1 - 代码注入介绍"></a>1 - 代码注入介绍</h2><h3 id="1-1-概述"><a href="#1-1-概述" class="headerlink" title="1.1 - 概述"></a>1.1 - 概述</h3><ul><li>代码注入是在项目之外将需要修改的代码动态插入到项目中的技术手段</li><li>直接修改源码是完全可以达到目的的，但是源码修改会破坏仓库的代码完整性，问题主要出现在需要对仓库进行更新的时候</li><li>修改过的仓库很容易在更新时引入冲突，那时候很可能需要面对自己都不记得为什么改的代码和完全不懂的项目代码做出取舍，实在是很危险、痛苦而且不优雅的</li><li>也就是说，我们又要调整项目代码功能，又要保持项目足够“干净”，以便享受将来的更新，此时代码注入的价值便显现出来了</li></ul><h3 id="1-2-hexo代码注入"><a href="#1-2-hexo代码注入" class="headerlink" title="1.2 - hexo代码注入"></a>1.2 - hexo代码注入</h3><ul><li><a href="https://hexo.io/zh-cn/api/injector.html">Hexo 注入器</a> 是 Hexo 5 版本自身加入的一项新功能，所以在所有 Hexo 主题都是支持这个功能的</li><li>注入器可以将 HTML 片段注入生成页面的 <code>head</code> 和 <code>body</code> 节点中</li><li>编写注入代码，需要在博客的根目录下创建 <code>scripts</code> 文件夹，然后在里面任意命名创建一个 js 文件即可。</li></ul><h4 id="1-2-1-实践示例-讲解"><a href="#1-2-1-实践示例-讲解" class="headerlink" title="1.2.1 - 实践示例 + 讲解"></a>1.2.1 - 实践示例 + 讲解</h4><p>（1）例如创建一个 <code>/blog/scripts/example.js</code>，内容为：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs js">hexo.<span class="hljs-property">extend</span>.<span class="hljs-property">injector</span>.<span class="hljs-title function_">register</span>(<span class="hljs-string">&#x27;body_end&#x27;</span>, <span class="hljs-string">&#x27;&lt;script src=&quot;/jquery.js&quot;&gt;&lt;/script&gt;&#x27;</span>, <span class="hljs-string">&#x27;default&#x27;</span>);<br></code></pre></td></tr></table></figure><p>上述代码会在生成的页面 <code>body</code> 中注入加载 <code>jquery.js</code> 的代码</p><p>（2）参数</p><ul><li><code>register</code> 函数可接受三个参数，第一个参数是代码片段注入的位置，接受以下值：</li></ul><table><thead><tr><th align="center">参数</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">head_begin</td><td align="center">head开头</td></tr><tr><td align="center">head_end</td><td align="center">head结尾</td></tr><tr><td align="center">body_begin</td><td align="center">body开头</td></tr><tr><td align="center">body_end</td><td align="center">body结尾</td></tr></tbody></table><ul><li>第二个参数是注入的片段，可以是字符串，也可以是一个返回值为字符串的函数。</li><li>第三个参数是注入的页面类型，接受以下值：</li></ul><table><thead><tr><th align="center">参数</th><th align="center">含义</th></tr></thead><tbody><tr><td align="center">default</td><td align="center">注入到每个页面（默认值）</td></tr><tr><td align="center">home</td><td align="center">只注入到主页（<code>is_home()</code> 为 <code>true</code> 的页面）</td></tr><tr><td align="center">post</td><td align="center">只注入到文章页面（<code>is_post()</code> 为 <code>true</code> 的页面）</td></tr><tr><td align="center">page</td><td align="center">只注入到独立页面（<code>is_page()</code> 为 <code>true</code> 的页面）</td></tr><tr><td align="center">archive</td><td align="center">只注入到归档页面（<code>is_archive()</code> 为 <code>true</code> 的页面）</td></tr><tr><td align="center">category</td><td align="center">只注入到分类页面（<code>is_category()</code> 为 <code>true</code> 的页面）</td></tr><tr><td align="center">tag</td><td align="center">只注入到标签页面（<code>is_tag()</code> 为 <code>true</code> 的页面）</td></tr></tbody></table><blockquote><p>或是其他自定义 layout 名称，例如在Fluid 主题中 <code>about</code> 对应关于页、<code>links</code> 对应友联页</p></blockquote><h3 id="1-3-fluid代码注入"><a href="#1-3-fluid代码注入" class="headerlink" title="1.3 - fluid代码注入"></a>1.3 - fluid代码注入</h3><ul><li>Fluid 主题也提供了一套注入代码功能，相较于 Hexo 注入功能更细致更丰富，并且支持注入 <code>ejs</code> 代码。</li><li>如果你想充分修改主题，又不想直接修改源码影响日后更新，本主题提供了代码注入功能，可以将代码无侵入式加入到主题里。</li><li>你可以直接注入 HTML 片段，不过建议你了解一下 <a href="https://ejs.bootcss.com/">EJS 模板引擎</a>，这样你就可以像主题里的 <code>ejs</code> 文件一样编写自己的组件再注入进去。</li><li>进入博客目录下 <code>scripts</code> 文件夹（如不存在则创建），在里面创建任意名称的 js 文件，在文件中写入如下内容：</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs javascript">hexo.<span class="hljs-property">extend</span>.<span class="hljs-property">filter</span>.<span class="hljs-title function_">register</span>(<span class="hljs-string">&#x27;theme_inject&#x27;</span>, <span class="hljs-keyword">function</span>(<span class="hljs-params">injects</span>) &#123;<br>  injects.<span class="hljs-property">header</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;default&#x27;</span>, <span class="hljs-string">&#x27;source/_inject/test1.ejs&#x27;</span>, &#123; <span class="hljs-attr">key</span>: <span class="hljs-string">&#x27;value&#x27;</span> &#125;, -<span class="hljs-number">1</span>);<br>  injects.<span class="hljs-property">footer</span>.<span class="hljs-title function_">raw</span>(<span class="hljs-string">&#x27;default&#x27;</span>, <span class="hljs-string">&#x27;&lt;script async src=&quot;https://xxxxxx&quot; crossorigin=&quot;anonymous&quot;&gt;&lt;/script&gt;&#x27;</span>);<br>&#125;);<br></code></pre></td></tr></table></figure><ul><li><p><code>header</code> 和 <code>footer</code> 是注入点的名称，表示代码注入到页面的什么位置；</p></li><li><p><code>file</code> 方法表示注入的是文件，第一个参数下面介绍，第二个参数则是文件的路径，第三个参数是传入文件的参数（可省略），第四个参数是顺序（可省略）；</p></li><li><p><code>raw</code> 方法表示注入的是原生代码，第一个参数下面介绍，第二个参数则是一句原生的 HTML 语句；</p></li><li><p><code>default</code> 表示注入的键名，可以使用任意键名，同一个注入点下的相同键名会使注入的内容覆盖，而不同键名则会让内容依次排列（默认按执行先后顺序，可通过 <code>file</code> 第四个参数指定），这里 default 为主题默认键名，通常会替换掉主题默认的组件；</p></li><li><p>主题目前提供的注入点如下：</p></li></ul><table><thead><tr><th align="center">注入点名称</th><th align="center">注入范围</th><th align="center">存在 <code>default</code> 键</th></tr></thead><tbody><tr><td align="center">head</td><td align="center"><code>head</code> 标签中的结尾</td><td align="center">无</td></tr><tr><td align="center">header</td><td align="center"><code>header</code> 标签中所有内容</td><td align="center">有</td></tr><tr><td align="center">bodyBegin</td><td align="center"><code>body</code> 标签中的开始</td><td align="center">无</td></tr><tr><td align="center">bodyEnd</td><td align="center"><code>body</code> 标签中的结尾</td><td align="center">无</td></tr><tr><td align="center">footer</td><td align="center"><code>footer</code> 标签中所有内容</td><td align="center">有</td></tr><tr><td align="center">postMetaTop</td><td align="center">文章页 <code>header</code> 标签中 meta 部分内容</td><td align="center">有</td></tr><tr><td align="center">postMetaBottom</td><td align="center">文章页底部<code>meta</code>部分内容</td><td align="center">有</td></tr><tr><td align="center">postMarkdownBegin</td><td align="center"><code>&lt;div class=&quot;markdown-body&quot;&gt;</code> 标签中的开始</td><td align="center">无</td></tr><tr><td align="center">postMarkdownEnd</td><td align="center"><code>&lt;div class=&quot;markdown-body&quot;&gt;</code> 标签中的结尾</td><td align="center">无</td></tr><tr><td align="center">postLeft</td><td align="center">文章页左侧边栏</td><td align="center">有</td></tr><tr><td align="center">postRight</td><td align="center">文章页右侧边栏</td><td align="center">有</td></tr><tr><td align="center">postCopyright</td><td align="center">文章页版权信息</td><td align="center">有</td></tr><tr><td align="center">postRight</td><td align="center">文章页右侧边栏</td><td align="center">无</td></tr><tr><td align="center">postComments</td><td align="center">文章页评论</td><td align="center">有</td></tr><tr><td align="center">pageComments</td><td align="center">自定义页评论</td><td align="center">有</td></tr><tr><td align="center">linksComments</td><td align="center">友链页评论</td><td align="center">有</td></tr></tbody></table><h2 id="2-视频背景注入实现"><a href="#2-视频背景注入实现" class="headerlink" title="2 - 视频背景注入实现"></a>2 - 视频背景注入实现</h2><h3 id="2-1-流程"><a href="#2-1-流程" class="headerlink" title="2.1 - 流程"></a>2.1 - 流程</h3><ul><li>创建注入配置文件</li><li>创建注入代码文件</li><li>创建背景图片+背景视频url json文件</li><li>修改主题配置文件</li></ul><h3 id="2-2-实现"><a href="#2-2-实现" class="headerlink" title="2.2 - 实现"></a>2.2 - 实现</h3><p>（1）创建配置注入路径及文件<code>blog/scripts/page.js</code>，内容为</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs javascript">hexo.<span class="hljs-property">extend</span>.<span class="hljs-property">filter</span>.<span class="hljs-title function_">register</span>(<span class="hljs-string">&#x27;theme_inject&#x27;</span>, <span class="hljs-keyword">function</span>(<span class="hljs-params">injects</span>) &#123;<br>  <span class="hljs-comment">//injects.header.file(&#x27;default&#x27;, &#x27;source/_inject/test1.ejs&#x27;, &#123; key: &#x27;value&#x27; &#125;, -1);</span><br>  injects.<span class="hljs-property">bodyBegin</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;default&#x27;</span>, <span class="hljs-string">&quot;source/_inject/bodyBegin.ejs&quot;</span>);<br>  injects.<span class="hljs-property">header</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;video-banner&#x27;</span>, <span class="hljs-string">&#x27;source/_inject/header.ejs&#x27;</span>, &#123; <span class="hljs-attr">key</span>: <span class="hljs-string">&#x27;value&#x27;</span> &#125;, -<span class="hljs-number">1</span>);<br>&#125;);<br></code></pre></td></tr></table></figure><p>（2）创建代码注入路径及文件</p><ul><li><p>路径为</p><ul><li><code>blog/source/_insert/header.ejs</code></li><li><code>blog/source/_insert/bodyBegin.ejs</code></li></ul></li><li><p>内容为：</p><ul><li><code>header.ejs</code></li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;%<br>var banner_video = theme.index.banner_video<br>var banner_img = page.banner_img || theme.index.banner_img<br>var banner_img_height = page.banner_img_height || theme.index.banner_img_height<br>var banner_mask_alpha = page.banner_mask_alpha || theme.index.banner_mask_alpha<br>%&gt;<br>&lt;script type=&quot;text/javascript&quot; src=&quot;/vvd_js/jquery.js&quot;&gt;&lt;/script&gt;<br><br>&lt;div class=&quot;banner&quot; id=&#x27;banner&#x27; &gt;<br><br>&lt;div class=&quot;full-bg-img&quot; &gt;<br><br>&lt;% if(banner_video)&#123; %&gt;<br>&lt;script&gt;<br>var ua = navigator.userAgent;<br>var ipad = ua.match(/(iPad).*OS\s([\d_]+)/),<br>isIphone = !ipad &amp;&amp; ua.match(/(iPhone\sOS)\s([\d_]+)/),<br>isAndroid = ua.match(/(Android)\s+([\d.]+)/),<br>isMobile = isIphone || isAndroid;<br><br>function set_video_attr(id)&#123;<br><br>var height = document.body.clientHeight<br>var width = document.body.clientWidth<br>var video_item = document.getElementById(id);<br><br>if (height / width &lt; 0.56)&#123;<br>video_item.setAttribute(&#x27;width&#x27;, &#x27;100%&#x27;);<br>video_item.setAttribute(&#x27;height&#x27;, &#x27;auto&#x27;);<br>&#125; else &#123;<br>video_item.setAttribute(&#x27;height&#x27;, &#x27;100%&#x27;);<br>video_item.setAttribute(&#x27;width&#x27;, &#x27;auto&#x27;);<br>&#125;<br>&#125;<br><br>$.getJSON(&#x27;/vvd_js/video_url.json&#x27;, function(data)&#123;<br>if (true)&#123;<br>var video_list_length = data.length<br>var seed = Math.random()<br>index = Math.floor(seed * video_list_length)<br><br>video_url = data[index][0]<br>pre_show_image_url = data[index][1]<br><br>banner_obj = document.getElementById(&quot;banner&quot;)<br>banner_obj.style.cssText = &quot;background: url(&#x27;&quot; + pre_show_image_url + &quot;&#x27;) no-repeat; background-size: cover;&quot;<br><br>vvd_banner_obj = document.getElementById(&quot;vvd_banner_img&quot;)<br><br>vvd_banner_content = &quot;&lt;img id=&#x27;banner_img_item&#x27; src=&#x27;&quot; + pre_show_image_url + &quot;&#x27; style=&#x27;height: 100%; position: fixed; z-index: -999&#x27;&gt;&quot;<br>vvd_banner_obj.innerHTML = vvd_banner_content<br>set_video_attr(&#x27;banner_img_item&#x27;)<br><br>if (!isMobile) &#123;<br>video_html_res = &quot;&lt;video id=&#x27;video_item&#x27; style=&#x27;position: fixed; z-index: -888;&#x27;  muted=&#x27;muted&#x27; src=&quot; + video_url + &quot; autoplay=&#x27;autoplay&#x27; loop=&#x27;loop&#x27;&gt;&lt;/video&gt;&quot;<br>document.getElementById(&quot;banner_video_insert&quot;).innerHTML = video_html_res;<br>set_video_attr(&#x27;video_item&#x27;)<br>&#125;<br>&#125;<br>&#125;);<br><br>if (!isMobile)&#123;<br>window.onresize = function()&#123;<br>set_video_attr(&#x27;video_item&#x27;)<br>&#125;<br>&#125;<br>&lt;/script&gt;<br>&lt;% &#125; %&gt;<br>&lt;/div&gt;<br>&lt;/div&gt;<br>    &lt;/div&gt;<br></code></pre></td></tr></table></figure><ul><li><code>bodyBegin.ejs</code></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs ejs">&lt;div&gt;<br>    &lt;!--其中rgba中的a，指的是mask文件的alpha，透明度--&gt;<br>&lt;div class=&#x27;real_mask&#x27; style=&quot;<br>background-color: rgba(0,0,0,0.3);<br>width: 100%;<br>height: 100%;<br>position: fixed;<br>z-index: -777;<br>&quot;&gt;&lt;/div&gt;<br>&lt;div id=&quot;banner_video_insert&quot;&gt;<br>&lt;/div&gt;<br>&lt;div id=&#x27;vvd_banner_img&#x27;&gt;<br>&lt;/div&gt;<br>&lt;/div&gt;<br>&lt;div id=&quot;banner&quot;&gt;&lt;/div&gt;<br></code></pre></td></tr></table></figure><p>（3）创建背景图片+背景视频url json文件</p><p>在<code>blog/source/vvd_js/</code>创建背景图片、视频地址json文件<code>video_url.json</code></p><p>内容格式为：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">[</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220318.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220318.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220356.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220356.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220415.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220415.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220556.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220556.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220626.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220626.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220640.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220640.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220742.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210808220742.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span><br><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210813194902.mp4&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;https://101.43.39.125/HexoFiles/vvd-dell-2021-win-10/20210813194902.jpg&quot;</span><span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p>同时在该文件夹下放置下载文件<code>jquery.js</code></p><p>（4）修改主题配置文件<code>_config.fluid.yml</code></p><ul><li>覆盖默认 banner 图为纯透明的 png 图像，其中banner图片指的是背景图片。将banner换成透明的，方便将自己的动态视频嵌入。<ul><li>将所有的 <code>banner_img</code> 替换为 <code>https://101.43.39.125/HexoFiles/new/bg-trans.png</code></li></ul></li><li>添加 <code>index/banner_video</code> ，设置为 true</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">#---------------------------</span><br><span class="hljs-comment"># 首页</span><br><span class="hljs-comment"># Home Page</span><br><span class="hljs-comment">#---------------------------</span><br><span class="hljs-attr">index:</span><br>  <span class="hljs-comment"># 首页 Banner 头图，可以是相对路径或绝对路径，以下相同</span><br>  <span class="hljs-comment"># Path of Banner image, can be a relative path or an absolute path, the same on other pages</span><br>  <span class="hljs-attr">banner_img:</span> <span class="hljs-string">https://101.43.39.125/HexoFiles/new/bg-trans.png</span><br>  <br>  <span class="hljs-comment"># 首页 Banner 使用随机视频</span><br>  <span class="hljs-comment"># true 开启  false 关闭</span><br>  <span class="hljs-attr">banner_video:</span> <span class="hljs-literal">true</span><br><br></code></pre></td></tr></table></figure><ul><li>将所有 <code>banner_mask_alpha</code> 设置为 ，这个参数将透明度设置为0</li></ul>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
      <category>主题</category>
      
      <category>fluid</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>4 - hexo为文章添加随机封面</title>
    <link href="/posts/915427997/"/>
    <url>/posts/915427997/</url>
    
    <content type="html"><![CDATA[<p>在主题配置文件中添加：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">#---------------------------</span><br><span class="hljs-comment"># 文章页</span><br><span class="hljs-comment"># Post Page</span><br><span class="hljs-comment">#---------------------------</span><br><span class="hljs-attr">post:</span><br><br>  <span class="hljs-comment"># 文章在首页的默认封面图，当没有指定 index_img 时会使用该图片，若两者都为空则不显示任何图片</span><br>  <span class="hljs-comment"># Path of the default post cover when `index_img` is not set. If both are empty, no image will be displayed</span><br>  <span class="hljs-attr">default_index_img:</span> <br>  <span class="hljs-bullet">-</span> <span class="hljs-string">https://img.xjh.me/random_img.php?type=bg&amp;ctype=nature&amp;return=302</span><br>  <span class="hljs-comment"># https://tuapi.eees.cc/api.php?category=meinv</span><br>  <span class="hljs-comment"># https://img.xjh.me/random_img.php?type=bg&amp;ctype=nature&amp;return=302</span><br>  <span class="hljs-comment"># https://img.xjh.me/random_img.php</span><br>  <span class="hljs-comment"># https://img.xjh.me/random_img.php?type=bg</span><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>3 - hexo自动为文章添加分类插件</title>
    <link href="/posts/3085432397/"/>
    <url>/posts/3085432397/</url>
    
    <content type="html"><![CDATA[<h2 id="√-步骤"><a href="#√-步骤" class="headerlink" title="[√] 步骤"></a>[√] 步骤</h2><hr><p>想要hexo根据<code>_posts</code>中的文件夹自动为文章生成分类</p><h4 id="√-安装"><a href="#√-安装" class="headerlink" title="[√] 安装"></a>[√] 安装</h4><hr><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install hexo-auto-category --save<br></code></pre></td></tr></table></figure><h4 id="√-配置站点文件"><a href="#√-配置站点文件" class="headerlink" title="[√] 配置站点文件"></a>[√] 配置站点文件</h4><hr><p>添加：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment"># Generate categories from directory-tree</span><br><span class="hljs-comment"># Dependencies: https://github.com/xu-song/hexo-auto-category</span><br><span class="hljs-comment"># depth: the depth of directory-tree you want to generate, should &gt; 0</span><br><span class="hljs-attr">auto_category:</span><br> <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br> <span class="hljs-attr">depth:</span><br><br></code></pre></td></tr></table></figure><p>如果只想生成第一级目录分类，可以设置depth属性，比如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">auto_category:</span><br> <span class="hljs-attr">enable:</span> <span class="hljs-literal">true</span><br> <span class="hljs-attr">depth:</span> <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><h4 id="√-效果"><a href="#√-效果" class="headerlink" title="[√] 效果"></a>[√] 效果</h4><hr><p>图为自动生成的分类</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212251437607.png" alt="image-20221225143714692"></p><h2 id="√-参考"><a href="#√-参考" class="headerlink" title="[√] 参考"></a>[√] 参考</h2><hr><p><a href="https://blog.csdn.net/Cryu_xuan/article/details/104232173%EF%BC%88%E2%88%9A%EF%BC%89">https://blog.csdn.net/Cryu_xuan/article/details/104232173（√）</a></p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>2 - hexo使用随机编号生成文章永久链接</title>
    <link href="/posts/1841524034/"/>
    <url>/posts/1841524034/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考文章：</p><p><a href="https://blog.51cto.com/u_13640625/3032262">https://blog.51cto.com/u_13640625&#x2F;3032262（√）</a></p></blockquote><p>1.安装abbrlink插件</p><p>在博客根目录（执行hexo命令的地方）安装插件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm install hexo-abbrlink --save<br></code></pre></td></tr></table></figure><p>2.编辑站点配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-comment">#permalink: :year/:month/:day/:title/</span><br><span class="hljs-comment">#permalink_defaults:</span><br><span class="hljs-attr">permalink:</span> <span class="hljs-string">posts/:abbrlink/</span><br><span class="hljs-attr">abbrlink:</span><br>  <span class="hljs-attr">alg:</span> <span class="hljs-string">crc32</span> <span class="hljs-comment">#support crc16(default) and crc32</span><br>  <span class="hljs-attr">rep:</span> <span class="hljs-string">dec</span>   <span class="hljs-comment">#support dec(default) and hex</span><br></code></pre></td></tr></table></figure><blockquote><p>设置示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-string">crc16</span> <span class="hljs-string">&amp;</span> <span class="hljs-string">hex</span><br><span class="hljs-string">https://post.zz173.com/posts/66c8.html</span><br><br><span class="hljs-string">crc16</span> <span class="hljs-string">&amp;</span> <span class="hljs-string">dec</span><br><span class="hljs-string">https://post.zz173.com/posts/65535.html</span><br><br><span class="hljs-string">crc32</span> <span class="hljs-string">&amp;</span> <span class="hljs-string">hex</span><br><span class="hljs-string">https://post.zz173.com/posts/8ddf18fb.html</span><br><br><span class="hljs-string">crc32</span> <span class="hljs-string">&amp;</span> <span class="hljs-string">dec</span><br><span class="hljs-string">https://post.zz173.com/posts/1690090958.html</span><br></code></pre></td></tr></table></figure></blockquote>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>1 - hexo编写.bat脚本实现Hexo一键部署</title>
    <link href="/posts/545180737/"/>
    <url>/posts/545180737/</url>
    
    <content type="html"><![CDATA[<p>使用hexo发布文章的时候，每次需要在hexo根目录打开git，然后依次执行<code>hexo clean, hexo g, hexo d</code>,不太方便。</p><p>后来将此过程简化为直接输入<code>hexo clean&amp;&amp;hexo g&amp;&amp;hexo d</code></p><p>再后来，实现编写bat脚本，一键运行。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">@echo off<br>hexo clean&amp;&amp;hexo g&amp;&amp;gulp&amp;&amp;hexo d<br></code></pre></td></tr></table></figure><p>其中，@echo off表示不显示后续命令行及当前命令行。</p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
    </categories>
    
    
    <tags>
      
      <tag>软件使用技巧</tag>
      
      <tag>hexo</tag>
      
      <tag>效率</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>0 - hexo博客搭建记录</title>
    <link href="/posts/1312067499/"/>
    <url>/posts/1312067499/</url>
    
    <content type="html"><![CDATA[<blockquote><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/35668237">超详细Hexo+Github博客搭建小白教程 - 知乎 - 字节跳动 AI Lab NLP算法工程师 - godweiyang</a></p></blockquote><h2 id="1-安装Node-js"><a href="#1-安装Node-js" class="headerlink" title="1 - 安装Node.js"></a>1 - 安装Node.js</h2><h2 id="2-安装Git"><a href="#2-安装Git" class="headerlink" title="2 - 安装Git"></a>2 - 安装Git</h2><h2 id="3-本地安装、运行hexo"><a href="#3-本地安装、运行hexo" class="headerlink" title="3 - 本地安装、运行hexo"></a>3 - 本地安装、运行hexo</h2><h3 id="3-1-新建文件夹"><a href="#3-1-新建文件夹" class="headerlink" title="3.1 - 新建文件夹"></a>3.1 - 新建文件夹</h3><p>在合适的地方新建一个文件夹，用来存放自己的博客文件，比如我的博客文件都存放在<code>D:\坚果云\blog</code>目录下。</p><p>在该目录下右键点击<code>Git Bash Here</code>，打开git的控制台窗口。</p><h3 id="3-2-安装hexo并验证"><a href="#3-2-安装hexo并验证" class="headerlink" title="3.2 - 安装hexo并验证"></a>3.2 - 安装hexo并验证</h3><p>定位到该目录下，输入<code>npm i hexo-cli -g</code>安装Hexo。会有几个报错，无视它就行。</p><p>安装完后输入<code>hexo -v</code>验证是否安装成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">npm i hexo-cli -g<br><br>hexo -v<br></code></pre></td></tr></table></figure><h3 id="3-3-初始化网站-安装必备组件"><a href="#3-3-初始化网站-安装必备组件" class="headerlink" title="3.3 - 初始化网站+安装必备组件"></a>3.3 - 初始化网站+安装必备组件</h3><p>输入<code>hexo init</code>初始化文件夹，接着输入<code>npm install</code>安装必备的组件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">hexo init<br><br>npm install<br></code></pre></td></tr></table></figure><p>3.4 - 本地运行，观察效果</p><p>删除网页静态文件缓存 + 生成新的静态文件 + 在本地4000端口运行程序</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">c = clean, g = generate, s = server</span><br>hexo c&amp;&amp;hexo g&amp;&amp;hexo s<br></code></pre></td></tr></table></figure><p>在浏览器打开网页：<a href="http://localhost:4000/">http://localhost:4000/</a></p><p>按<code>ctrl+c</code>关闭本地服务器。</p>]]></content>
    
    
    <categories>
      
      <category>工具笔记</category>
      
      <category>hexo</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>JVM题目整理</title>
    <link href="/posts/1491831089/"/>
    <url>/posts/1491831089/</url>
    
    <content type="html"><![CDATA[<h3 id="001-关于垃圾收集的哪些叙述是正确的-√-。"><a href="#001-关于垃圾收集的哪些叙述是正确的-√-。" class="headerlink" title="001 - 关于垃圾收集的哪些叙述是正确的(√)。"></a>001 - 关于垃圾收集的哪些叙述是正确的(√)。</h3><p>程序开发者必须自己创建一个线程进行内存释放的工作</p><p>垃圾收集允许程序开发者明确指定并立即释放该内存</p><p>垃圾收集将检查并释放不再使用的内存</p><p>垃圾收集能够在期望的时间释放被java对象使用的内存</p><p>自己选择：C</p><p>答案：C</p><hr><p>A选项，开发者无需自己创建线程来进行垃圾回收。</p><p>B选项，开发者可以指定进行垃圾回收，但是只是建议，不一定是立即执行。</p><p>D选项，不能明确的说一定可以在期望的时间内释放内存。</p>]]></content>
    
    
    <categories>
      
      <category>Java技术栈</category>
      
      <category>Java虚拟机</category>
      
      <category>笔记</category>
      
      <category>0 - 知识点收集</category>
      
    </categories>
    
    
    <tags>
      
      <tag>题目整理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>sleep、yield、wait、join的区别</title>
    <link href="/posts/1811526594/"/>
    <url>/posts/1811526594/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  参考文章：</p><p>  <a href="">sleep、yield、wait、join的区别(阿里) - 博客园 - 柴飞飞（√）</a></p></blockquote><h2 id="零碎点"><a href="#零碎点" class="headerlink" title="零碎点"></a>零碎点</h2><p>只有runnable到running时才会<code>占用cpu时间片</code>，其他都会<code>出让cpu时间片</code>。</p><hr><p><strong>线程状态转换图</strong></p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211221729611.png" alt="image-20221122172926319"> </p><h2 id="sleep-和-wait-的辨析"><a href="#sleep-和-wait-的辨析" class="headerlink" title="sleep 和 wait 的辨析"></a>sleep 和 wait 的辨析</h2><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>线程的资源有不少，但应该包含<code>CPU资源</code>和<code>锁资源</code>这两类。</p><ul><li>sleep(long mills)：让出CPU资源，但是不会释放锁资源</li><li>wait()：让出CPU资源和锁资源</li></ul><hr><p>wait用于锁机制，sleep不是，这就是为啥sleep不释放锁，wait释放锁的原因，sleep是线程的方法，跟锁没半毛钱关系，wait，notify,notifyall 都是Object对象的方法，是一起使用的，用于锁机制</p><h2 id="sleep-和-yield-的辨析"><a href="#sleep-和-yield-的辨析" class="headerlink" title="sleep 和 yield 的辨析"></a>sleep 和 yield 的辨析</h2><h3 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h3><p>Thread.sleep(long) 和 Thread.yield() 都是 Thread 类的静态方法，在调用的时候都是 Thread.sleep(long) &#x2F; Thread.yield() 的方式进行调用</p><p>而 join() 是由线程对象来调用</p><h2 id="wait"><a href="#wait" class="headerlink" title="wait"></a>wait</h2><p>Object类的方法(notify()、notifyAll()  也是Object对象)，必须放在循环体和同步代码块中，执行该方法的线程会释放CPU资源和锁资源。然后该线程进入线程等待池中等待被再次唤醒(notify随机唤醒，notifyAll全部唤醒，线程结束自动唤醒)。线程在等待池中被唤醒之后，进入锁池，重新竞争获取同步锁。</p><hr><p>wait() 和 notify()、notifyAll() 这三个方法都是 java.lang.Object 的方法。</p><hr><p>都必须在 Synchronized 语句块内调用：它们都是用于协调多个线程对共享数据的存取，所以必须在Synchronized语句块内使用这三个方法。</p><hr><p>前面说过Synchronized这个关键字用于保护共享数据，阻止其他线程对共享数据的存取。但是这样程序的流程就很不灵活了，如何才能在当前线程还没退出Synchronized数据块时让其他线程也有机会访问共享数据呢？此时就用这三个方法来灵活控制。 </p><ul><li>（1）wait()方法使当前线程暂停执行并释放对象锁标志，让其他线程可以进入Synchronized数据块，当前线程被放入对象等待池中。</li><li>（2）当调用 notify()方法后，将从对象的等待池中移走一个任意的线程并放到锁标志等待池中，只有锁标志等待池中的线程能够获取锁标志；如果锁标志等待池中没有线程，则notify()不起作用。 </li><li>（3）notifyAll()则从对象等待池中移走所有等待那个对象的线程并放到锁标志等待池中。</li></ul><hr><p>在java中，Thread类线程执行完run()方法后，一定会自动执行notifyAll()方法</p><h2 id="sleep"><a href="#sleep" class="headerlink" title="sleep"></a>sleep</h2><p>Thread类的方法，必须带一个时间参数。会让当前线程休眠进入阻塞状态并释放CPU资源，但是不会释放锁资源。</p><p>提供其他线程运行的机会且不考虑优先级</p><p>如果有同步锁则sleep不会释放锁即其他线程无法获得同步锁</p><p>可通过调用interrupt()方法来唤醒休眠线程。</p><p>阿里面试题 Sleep释放CPU，wait 也会释放cpu，因为cpu资源太宝贵了，只有在线程running的时候，才会获取cpu片段。</p><h2 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h2><p>让出CPU调度，Thread类的方法，类似sleep只是不能由用户指定暂停多长时间 </p><p>并且yield()方法只能让同优先级的线程有执行的机会，优先级不同的线程，无法获得运行机会。</p><p>yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。</p><p>调用yield方法只是一个建议，告诉线程调度器我的工作已经做的差不多了，可以让别的相同优先级的线程使用CPU了，没有任何机制保证采纳。</p><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><p>一种特殊的wait，当前运行线程 A 调用另一个线程 B 的 join 方法，然后当前线程 A 进入阻塞状态直到另一个线程 B 运行结束，然后 A 再继续运行。 注意该方法也需要捕捉异常。</p><p>join()方法就是通过wait()方法实现的。</p><p>形象的讲，就是在线程 A 运行的过程中，执行了 <code>B.join()</code>，那么 A 就知道 B 加入进来了，那么 A 就让 B 先执行，B 执行完之后，A 再继续执行。相当于 A 在运行过程中，让 B 插了个队，</p><p>代码示例：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        <span class="hljs-type">Runnable</span> <span class="hljs-variable">runnable</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Runnable</span>() &#123;<br>            <span class="hljs-meta">@Override</span><br>            <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">run</span><span class="hljs-params">()</span> &#123;<br>                <span class="hljs-keyword">try</span> &#123;<br>                    Thread.sleep(<span class="hljs-number">1000</span>);<br>                &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>                    e.printStackTrace();<br>                &#125;<br>                System.out.println(<span class="hljs-string">&quot;子线程执行&quot;</span>);<br>            &#125;<br>        &#125;;<br>        <span class="hljs-type">Thread</span> <span class="hljs-variable">thread1</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(runnable);<br>        <span class="hljs-type">Thread</span> <span class="hljs-variable">thread2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Thread</span>(runnable);<br>        thread1.start();<br>        thread2.start();<br>        <span class="hljs-keyword">try</span> &#123;<br>            <span class="hljs-comment">//主线程开始等待子线程thread1，thread2</span><br>            thread1.join();<br>            thread2.join();<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            e.printStackTrace();<br>        &#125;<br>        <span class="hljs-comment">//等待两个线程都执行完（不活动）了，才执行下行打印</span><br>        System.out.println(<span class="hljs-string">&quot;执行完毕&quot;</span>);<br>    &#125;<br><br>&#125;<br><span class="hljs-comment">/*</span><br><span class="hljs-comment">子线程执行</span><br><span class="hljs-comment">子线程执行</span><br><span class="hljs-comment">执行完毕</span><br><span class="hljs-comment">*/</span><br></code></pre></td></tr></table></figure><p>该示例中，就是主线程让 thread1 和 thread2 这两个线程插队。</p>]]></content>
    
    
    <categories>
      
      <category>Java技术栈</category>
      
      <category>Java并发</category>
      
      <category>笔记</category>
      
      <category>线程基础</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>反射笔记</title>
    <link href="/posts/1454350640/"/>
    <url>/posts/1454350640/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  【参考】</p><p>  <a href="https://javaguide.cn/java/basis/reflection.html#%E4%BD%95%E4%B8%BA%E5%8F%8D%E5%B0%84">Java 反射机制详解 - JavaGuide（√）</a></p></blockquote><h2 id="1-零碎点"><a href="#1-零碎点" class="headerlink" title="1 - 零碎点"></a>1 - 零碎点</h2><p>反射是框架的灵魂，反射能够在运行时分析类、并且执行类中的方法。</p><p>反射能够获取任意一个类的所有的方法和属性，并且还能够调用这些方法和属性。</p><h2 id="2-反射的应用场景"><a href="#2-反射的应用场景" class="headerlink" title="2 - 反射的应用场景"></a>2 - 反射的应用场景</h2><h3 id="2-1-概述"><a href="#2-1-概述" class="headerlink" title="2.1 - 概述"></a>2.1 - 概述</h3><p>框架、动态代理、注解。这三个内容会用到反射。</p><h3 id="2-2-展开"><a href="#2-2-展开" class="headerlink" title="2.2 - 展开"></a>2.2 - 展开</h3><p>&#x3D;&#x3D;框架&#x3D;&#x3D;</p><p>通过反射，能够大量的使用各种框架。像 Spring&#x2F;Spring Boot、MyBatis 等等框架中都大量使用了反射机制。</p><hr><p>&#x3D;&#x3D;动态代理&#x3D;&#x3D;</p><p>各种框架中也大量使用了动态代理，动态代理的实现也依赖反射。</p><blockquote><p>  比如在动态代理的实现过程中，会使用反射类<code>Method</code>来调用指定的方法。</p>  <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">DebugInvocationHandler</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">InvocationHandler</span> &#123;<br>    <span class="hljs-comment">/**</span><br><span class="hljs-comment">     * 代理类中的真实对象</span><br><span class="hljs-comment">     */</span><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> Object target;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">DebugInvocationHandler</span><span class="hljs-params">(Object target)</span> &#123;<br>        <span class="hljs-built_in">this</span>.target = target;<br>    &#125;<br><br><br>    <span class="hljs-keyword">public</span> Object <span class="hljs-title function_">invoke</span><span class="hljs-params">(Object proxy, Method method, Object[] args)</span> <span class="hljs-keyword">throws</span> InvocationTargetException, IllegalAccessException &#123;<br>        System.out.println(<span class="hljs-string">&quot;before method &quot;</span> + method.getName());<br>        <span class="hljs-type">Object</span> <span class="hljs-variable">result</span> <span class="hljs-operator">=</span> method.invoke(target, args);<br>        System.out.println(<span class="hljs-string">&quot;after method &quot;</span> + method.getName());<br>        <span class="hljs-keyword">return</span> result;<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure></blockquote><hr><p>&#x3D;&#x3D;注解&#x3D;&#x3D;</p><p>Java中的注解也用到了反射。</p><p>比如使用Spring的时候，@Component注解能够将一个类声明为Spring Bean，@Value注解能够读取到配置文件中的值。</p><p>这些都是通过反射分析类，来获取类&#x2F;属性&#x2F;方法&#x2F;方法的参数上的注解，然后根据注解做进一步的分析。</p><h2 id="3-反射机制的优缺点"><a href="#3-反射机制的优缺点" class="headerlink" title="3 - 反射机制的优缺点"></a>3 - 反射机制的优缺点</h2><h3 id="3-1-优点"><a href="#3-1-优点" class="headerlink" title="3.1 - 优点"></a>3.1 - 优点</h3><p>能够让代码更灵活，比如用于框架、动态代理来扩展切面功能、注解来方便开发</p><h3 id="3-2-缺点"><a href="#3-2-缺点" class="headerlink" title="3.2 - 缺点"></a>3.2 - 缺点</h3><p>（1）&#x3D;&#x3D;安全问题：&#x3D;&#x3D;运行的时候能够分析并操作类，会增加安全问题，比如运行时使用反射创建对象会无视泛型的安全检查。</p><p>（2）&#x3D;&#x3D;性能问题：&#x3D;&#x3D;相对来说，发射的性能会比正射要差一些。</p><h2 id="4-反射实战"><a href="#4-反射实战" class="headerlink" title="4 - 反射实战"></a>4 - 反射实战</h2><h3 id="4-1-获取-Class-对象的四种方式"><a href="#4-1-获取-Class-对象的四种方式" class="headerlink" title="4.1 - 获取 Class 对象的四种方式"></a>4.1 - 获取 Class 对象的四种方式</h3><p>获取一个类的方法、变量等信息需要从 Class 对象获取，因此要想动态的获取这些信息则需要获取Class对象。</p><hr><p>Java 提供了四种获取一个类的 Class 对象的方式。</p><p>（1）通过类名获取：TargetObject.class</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Class</span> <span class="hljs-variable">alunbarClass</span> <span class="hljs-operator">=</span> TargetObject.class;<br></code></pre></td></tr></table></figure><p>通过此方式获取 Class 对象不会进行初始化</p><p>（2）通过类的完整名称（路径）获取：Class.forName(“cn.javaguide.TargetObject”)</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">Class</span> <span class="hljs-variable">alunbarClass1</span> <span class="hljs-operator">=</span> Class.forName(<span class="hljs-string">&quot;cn.javaguide.TargetObject&quot;</span>);<br></code></pre></td></tr></table></figure><p>（3）通过类的对象获取：o.getClass()</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">TargetObject</span> <span class="hljs-variable">o</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">TargetObject</span>();<br><span class="hljs-type">Class</span> <span class="hljs-variable">alunbarClass2</span> <span class="hljs-operator">=</span> o.getClass();<br></code></pre></td></tr></table></figure><p>（4）通过类加载器获取：xxxClassLoader.loadClass()传入类路径</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">ClassLoader.getSystemClassLoader().loadClass(<span class="hljs-string">&quot;cn.javaguide.TargetObject&quot;</span>);<br></code></pre></td></tr></table></figure><p>通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行</p><h3 id="4-2-反射的一些基本操作"><a href="#4-2-反射的一些基本操作" class="headerlink" title="4.2 - 反射的一些基本操作"></a>4.2 - 反射的一些基本操作</h3><p>（1）创建要使用反射操作的类：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TargetObject</span> &#123;<br>    <span class="hljs-keyword">private</span> String value;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">TargetObject</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-built_in">this</span>.value = <span class="hljs-string">&quot;alec&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">publicMethod</span><span class="hljs-params">(String s)</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;I am &quot;</span> + s);<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">privateMethod</span><span class="hljs-params">()</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;value is &quot;</span> + value);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><p>（2）使用反射操作这个类的方法和参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.lang.reflect.Field;<br><span class="hljs-keyword">import</span> java.lang.reflect.InvocationTargetException;<br><span class="hljs-keyword">import</span> java.lang.reflect.Method;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException &#123;<br>        <span class="hljs-comment">// 通过反射创建对象：获取要操作的类的 Class 对象，并创建这个类的实例</span><br>        Class&lt;?&gt; targetClass = Class.forName(<span class="hljs-string">&quot;TargetObject&quot;</span>);<br>        <span class="hljs-type">TargetObject</span> <span class="hljs-variable">targetObject</span> <span class="hljs-operator">=</span> (TargetObject)targetClass.newInstance();<br><br>        <span class="hljs-comment">// 获取要操作的类中定义的所有的方法</span><br>        Method[] methods = targetClass.getDeclaredMethods();<br>        <span class="hljs-keyword">for</span> (Method e : methods) &#123;<br>            System.out.println(e.getName());<br>        &#125;<br><br>        <span class="hljs-comment">// 获取指定的public方法并调用</span><br>        <span class="hljs-type">Method</span> <span class="hljs-variable">publicMethod</span> <span class="hljs-operator">=</span> targetClass.getDeclaredMethod(<span class="hljs-string">&quot;publicMethod&quot;</span>, String.class);<br>        publicMethod.invoke(targetObject, <span class="hljs-string">&quot;alec&quot;</span>);<br><br>        <span class="hljs-comment">// 取消private方法的安全检查然后调用</span><br>        <span class="hljs-type">Method</span> <span class="hljs-variable">privateMethod</span> <span class="hljs-operator">=</span> targetClass.getDeclaredMethod(<span class="hljs-string">&quot;privateMethod&quot;</span>);<br>        privateMethod.setAccessible(<span class="hljs-literal">true</span>);<br>        privateMethod.invoke(targetObject);<br><br>        <span class="hljs-comment">// 获取指定的参数并对参数进行修改</span><br>        <span class="hljs-type">Field</span> <span class="hljs-variable">field</span> <span class="hljs-operator">=</span> targetClass.getDeclaredField(<span class="hljs-string">&quot;value&quot;</span>);<br>        field.setAccessible(<span class="hljs-literal">true</span>);<br>        field.set(targetObject, <span class="hljs-string">&quot;alec_changed&quot;</span>);<br>        privateMethod.invoke(targetObject);<br>        <br>    &#125;<br>&#125;<br><span class="hljs-comment">/*结果*/</span><br>publicMethod<br>privateMethod<br>I am alec<br>value is alec<br>value is alec_changed<br></code></pre></td></tr></table></figure><h2 id="5-反射类的常用方法总结"><a href="#5-反射类的常用方法总结" class="headerlink" title="5 - 反射类的常用方法总结"></a>5 - 反射类的常用方法总结</h2><h3 id="5-1-Class-getDeclaredFields-和-Class-getMethods-的区别"><a href="#5-1-Class-getDeclaredFields-和-Class-getMethods-的区别" class="headerlink" title="5.1 - Class.getDeclaredFields() 和 Class.getMethods() 的区别"></a>5.1 - Class.getDeclaredFields() 和 Class.getMethods() 的区别</h3><blockquote><p>  参考：<a href="https://www.cnblogs.com/wy697495/p/9631909.html">https://www.cnblogs.com/wy697495/p/9631909.html</a></p></blockquote><p>&#x3D;&#x3D;区别：&#x3D;&#x3D;</p><p>两个方法的区别主要在于：getMethods()返回的是该类以及超类的公共方法。getDeclaredMethods()返回该类本身自己声明的包括公共、保护、默认（包）访问和私有方法，但并不包括超类中的方法。</p><p>总结：其实Class中有很多相似的方法比如：getAnnotations()和getDeclaredAnnotations()，以及getFields()和getDeclaredFields()等等，不同之处和上面基本一样</p><p>&#x3D;&#x3D;例子：&#x3D;&#x3D;</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/*定义要被反射获取信息的类*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TargetClass</span> &#123;<br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">TargetClass</span><span class="hljs-params">()</span> &#123;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">publicMethod_1</span><span class="hljs-params">()</span>&#123;<br><br>    &#125;<br><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">protectedMethod_2</span><span class="hljs-params">()</span>&#123;<br><br>    &#125;<br><br>    <span class="hljs-keyword">void</span> <span class="hljs-title function_">defaultMethod_3</span><span class="hljs-params">()</span>&#123;<br><br>    &#125;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">privateMethod_4</span><span class="hljs-params">()</span>&#123;<br><br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/*方法测试*/</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TargetObject</span> &#123;<br>    <span class="hljs-keyword">private</span> String value;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-title function_">TargetObject</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-built_in">this</span>.value = <span class="hljs-string">&quot;alec&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">publicMethod</span><span class="hljs-params">(String s)</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;I am &quot;</span> + s);<br>    &#125;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">privateMethod</span><span class="hljs-params">()</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;value is &quot;</span> + value);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">/*结果*/</span><br>=============getMethods==================<br>publicMethod_1<br>wait<br>wait<br>wait<br>equals<br>toString<br>hashCode<br>getClass<br>notify<br>notifyAll<br>===========getDeclaredMethods=============<br>protectedMethod_2<br>privateMethod_4<br>defaultMethod_3<br>publicMethod_1<br>==========================================<br></code></pre></td></tr></table></figure><h3 id="5-2-通过反射调用类的各种元素"><a href="#5-2-通过反射调用类的各种元素" class="headerlink" title="5.2 - 通过反射调用类的各种元素"></a>5.2 - 通过反射调用类的各种元素</h3><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211141930778.png" alt="image-20221114193018687"></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">// 通过反射可以得到</span><br>Classes，内部类;<br>Field，属性;<br>Constructor，构造器;<br>Method，方法;<br>Annotation，注解;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Java技术栈</category>
      
      <category>Java基础</category>
      
      <category>笔记</category>
      
      <category>反射</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JavaSE知识点</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java缓冲流如何提高读写性能</title>
    <link href="/posts/3366410150/"/>
    <url>/posts/3366410150/</url>
    
    <content type="html"><![CDATA[<blockquote><p>  参考：</p><p>  <a href="https://blog.csdn.net/reliveIT/article/details/45819529">【Java】缓冲流如何提高性能 - CSDN - 扶我起来我还要写代码（√）</a></p></blockquote><h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 - 前言"></a>0 - 前言</h2><p>先说结论，使用 BufferedInputStream，BufferedOutputStream 能够提高读写性能。</p><p>传统的Java IO是基于阻塞的，他的工作状态就是“读&#x2F;写，等待，读&#x2F;写，等待······”。</p><p>缓冲流有字节和字符两种，原理上几乎差不读，本处以字节缓冲路来进行讨论。</p><h2 id="1-缓冲输入流"><a href="#1-缓冲输入流" class="headerlink" title="1 - 缓冲输入流"></a>1 - 缓冲输入流</h2><h3 id="1-0-零碎点"><a href="#1-0-零碎点" class="headerlink" title="1.0 - 零碎点"></a>1.0 - 零碎点</h3><p>BufferedInputStream extends FileInputStream</p><p>缓冲流的设计思想是基于装饰器设计模式的，需要在构造缓冲流的时候传入一个节点流。 采用了缓冲技术的read(arr)方法，如果arr.length&gt;&#x3D;buf.length，那么将不会在使用buf，而是直接将磁盘上的数据填充到arr，这样才能保证最好的性能，但是可能引入的风险是arr的大小没有控制好，导致内存紧张；如果arr.length&lt;buf.length，那么还是依旧读满整个buf，然后从buf中将数据System.arrayCopy到arr中，没有了再次读取磁盘到buf，如此重复，实际上最终和磁盘交互的并不是BufferedInputStream，而是通过构造器注入的其他节点流的native read(arr[])来实现。</p><h3 id="1-1-为什么缓冲输入流能提高效率？"><a href="#1-1-为什么缓冲输入流能提高效率？" class="headerlink" title="1.1 - 为什么缓冲输入流能提高效率？"></a>1.1 - 为什么缓冲输入流能提高效率？</h3><p>思想就是空间换时间的思想。一次读入足够多的数据到内存中，然后之后直接从内存中取，因此快。即牺牲部分内存空间，换快。</p><p>具体是一次读取buf个字节到内存中，默认buf是8192个字节。</p><p>调用read方法，read方法虽然是一个字节一个字节的返回数据，但是他实际上是一次就读取了buf个字节到内存中等着。</p><p>传统的IO是阻塞式的，没有采取缓冲技术，也就意味着，读取一个字节，使用IO资源，然后阻塞，然后再使用IO资源，再阻塞，直至文件读完，流关闭释放IO资源。</p><h2 id="2-缓冲输出流"><a href="#2-缓冲输出流" class="headerlink" title="2 - 缓冲输出流"></a>2 - 缓冲输出流</h2><h3 id="2-0-零碎点"><a href="#2-0-零碎点" class="headerlink" title="2.0 - 零碎点"></a>2.0 - 零碎点</h3><p>BufferedOutputStream extends FileOutputStream。</p><p>read(int)方法的思想还是空间换时间，使用缓冲技术，则每次都是写buf，直到buf写满了才会把数据刷到磁盘。如果没有使用缓冲技术，那么每个字节都需要消耗本地的IO资源，写一个字节，使用一次IO资源，然后再阻塞再写，如此重复。</p><p>刷盘的过程是调用构造方法中传入的节点流的write(arr[])来实现的，而不是直接调用native write(int)实现。wirte(arr[])最终调用write(arr[], int, int)，思想是将arr中的数据刷到磁盘。使用了缓冲流技术，如果arr.length&gt;&#x3D;buf.length，则直接将arr中的数据刷盘；如果arr.length&lt;buf.length，则将数据写入buf，直到buf写满了才会刷盘，刷盘的过程也是调用构造方法中传入的节点流的write(arr[], int, int)完成。</p>]]></content>
    
    
    <categories>
      
      <category>Java技术栈</category>
      
      <category>Java基础</category>
      
      <category>笔记</category>
      
      <category>JavaIO</category>
      
      <category>0 - 知识点收集</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JavaIO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java使用FileWriter类向文件写入内容</title>
    <link href="/posts/975683122/"/>
    <url>/posts/975683122/</url>
    
    <content type="html"><![CDATA[<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.io.File;<br><span class="hljs-keyword">import</span> java.io.FileWriter;<br><span class="hljs-keyword">import</span> java.io.IOException;<br><span class="hljs-keyword">import</span> java.io.Writer;<br><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Main</span> &#123;<br>    <span class="hljs-comment">//使用FileWriter向文本文件中写信息</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">str</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;Hello World&quot;</span>;<br>        <span class="hljs-comment">//1.创建流</span><br>        <span class="hljs-type">Writer</span> <span class="hljs-variable">fw</span> <span class="hljs-operator">=</span> <span class="hljs-literal">null</span>;<br>        <span class="hljs-keyword">try</span> &#123;<br>            <span class="hljs-comment">/*创建txt文件*/</span><br>            <span class="hljs-type">File</span> <span class="hljs-variable">file</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">File</span>(<span class="hljs-string">&quot;D:\\hello.txt&quot;</span>);<br>            <span class="hljs-keyword">if</span> (!file.exists()) &#123;<br>                file.createNewFile();<br>            &#125;<br>            fw = <span class="hljs-keyword">new</span> <span class="hljs-title class_">FileWriter</span>(<span class="hljs-string">&quot;D:\\hello.txt&quot;</span>);<span class="hljs-comment">//1</span><br>            <span class="hljs-comment">//2.写入信息</span><br>            fw.write(str);<br>            <span class="hljs-comment">// 3.刷新缓冲区，即写入内容</span><br>            fw.flush();<br>            <span class="hljs-keyword">if</span> (fw != <span class="hljs-literal">null</span>) &#123;<br>                <span class="hljs-comment">// 4.关闭流,关闭缓冲流时，也会刷新一次缓冲区</span><br>                fw.close();<br><br>            &#125;<br>        &#125; <span class="hljs-keyword">catch</span> (IOException e) &#123;<br>            e.printStackTrace();<br>        &#125;<br>    &#125;<br>&#125;<br><br></code></pre></td></tr></table></figure><p>结果：</p><p><img src="https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202211141442388.png" alt="image-20221114144244343"></p>]]></content>
    
    
    <categories>
      
      <category>Java技术栈</category>
      
      <category>Java基础</category>
      
      <category>笔记</category>
      
      <category>JavaIO</category>
      
      <category>0 - 知识点收集</category>
      
    </categories>
    
    
    <tags>
      
      <tag>JavaIO</tag>
      
    </tags>
    
  </entry>
  
  
  
  
  
  
  <entry>
    <title>音乐</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[    <div id="aplayer-yQlAPjpD" class="aplayer aplayer-tag-marker meting-tag-marker"         data-id="7729098320" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"    ></div>    <div id="aplayer-WaQlrcZm" class="aplayer aplayer-tag-marker meting-tag-marker"         data-id="2305794885" data-server="netease" data-type="playlist" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#ad7a86"    ></div>]]></content>
    
  </entry>
  
  
  
  <entry>
    <title>关于本站</title>
    <link href="/"/>
    <url>/</url>
    
    <content type="html"><![CDATA[<h3 id="have-a-nice-day"><a href="#have-a-nice-day" class="headerlink" title="have a nice day~"></a>have a nice day~</h3>]]></content>
    
  </entry>
  
  
  
</search>
