

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/photo.png">
  <link rel="icon" href="/img/photo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Shuai Zhao">
  <meta name="keywords" content="人工智能, 深度学习, 软件开发, 个人博客, 所思所想">
  
    <meta name="description" content="[√] 第4章 - 前馈神经网络神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。 在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 图">
<meta property="og:type" content="article">
<meta property="og:title" content="第4章 - 前馈神经网络 - 书籍">
<meta property="og:url" content="https://alec-97.github.io/posts/75512103/index.html">
<meta property="og:site_name" content="要走起来，你才知道方向。">
<meta property="og:description" content="[√] 第4章 - 前馈神经网络神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。 在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 图">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214165655381.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172332002.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172608024.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214204504748.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/dbcf147a4e00446792eb2b93834e0f3154936e08ea124242af8631fde204381c">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172620106.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213744716.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/8562dfb10d464396948d05ee3620cec1d057025dddee43ff92dae3fbb72e8f65.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214205337207.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222637659.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214224725288.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214221440242.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160241372.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213927961.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215175316781.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160106452.png">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222430247.png">
<meta property="article:published_time" content="2022-12-19T10:16:44.000Z">
<meta property="article:modified_time" content="2023-04-16T05:32:00.313Z">
<meta property="article:author" content="Shuai Zhao">
<meta property="article:tag" content="人工智能, 深度学习, 软件开发, 个人博客, 所思所想">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>第4章 - 前馈神经网络 - 书籍 - 要走起来，你才知道方向。</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/alec_diy/css/alec_custom.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alec-97.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":80,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"->"},"progressbar":{"enable":true,"height_px":3,"color":"#00FF7F","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <div>
	<div class='real_mask' style="
		background-color: rgba(0,0,0,0.3);
		width: 100%;
		height: 100%;
		position: fixed;
		z-index: -777;
	"></div>
	<div id="banner_video_insert">
	</div>	
	<div id='vvd_banner_img'>
	</div>
</div>
<div id="banner"></div>
	<script type="text/javascript">
	  /*窗口监视*/
	  var originalTitle = document.title;
	  window.onblur = function(){document.title = "往事随风"};
	  window.onfocus = function(){document.title = originalTitle};
	</script>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Alec</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/playlist/">
                <i class="iconfont icon-music"></i>
                <span>音乐</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">第4章 - 前馈神经网络 - 书籍</span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Shuai Zhao
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-19 18:16" pubdate>
          2022年12月19日 晚上
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          54k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          453 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

	<script type="text/javascript" src="/vvd_js/jquery.js"></script>

	<div class="banner" id='banner' >

		<div class="full-bg-img" >

			
				<script>
					var ua = navigator.userAgent;
					var ipad = ua.match(/(iPad).*OS\s([\d_]+)/),
						isIphone = !ipad && ua.match(/(iPhone\sOS)\s([\d_]+)/),
						isAndroid = ua.match(/(Android)\s+([\d.]+)/),
						isMobile = isIphone || isAndroid;

					function set_video_attr(id){

						var height = document.body.clientHeight
						var width = document.body.clientWidth
						var video_item = document.getElementById(id);

						if (height / width < 0.56){
							video_item.setAttribute('width', '100%');
							video_item.setAttribute('height', 'auto');
						} else {
							video_item.setAttribute('height', '100%');
							video_item.setAttribute('width', 'auto');
						}
					}



					$.getJSON('/vvd_js/video_url.json', function(data){
						if (true){
							var video_list_length = data.length
							var seed = Math.random()
							index = Math.floor(seed * video_list_length)
							
							video_url = data[index][0]
							pre_show_image_url = data[index][1]

							// alec insert, 弹出当前是哪个视频
							// var info = index+"/"+video_list_length
							// alert(info)

							
							banner_obj = document.getElementById("banner")
							banner_obj.style.cssText = "background: url('" + pre_show_image_url + "') no-repeat; background-size: cover;"

							vvd_banner_obj = document.getElementById("vvd_banner_img")

							vvd_banner_content = "<img id='banner_img_item' src='" + pre_show_image_url + "' style='height: 100%; position: fixed; z-index: -999'>"
							vvd_banner_obj.innerHTML = vvd_banner_content
							set_video_attr('banner_img_item')

							if (!isMobile) {
								video_html_res = "<video id='video_item' style='position: fixed; z-index: -888;'  muted='muted' src=" + video_url + " autoplay='autoplay' loop='loop'></video>"
								document.getElementById("banner_video_insert").innerHTML = video_html_res;
								set_video_attr('video_item')
							}
						}
					});

					if (!isMobile){
						window.onresize = function(){
							set_video_attr('video_item')
							}
						}
				</script>
			
			</div>
		</div>
    </div>



  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">第4章 - 前馈神经网络 - 书籍</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：1 小时前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <h2 id="√-第4章-前馈神经网络"><a href="#√-第4章-前馈神经网络" class="headerlink" title="[√] 第4章 - 前馈神经网络"></a>[√] 第4章 - 前馈神经网络</h2><p>神经网络是由神经元按照一定的连接结构组合而成的网络。神经网络可以看作一个函数，通过简单非线性函数的多次复合，实现输入空间到输出空间的复杂映射 。<br>前馈神经网络是最早发明的简单人工神经网络。整个网络中的信息单向传播，可以用一个有向无环路图表示，这种网络结构简单，易于实现。</p>
<p>在学习本章内容前，建议先阅读《神经网络与深度学习》第2章：机器学习概述的相关内容，关键知识点如 <strong>图4.1</strong> 所示，以便更好的理解和掌握相应的理论知识，及其在实践中的应用方法。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/0ae8775e92a04173928b4e3fed98b5934f0aad6582dc4143b2205669bfae6b55" srcset="/img/loading.gif" lazyload width=500></center>

<center>图4.1 《神经网络与深度学习》关键知识点回顾</center>

<p>本实践基于 <strong>《神经网络与深度学习》第4章：前馈神经网络</strong> 相关内容进行设计，主要包含两部分：</p>
<ul>
<li><strong>模型解读</strong>：介绍前馈神经网络的基本概念、网络结构及代码实现，利用前馈神经网络完成一个分类任务，并通过两个简单的实验，观察前馈神经网络的梯度消失问题和死亡ReLU问题，以及对应的优化策略；</li>
<li><strong>案例与实践</strong>：基于前馈神经网络完成鸢尾花分类任务。</li>
</ul>
<hr>
<h3 id="√-4-1-神经元"><a href="#√-4-1-神经元" class="headerlink" title="[√] 4.1 - 神经元"></a>[√] 4.1 - 神经元</h3><p>神经网络的基本组成单元为带有非线性激活函数的神经元，其结构如如<strong>图4.2</strong>所示。神经元是对生物神经元的结构和特性的一种简化建模，接收一组输入信号并产生输出。</p>
<p>&#x3D;&#x3D;带有非线性激活函数&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;接收一组信号并产生输出&#x3D;&#x3D;</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214165655381.png" srcset="/img/loading.gif" lazyload alt="image-20221214165655381"></p>
<hr>
<h4 id="√-4-1-1-净活性值"><a href="#√-4-1-1-净活性值" class="headerlink" title="[√] 4.1.1 - 净活性值"></a>[√] 4.1.1 - 净活性值</h4><p>&#x3D;&#x3D;z&#x3D;wx+b 这个公司计算的是净活性值&#x3D;&#x3D;</p>
<p>假设一个神经元接收的输入为$\mathbf{x}\in \mathbb{R}^D$，其权重向量为$\mathbf{w}\in \mathbb{R}^D$，神经元所获得的输入信号，即净活性值$z$的计算方法为</p>
<p>$$<br>z &#x3D;\mathbf{w}^T\mathbf{x}+b，（4.1）<br>$$</p>
<p>其中$b$为偏置。</p>
<p>为了提高预测样本的效率，我们通常会将$N$个样本归为一组进行成批地预测。</p>
<p>$$<br>\boldsymbol{z} &#x3D;\boldsymbol{X} \boldsymbol{w} + b, (4.2)<br>$$</p>
<p>其中$\boldsymbol{X}\in \mathbb{R}^{N\times D}$为$N$个样本的特征矩阵，$\boldsymbol{z}\in \mathbb{R}^N$为$N$个预测值组成的列向量。</p>
<p>使用Paddle计算一组输入的净活性值。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><br><span class="hljs-comment"># 2个特征数为5的样本</span><br>X = paddle.rand(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>])<br><br><span class="hljs-comment"># 含有5个参数的权重向量</span><br>w = paddle.rand(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">1</span>])<br><span class="hljs-comment"># 偏置项</span><br>b = paddle.rand(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br><br><span class="hljs-comment"># 使用&#x27;paddle.matmul&#x27;实现矩阵相乘</span><br>z = paddle.matmul(X, w) + b<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input X:&quot;</span>, X)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;weight w:&quot;</span>, w, <span class="hljs-string">&quot;\nbias b:&quot;</span>, b)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output z:&quot;</span>, z)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">input</span> X: Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.79964578</span>, <span class="hljs-number">0.80879998</span>, <span class="hljs-number">0.94919258</span>, <span class="hljs-number">0.90140802</span>, <span class="hljs-number">0.99157101</span>],<br>        [<span class="hljs-number">0.68319607</span>, <span class="hljs-number">0.18029618</span>, <span class="hljs-number">0.31775340</span>, <span class="hljs-number">0.69175428</span>, <span class="hljs-number">0.23035321</span>]])<br>weight w: Tensor(shape=[<span class="hljs-number">5</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.40206695</span>],<br>        [<span class="hljs-number">0.33055961</span>],<br>        [<span class="hljs-number">0.66693956</span>],<br>        [<span class="hljs-number">0.91678756</span>],<br>        [<span class="hljs-number">0.37521145</span>]]) <br>bias b: Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">0.04227288</span>]])<br>output z: Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>], dtype=float32, place=CPUPlace, stop_gradient=<span class="hljs-literal">True</span>,<br>       [[<span class="hljs-number">2.46264338</span>],<br>        [<span class="hljs-number">1.30910742</span>]])<br><br></code></pre></td></tr></table></figure>

<p><strong>说明</strong></p>
<p>在飞桨中，可以使用<strong>nn.Linear</strong>完成输入张量的上述变换。</p>
<hr>
<h4 id="√-4-1-2-激活函数"><a href="#√-4-1-2-激活函数" class="headerlink" title="[√] 4.1.2 - 激活函数"></a>[√] 4.1.2 - 激活函数</h4><p>&#x3D;&#x3D;激活函数通常为非线性函数，可以增强神经网络的表示能力和学习能力。常用的激活函数有S型函数和ReLU函数。&#x3D;&#x3D;</p>
<p>净活性值$z$再经过一个非线性函数$f(·)$后，得到神经元的活性值$a$。</p>
<p>$$<br>a &#x3D; f(z)，（4.3）<br>$$</p>
<p>激活函数通常为非线性函数，可以增强神经网络的表示能力和学习能力。常用的激活函数有S型函数和ReLU函数。</p>
<hr>
<h5 id="√-4-1-2-1-Sigmoid-型函数"><a href="#√-4-1-2-1-Sigmoid-型函数" class="headerlink" title="[√] 4.1.2.1 - Sigmoid 型函数"></a>[√] 4.1.2.1 - Sigmoid 型函数</h5><p>&#x3D;&#x3D;Sigmoid 型函数是指一类S型曲线函数，为两端饱和函数。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;常用的 Sigmoid 型函数有 Logistic 函数和 Tanh 函数&#x3D;&#x3D;</p>
<p>其数学表达式为：</p>
<p>Logistic 函数：</p>
<p>$$<br>\sigma(z) &#x3D; \frac{1}{1+\exp(-z)}。（4.4）<br>$$</p>
<p>Tanh 函数：</p>
<p>$$<br>\mathrm{tanh}(z) &#x3D; \frac{\exp(z)-\exp(-z)}{\exp(z)+\exp(-z)}。（4.5）<br>$$</p>
<p>Logistic函数和Tanh函数的代码实现和可视化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">%matplotlib inline<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># Logistic函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">logistic</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-z))<br><br><span class="hljs-comment"># Tanh函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tanh</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> (paddle.exp(z) - paddle.exp(-z)) / (paddle.exp(z) + paddle.exp(-z))<br><br><span class="hljs-comment"># 在[-10,10]的范围内生成10000个输入值，用于绘制函数曲线</span><br>z = paddle.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10000</span>)<br><br>plt.figure()<br>plt.plot(z.tolist(), logistic(z).tolist(), color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Logistic Function&quot;</span>)<br>plt.plot(z.tolist(), tanh(z).tolist(), color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle =<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Tanh Function&quot;</span>)<br><br>ax = plt.gca() <span class="hljs-comment"># 获取轴，默认有4个</span><br><span class="hljs-comment"># 隐藏两个轴，通过把颜色设置成none</span><br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br><span class="hljs-comment"># 调整坐标轴位置   </span><br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br><br>plt.savefig(<span class="hljs-string">&#x27;fw-logistic-tanh.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172332002.png" srcset="/img/loading.gif" lazyload alt="image-20221214172332002"></p>
<p><strong>说明</strong></p>
<p>在飞桨中，可以通过调用<code>paddle.nn.functional.sigmoid</code>和<code>paddle.nn.functional.tanh</code>实现对张量的Logistic和Tanh计算。</p>
<hr>
<h5 id="√-4-1-2-2-ReLU-型函数"><a href="#√-4-1-2-2-ReLU-型函数" class="headerlink" title="[√] 4.1.2.2 - ReLU 型函数"></a>[√] 4.1.2.2 - ReLU 型函数</h5><p>常见的ReLU函数有ReLU和带泄露的ReLU（Leaky ReLU），数学表达式分别为：</p>
<p>$$<br>\mathrm{ReLU}(z) &#x3D; \max(0,z),（4.6）<br>$$</p>
<p>$$<br>\mathrm{LeakyReLU}(z) &#x3D; \max(0,z)+\lambda \min(0,z),（4.7）<br>$$</p>
<p>其中$\lambda$为超参数。</p>
<p>可视化ReLU和带泄露的ReLU的函数的代码实现和可视化如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># ReLU</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">relu</span>(<span class="hljs-params">z</span>):<br>    <span class="hljs-keyword">return</span> paddle.maximum(z, paddle.to_tensor(<span class="hljs-number">0.</span>))<br><br><span class="hljs-comment"># 带泄露的ReLU</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">leaky_relu</span>(<span class="hljs-params">z, negative_slope=<span class="hljs-number">0.1</span></span>):<br>    <span class="hljs-comment"># 当前版本paddle暂不支持直接将bool类型转成int类型，因此调用了paddle的cast函数来进行显式转换</span><br>    a1 = (paddle.cast((z &gt; <span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) * z) <br>    a2 = (paddle.cast((z &lt;= <span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>) * (negative_slope * z))<br>    <span class="hljs-keyword">return</span> a1 + a2<br><br><span class="hljs-comment"># 在[-10,10]的范围内生成一系列的输入值，用于绘制relu、leaky_relu的函数曲线</span><br>z = paddle.linspace(-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10000</span>)<br><br>plt.figure()<br>plt.plot(z.tolist(), relu(z).tolist(), color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;ReLU Function&quot;</span>)<br>plt.plot(z.tolist(), leaky_relu(z).tolist(), color=<span class="hljs-string">&quot;#E20079&quot;</span>, linestyle=<span class="hljs-string">&quot;--&quot;</span>, label=<span class="hljs-string">&quot;LeakyReLU Function&quot;</span>)<br><br>ax = plt.gca()<br>ax.spines[<span class="hljs-string">&#x27;top&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;right&#x27;</span>].set_color(<span class="hljs-string">&#x27;none&#x27;</span>)<br>ax.spines[<span class="hljs-string">&#x27;left&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>ax.spines[<span class="hljs-string">&#x27;bottom&#x27;</span>].set_position((<span class="hljs-string">&#x27;data&#x27;</span>,<span class="hljs-number">0</span>))<br>plt.legend(loc=<span class="hljs-string">&#x27;upper left&#x27;</span>, fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-relu-leakyrelu.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172608024.png" srcset="/img/loading.gif" lazyload alt="image-20221214172608024"></p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214204504748.png" srcset="/img/loading.gif" lazyload alt="image-20221214172620106"></p>
<p><strong>说明</strong></p>
<p>在飞桨中，可以通过调用<code>paddle.nn.functional.relu</code>和<code>paddle.nn.functional.leaky_relu</code>完成ReLU与带泄露的ReLU的计算。</p>
<hr>
<p><strong>动手练习</strong><br>本节重点介绍和实现了几个经典的Sigmoid函数和ReLU函数。<br>请动手实现《神经网络与深度学习》4.1节中提到的其他激活函数，如：Hard-Logistic、Hard-Tanh、ELU、Softplus、Swish等。</p>
<hr>
<h3 id="√-4-2-基于前馈神经网络的二分类任务"><a href="#√-4-2-基于前馈神经网络的二分类任务" class="headerlink" title="[√] 4.2 - 基于前馈神经网络的二分类任务"></a>[√] 4.2 - 基于前馈神经网络的二分类任务</h3><p>前馈神经网络的网络结构如<strong>图4.3</strong>所示。每一层获取前一层神经元的活性值，并重复上述计算得到该层的活性值，传入到下一层。整个网络中无反馈，信号从输入层向输出层逐层的单向传播，得到网络最后的输出 $\boldsymbol{a}^{(L)}$。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/dbcf147a4e00446792eb2b93834e0f3154936e08ea124242af8631fde204381c" srcset="/img/loading.gif" lazyload ></center>
<center><br>图4.3: 前馈神经网络结构</br></center>



<hr>
<h4 id="√-4-2-1-数据集构建"><a href="#√-4-2-1-数据集构建" class="headerlink" title="[√] 4.2.1 - 数据集构建"></a>[√] 4.2.1 - 数据集构建</h4><p>这里，我们使用第3.1.1节中构建的二分类数据集：Moon1000数据集，其中训练集640条、验证集160条、测试集200条。 该数据集的数据是从两个带噪音的弯月形状数据分布中采样得到，每个样本包含2个特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.dataset <span class="hljs-keyword">import</span> make_moons<br><br><span class="hljs-comment"># 采样1000个样本</span><br>n_samples = <span class="hljs-number">1000</span><br>X, y = make_moons(n_samples=n_samples, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">0.5</span>)<br><br>num_train = <span class="hljs-number">640</span><br>num_dev = <span class="hljs-number">160</span><br>num_test = <span class="hljs-number">200</span><br><br>X_train, y_train = X[:num_train], y[:num_train]<br>X_dev, y_dev = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>X_test, y_test = X[num_train + num_dev:], y[num_train + num_dev:]<br><br>y_train = y_train.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_dev = y_dev.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br>y_test = y_test.reshape([-<span class="hljs-number">1</span>,<span class="hljs-number">1</span>])<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">outer_circ_x.shape: [<span class="hljs-number">500</span>] outer_circ_y.shape: [<span class="hljs-number">500</span>]<br>inner_circ_x.shape: [<span class="hljs-number">500</span>] inner_circ_y.shape: [<span class="hljs-number">500</span>]<br>after concat shape: [<span class="hljs-number">1000</span>]<br>X shape: [<span class="hljs-number">1000</span>, <span class="hljs-number">2</span>]<br>y shape: [<span class="hljs-number">1000</span>]<br></code></pre></td></tr></table></figure>

<hr>
<h4 id="√-4-2-2-模型构建"><a href="#√-4-2-2-模型构建" class="headerlink" title="[√] 4.2.2 - 模型构建"></a>[√] 4.2.2 - 模型构建</h4><p>&#x3D;&#x3D;经过仿射变换，得到该层神经元的净活性值z&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;再输入到激活函数得到该层神经元的活性值a&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;在实践中，为了提高模型的处理效率，通常将N个样本归为一组进行成批地计算。&#x3D;&#x3D;</p>
<p>为了更高效的构建前馈神经网络，我们先定义每一层的算子，然后再通过算子组合构建整个前馈神经网络。</p>
<p>假设网络的第$l$层的输入为第$l-1$层的神经元活性值$\boldsymbol{a}^{(l-1)}$，经过一个仿射变换，得到该层神经元的净活性值$\boldsymbol{z}$，再输入到激活函数得到该层神经元的活性值$\boldsymbol{a}$。</p>
<p>在实践中，为了提高模型的处理效率，通常将$N$个样本归为一组进行成批地计算。假设网络第$l$层的输入为$\boldsymbol{A}^{(l-1)}\in \mathbb{R}^{N\times M_{l-1}}$，其中每一行为一个样本，则前馈网络中第$l$层的计算公式为</p>
<p>$$<br>\mathbf Z^{(l)}&#x3D;\mathbf A^{(l-1)} \mathbf W^{(l)} +\mathbf b^{(l)}  \in \mathbb{R}^{N\times M_{l}}, (4.8)<br>$$<br>$$<br>\mathbf A^{(l)}&#x3D;f_l(\mathbf Z^{(l)}) \in \mathbb{R}^{N\times M_{l}}, (4.9)<br>$$<br>其中$\mathbf Z^{(l)}$为$N$个样本第$l$层神经元的净活性值，$\mathbf A^{(l)}$为$N$个样本第$l$层神经元的活性值，$\boldsymbol{W}^{(l)}\in \mathbb{R}^{M_{l-1}\times M_{l}}$为第$l$层的权重矩阵，$\boldsymbol{b}^{(l)}\in \mathbb{R}^{1\times M_{l}}$为第$l$层的偏置。</p>
<hr>
<p>为了和代码的实现保存一致性，这里使用形状为$(样本数量\times 特征维度)$的张量来表示一组样本。样本的矩阵$\boldsymbol{X}$是由$N$个$\boldsymbol{x}$的<strong>行向量</strong>组成。而《神经网络与深度学习》中$\boldsymbol{x}$为列向量，因此这里的权重矩阵$\boldsymbol{W}$和偏置$\boldsymbol{b}$和《神经网络与深度学习》中的表示刚好为转置关系。</p>
<hr>
<p>为了使后续的模型搭建更加便捷，我们将神经层的计算，即公式(4.8)和(4.9)，都封装成算子，这些算子都继承<code>Op</code>基类。</p>
<hr>
<h5 id="√-4-2-2-1-线性层算子"><a href="#√-4-2-2-1-线性层算子" class="headerlink" title="[√] 4.2.2.1 - 线性层算子"></a>[√] 4.2.2.1 - 线性层算子</h5><p>公式（4.8）对应一个线性层算子，权重参数采用默认的随机初始化，偏置采用默认的零初始化。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.op <span class="hljs-keyword">import</span> Op<br><br><span class="hljs-comment"># 实现线性层算子</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - input_size：输入数据维度</span><br><span class="hljs-string">            - output_size：输出数据维度</span><br><span class="hljs-string">            - name：算子名称</span><br><span class="hljs-string">            - weight_init：权重初始化方式，默认使用&#x27;paddle.standard_normal&#x27;进行标准正态分布初始化</span><br><span class="hljs-string">            - bias_init：偏置初始化方式，默认使用全0初始化</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <br>        self.params = &#123;&#125;<br>        <span class="hljs-comment"># 初始化权重</span><br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size,output_size])<br>        <span class="hljs-comment"># 初始化偏置</span><br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="hljs-number">1</span>,output_size])<br>        self.inputs = <span class="hljs-literal">None</span><br><br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs：shape=[N,input_size], N是样本数量</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：预测值，shape=[N,output_size]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.inputs = inputs<br><br>        outputs = paddle.matmul(self.inputs, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-2-2-Logistic算子"><a href="#√-4-2-2-2-Logistic算子" class="headerlink" title="[√] 4.2.2.2 - Logistic算子"></a>[√] 4.2.2.2 - Logistic算子</h5><p>本节我们采用Logistic函数来作为公式(4.9)中的激活函数。这里也将Logistic函数实现一个算子，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Logistic</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.inputs = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - inputs: shape=[N,D]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - outputs：shape=[N,D]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        outputs = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-inputs))<br>        self.outputs = outputs<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-2-3-层的串行组合"><a href="#√-4-2-2-3-层的串行组合" class="headerlink" title="[√] 4.2.2.3 - 层的串行组合"></a>[√] 4.2.2.3 - 层的串行组合</h5><p>&#x3D;&#x3D;在定义了神经层的线性层算子和激活函数算子之后，我们可以不断交叉重复使用它们来构建一个多层的神经网络。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;使用激活函数的作用是为了激活特征，让特征更有活力。将特征归一化，这样可以防止过拟合，通过这种正则化方式抑制模型的能力。通过归一化特征值，可以防止特征值量纲的不同导致的差异。&#x3D;&#x3D;</p>
<p>下面我们实现一个两层的用于二分类任务的前馈神经网络，选用Logistic作为激活函数，可以利用上面实现的线性层和激活函数算子来组装。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现一个两层前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - input_size：输入维度</span><br><span class="hljs-string">            - hidden_size：隐藏层神经元数量</span><br><span class="hljs-string">            - output_size：输出维度</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.fc1 = Linear(input_size, hidden_size, name=<span class="hljs-string">&quot;fc1&quot;</span>)<br>        self.act_fn1 = Logistic()<br>        self.fc2 = Linear(hidden_size, output_size, name=<span class="hljs-string">&quot;fc2&quot;</span>)<br>        self.act_fn2 = Logistic()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - X：shape=[N,input_size], N是样本数量</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - a2：预测值，shape=[N,output_size]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        z1 = self.fc1(X)<br>        a1 = self.act_fn1(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn2(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure>

<p><strong>测试一下</strong></p>
<p>现在，我们实例化一个两层的前馈网络，令其输入层维度为5，隐藏层维度为10，输出层维度为1。 并随机生成一条长度为5的数据输入两层神经网络，观察输出结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化模型</span><br>model = Model_MLP_L2(input_size=<span class="hljs-number">5</span>, hidden_size=<span class="hljs-number">10</span>, output_size=<span class="hljs-number">1</span>)<br><span class="hljs-comment"># 随机生成1条长度为5的数据</span><br>X = paddle.rand(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">5</span>])<br>result = model(X)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;result: &quot;</span>, result)<br></code></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros">result:  Tensor(shape=[1, 1], <span class="hljs-attribute">dtype</span>=float32, <span class="hljs-attribute">place</span>=CPUPlace, <span class="hljs-attribute">stop_gradient</span>=<span class="hljs-literal">True</span>,<br>       [[0.65805507]])<br></code></pre></td></tr></table></figure>

<hr>
<h4 id="√-4-2-3-损失函数"><a href="#√-4-2-3-损失函数" class="headerlink" title="[√] 4.2.3 - 损失函数"></a>[√] 4.2.3 - 损失函数</h4><p>二分类交叉熵损失函数见第三章，这里不再赘述。</p>
<hr>
<h4 id="√-4-2-4-模型优化"><a href="#√-4-2-4-模型优化" class="headerlink" title="[√] 4.2.4 - 模型优化"></a>[√] 4.2.4 - 模型优化</h4><p>&#x3D;&#x3D;神经网络的参数主要是通过<strong>梯度下降法</strong>进行优化的，因此需要计算最终损失对每个参数的梯度。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;由于神经网络的层数通常比较深，其梯度计算和上一章中的线性分类模型的不同的点在于：线性模型通常比较简单可以直接计算梯度，而神经网络相当于一个复合函数，需要利用链式法则进行反向传播来计算梯度。&#x3D;&#x3D;</p>
<hr>
<h5 id="√-4-2-4-1-反向传播算法"><a href="#√-4-2-4-1-反向传播算法" class="headerlink" title="[√] 4.2.4.1 - 反向传播算法"></a>[√] 4.2.4.1 - 反向传播算法</h5><p>前馈神经网络的参数梯度通常使用<strong>误差反向传播</strong>算法来计算。使用误差反向传播算法的前馈神经网络训练过程可以分为以下三步：</p>
<ol>
<li>前馈计算每一层的净活性值$\boldsymbol{Z}^{(l)}$和激活值$\boldsymbol{A}^ {(l)}$，直到最后一层；</li>
<li>反向传播计算每一层的误差项$\delta^{(l)}&#x3D;\frac{\partial R}{\partial \boldsymbol{Z}^{(l)}}$；</li>
<li>计算每一层参数的梯度，并更新参数。</li>
</ol>
<p>在上面实现算子的基础上，来实现误差反向传播算法。在上面的三个步骤中，</p>
<ol>
<li>第1步是前向计算，可以利用算子的<code>forward()</code>方法来实现；</li>
<li>第2步是反向计算梯度，可以利用算子的<code>backward()</code>方法来实现；</li>
<li>第3步中的计算参数梯度也放到<code>backward()</code>中实现，更新参数放到另外的<strong>优化器</strong>中专门进行。</li>
</ol>
<p>这样，在模型训练过程中，我们首先执行模型的<code>forward()</code>，再执行模型的<code>backward()</code>，就得到了所有参数的梯度，之后再利用优化器迭代更新参数。</p>
<p>以这我们这节中构建的两层全连接前馈神经网络<code>Model_MLP_L2</code>为例，下图给出了其前向和反向计算过程：</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214172620106.png" srcset="/img/loading.gif" lazyload></p>
<p>下面我们按照反向的梯度传播顺序，为每个算子添加<code>backward()</code>方法，并在其中实现每一层参数的梯度的计算。</p>
<hr>
<h5 id="√-4-2-4-2-损失函数"><a href="#√-4-2-4-2-损失函数" class="headerlink" title="[√] 4.2.4.2 - 损失函数"></a>[√] 4.2.4.2 - 损失函数</h5><p>二分类交叉熵损失函数对神经网络的输出$\hat{\boldsymbol{y}}$的偏导数为:<br>$$<br>\frac{\partial R}{\partial \hat{\boldsymbol{y}}} &#x3D;  -\frac{1}{N}(\mathrm{dialog}(\frac{1}{\hat{\boldsymbol{y}}})\boldsymbol{y}-\mathrm{dialog}(\frac{1}{1-\hat{\boldsymbol{y}}})(1-\boldsymbol{y})) (4.10) \<br>&#x3D; -\frac{1}{N}(\frac{1}{\hat{\boldsymbol{y}}}\odot\boldsymbol{y}-\frac{1}{1-\hat{\boldsymbol{y}}}\odot(1-\boldsymbol{y})), (4.11)<br>$$<br>其中$dialog(\boldsymbol{x})$表示以向量$\boldsymbol{x}$为对角元素的对角阵，$\frac{1}{\boldsymbol{x}}&#x3D;\frac{1}{x_1},…,\frac{1}{x_N}$表示逐元素除，$\odot$表示逐元素积。</p>
<ul>
<li>实现损失函数的<code>backward()</code>，代码实现如下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实现交叉熵损失函数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BinaryCrossEntropyLoss</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model</span>):<br>        self.predicts = <span class="hljs-literal">None</span><br>        self.labels = <span class="hljs-literal">None</span><br>        self.num = <span class="hljs-literal">None</span><br><br>        self.model = model<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-keyword">return</span> self.forward(predicts, labels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, predicts, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - predicts：预测值，shape=[N, 1]，N为样本数量</span><br><span class="hljs-string">            - labels：真实标签，shape=[N, 1]</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失值：shape=[1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.predicts = predicts<br>        self.labels = labels<br>        self.num = self.predicts.shape[<span class="hljs-number">0</span>]<br>        loss = -<span class="hljs-number">1.</span> / self.num * (paddle.matmul(self.labels.t(), paddle.log(self.predicts)) <br>                + paddle.matmul((<span class="hljs-number">1</span>-self.labels.t()), paddle.log(<span class="hljs-number">1</span>-self.predicts)))<br><br>        loss = paddle.squeeze(loss, axis=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> loss<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 计算损失函数对模型预测的导数</span><br>        loss_grad_predicts = -<span class="hljs-number">1.0</span> * (self.labels / self.predicts - <br>                       (<span class="hljs-number">1</span> - self.labels) / (<span class="hljs-number">1</span> - self.predicts)) / self.num<br>        <br>        <span class="hljs-comment"># 梯度反向传播</span><br>        self.model.backward(loss_grad_predicts)<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-4-3-Logistic算子"><a href="#√-4-2-4-3-Logistic算子" class="headerlink" title="[√] 4.2.4.3 - Logistic算子"></a>[√] 4.2.4.3 - Logistic算子</h5><p>在本节中，我们使用Logistic激活函数，所以这里为Logistic算子增加的反向函数。</p>
<p>Logistic算子的前向过程表示为$\boldsymbol{A}&#x3D;\sigma(\boldsymbol{Z})$，其中$\sigma$为Logistic函数，$\boldsymbol{Z} \in R^{N \times D}$和$\boldsymbol{A} \in R^{N \times D}$的每一行表示一个样本。</p>
<p>为了简便起见，我们分别用向量$\boldsymbol{a} \in R^D$ 和 $\boldsymbol{z} \in R^D$表示同一个样本在激活函数前后的表示，则$\boldsymbol{a}$对$\boldsymbol{z}$的偏导数为：<br>$$<br>\frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}&#x3D;diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\in R^{D \times D}, (4.12)<br>$$<br>按照反向传播算法，令$\delta_{\boldsymbol{a}}&#x3D;\frac{\partial R}{\partial \boldsymbol{a}} \in R^D$表示最终损失$R$对Logistic算子的单个输出$\boldsymbol{a}$的梯度，则<br>$$<br>\delta_{\boldsymbol{z}} \triangleq \frac{\partial R}{\partial \boldsymbol{z}} &#x3D; \frac{\partial \boldsymbol{a}}{\partial \boldsymbol{z}}\delta_{\boldsymbol{a}}  (4.13) \<br>&#x3D; diag(\boldsymbol{a}\odot(1-\boldsymbol{a}))\delta_{\boldsymbol(a)}, (4.14) \<br>&#x3D; \boldsymbol{a}\odot(1-\boldsymbol{a})\odot\delta_{\boldsymbol(a)}。 (4.15)<br>$$</p>
<p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{A}} &#x3D;\frac{\partial R}{\partial \boldsymbol{A}} \in R^{N \times D}$表示最终损失$R$对Logistic算子输出$A$的梯度，损失函数对Logistic函数输入$\boldsymbol{Z}$的导数为<br>$$<br>\delta_{\boldsymbol{Z}}&#x3D;\boldsymbol{A} \odot (1-\boldsymbol{A})\odot \delta_{\boldsymbol{A}} \in R^{N \times D},(4.16)<br>$$<br>$\delta_{\boldsymbol{Z}}$为Logistic算子反向传播的输出。</p>
<p>由于Logistic函数中没有参数，这里不需要在<code>backward()</code>方法中计算该算子参数的梯度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Logistic</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.inputs = <span class="hljs-literal">None</span><br>        self.outputs = <span class="hljs-literal">None</span><br>        self.params = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = <span class="hljs-number">1.0</span> / (<span class="hljs-number">1.0</span> + paddle.exp(-inputs))<br>        self.outputs = outputs<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        <span class="hljs-comment"># 计算Logistic激活函数对输入的导数</span><br>        outputs_grad_inputs = paddle.multiply(self.outputs, (<span class="hljs-number">1.0</span> - self.outputs))<br>        <span class="hljs-keyword">return</span> paddle.multiply(grads,outputs_grad_inputs)<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-4-4-线性层"><a href="#√-4-2-4-4-线性层" class="headerlink" title="[√] 4.2.4.4 - 线性层"></a>[√] 4.2.4.4 - 线性层</h5><p>线性层算子Linear的前向过程表示为$\boldsymbol{Y}&#x3D;\boldsymbol{X}\boldsymbol{W}+\boldsymbol{b}$，其中输入为$\boldsymbol{X} \in R^{N \times M}$，输出为$\boldsymbol{Y} \in R^{N \times D}$，参数为权重矩阵$\boldsymbol{W} \in R^{M \times D}$和偏置$\boldsymbol{b} \in R^{1 \times D}$。$\boldsymbol{X}$和$\boldsymbol{Y}$中的每一行表示一个样本。</p>
<p>为了简便起见，我们用向量$\boldsymbol{x}\in R^M$和$\boldsymbol{y}\in R^D$表示同一个样本在线性层算子中的输入和输出，则有$\boldsymbol{y}&#x3D;\boldsymbol{W}^T\boldsymbol{x}+\boldsymbol{b}^T$。$\boldsymbol{y}$对输入$\boldsymbol{x}$的偏导数为<br>$$<br>\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}} &#x3D; \boldsymbol{W}\in R^{D \times M}。(4.17)<br>$$</p>
<p>&#x3D;&#x3D;线性层输入的梯度&#x3D;&#x3D; </p>
<p>按照反向传播算法，令$\delta_{\boldsymbol{y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{y}}\in R^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则<br>$$<br>\delta_{\boldsymbol{x}} \triangleq \frac{\partial R}{\partial \boldsymbol{x}}&#x3D; \boldsymbol{W} \delta_{\boldsymbol{y}}。(4.18)<br>$$</p>
<p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，公式可以重写为<br>$$<br>\delta_{\boldsymbol{X}} &#x3D;\delta_{\boldsymbol{Y}} \boldsymbol{W}^T,(4.19)<br>$$<br>其中$\delta_{\boldsymbol{X}}$为线性层算子反向函数的输出。</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213744716.png" srcset="/img/loading.gif" lazyload alt="image-20221214204504748"></p>
<p>&#x3D;&#x3D;线性层参数的梯度&#x3D;&#x3D;</p>
<p><strong>计算线性层参数的梯度</strong> 由于线性层算子中包含有可学习的参数$\boldsymbol{W}$和$\boldsymbol{b}$，因此<code>backward()</code>除了实现梯度反传外，还需要计算算子内部的参数的梯度。</p>
<p>令$\delta_{\boldsymbol{y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{y}}\in \mathbb{R}^D$表示最终损失$R$对线性层算子的单个输出$\boldsymbol{y}$的梯度，则<br>$$<br>\delta_{\boldsymbol{W}} \triangleq \frac{\partial R}{\partial \boldsymbol{W}} &#x3D; \boldsymbol{x}\delta_{\boldsymbol{y}}^T,(4.20) \<br>\delta_{\boldsymbol{b}} \triangleq \frac{\partial R}{\partial \boldsymbol{b}} &#x3D; \delta_{\boldsymbol{y}}^T。(4.21)<br>$$</p>
<p>将上面公式利用批量数据表示的方式重写，令$\delta_{\boldsymbol{Y}}&#x3D;\frac{\partial R}{\partial \boldsymbol{Y}}\in \mathbb{R}^{N\times D}$表示最终损失$R$对线性层算子输出$\boldsymbol{Y}$的梯度，则公式可以重写为<br>$$<br>\delta_{\boldsymbol{W}} &#x3D; \boldsymbol{X}^T \delta_{\boldsymbol{Y}},(4.22) \<br>\delta_{\boldsymbol{b}} &#x3D; \mathbf{1}^T \delta_{\boldsymbol{Y}}。(4.23)<br>$$</p>
<p>具体实现代码如下：</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/8562dfb10d464396948d05ee3620cec1d057025dddee43ff92dae3fbb72e8f65.png" srcset="/img/loading.gif" lazyload alt="image-20221214205337207"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Linear</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, name, weight_init=paddle.standard_normal, bias_init=paddle.zeros</span>):<br>        self.params = &#123;&#125;<br>        self.params[<span class="hljs-string">&#x27;W&#x27;</span>] = weight_init(shape=[input_size, output_size])<br>        self.params[<span class="hljs-string">&#x27;b&#x27;</span>] = bias_init(shape=[<span class="hljs-number">1</span>, output_size])<br><br>        self.inputs = <span class="hljs-literal">None</span><br>        self.grads = &#123;&#125;<br><br>        self.name = name<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        self.inputs = inputs<br>        outputs = paddle.matmul(self.inputs, self.params[<span class="hljs-string">&#x27;W&#x27;</span>]) + self.params[<span class="hljs-string">&#x27;b&#x27;</span>]<br>        <span class="hljs-keyword">return</span> outputs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, grads</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">            - grads：损失函数对当前层输出的导数</span><br><span class="hljs-string">        输出：</span><br><span class="hljs-string">            - 损失函数对当前层输入的导数</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        self.grads[<span class="hljs-string">&#x27;W&#x27;</span>] = paddle.matmul(self.inputs.T, grads)<br>        self.grads[<span class="hljs-string">&#x27;b&#x27;</span>] = paddle.<span class="hljs-built_in">sum</span>(grads, axis=<span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 线性层输入的梯度</span><br>        <span class="hljs-keyword">return</span> paddle.matmul(grads, self.params[<span class="hljs-string">&#x27;W&#x27;</span>].T)<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-4-5-整个网络"><a href="#√-4-2-4-5-整个网络" class="headerlink" title="[√] 4.2.4.5 - 整个网络"></a>[√] 4.2.4.5 - 整个网络</h5><p>实现完整的两层神经网络的前向和反向计算。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2</span>(<span class="hljs-title class_ inherited__">Op</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-comment"># 线性层</span><br>        self.fc1 = Linear(input_size, hidden_size, name=<span class="hljs-string">&quot;fc1&quot;</span>)<br>        <span class="hljs-comment"># Logistic激活函数层</span><br>        self.act_fn1 = Logistic()<br>        self.fc2 = Linear(hidden_size, output_size, name=<span class="hljs-string">&quot;fc2&quot;</span>)<br>        self.act_fn2 = Logistic()<br><br>        self.layers = [self.fc1, self.act_fn1, self.fc2, self.act_fn2]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.forward(X)<br><br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        z1 = self.fc1(X)<br>        a1 = self.act_fn1(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn2(z2)<br>        <span class="hljs-keyword">return</span> a2<br>        <br>    <span class="hljs-comment"># 反向计算，因为是反向传播，因此对输入的倒数，通过对输出的倒数推得，←</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">backward</span>(<span class="hljs-params">self, loss_grad_a2</span>):<br>        loss_grad_z2 = self.act_fn2.backward(loss_grad_a2)<br>        loss_grad_a1 = self.fc2.backward(loss_grad_z2)<br>        loss_grad_z1 = self.act_fn1.backward(loss_grad_a1)<br>        loss_grad_inputs = self.fc1.backward(loss_grad_z1)<br></code></pre></td></tr></table></figure>



<hr>
<h5 id="√-4-2-4-6-优化器"><a href="#√-4-2-4-6-优化器" class="headerlink" title="[√] 4.2.4.6 - 优化器"></a>[√] 4.2.4.6 - 优化器</h5><p>在计算好神经网络参数的梯度之后，我们将梯度下降法中参数的更新过程实现在优化器中。</p>
<p>与第3章中实现的梯度下降优化器<code>SimpleBatchGD</code>不同的是，此处的优化器需要遍历每层，对每层的参数分别做更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.opitimizer <span class="hljs-keyword">import</span> Optimizer<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BatchGD</span>(<span class="hljs-title class_ inherited__">Optimizer</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, init_lr, model</span>):<br>        <span class="hljs-built_in">super</span>(BatchGD, self).__init__(init_lr=init_lr, model=model)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 参数更新</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> layer.params.keys():<br>                    layer.params[key] = layer.params[key] - self.init_lr * layer.grads[key]<br></code></pre></td></tr></table></figure>





<hr>
<h4 id="√-4-2-5-完善Runner类：RunnerV2-1"><a href="#√-4-2-5-完善Runner类：RunnerV2-1" class="headerlink" title="[√] 4.2.5 - 完善Runner类：RunnerV2_1"></a>[√] 4.2.5 - 完善Runner类：RunnerV2_1</h4><p>基于3.1.6实现的 RunnerV2 类主要针对比较简单的模型。而在本章中，模型由多个算子组合而成，通常比较复杂，因此本节继续完善并实现一个改进版： <code>RunnerV2_1</code>类，其主要加入的功能有：</p>
<ol>
<li>支持自定义算子的梯度计算，在训练过程中调用<code>self.loss_fn.backward()</code>从损失函数开始反向计算梯度；</li>
<li>每层的模型保存和加载，将每一层的参数分别进行保存和加载。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV2_1</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, metric, loss_fn, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric<br><br>        <span class="hljs-comment"># 记录训练过程中的评估指标变化情况</span><br>        self.train_scores = []<br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.train_loss = []<br>        self.dev_loss = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_set, dev_set, **kwargs</span>):<br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_epochs = kwargs.get(<span class="hljs-string">&quot;log_epochs&quot;</span>, <span class="hljs-number">100</span>)<br><br>        <span class="hljs-comment"># 传入模型保存路径</span><br>        save_dir = kwargs.get(<span class="hljs-string">&quot;save_dir&quot;</span>, <span class="hljs-literal">None</span>)<br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        best_score = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            X, y = train_set<br>            <span class="hljs-comment"># 获取模型预测</span><br>            logits = self.model(X)<br>            <span class="hljs-comment"># 计算交叉熵损失</span><br>            trn_loss = self.loss_fn(logits, y) <span class="hljs-comment"># return a tensor</span><br>            <br>            self.train_loss.append(trn_loss.item())<br>            <span class="hljs-comment"># 计算评估指标</span><br>            trn_score = self.metric(logits, y).item()<br>            self.train_scores.append(trn_score)<br><br>            <span class="hljs-comment"># alec：此处是主动计算梯度</span><br>            self.loss_fn.backward() <span class="hljs-comment"># 反向传播计算梯度</span><br><br>            <span class="hljs-comment"># 参数更新</span><br>            self.optimizer.step() <span class="hljs-comment"># 梯度下降方法更新梯度</span><br>           <br>            dev_score, dev_loss = self.evaluate(dev_set)<br>            <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>            <span class="hljs-keyword">if</span> dev_score &gt; best_score:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                best_score = dev_score<br>                <span class="hljs-keyword">if</span> save_dir:<br>                    self.save_model(save_dir)<br><br>            <span class="hljs-keyword">if</span> log_epochs <span class="hljs-keyword">and</span> epoch % log_epochs == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, loss: <span class="hljs-subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)<br>    <br>    <span class="hljs-comment"># 验证不需要计算梯度和更新参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set</span>):<br>        X, y = data_set<br>        <span class="hljs-comment"># 计算模型输出</span><br>        logits = self.model(X)<br>        <span class="hljs-comment"># 计算损失函数</span><br>        loss = self.loss_fn(logits, y).item()<br>        self.dev_loss.append(loss)<br>        <span class="hljs-comment"># 计算评估指标</span><br>        score = self.metric(logits, y).item()<br>        self.dev_scores.append(score)<br>        <span class="hljs-keyword">return</span> score, loss<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-keyword">return</span> self.model(X)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_dir</span>):<br>        <span class="hljs-comment"># 对模型每层参数分别进行保存，保存文件名称与该层名称相同</span><br>        <span class="hljs-comment"># 按层保存模型</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                paddle.save(layer.params, os.path.join(save_dir, layer.name+<span class="hljs-string">&quot;.pdparams&quot;</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_dir</span>):<br>        <span class="hljs-comment"># 获取所有层参数名称和保存路径之间的对应关系</span><br>        model_file_names = os.listdir(model_dir)<br>        name_file_dict = &#123;&#125;<br>        <span class="hljs-keyword">for</span> file_name <span class="hljs-keyword">in</span> model_file_names:<br>            name = file_name.replace(<span class="hljs-string">&quot;.pdparams&quot;</span>,<span class="hljs-string">&quot;&quot;</span>)<br>            name_file_dict[name] = os.path.join(model_dir, file_name)<br><br>        <span class="hljs-comment"># 加载每层参数</span><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.model.layers: <span class="hljs-comment"># 遍历所有层</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(layer.params, <span class="hljs-built_in">dict</span>):<br>                name = layer.name<br>                file_path = name_file_dict[name]<br>                layer.params = paddle.load(file_path)<br></code></pre></td></tr></table></figure>





<hr>
<h4 id="√-4-2-6-模型训练"><a href="#√-4-2-6-模型训练" class="headerlink" title="[√] 4.2.6 - 模型训练"></a>[√] 4.2.6 - 模型训练</h4><p>基于<code>RunnerV2_1</code>，使用训练集和验证集进行模型训练，共训练2000个epoch。评价指标为第章介绍的<code>accuracy</code>。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nndl.metric <span class="hljs-keyword">import</span> accuracy<br>paddle.seed(<span class="hljs-number">123</span>)<br>epoch_num = <span class="hljs-number">1000</span><br><br>model_saved_dir = <span class="hljs-string">&quot;model&quot;</span><br><br><span class="hljs-comment"># 输入层维度为2</span><br>input_size = <span class="hljs-number">2</span><br><span class="hljs-comment"># 隐藏层维度为5</span><br>hidden_size = <span class="hljs-number">5</span><br><span class="hljs-comment"># 输出层维度为1</span><br>output_size = <span class="hljs-number">1</span><br><br><span class="hljs-comment"># 定义网络</span><br>model = Model_MLP_L2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = BinaryCrossEntropyLoss(model)<br><br><span class="hljs-comment"># 优化器</span><br>learning_rate = <span class="hljs-number">0.2</span><br>optimizer = BatchGD(learning_rate, model)<br><br><span class="hljs-comment"># 评价方法</span><br>metric = accuracy<br><br><span class="hljs-comment"># 实例化RunnerV2_1类，并传入训练配置</span><br>runner = RunnerV2_1(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="hljs-number">50</span>, save_dir=model_saved_dir)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><code class="hljs python">运行耗时: <span class="hljs-number">2</span>秒<span class="hljs-number">843</span>毫秒<br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.20000</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.7360124588012695</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.20000</span> --&gt; <span class="hljs-number">0.21875</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.21875</span> --&gt; <span class="hljs-number">0.29375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.29375</span> --&gt; <span class="hljs-number">0.32500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.32500</span> --&gt; <span class="hljs-number">0.39375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.39375</span> --&gt; <span class="hljs-number">0.44375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.44375</span> --&gt; <span class="hljs-number">0.67500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.67500</span> --&gt; <span class="hljs-number">0.70000</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.70000</span> --&gt; <span class="hljs-number">0.71250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.71250</span> --&gt; <span class="hljs-number">0.72500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.72500</span> --&gt; <span class="hljs-number">0.73125</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.73125</span> --&gt; <span class="hljs-number">0.74375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.74375</span> --&gt; <span class="hljs-number">0.75000</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.75000</span> --&gt; <span class="hljs-number">0.75625</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.75625</span> --&gt; <span class="hljs-number">0.76250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.76250</span> --&gt; <span class="hljs-number">0.76875</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.76875</span> --&gt; <span class="hljs-number">0.77500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.77500</span> --&gt; <span class="hljs-number">0.78125</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.78125</span> --&gt; <span class="hljs-number">0.78750</span><br>[Train] epoch: <span class="hljs-number">50</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.6630627512931824</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.78750</span> --&gt; <span class="hljs-number">0.79375</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.79375</span> --&gt; <span class="hljs-number">0.80000</span><br>[Train] epoch: <span class="hljs-number">100</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.5919685959815979</span><br>[Train] epoch: <span class="hljs-number">150</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.5248624086380005</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80000</span> --&gt; <span class="hljs-number">0.80625</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.80625</span> --&gt; <span class="hljs-number">0.81250</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.81250</span> --&gt; <span class="hljs-number">0.81875</span><br>[Train] epoch: <span class="hljs-number">200</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.48363637924194336</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.81875</span> --&gt; <span class="hljs-number">0.82500</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.82500</span> --&gt; <span class="hljs-number">0.83125</span><br>[Train] epoch: <span class="hljs-number">250</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.46238335967063904</span><br>[Train] epoch: <span class="hljs-number">300</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4515562951564789</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.83125</span> --&gt; <span class="hljs-number">0.83750</span><br>[Train] epoch: <span class="hljs-number">350</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44589540362358093</span><br>[Train] epoch: <span class="hljs-number">400</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44286662340164185</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.83750</span> --&gt; <span class="hljs-number">0.84375</span><br>[Train] epoch: <span class="hljs-number">450</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44121456146240234</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.84375</span> --&gt; <span class="hljs-number">0.85000</span><br>[Train] epoch: <span class="hljs-number">500</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.44029098749160767</span><br>[Train] epoch: <span class="hljs-number">550</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43975430727005005</span><br>[Train] epoch: <span class="hljs-number">600</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4394233822822571</span><br>[Train] epoch: <span class="hljs-number">650</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43920236825942993</span><br>[Train] epoch: <span class="hljs-number">700</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4390408992767334</span><br>[Train] epoch: <span class="hljs-number">750</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4389124810695648</span><br>[Train] epoch: <span class="hljs-number">800</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43880319595336914</span><br>[Train] epoch: <span class="hljs-number">850</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.4387057423591614</span><br>[Train] epoch: <span class="hljs-number">900</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43861618638038635</span><br>[Train] epoch: <span class="hljs-number">950</span>/<span class="hljs-number">1000</span>, loss: <span class="hljs-number">0.43853235244750977</span><br></code></pre></td></tr></table></figure>

<p>可视化观察训练集与验证集的损失函数变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印训练集和验证集的损失</span><br>plt.figure()<br>plt.plot(<span class="hljs-built_in">range</span>(epoch_num), runner.train_loss, color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>plt.plot(<span class="hljs-built_in">range</span>(epoch_num), runner.dev_loss, color=<span class="hljs-string">&quot;#E20079&quot;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.legend(fontsize=<span class="hljs-string">&#x27;large&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-loss2.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214205337207.png" srcset="/img/loading.gif" lazyload alt="image-20221214213744716"></p>
<hr>
<h4 id="√-4-2-7-性能评价"><a href="#√-4-2-7-性能评价" class="headerlink" title="[√] 4.2.7 - 性能评价"></a>[√] 4.2.7 - 性能评价</h4><p>使用测试集对训练中的最优模型进行评价，观察模型的评价指标。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载训练好的模型</span><br>runner.load_model(model_saved_dir)<br><span class="hljs-comment"># 在测试集上对模型进行评价</span><br>score, loss = runner.evaluate([X_test, y_test])<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] score/loss: <span class="hljs-number">0.8350</span>/<span class="hljs-number">0.4016</span><br></code></pre></td></tr></table></figure>

<p>从结果来看，模型在测试集上取得了较高的准确率。</p>
<p>下面对结果进行可视化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><br><span class="hljs-comment"># 均匀生成40000个数据点</span><br>x1, x2 = paddle.meshgrid(paddle.linspace(-math.pi, math.pi, <span class="hljs-number">200</span>), paddle.linspace(-math.pi, math.pi, <span class="hljs-number">200</span>))<br>x = paddle.stack([paddle.flatten(x1), paddle.flatten(x2)], axis=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 预测对应类别</span><br>y = runner.predict(x)<br>y = paddle.squeeze(paddle.cast((y&gt;=<span class="hljs-number">0.5</span>),dtype=<span class="hljs-string">&#x27;float32&#x27;</span>),axis=-<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># 绘制类别区域</span><br>plt.ylabel(<span class="hljs-string">&#x27;x2&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;x1&#x27;</span>)<br>plt.scatter(x[:,<span class="hljs-number">0</span>].tolist(), x[:,<span class="hljs-number">1</span>].tolist(), c=y.tolist(), cmap=plt.cm.Spectral)<br><br>plt.scatter(X_train[:, <span class="hljs-number">0</span>].tolist(), X_train[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_train,axis=-<span class="hljs-number">1</span>).tolist())<br>plt.scatter(X_dev[:, <span class="hljs-number">0</span>].tolist(), X_dev[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_dev,axis=-<span class="hljs-number">1</span>).tolist())<br>plt.scatter(X_test[:, <span class="hljs-number">0</span>].tolist(), X_test[:, <span class="hljs-number">1</span>].tolist(), marker=<span class="hljs-string">&#x27;*&#x27;</span>, c=paddle.squeeze(y_test,axis=-<span class="hljs-number">1</span>).tolist())<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222637659.png" srcset="/img/loading.gif" lazyload alt="image-20221214213927961"></p>
<hr>
<h3 id="√-4-3-自动梯度计算和预定义算子"><a href="#√-4-3-自动梯度计算和预定义算子" class="headerlink" title="[√] 4.3 - 自动梯度计算和预定义算子"></a>[√] 4.3 - 自动梯度计算和预定义算子</h3><p>虽然我们能够通过模块化的方式比较好地对神经网络进行组装，但是每个模块的梯度计算过程仍然十分繁琐且容易出错。在深度学习框架中，已经封装了自动梯度计算的功能，我们只需要聚焦模型架构，不再需要耗费精力进行计算梯度。</p>
<p>飞桨提供了<code>paddle.nn.Layer</code>类，来方便快速的实现自己的层和模型。模型和层都可以基于<code>paddle.nn.Layer</code>扩充实现，模型只是一种特殊的层。</p>
<p>继承了<code>paddle.nn.Layer</code>类的算子中，可以在内部直接调用其它继承<code>paddle.nn.Layer</code>类的算子，飞桨框架会自动识别算子中内嵌的<code>paddle.nn.Layer</code>类算子，并自动计算它们的梯度，并在优化时更新它们的参数。</p>
<hr>
<h4 id="√-4-3-1-利用预定义算子重新实现前馈神经网络"><a href="#√-4-3-1-利用预定义算子重新实现前馈神经网络" class="headerlink" title="[√] 4.3.1 - 利用预定义算子重新实现前馈神经网络"></a>[√] 4.3.1 - 利用预定义算子重新实现前馈神经网络</h4><p>下面我们使用Paddle的预定义算子来重新实现二分类任务。 主要使用到的预定义算子为<code>paddle.nn.Linear</code>：</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs reasonml"><span class="hljs-keyword">class</span> paddle.nn.<span class="hljs-constructor">Linear(<span class="hljs-params">in_features</span>, <span class="hljs-params">out_features</span>, <span class="hljs-params">weight_attr</span>=None, <span class="hljs-params">bias_attr</span>=None, <span class="hljs-params">name</span>=None)</span><br></code></pre></td></tr></table></figure>

<p><code>paddle.nn.Linear</code>算子可以接受一个形状为[batch_size,∗,in_features]的<strong>输入张量</strong>，其中”∗”表示张量中可以有任意的其它额外维度，并计算它与形状为[in_features, out_features]的<strong>权重矩阵</strong>的乘积，然后生成形状为[batch_size,∗,out_features]的<strong>输出张量</strong>。 <code>paddle.nn.Linear</code>算子默认有偏置参数，可以通过<code>bias_attr=False</code>设置不带偏置。</p>
<hr>
<p>paddle.nn 目录下包含飞桨框架支持的神经网络层和相关函数的相关API。paddle.nn.functional下都是一些函数实现。</p>
<hr>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> paddle.nn.initializer <span class="hljs-keyword">import</span> Constant, Normal, Uniform<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V2</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V2, self).__init__()<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span><br>        <span class="hljs-comment"># 其中第一个参数（in_features）为线性层输入维度；第二个参数（out_features）为线性层输出维度</span><br>        <span class="hljs-comment"># weight_attr为权重参数属性，这里使用&#x27;paddle.nn.initializer.Normal&#x27;进行随机高斯分布初始化</span><br>        <span class="hljs-comment"># bias_attr为偏置参数属性，这里使用&#x27;paddle.nn.initializer.Constant&#x27;进行常量初始化</span><br>        self.fc1 = nn.Linear(input_size, hidden_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="hljs-number">0.</span>, std=<span class="hljs-number">1.</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        self.fc2 = nn.Linear(hidden_size, output_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Normal(mean=<span class="hljs-number">0.</span>, std=<span class="hljs-number">1.</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span><br>        self.act_fn = F.sigmoid<br>        <br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        z1 = self.fc1(inputs)<br>        a1 = self.act_fn(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure>



<hr>
<h4 id="√-4-3-2-完善Runner类"><a href="#√-4-3-2-完善Runner类" class="headerlink" title="[√] 4.3.2 - 完善Runner类"></a>[√] 4.3.2 - 完善Runner类</h4><p>基于上一节实现的 <code>RunnerV2_1</code> 类，本节的 RunnerV2_2 类在训练过程中使用自动梯度计算；模型保存时，使用<code>state_dict</code>方法获取模型参数；模型加载时，使用<code>set_state_dict</code>方法加载模型参数.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV2_2</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, metric, loss_fn, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric<br><br>        <span class="hljs-comment"># 记录训练过程中的评估指标变化情况</span><br>        self.train_scores = []<br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.train_loss = []<br>        self.dev_loss = []<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_set, dev_set, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型切换为训练模式</span><br>        self.model.train()<br><br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_epochs = kwargs.get(<span class="hljs-string">&quot;log_epochs&quot;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span><br>        save_path = kwargs.get(<span class="hljs-string">&quot;save_path&quot;</span>, <span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br>        <span class="hljs-comment"># log打印函数，如果没有传入则默认为&quot;None&quot;</span><br>        custom_print_log = kwargs.get(<span class="hljs-string">&quot;custom_print_log&quot;</span>, <span class="hljs-literal">None</span>) <br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        best_score = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            X, y = train_set<br>            <span class="hljs-comment"># 获取模型预测</span><br>            logits = self.model(X)<br>            <span class="hljs-comment"># 计算交叉熵损失</span><br>            trn_loss = self.loss_fn(logits, y)<br>            self.train_loss.append(trn_loss.item())<br>            <span class="hljs-comment"># 计算评估指标</span><br>            trn_score = self.metric(logits, y).item()<br>            self.train_scores.append(trn_score)<br><br>            <span class="hljs-comment"># 自动计算参数梯度</span><br>            <span class="hljs-comment"># alec：此处通过trn_loss自动计算梯度</span><br>            trn_loss.backward()<br>            <span class="hljs-keyword">if</span> custom_print_log <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-comment"># 打印每一层的梯度</span><br>                custom_print_log(self)<br><br>            <span class="hljs-comment"># 参数更新</span><br>            self.optimizer.step()<br>            <span class="hljs-comment"># 清空梯度</span><br>            self.optimizer.clear_grad()<br><br>            dev_score, dev_loss = self.evaluate(dev_set)<br>            <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>            <span class="hljs-keyword">if</span> dev_score &gt; best_score:<br>                self.save_model(save_path)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                best_score = dev_score<br><br>            <span class="hljs-keyword">if</span> log_epochs <span class="hljs-keyword">and</span> epoch % log_epochs == <span class="hljs-number">0</span>:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, loss: <span class="hljs-subst">&#123;trn_loss.item()&#125;</span>&quot;</span>)<br>                <br>    <span class="hljs-comment"># 模型评估阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, data_set</span>):<br>        <span class="hljs-comment"># 将模型切换为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br><br>        X, y = data_set<br>        <span class="hljs-comment"># 计算模型输出</span><br>        logits = self.model(X)<br>        <span class="hljs-comment"># 计算损失函数</span><br>        loss = self.loss_fn(logits, y).item()<br>        self.dev_loss.append(loss)<br>        <span class="hljs-comment"># 计算评估指标</span><br>        score = self.metric(logits, y).item()<br>        self.dev_scores.append(score)<br>        <span class="hljs-keyword">return</span> score, loss<br>    <br>    <span class="hljs-comment"># 模型测试阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 将模型切换为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">return</span> self.model(X)<br><br>    <span class="hljs-comment"># 使用&#x27;model.state_dict()&#x27;获取模型参数，并进行保存</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, saved_path</span>):<br>        paddle.save(self.model.state_dict(), saved_path)<br><br>    <span class="hljs-comment"># 使用&#x27;model.set_state_dict&#x27;加载模型参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        state_dict = paddle.load(model_path)<br>        self.model.set_state_dict(state_dict)<br></code></pre></td></tr></table></figure>



<hr>
<h4 id="√-4-3-3-模型训练"><a href="#√-4-3-3-模型训练" class="headerlink" title="[√] 4.3.3 - 模型训练"></a>[√] 4.3.3 - 模型训练</h4><p>实例化RunnerV2类，并传入训练配置，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置模型</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size = <span class="hljs-number">5</span><br>output_size = <span class="hljs-number">1</span><br>model = Model_MLP_L2_V2(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 设置损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 设置优化器</span><br>learning_rate = <span class="hljs-number">0.2</span><br>optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())<br><br><span class="hljs-comment"># 设置评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 其他参数</span><br>epoch_num = <span class="hljs-number">1000</span><br>saved_path = <span class="hljs-string">&#x27;best_model.pdparams&#x27;</span><br><br><span class="hljs-comment"># 实例化RunnerV2类，并传入训练配置</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=epoch_num, log_epochs=<span class="hljs-number">50</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>将训练过程中训练集与验证集的准确率变化情况进行可视化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 可视化观察训练集与验证集的指标变化情况</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot</span>(<span class="hljs-params">runner, fig_name</span>):<br>    plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))<br>    epochs = [i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(runner.train_scores))]<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>    plt.plot(epochs, runner.train_loss, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    plt.plot(epochs, runner.dev_loss, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>    plt.plot(epochs, runner.train_scores, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train accuracy&quot;</span>)<br>    plt.plot(epochs, runner.dev_scores, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev accuracy&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;score&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;epoch&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;lower right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    <br>    plt.savefig(fig_name)<br>    plt.show()<br><br>plot(runner, <span class="hljs-string">&#x27;fw-acc.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214224725288.png" srcset="/img/loading.gif" lazyload alt="image-20221214221440242"></p>
<hr>
<h4 id="√-4-3-4-性能评价"><a href="#√-4-3-4-性能评价" class="headerlink" title="[√] 4.3.4 - 性能评价"></a>[√] 4.3.4 - 性能评价</h4><p>使用测试数据对训练完成后的最优模型进行评价，观察模型在测试集上的准确率以及loss情况。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 模型评价</span><br>runner.load_model(<span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br>score, loss = runner.evaluate([X_test, y_test])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] score/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">运行耗时: <span class="hljs-number">7</span>毫秒<br>[Test] score/loss: <span class="hljs-number">0.8300</span>/<span class="hljs-number">0.4035</span><br></code></pre></td></tr></table></figure>

<p>从结果来看，模型在测试集上取得了较高的准确率。</p>
<hr>
<h3 id="√-4-4-优化问题"><a href="#√-4-4-优化问题" class="headerlink" title="[√] 4.4 - 优化问题"></a>[√] 4.4 - 优化问题</h3><p>在本节中，我们通过实践来发现神经网络模型的优化问题，并思考如何改进。</p>
<hr>
<h4 id="√-4-4-1-参数初始化"><a href="#√-4-4-1-参数初始化" class="headerlink" title="[√] 4.4.1 - 参数初始化"></a>[√] 4.4.1 - 参数初始化</h4><p>实现一个神经网络前，需要先初始化模型参数。如果对每一层的权重和偏置都用0初始化，那么通过第一遍前向计算，所有隐藏层神经元的激活值都相同；在反向传播时，所有权重的更新也都相同，这样会导致隐藏层神经元没有差异性，出现<strong>对称权重现象</strong>。</p>
<p>接下来，将模型参数全都初始化为0，看实验结果。这里重新定义了一个类<code>TwoLayerNet_Zeros</code>，两个线性层的参数全都初始化为0。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> paddle.nn.initializer <span class="hljs-keyword">import</span> Constant, Normal, Uniform<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V4</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, output_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V4, self).__init__()<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.Linear&#x27;定义线性层。</span><br>        <span class="hljs-comment"># 其中in_features为线性层输入维度；out_features为线性层输出维度</span><br>        <span class="hljs-comment"># weight_attr为权重参数属性</span><br>        <span class="hljs-comment"># bias_attr为偏置参数属性</span><br>        self.fc1 = nn.Linear(input_size, hidden_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        self.fc2 = nn.Linear(hidden_size, output_size,<br>                                weight_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)),<br>                                bias_attr=paddle.ParamAttr(initializer=Constant(value=<span class="hljs-number">0.0</span>)))<br>        <span class="hljs-comment"># 使用&#x27;paddle.nn.functional.sigmoid&#x27;定义 Logistic 激活函数</span><br>        self.act_fn = F.sigmoid<br>        <br>    <span class="hljs-comment"># 前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        z1 = self.fc1(inputs)<br>        a1 = self.act_fn(z1)<br>        z2 = self.fc2(a1)<br>        a2 = self.act_fn(z2)<br>        <span class="hljs-keyword">return</span> a2<br></code></pre></td></tr></table></figure>



<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_weights</span>(<span class="hljs-params">runner</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The weights of the Layers：&#x27;</span>)<br>    <br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> runner.model.sublayers():<br>        <span class="hljs-built_in">print</span>(item.full_name())<br>        <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> item.parameters():<br>            <span class="hljs-built_in">print</span>(param.numpy())<br>        <br></code></pre></td></tr></table></figure>

<p>利用Runner类训练模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 设置模型</span><br>input_size = <span class="hljs-number">2</span><br>hidden_size = <span class="hljs-number">5</span><br>output_size = <span class="hljs-number">1</span><br>model = Model_MLP_L2_V4(input_size=input_size, hidden_size=hidden_size, output_size=output_size)<br><br><span class="hljs-comment"># 设置损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 设置优化器</span><br>learning_rate = <span class="hljs-number">0.2</span> <span class="hljs-comment">#5e-2</span><br>optimizer = paddle.optimizer.SGD(learning_rate=learning_rate, parameters=model.parameters())<br><br><span class="hljs-comment"># 设置评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 其他参数</span><br>epoch = <span class="hljs-number">2000</span><br>saved_path = <span class="hljs-string">&#x27;best_model.pdparams&#x27;</span><br><br><span class="hljs-comment"># 实例化RunnerV2类，并传入训练配置</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br>runner.train([X_train, y_train], [X_dev, y_dev], num_epochs=<span class="hljs-number">5</span>, log_epochs=<span class="hljs-number">50</span>, save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>,custom_print_log=print_weights)<br></code></pre></td></tr></table></figure>

<p>可视化训练和验证集上的主准确率和loss变化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">plot(runner, <span class="hljs-string">&quot;fw-zero.pdf&quot;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214221440242.png" srcset="/img/loading.gif" lazyload alt="image-20221214222430247"></p>
<p>&#x3D;&#x3D;从输出结果看，二分类准确率为50%左右，说明模型没有学到任何内容。训练和验证loss几乎没有怎么下降。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;为了避免对称权重现象，可以使用高斯分布或均匀分布初始化神经网络的参数。&#x3D;&#x3D;</p>
<p>高斯分布和均匀分布采样的实现和可视化代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 使用&#x27;paddle.normal&#x27;实现高斯分布采样，其中&#x27;mean&#x27;为高斯分布的均值，&#x27;std&#x27;为高斯分布的标准差，&#x27;shape&#x27;为输出形状</span><br>gausian_weights = paddle.normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">1.0</span>, shape=[<span class="hljs-number">10000</span>])<br><span class="hljs-comment"># 使用&#x27;paddle.uniform&#x27;实现在[min,max)范围内的均匀分布采样，其中&#x27;shape&#x27;为输出形状</span><br>uniform_weights = paddle.uniform(shape=[<span class="hljs-number">10000</span>], <span class="hljs-built_in">min</span>=- <span class="hljs-number">1.0</span>, <span class="hljs-built_in">max</span>=<span class="hljs-number">1.0</span>)<br><br><span class="hljs-comment"># 绘制两种参数分布</span><br>plt.figure()<br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>plt.title(<span class="hljs-string">&#x27;Gausian Distribution&#x27;</span>)<br>plt.hist(gausian_weights, bins=<span class="hljs-number">200</span>, density=<span class="hljs-literal">True</span>, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>)<br>plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>plt.title(<span class="hljs-string">&#x27;Uniform Distribution&#x27;</span>)<br>plt.hist(uniform_weights, bins=<span class="hljs-number">200</span>, density=<span class="hljs-literal">True</span>, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>)<br>plt.savefig(<span class="hljs-string">&#x27;fw-gausian-uniform.pdf&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160241372.png" srcset="/img/loading.gif" lazyload alt="image-20221214222637659"></p>
<blockquote>
<p>alec总结：</p>
<p>如果网络的参数初始化的时候，将其全部初始化为0，那么将学不到任何东西。因此需要通过高斯分布、均匀分布等初始化，而不能直接全零初始化。</p>
</blockquote>
<hr>
<h4 id="√-4-4-2-梯度消失问题"><a href="#√-4-4-2-梯度消失问题" class="headerlink" title="[√] 4.4.2 - 梯度消失问题"></a>[√] 4.4.2 - 梯度消失问题</h4><blockquote>
<p>alec记录：</p>
<p>（1）随着网络层数的加深，容易出现梯度消失的问题。</p>
<p>（2）对于sigmoid型的激活函数，这种函数在饱和区（x趋向于很大或者很小）的时候，梯度很小，再加上网络层数很深，误差不断的衰减，因此最终梯度很小，就出现了梯度消失问题。解决这种问题的简单有效的方法之一是使用导数比较大的激活函数，比如ReLU激活函数。（ReLU激活函数在x为正的时候，梯度始终等于1，因此能够处理梯度消失问题。）</p>
</blockquote>
<p>在神经网络的构建过程中，随着网络层数的增加，理论上网络的拟合能力也应该是越来越好的。但是随着网络变深，参数学习更加困难，容易出现梯度消失问题。</p>
<p>由于Sigmoid型函数的饱和性，饱和区的导数更接近于0，误差经过每一层传递都会不断衰减。当网络层数很深时，梯度就会不停衰减，甚至消失，使得整个网络很难训练，这就是所谓的梯度消失问题。 在深度神经网络中，减轻梯度消失问题的方法有很多种，一种简单有效的方式就是使用导数比较大的激活函数，如：ReLU。</p>
<p>下面通过一个简单的实验观察前馈神经网络的梯度消失现象和改进方法。</p>
<hr>
<p>#####[√] 4.4.2.1 - 模型构建</p>
<p>定义一个前馈神经网络，包含4个隐藏层和1个输出层，通过传入的参数指定激活函数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义多层前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L5</span>(paddle.nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, act=<span class="hljs-string">&#x27;sigmoid&#x27;</span>, w_init=Normal(<span class="hljs-params">mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span></span>), b_init=Constant(<span class="hljs-params">value=<span class="hljs-number">1.0</span></span>)</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L5, self).__init__()<br>        self.fc1 = paddle.nn.Linear(input_size, <span class="hljs-number">3</span>)<br>        self.fc2 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc3 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc4 = paddle.nn.Linear(<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)<br>        self.fc5 = paddle.nn.Linear(<span class="hljs-number">3</span>, output_size)<br>        <span class="hljs-comment"># 定义网络使用的激活函数</span><br>        <span class="hljs-keyword">if</span> act == <span class="hljs-string">&#x27;sigmoid&#x27;</span>:<br>            self.act = F.sigmoid<br>        <span class="hljs-keyword">elif</span> act == <span class="hljs-string">&#x27;relu&#x27;</span>:<br>            self.act = F.relu<br>        <span class="hljs-keyword">elif</span> act == <span class="hljs-string">&#x27;lrelu&#x27;</span>:<br>            self.act = F.leaky_relu<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Please enter sigmoid, relu or lrelu!&quot;</span>)<br>        <span class="hljs-comment"># 初始化线性层权重和偏置参数</span><br>        self.init_weights(w_init, b_init)<br><br>    <span class="hljs-comment"># 初始化线性层权重和偏置参数（初始化的是线性层的参数，不含激活层（非线性层）的参数）</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">self, w_init, b_init</span>):<br>        <span class="hljs-comment"># 使用&#x27;named_sublayers&#x27;遍历所有网络层</span><br>        <span class="hljs-keyword">for</span> n, m <span class="hljs-keyword">in</span> self.named_sublayers():<br>            <span class="hljs-comment"># 如果是线性层，则使用指定方式进行参数初始化</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                w_init(m.weight)<br>                b_init(m.bias)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc1(inputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc2(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc3(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc4(outputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc5(outputs)<br>        outputs = F.sigmoid(outputs)<br>        <span class="hljs-keyword">return</span> outputs<br></code></pre></td></tr></table></figure>





<hr>
<h5 id="√-4-4-2-2-使用Sigmoid型函数进行训练"><a href="#√-4-4-2-2-使用Sigmoid型函数进行训练" class="headerlink" title="[√] 4.4.2.2 - 使用Sigmoid型函数进行训练"></a>[√] 4.4.2.2 - 使用Sigmoid型函数进行训练</h5><p>使用Sigmoid型函数作为激活函数，为了便于观察梯度消失现象，只进行一轮网络优化。代码实现如下：</p>
<p>定义梯度打印函数：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">print_grads</span>(<span class="hljs-params">runner</span>):<br>    <span class="hljs-comment"># 打印每一层的权重的模</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;The gradient of the Layers：&#x27;</span>)<br>    <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> runner.model.sublayers():<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(item.parameters())==<span class="hljs-number">2</span>:<br>            <span class="hljs-built_in">print</span>(item.full_name(), paddle.norm(item.parameters()[<span class="hljs-number">0</span>].grad, p=<span class="hljs-number">2.</span>).numpy()[<span class="hljs-number">0</span>])<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">102</span>)<br><span class="hljs-comment"># 学习率大小</span><br>lr = <span class="hljs-number">0.01</span><br><br><span class="hljs-comment"># 定义网络，激活函数使用sigmoid</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;sigmoid&#x27;</span>)<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数，使用交叉熵损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 定义评价指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 指定梯度打印函数</span><br>custom_print_log=print_grads<br></code></pre></td></tr></table></figure>

<p>实例化RunnerV2_2类，并传入训练配置。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br></code></pre></td></tr></table></figure>

<p>模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_2 <span class="hljs-number">0.0007373723</span><br>linear_3 <span class="hljs-number">0.006756582</span><br></code></pre></td></tr></table></figure>

<p>观察实验结果可以发现，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。</p>
<blockquote>
<p>alec分析：</p>
<p>使用sigmoid型的激活函数，随着网络的加深，出现了梯度消失的现象，这可能导致网络无法继续学习下去。因此梯度为0，无法更新参数了。</p>
</blockquote>
<hr>
<h5 id="√-4-4-2-3-使用ReLU函数进行模型训练"><a href="#√-4-4-2-3-使用ReLU函数进行模型训练" class="headerlink" title="[√] 4.4.2.3 - 使用ReLU函数进行模型训练"></a>[√] 4.4.2.3 - 使用ReLU函数进行模型训练</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">102</span>)<br>lr = <span class="hljs-number">0.01</span>  <span class="hljs-comment"># 学习率大小</span><br><br><span class="hljs-comment"># 定义网络，激活函数使用relu</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;relu&#x27;</span>) <span class="hljs-comment"># 使用ReLU作为激活函数</span><br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-comment"># 定义损失函数，这里使用交叉熵损失函数</span><br>loss_fn = F.binary_cross_entropy<br><br><span class="hljs-comment"># 定义评估指标</span><br>metric = accuracy<br><br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_9 <span class="hljs-number">4.828196e-08</span><br>linear_10 <span class="hljs-number">3.2609435e-06</span><br>linear_11 <span class="hljs-number">0.0001859922</span><br>linear_12 <span class="hljs-number">0.011442569</span><br>linear_13 <span class="hljs-number">0.43247733</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.53750</span><br></code></pre></td></tr></table></figure>

<p><strong>图4.4</strong> 展示了使用不同激活函数时，网络每层梯度值的ℓ2\ell_2ℓ2范数情况。从结果可以看到，5层的全连接前馈神经网络使用Sigmoid型函数作为激活函数时，梯度经过每一个神经层的传递都会不断衰减，最终传递到第一个神经层时，梯度几乎完全消失。改为ReLU激活函数后，梯度消失现象得到了缓解，每一层的参数都具有梯度值。</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214213927961.png" srcset="/img/loading.gif" lazyload alt="image-20221214224725288"></p>
<blockquote>
<p>alec结论：</p>
<p>使用ReLU激活函数能够较好的解决使用sigmoid激活函数导致的梯度消失问题。</p>
</blockquote>
<hr>
<h4 id="√-4-4-3-死亡-ReLU-问题"><a href="#√-4-4-3-死亡-ReLU-问题" class="headerlink" title="[√] 4.4.3 - 死亡 ReLU 问题"></a>[√] 4.4.3 - 死亡 ReLU 问题</h4><p>&#x3D;&#x3D;ReLU函数的利与弊：ReLU激活函数可以一定程度上改善梯度消失问题，但是ReLU函数在某些情况下容易出现死亡 ReLU问题，使得网络难以训练。&#x3D;&#x3D;</p>
<p>这是由于当$x&lt;0$时，ReLU函数的输出恒为0。在训练过程中，如果参数在一次不恰当的更新后，某个ReLU神经元在所有训练数据上都不能被激活（即输出为0），那么这个神经元自身参数的梯度永远都会是0，在以后的训练过程中永远都不能被激活。而一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU的变种。</p>
<blockquote>
<p>alec：</p>
<p>因为ReLU函数在x轴的负数部分恒为0，因此这就导致梯度永远是0，出现了死亡梯度问题。</p>
<p>解决这个问题的方法就是，将负半轴改变，例如可以使用Leaky ReLU、ELU等ReLU的变种。</p>
</blockquote>
<hr>
<h5 id="√-4-4-3-1-使用ReLU进行模型训练"><a href="#√-4-4-3-1-使用ReLU进行模型训练" class="headerlink" title="[√] 4.4.3.1 - 使用ReLU进行模型训练"></a>[√] 4.4.3.1 - 使用ReLU进行模型训练</h5><p>使用第4.4.2节中定义的多层全连接前馈网络进行实验，使用ReLU作为激活函数，观察死亡ReLU现象和优化方法。当神经层的偏置被初始化为一个相对于权重较大的负值时，可以想像，输入经过神经层的处理，最终的输出会为负值，从而导致死亡ReLU现象。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义网络，并使用较大的负值来初始化偏置</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;relu&#x27;</span>, b_init=Constant(value=-<span class="hljs-number">8.0</span>))<br></code></pre></td></tr></table></figure>

<p>实例化RunnerV2类，启动模型训练，打印网络每层梯度值的$\ell_2$范数。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochs=<span class="hljs-number">0</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_14 <span class="hljs-number">0.0</span><br>linear_15 <span class="hljs-number">0.0</span><br>linear_16 <span class="hljs-number">0.0</span><br>linear_17 <span class="hljs-number">0.0</span><br>linear_18 <span class="hljs-number">0.0</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.50625</span><br></code></pre></td></tr></table></figure>

<p>从输出结果可以发现，使用 ReLU 作为激活函数，当满足条件时，会发生死亡ReLU问题，网络训练过程中 ReLU 神经元的梯度始终为0，参数无法更新。</p>
<p>针对死亡ReLU问题，一种简单有效的优化方式就是将激活函数更换为Leaky ReLU、ELU等ReLU 的变种。接下来，观察将激活函数更换为 Leaky ReLU时的梯度情况。</p>
<hr>
<h5 id="√-4-4-3-2-使用Leaky-ReLU进行模型训练"><a href="#√-4-4-3-2-使用Leaky-ReLU进行模型训练" class="headerlink" title="[√] 4.4.3.2 - 使用Leaky ReLU进行模型训练"></a>[√] 4.4.3.2 - 使用Leaky ReLU进行模型训练</h5><p>将激活函数更换为Leaky ReLU进行模型训练，观察梯度情况。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 重新定义网络，使用Leaky ReLU激活函数</span><br>model =  Model_MLP_L5(input_size=<span class="hljs-number">2</span>, output_size=<span class="hljs-number">1</span>, act=<span class="hljs-string">&#x27;lrelu&#x27;</span>, b_init=Constant(value=-<span class="hljs-number">8.0</span>))<br><br><span class="hljs-comment"># 实例化Runner类</span><br>runner = RunnerV2_2(model, optimizer, metric, loss_fn)<br><br><span class="hljs-comment"># 启动训练</span><br>runner.train([X_train, y_train], [X_dev, y_dev], <br>            num_epochs=<span class="hljs-number">1</span>, log_epochps=<span class="hljs-literal">None</span>, <br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>, <br>            custom_print_log=custom_print_log)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">The gradient of the Layers：<br>linear_19 <span class="hljs-number">2.0111758e-16</span><br>linear_20 <span class="hljs-number">1.8527564e-13</span><br>linear_21 <span class="hljs-number">1.6659853e-09</span><br>linear_22 <span class="hljs-number">1.1705935e-05</span><br>linear_23 <span class="hljs-number">0.06902571</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.50625</span><br>[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">1</span>, loss: <span class="hljs-number">3.988154649734497</span><br></code></pre></td></tr></table></figure>

<p>从输出结果可以看到，将激活函数更换为Leaky ReLU后，死亡ReLU问题得到了改善，梯度恢复正常，参数也可以正常更新。但是由于 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率默认只有0.01，所以反向传播时，随着网络层数的加深，梯度值越来越小。如果想要改善这一现象，将 Leaky ReLU 中，$\mathcal{x&lt;0}$ 时的斜率调大即可。</p>
<h3 id="√-4-5-实践：基于前馈神经网络完成鸢尾花分类"><a href="#√-4-5-实践：基于前馈神经网络完成鸢尾花分类" class="headerlink" title="[√] 4.5 - 实践：基于前馈神经网络完成鸢尾花分类"></a>[√] 4.5 - 实践：基于前馈神经网络完成鸢尾花分类</h3><p>在本实践中，我们继续使用第三章中的鸢尾花分类任务，将Softmax分类器替换为本章介绍的前馈神经网络。 在本实验中，我们使用的损失函数为交叉熵损失；优化器为随机梯度下降法；评价指标为准确率。</p>
<hr>
<h4 id="√-4-5-1-小批量梯度下降法"><a href="#√-4-5-1-小批量梯度下降法" class="headerlink" title="[√] 4.5.1 - 小批量梯度下降法"></a>[√] 4.5.1 - 小批量梯度下降法</h4><p>在梯度下降法中，目标函数是整个训练集上的风险函数，这种方式称为<strong>批量梯度下降法（Batch Gradient Descent，BGD）</strong>。 批量梯度下降法在每次迭代时需要计算每个样本上损失函数的梯度并求和。当训练集中的样本数量NN<em>N</em>很大时，空间复杂度比较高，每次迭代的计算开销也很大。</p>
<p>为了减少每次迭代的计算复杂度，我们可以在每次迭代时只采集一小部分样本，计算在这组样本上损失函数的梯度并更新参数，这种优化方式称为 小批量梯度下降法（Mini-Batch Gradient Descent，Mini-Batch GD）。</p>
<p>第$t$次迭代时，随机选取一个包含$K$个样本的子集$\mathcal{B}<em>t$，计算这个子集上每个样本损失函数的梯度并进行平均，然后再进行参数更新。<br>$$<br>\theta</em>{t+1} \leftarrow \theta_t  - \alpha \frac{1}{K} \sum_{(\boldsymbol{x},y)\in \mathcal{S}_t} \frac{\partial \mathcal{L}\Big(y,f(\boldsymbol{x};\theta)\Big)}{\partial \theta},<br>$$<br>&#x3D;&#x3D;其中$K$为**批量大小(Batch Size)**。$K$通常不会设置很大，一般在$1\sim100$之间。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;在实际应用中为了提高计算效率，通常设置为2的幂$2^n$。&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;在实际应用中，小批量随机梯度下降法有收敛快、计算开销小的优点，因此逐渐成为大规模的机器学习中的主要优化算法。&#x3D;&#x3D;<br>&#x3D;&#x3D;此外，随机梯度下降相当于在批量梯度下降的梯度上引入了随机噪声。在非凸优化问题中，随机梯度下降更容易逃离局部最优点。&#x3D;&#x3D;</p>
<blockquote>
<p>alec:</p>
<p>小批量梯度下降收敛快、开销小。</p>
<p>随机梯度下降在梯度下降的基础上引入了随机噪声，因此更加容易逃离局部最优点。</p>
</blockquote>
<p>小批量随机梯度下降法的训练过程如下：</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215175316781.png" srcset="/img/loading.gif" lazyload alt="image-20221215160106452"></p>
<hr>
<h5 id="√-4-5-1-1-数据分组"><a href="#√-4-5-1-1-数据分组" class="headerlink" title="[√] 4.5.1.1 - 数据分组"></a>[√] 4.5.1.1 - 数据分组</h5><p>&#x3D;&#x3D;为了小批量梯度下降法，我们需要对数据进行随机分组。目前，机器学习中通常做法是构建一个数据迭代器，每个迭代过程中从全部数据集中获取一批指定数量的数据。&#x3D;&#x3D;</p>
<p>数据迭代器的实现原理如下图所示：</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221215160106452.png" srcset="/img/loading.gif" lazyload alt="image-20221215160241372"></p>
<ol>
<li>首先，将数据集封装为Dataset类，传入一组索引值，根据索引从数据集合中获取数据；</li>
<li>其次，构建DataLoader类，需要指定数据批量的大小和是否需要对数据进行乱序，通过该类即可批量获取数据。</li>
</ol>
<p>在实践过程中，通常使用进行参数优化。在飞桨中，使用<code>paddle.io.DataLoader</code>加载minibatch的数据， <code>paddle.io.DataLoader</code> API可以生成一个迭代器，其中通过设置<code>batch_size</code>参数来指定minibatch的长度，通过设置shuffle参数为True，可以在生成<code>minibatch</code>的索引列表时将索引顺序打乱。</p>
<hr>
<h4 id="√-4-5-2-数据处理"><a href="#√-4-5-2-数据处理" class="headerlink" title="[√] 4.5.2 - 数据处理"></a>[√] 4.5.2 - 数据处理</h4><p>构造IrisDataset类进行数据读取，继承自<code>paddle.io.Dataset</code>类。<code>paddle.io.Dataset</code>是用来封装 Dataset的方法和行为的抽象类，通过一个索引获取指定的样本，同时对该样本进行数据处理。当继承<code>paddle.io.Dataset</code>来定义数据读取类时，实现如下方法：</p>
<ul>
<li><code>__getitem__</code>：根据给定索引获取数据集中指定样本，并对样本进行数据处理；</li>
<li><code>__len__</code>：返回数据集样本个数。</li>
</ul>
<p>代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.io <span class="hljs-keyword">as</span> io<br><span class="hljs-keyword">from</span> nndl.dataset <span class="hljs-keyword">import</span> load_data<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">IrisDataset</span>(io.Dataset):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, mode=<span class="hljs-string">&#x27;train&#x27;</span>, num_train=<span class="hljs-number">120</span>, num_dev=<span class="hljs-number">15</span></span>):<br>        <span class="hljs-built_in">super</span>(IrisDataset, self).__init__()<br>        <span class="hljs-comment"># 调用第三章中的数据读取函数，其中不需要将标签转成one-hot类型</span><br>        X, y = load_data(shuffle=<span class="hljs-literal">True</span>)<br>        <span class="hljs-keyword">if</span> mode == <span class="hljs-string">&#x27;train&#x27;</span>:<br>            self.X, self.y = X[:num_train], y[:num_train]<br>        <span class="hljs-keyword">elif</span> mode == <span class="hljs-string">&#x27;dev&#x27;</span>:<br>            self.X, self.y = X[num_train:num_train + num_dev], y[num_train:num_train + num_dev]<br>        <span class="hljs-keyword">else</span>:<br>            self.X, self.y = X[num_train + num_dev:], y[num_train + num_dev:]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.X[idx], self.y[idx]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.y)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">12</span>)<br>train_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;train&#x27;</span>)<br>dev_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;dev&#x27;</span>)<br>test_dataset = IrisDataset(mode=<span class="hljs-string">&#x27;test&#x27;</span>)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 打印训练集长度</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;length of train set: &quot;</span>, <span class="hljs-built_in">len</span>(train_dataset))<br></code></pre></td></tr></table></figure>

<hr>
<h5 id="√-4-5-2-2-用DataLoader进行封装"><a href="#√-4-5-2-2-用DataLoader进行封装" class="headerlink" title="[√] 4.5.2.2 - 用DataLoader进行封装"></a>[√] 4.5.2.2 - 用DataLoader进行封装</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 批量大小</span><br>batch_size = <span class="hljs-number">16</span><br><br><span class="hljs-comment"># 加载数据</span><br>train_loader = io.DataLoader(train_dataset, batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>)<br>dev_loader = io.DataLoader(dev_dataset, batch_size=batch_size)<br>test_loader = io.DataLoader(test_dataset, batch_size=batch_size)<br></code></pre></td></tr></table></figure>





<hr>
<h4 id="√-4-5-3-模型构建"><a href="#√-4-5-3-模型构建" class="headerlink" title="[√] 4.5.3 - 模型构建"></a>[√] 4.5.3 - 模型构建</h4><p>构建一个简单的前馈神经网络进行鸢尾花分类实验。其中输入层神经元个数为4，输出层神经元个数为3，隐含层神经元个数为6。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-comment"># 定义前馈神经网络</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_MLP_L2_V3</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, output_size, hidden_size</span>):<br>        <span class="hljs-built_in">super</span>(Model_MLP_L2_V3, self).__init__()<br>        <span class="hljs-comment"># 构建第一个全连接层</span><br>        self.fc1 = nn.Linear(<br>            input_size,<br>            hidden_size,<br>            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)),<br>            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">1.0</span>))<br>        )<br>        <span class="hljs-comment"># 构建第二全连接层</span><br>        self.fc2 = nn.Linear(<br>            hidden_size,<br>            output_size,<br>            weight_attr=paddle.ParamAttr(initializer=nn.initializer.Normal(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.01</span>)),<br>            bias_attr=paddle.ParamAttr(initializer=nn.initializer.Constant(value=<span class="hljs-number">1.0</span>))<br>        )<br>        <span class="hljs-comment"># 定义网络使用的激活函数</span><br>        self.act = nn.Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        outputs = self.fc1(inputs)<br>        outputs = self.act(outputs)<br>        outputs = self.fc2(outputs)<br>        <span class="hljs-keyword">return</span> outputs<br><br>fnn_model = Model_MLP_L2_V3(input_size=<span class="hljs-number">4</span>, output_size=<span class="hljs-number">3</span>, hidden_size=<span class="hljs-number">6</span>)<br></code></pre></td></tr></table></figure>





<hr>
<h4 id="√-4-5-4-完善Runner类"><a href="#√-4-5-4-完善Runner类" class="headerlink" title="[√] 4.5.4 - 完善Runner类"></a>[√] 4.5.4 - 完善Runner类</h4><p>&#x3D;&#x3D;基于RunnerV2类进行完善实现了RunnerV3类。其中训练过程使用自动梯度计算&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;使用<code>DataLoader</code>加载批量数据&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;使用随机梯度下降法进行参数优化&#x3D;&#x3D;</p>
<p>模型保存时，使用<code>state_dict</code>方法获取模型参数</p>
<p>模型加载时，使用<code>set_state_dict</code>方法加载模型参数.</p>
<p>&#x3D;&#x3D;由于这里使用随机梯度下降法对参数优化，所以数据以批次的形式输入到模型中进行训练&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;那么评价指标计算也是分别在每个批次进行的&#x3D;&#x3D;</p>
<p>&#x3D;&#x3D;要想获得每个epoch整体的评价结果，需要对历史评价结果进行累积。&#x3D;&#x3D;</p>
<p>这里定义<code>Accuracy</code>类实现该功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.metric <span class="hljs-keyword">import</span> Metric<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Accuracy</span>(<span class="hljs-title class_ inherited__">Metric</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, is_logist=<span class="hljs-literal">True</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - is_logist: outputs是logist还是激活后的值</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 用于统计正确的样本个数</span><br>        self.num_correct = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># 用于统计样本的总数</span><br>        self.num_count = <span class="hljs-number">0</span><br><br>        self.is_logist = is_logist<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">update</span>(<span class="hljs-params">self, outputs, labels</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        输入：</span><br><span class="hljs-string">           - outputs: 预测值, shape=[N,class_num]</span><br><span class="hljs-string">           - labels: 标签值, shape=[N,1]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br><br>        <span class="hljs-comment"># 判断是二分类任务还是多分类任务，shape[1]=1时为二分类任务，shape[1]&gt;1时为多分类任务</span><br>        <span class="hljs-keyword">if</span> outputs.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">1</span>: <span class="hljs-comment"># 二分类</span><br>            outputs = paddle.squeeze(outputs, axis=-<span class="hljs-number">1</span>)<br>            <span class="hljs-keyword">if</span> self.is_logist:<br>                <span class="hljs-comment"># logist判断是否大于0</span><br>                preds = paddle.cast((outputs&gt;=<span class="hljs-number">0</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                <span class="hljs-comment"># 如果不是logist，判断每个概率值是否大于0.5，当大于0.5时，类别为1，否则类别为0</span><br>                preds = paddle.cast((outputs&gt;=<span class="hljs-number">0.5</span>), dtype=<span class="hljs-string">&#x27;float32&#x27;</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 多分类时，使用&#x27;paddle.argmax&#x27;计算最大元素索引作为类别</span><br>            preds = paddle.argmax(outputs, axis=<span class="hljs-number">1</span>, dtype=<span class="hljs-string">&#x27;int64&#x27;</span>)<br><br>        <span class="hljs-comment"># 获取本批数据中预测正确的样本个数</span><br>        labels = paddle.squeeze(labels, axis=-<span class="hljs-number">1</span>)<br>        batch_correct = paddle.<span class="hljs-built_in">sum</span>(paddle.cast(preds==labels, dtype=<span class="hljs-string">&quot;float32&quot;</span>)).numpy()[<span class="hljs-number">0</span>]<br>        batch_count = <span class="hljs-built_in">len</span>(labels)<br><br>        <span class="hljs-comment"># 更新num_correct 和 num_count</span><br>        self.num_correct += batch_correct<br>        self.num_count += batch_count<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">accumulate</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 使用累计的数据，计算总的指标</span><br>        <span class="hljs-keyword">if</span> self.num_count == <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">return</span> self.num_correct / self.num_count<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reset</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 重置正确的数目和总数</span><br>        self.num_correct = <span class="hljs-number">0</span><br>        self.num_count = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">name</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Accuracy&quot;</span><br><br></code></pre></td></tr></table></figure>

<p>RunnerV3类的代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">RunnerV3</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, optimizer, loss_fn, metric, **kwargs</span>):<br>        self.model = model<br>        self.optimizer = optimizer<br>        self.loss_fn = loss_fn<br>        self.metric = metric <span class="hljs-comment"># 只用于计算评价指标</span><br><br>        <span class="hljs-comment"># 记录训练过程中的评价指标变化情况</span><br>        self.dev_scores = []<br><br>        <span class="hljs-comment"># 记录训练过程中的损失函数变化情况</span><br>        self.train_epoch_losses = [] <span class="hljs-comment"># 一个epoch记录一次loss</span><br>        self.train_step_losses = []  <span class="hljs-comment"># 一个step记录一次loss</span><br>        self.dev_losses = []<br>        <br>        <span class="hljs-comment"># 记录全局最优指标</span><br>        self.best_score = <span class="hljs-number">0</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self, train_loader, dev_loader=<span class="hljs-literal">None</span>, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型切换为训练模式</span><br>        self.model.train()<br><br>        <span class="hljs-comment"># 传入训练轮数，如果没有传入值则默认为0</span><br>        num_epochs = kwargs.get(<span class="hljs-string">&quot;num_epochs&quot;</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-comment"># 传入log打印频率，如果没有传入值则默认为100</span><br>        log_steps = kwargs.get(<span class="hljs-string">&quot;log_steps&quot;</span>, <span class="hljs-number">100</span>)<br>        <span class="hljs-comment"># 评价频率</span><br>        eval_steps = kwargs.get(<span class="hljs-string">&quot;eval_steps&quot;</span>, <span class="hljs-number">0</span>)<br><br>        <span class="hljs-comment"># 传入模型保存路径，如果没有传入值则默认为&quot;best_model.pdparams&quot;</span><br>        save_path = kwargs.get(<span class="hljs-string">&quot;save_path&quot;</span>, <span class="hljs-string">&quot;best_model.pdparams&quot;</span>)<br><br>        custom_print_log = kwargs.get(<span class="hljs-string">&quot;custom_print_log&quot;</span>, <span class="hljs-literal">None</span>) <br>       <br>        <span class="hljs-comment"># 训练总的步数</span><br>        <span class="hljs-comment"># len(train_loader)为 mini-batch 的数量</span><br>        num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_loader)<br><br>        <span class="hljs-keyword">if</span> eval_steps:<br>            <span class="hljs-keyword">if</span> self.metric <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Error: Metric can not be None!&#x27;</span>)<br>            <span class="hljs-keyword">if</span> dev_loader <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">raise</span> RuntimeError(<span class="hljs-string">&#x27;Error: dev_loader can not be None!&#x27;</span>)<br>            <br>        <span class="hljs-comment"># 运行的step数目</span><br>        global_step = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 进行num_epochs轮训练</span><br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>            <span class="hljs-comment"># 用于统计训练集的损失</span><br>            total_loss = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>                X, y = data<br>                <span class="hljs-comment"># 获取模型预测</span><br>                logits = self.model(X)<br>                loss = self.loss_fn(logits, y) <span class="hljs-comment"># 默认求mean</span><br>                total_loss += loss <span class="hljs-comment"># 统计累加损失</span><br><br>                <span class="hljs-comment"># 训练过程中，每个step的loss进行保存</span><br>                self.train_step_losses.append((global_step,loss.item()))<br><br>                <span class="hljs-keyword">if</span> log_steps <span class="hljs-keyword">and</span> global_step%log_steps==<span class="hljs-number">0</span>:<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Train] epoch: <span class="hljs-subst">&#123;epoch&#125;</span>/<span class="hljs-subst">&#123;num_epochs&#125;</span>, step: <span class="hljs-subst">&#123;global_step&#125;</span>/<span class="hljs-subst">&#123;num_training_steps&#125;</span>, loss: <span class="hljs-subst">&#123;loss.item():<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                <br>                <span class="hljs-comment"># 梯度反向传播，计算每个参数的梯度值</span><br>                <span class="hljs-comment"># alec：计算每个参数的梯度</span><br>                loss.backward() <br><br>                <span class="hljs-keyword">if</span> custom_print_log:<br>                   custom_print_log(self)<br>                <br>                <span class="hljs-comment"># 小批量梯度下降进行参数更新</span><br>                <span class="hljs-comment"># 更新参数</span><br>                self.optimizer.step()<br>                <span class="hljs-comment"># 梯度归零</span><br>                <span class="hljs-comment"># 梯度归零</span><br>                self.optimizer.clear_grad()<br><br>                <span class="hljs-comment"># 判断是否需要评价</span><br>                <span class="hljs-keyword">if</span> eval_steps&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> global_step&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> \<br>                    (global_step%eval_steps == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> global_step==(num_training_steps-<span class="hljs-number">1</span>)):<br><br>                    dev_score, dev_loss = self.evaluate(dev_loader, global_step=global_step)<br>                    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate]  dev score: <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>, dev loss: <span class="hljs-subst">&#123;dev_loss:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>) <br><br>                    <span class="hljs-comment"># 将模型切换为训练模式</span><br>                    self.model.train()<br><br>                    <span class="hljs-comment"># 如果当前指标为最优指标，保存该模型</span><br>                    <span class="hljs-comment"># 通过计算在验证集上的验证得分来判断是否是最优的模型</span><br>                    <span class="hljs-keyword">if</span> dev_score &gt; self.best_score:<br>                        self.save_model(save_path)<br>                        <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Evaluate] best accuracy performence has been updated: <span class="hljs-subst">&#123;self.best_score:<span class="hljs-number">.5</span>f&#125;</span> --&gt; <span class="hljs-subst">&#123;dev_score:<span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br>                        self.best_score = dev_score<br><br>                global_step += <span class="hljs-number">1</span><br>            <br>            <span class="hljs-comment"># 当前epoch 训练loss累计值 </span><br>            <span class="hljs-comment"># 当前epoch的损失的平均值</span><br>            trn_loss = (total_loss / <span class="hljs-built_in">len</span>(train_loader)).item()<br>            <span class="hljs-comment"># epoch粒度的训练loss保存</span><br>            self.train_epoch_losses.append(trn_loss)<br>            <br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Train] Training done!&quot;</span>)<br><br>    <span class="hljs-comment"># 模型评估阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br>    <span class="hljs-comment"># 评估阶段，不需要计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate</span>(<span class="hljs-params">self, dev_loader, **kwargs</span>):<br>        <span class="hljs-keyword">assert</span> self.metric <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># 将模型设置为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br><br>        global_step = kwargs.get(<span class="hljs-string">&quot;global_step&quot;</span>, -<span class="hljs-number">1</span>) <br><br>        <span class="hljs-comment"># 用于统计训练集的损失</span><br>        total_loss = <span class="hljs-number">0</span><br><br>        <span class="hljs-comment"># 重置评价</span><br>        self.metric.reset() <br>        <br>        <span class="hljs-comment"># 遍历验证集每个批次  </span><br>        <span class="hljs-comment"># alec：第一个参数是指第几个mini-batch，第二个参数是存储该批次的数据对</span><br>        <span class="hljs-keyword">for</span> batch_id, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dev_loader):<br>            X, y = data<br>    <br>            <span class="hljs-comment"># 计算模型输出</span><br>            logits = self.model(X)<br>            <br>            <span class="hljs-comment"># 计算损失函数</span><br>            loss = self.loss_fn(logits, y).item()<br>            <span class="hljs-comment"># 累积损失</span><br>            total_loss += loss <br><br>            <span class="hljs-comment"># 累积评价</span><br>            self.metric.update(logits, y)<br><br>        dev_loss = (total_loss/<span class="hljs-built_in">len</span>(dev_loader))<br>        dev_score = self.metric.accumulate() <br><br>        <span class="hljs-comment"># 记录验证集loss</span><br>        <span class="hljs-keyword">if</span> global_step!=-<span class="hljs-number">1</span>:<br>            self.dev_losses.append((global_step, dev_loss))<br>            self.dev_scores.append(dev_score)<br>        <br>        <span class="hljs-keyword">return</span> dev_score, dev_loss<br>    <br>    <span class="hljs-comment"># 模型预测阶段，使用&#x27;paddle.no_grad()&#x27;控制不计算和存储梯度</span><br><span class="hljs-meta">    @paddle.no_grad()</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">predict</span>(<span class="hljs-params">self, x, **kwargs</span>):<br>        <span class="hljs-comment"># 将模型设置为评估模式</span><br>        self.model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-comment"># 运行模型前向计算，得到预测值</span><br>        logits = self.model(x)<br>        <span class="hljs-keyword">return</span> logits<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">save_model</span>(<span class="hljs-params">self, save_path</span>):<br>        paddle.save(self.model.state_dict(), save_path)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_model</span>(<span class="hljs-params">self, model_path</span>):<br>        model_state_dict = paddle.load(model_path)<br>        self.model.set_state_dict(model_state_dict)<br></code></pre></td></tr></table></figure>





<hr>
<h4 id="√-4-5-5-模型训练"><a href="#√-4-5-5-模型训练" class="headerlink" title="[√] 4.5.5 - 模型训练"></a>[√] 4.5.5 - 模型训练</h4><p>实例化RunnerV3类，并传入训练配置，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.optimizer <span class="hljs-keyword">as</span> opt<br><br>lr = <span class="hljs-number">0.2</span><br><br><span class="hljs-comment"># 定义网络</span><br>model = fnn_model<br><br><span class="hljs-comment"># 定义优化器</span><br>optimizer = opt.SGD(learning_rate=lr, parameters=model.parameters())<br><br><span class="hljs-comment"># 定义损失函数。softmax+交叉熵</span><br>loss_fn = F.cross_entropy<br><br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy(is_logist=<span class="hljs-literal">True</span>)<br><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br></code></pre></td></tr></table></figure>

<p>使用训练集和验证集进行模型训练，共训练150个epoch。在实验中，保存准确率最高的模型作为最佳模型。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 启动训练</span><br>log_steps = <span class="hljs-number">100</span><br>eval_steps = <span class="hljs-number">50</span><br>runner.train(train_loader, dev_loader, <br>            num_epochs=<span class="hljs-number">150</span>, log_steps=log_steps, eval_steps = eval_steps,<br>            save_path=<span class="hljs-string">&quot;best_model.pdparams&quot;</span>) <br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">[Train] epoch: <span class="hljs-number">0</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">0</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.09866</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.09026</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.00000</span> --&gt; <span class="hljs-number">0.40000</span><br>[Train] epoch: <span class="hljs-number">12</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">100</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.12618</span><br>[Evaluate]  dev score: <span class="hljs-number">0.33333</span>, dev loss: <span class="hljs-number">1.08850</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.08677</span><br>[Train] epoch: <span class="hljs-number">25</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">200</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.08003</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.08576</span><br>[Evaluate]  dev score: <span class="hljs-number">0.26667</span>, dev loss: <span class="hljs-number">1.09782</span><br>[Train] epoch: <span class="hljs-number">37</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">300</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">1.10121</span><br>[Evaluate]  dev score: <span class="hljs-number">0.40000</span>, dev loss: <span class="hljs-number">1.06987</span><br>[Evaluate]  dev score: <span class="hljs-number">0.73333</span>, dev loss: <span class="hljs-number">1.03954</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.40000</span> --&gt; <span class="hljs-number">0.73333</span><br>[Train] epoch: <span class="hljs-number">50</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">400</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.94351</span><br>[Evaluate]  dev score: <span class="hljs-number">0.53333</span>, dev loss: <span class="hljs-number">0.93444</span><br>[Evaluate]  dev score: <span class="hljs-number">0.66667</span>, dev loss: <span class="hljs-number">0.79328</span><br>[Train] epoch: <span class="hljs-number">62</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">500</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.67799</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.64378</span><br>[Evaluate] best accuracy performence has been updated: <span class="hljs-number">0.73333</span> --&gt; <span class="hljs-number">1.00000</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.54416</span><br>[Train] epoch: <span class="hljs-number">75</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">600</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.35588</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80000</span>, dev loss: <span class="hljs-number">0.48563</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.44295</span><br>[Train] epoch: <span class="hljs-number">87</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">700</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.43655</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.41185</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.38773</span><br>[Train] epoch: <span class="hljs-number">100</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">800</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.35982</span><br>[Evaluate]  dev score: <span class="hljs-number">0.80000</span>, dev loss: <span class="hljs-number">0.37680</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.34835</span><br>[Train] epoch: <span class="hljs-number">112</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">900</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.24126</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.32392</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.30578</span><br>[Train] epoch: <span class="hljs-number">125</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">1000</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.27697</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.28536</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.27146</span><br>[Train] epoch: <span class="hljs-number">137</span>/<span class="hljs-number">150</span>, step: <span class="hljs-number">1100</span>/<span class="hljs-number">1200</span>, loss: <span class="hljs-number">0.25255</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.25523</span><br>[Evaluate]  dev score: <span class="hljs-number">0.93333</span>, dev loss: <span class="hljs-number">0.24195</span><br>[Evaluate]  dev score: <span class="hljs-number">1.00000</span>, dev loss: <span class="hljs-number">0.23231</span><br>[Train] Training done!<br></code></pre></td></tr></table></figure>

<p>可视化观察训练集损失和训练集loss变化情况。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 绘制训练集和验证集的损失变化以及验证集上的准确率变化曲线</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_training_loss_acc</span>(<span class="hljs-params">runner, fig_name, </span><br><span class="hljs-params">    fig_size=(<span class="hljs-params"><span class="hljs-number">16</span>, <span class="hljs-number">6</span></span>), </span><br><span class="hljs-params">    sample_step=<span class="hljs-number">20</span>, </span><br><span class="hljs-params">    loss_legend_loc=<span class="hljs-string">&quot;upper right&quot;</span>, </span><br><span class="hljs-params">    acc_legend_loc=<span class="hljs-string">&quot;lower right&quot;</span>,</span><br><span class="hljs-params">    train_color=<span class="hljs-string">&quot;#8E004D&quot;</span>,</span><br><span class="hljs-params">    dev_color=<span class="hljs-string">&#x27;#E20079&#x27;</span>,</span><br><span class="hljs-params">    fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>,</span><br><span class="hljs-params">    train_linestyle=<span class="hljs-string">&quot;-&quot;</span>,</span><br><span class="hljs-params">    dev_linestyle=<span class="hljs-string">&#x27;--&#x27;</span></span>):<br><br>    plt.figure(figsize=fig_size)<br><br>    plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)<br>    train_items = runner.train_step_losses[::sample_step]<br>    train_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    train_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br><br>    plt.plot(train_steps, train_losses, color=train_color, linestyle=train_linestyle, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(runner.dev_losses)&gt;<span class="hljs-number">0</span>:<br>        dev_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>        dev_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>        plt.plot(dev_steps, dev_losses, color=dev_color, linestyle=dev_linestyle, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br>    <span class="hljs-comment"># 绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=fontsize)<br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=fontsize)<br>    plt.legend(loc=loss_legend_loc, fontsize=fontsize)<br><br>    <span class="hljs-comment"># 绘制评价准确率变化曲线</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(runner.dev_scores)&gt;<span class="hljs-number">0</span>:<br>        plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)<br>        plt.plot(dev_steps, runner.dev_scores,<br>            color=dev_color, linestyle=dev_linestyle, label=<span class="hljs-string">&quot;Dev accuracy&quot;</span>)<br>    <br>        <span class="hljs-comment"># 绘制坐标轴和图例</span><br>        plt.ylabel(<span class="hljs-string">&quot;score&quot;</span>, fontsize=fontsize)<br>        plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=fontsize)<br>        plt.legend(loc=acc_legend_loc, fontsize=fontsize)<br><br>    plt.savefig(fig_name)<br>    plt.show()<br><br>plot_training_loss_acc(runner, <span class="hljs-string">&#x27;fw-loss.pdf&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221214222430247.png" srcset="/img/loading.gif" lazyload alt="image-20221215175316781"></p>
<p>从输出结果可以看出准确率随着迭代次数增加逐渐上升，损失函数下降。</p>
<hr>
<h4 id="√-4-5-6-模型评价"><a href="#√-4-5-6-模型评价" class="headerlink" title="[√] 4.5.6 - 模型评价"></a>[√] 4.5.6 - 模型评价</h4><p>使用测试数据对在训练过程中保存的最佳模型进行评价，观察模型在测试集上的准确率以及Loss情况。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 加载最优模型</span><br>runner.load_model(<span class="hljs-string">&#x27;best_model.pdparams&#x27;</span>)<br><span class="hljs-comment"># 模型评价</span><br>score, loss = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Test] accuracy/loss: &#123;:.4f&#125;/&#123;:.4f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(score, loss))<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">[Test] accuracy/loss: <span class="hljs-number">0.8000</span>/<span class="hljs-number">0.6891</span><br></code></pre></td></tr></table></figure>



<hr>
<h4 id="√-4-5-7-模型预测"><a href="#√-4-5-7-模型预测" class="headerlink" title="[√] 4.5.7 - 模型预测"></a>[√] 4.5.7 - 模型预测</h4><p>同样地，也可以使用保存好的模型，对测试集中的某一个数据进行模型预测，观察模型效果。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 获取测试集中第一条数据</span><br>X, label = <span class="hljs-built_in">next</span>(test_loader())<br>logits = runner.predict(X)<br><br>pred_class = paddle.argmax(logits[<span class="hljs-number">0</span>]).numpy()<br>label = label[<span class="hljs-number">0</span>][<span class="hljs-number">0</span>].numpy()<br><br><span class="hljs-comment"># 输出真实类别与预测类别</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;The true category is &#123;&#125; and the predicted category is &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(label, pred_class))<br></code></pre></td></tr></table></figure>

<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs inform7">The true category <span class="hljs-keyword">is</span> <span class="hljs-comment">[2]</span> and the predicted category <span class="hljs-keyword">is</span> <span class="hljs-comment">[2]</span><br></code></pre></td></tr></table></figure>



<hr>
<h3 id="√-4-6-小结"><a href="#√-4-6-小结" class="headerlink" title="[√] 4.6 - 小结"></a>[√] 4.6 - 小结</h3><p>本章介绍前馈神经网络的基本概念、网络结构及代码实现，利用前馈神经网络完成一个分类任务，并通过两个简单的实验，观察前馈神经网络的梯度消失问题和死亡ReLU问题，以及对应的优化策略。 此外，还实践了基于前馈神经网络完成鸢尾花分类任务。</p>
<hr>
<h3 id="√-4-7-实验拓展"><a href="#√-4-7-实验拓展" class="headerlink" title="[√] 4.7 - 实验拓展"></a>[√] 4.7 - 实验拓展</h3><p>尝试基于MNIST手写数字识别数据集，设计合适的前馈神经网络进行实验，并取得95%以上的准确率。</p>
<hr>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/" class="category-chain-item">深度学习技术栈</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E8%B7%B5%E5%AD%A6%E4%B9%A0/" class="category-chain-item">实践学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E8%B7%B5%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5-%E9%A3%9E%E6%A1%A8-%E9%82%B1%E9%94%A1%E9%B9%8F/" class="category-chain-item">神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</a>
  
  

  

  

  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>第4章 - 前馈神经网络 - 书籍</div>
      <div>https://alec-97.github.io/posts/75512103/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Shuai Zhao</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年12月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/1943031638/" title="第4章 - 前馈神经网络 - 视频">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">第4章 - 前馈神经网络 - 视频</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/3859538395/" title="typora + GitHub + jsDelivr + PicGo">
                        <span class="hidden-mobile">typora + GitHub + jsDelivr + PicGo</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'alec-97/alec-97.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'Comment');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
      <div class="col-lg-7 mx-auto nopadding-x-md">
        <div class="container custom mx-auto">
           <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"> <script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script> <div id="player" class="aplayer aplayer-withlist aplayer-fixed" data-id="7729098320" data-server="netease" data-type="playlist" data-lrctype="0" data-order="random" data-fixed="true" data-listfolded="true" data-theme="#2D8CF0"></div> 
        </div>
      </div>
    
  </main>

  <footer>
    <div class="footer-inner" style="font-size: 0.85rem">
  <div class="alec_diy_footer">
  <!-- color:#d9dbdc -->
    
      <div class="footer-content">
         <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Fluid</span></a> <i class="iconfont icon-love"></i> <a href="https://https://alec-97.github.io/" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Alec</span></a>
<div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/vvd_js/duration.js"></script> </div>

      </div>
    

    
      <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

    

    
      <div class="footer-content">
        <a target="_blank" rel="noopener" href="https://developer.hitokoto.cn/" id="hitokoto_text"><span style="color: #d9dbdc;"  id="hitokoto"></span></a> <script src="https://v1.hitokoto.cn/?encode=js&select=%23hitokoto" defer></script> 
      </div>
    

    

    

  </div>  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/alec_diy/mouse_click/love.js"></script>
<script src="/alec_diy/live2d-widget/autoload.js"></script>
<script src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>




</body>
</html>
