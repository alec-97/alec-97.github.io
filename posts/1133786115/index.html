

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/photo.png">
  <link rel="icon" href="/img/photo.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Shuai Zhao">
  <meta name="keywords" content="人工智能, 深度学习, 软件开发, 个人博客, 所思所想">
  
    <meta name="description" content="alec：  在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构 循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．   循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．">
<meta property="og:type" content="article">
<meta property="og:title" content="第6章 - 循环神经网络 - 书籍">
<meta property="og:url" content="https://alec-97.github.io/posts/1133786115/index.html">
<meta property="og:site_name" content="要走起来，你才知道方向。">
<meta property="og:description" content="alec：  在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构 循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．   循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220155549904.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/a813c79080c84187ace2267f0c40352c61f69c8f5c7a4fa3a1f57eb24ed9fa27">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/40d2e25dfa5a44a386c6f98ff93e2c2a2a44bab0c1764c0ab22aec45c18e61f5">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/4bdc5e83a4e24feba0c3300535454b5a2030e48a992b4c3184f86c032484a928">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/22d9a00278914221a4074618567af6492323e5b2ed0c47849c8669540ece7dfb">
<meta property="og:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220110618357.png">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/8942af416fdc461cb0115956598b32a4d83f147aee0d4ec8a9ebff43e65e85bf">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/c80fb991231c417dad312c6c1396fdbd0db944566c2c46f29c4772387bcee278">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/6ddebc52f6494af49d1c77c1548ec4086c9a466869fd4389b3a02df3333e271a">
<meta property="og:image" content="https://ai-studio-static-online.cdn.bcebos.com/f319b75922c74c0c822cb0ab6383a22ca2a8d473ffa9472589e4db90cb70a062">
<meta property="article:published_time" content="2022-12-19T05:00:56.000Z">
<meta property="article:modified_time" content="2023-04-16T05:37:33.590Z">
<meta property="article:author" content="Shuai Zhao">
<meta property="article:tag" content="人工智能, 深度学习, 软件开发, 个人博客, 所思所想">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220155549904.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>第6章 - 循环神经网络 - 书籍 - 要走起来，你才知道方向。</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/alec_diy/css/alec_custom.css">
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/font-awesome/css/font-awesome.min.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"alec-97.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":80,"cursorChar":"_","loop":false,"scope":["home"]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"->"},"progressbar":{"enable":true,"height_px":3,"color":"#00FF7F","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  <div>
	<div class='real_mask' style="
		background-color: rgba(0,0,0,0.3);
		width: 100%;
		height: 100%;
		position: fixed;
		z-index: -777;
	"></div>
	<div id="banner_video_insert">
	</div>	
	<div id='vvd_banner_img'>
	</div>
</div>
<div id="banner"></div>
	<script type="text/javascript">
	  /*窗口监视*/
	  var originalTitle = document.title;
	  window.onblur = function(){document.title = "往事随风"};
	  window.onfocus = function(){document.title = originalTitle};
	</script>
  

  <header>
    

<div class="header-inner" style="height: 80vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Alec</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/study/">
                <i class="iconfont icon-books"></i>
                <span>学习进度</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/playlist/">
                <i class="iconfont icon-music"></i>
                <span>音乐</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">第6章 - 循环神经网络 - 书籍</span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Shuai Zhao
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-12-19 13:00" pubdate>
          2022年12月19日 下午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          46k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          381 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
        <div class="scroll-down-bar">
          <i class="iconfont icon-arrowdown"></i>
        </div>
      
    </div>
  </div>
</div>

</div>

	<script type="text/javascript" src="/vvd_js/jquery.js"></script>

	<div class="banner" id='banner' >

		<div class="full-bg-img" >

			
				<script>
					var ua = navigator.userAgent;
					var ipad = ua.match(/(iPad).*OS\s([\d_]+)/),
						isIphone = !ipad && ua.match(/(iPhone\sOS)\s([\d_]+)/),
						isAndroid = ua.match(/(Android)\s+([\d.]+)/),
						isMobile = isIphone || isAndroid;

					function set_video_attr(id){

						var height = document.body.clientHeight
						var width = document.body.clientWidth
						var video_item = document.getElementById(id);

						if (height / width < 0.56){
							video_item.setAttribute('width', '100%');
							video_item.setAttribute('height', 'auto');
						} else {
							video_item.setAttribute('height', '100%');
							video_item.setAttribute('width', 'auto');
						}
					}



					$.getJSON('/vvd_js/video_url.json', function(data){
						if (true){
							var video_list_length = data.length
							var seed = Math.random()
							index = Math.floor(seed * video_list_length)
							
							video_url = data[index][0]
							pre_show_image_url = data[index][1]

							// alec insert, 弹出当前是哪个视频
							// var info = index+"/"+video_list_length
							// alert(info)

							
							banner_obj = document.getElementById("banner")
							banner_obj.style.cssText = "background: url('" + pre_show_image_url + "') no-repeat; background-size: cover;"

							vvd_banner_obj = document.getElementById("vvd_banner_img")

							vvd_banner_content = "<img id='banner_img_item' src='" + pre_show_image_url + "' style='height: 100%; position: fixed; z-index: -999'>"
							vvd_banner_obj.innerHTML = vvd_banner_content
							set_video_attr('banner_img_item')

							if (!isMobile) {
								video_html_res = "<video id='video_item' style='position: fixed; z-index: -888;'  muted='muted' src=" + video_url + " autoplay='autoplay' loop='loop'></video>"
								document.getElementById("banner_video_insert").innerHTML = video_html_res;
								set_video_attr('video_item')
							}
						}
					});

					if (!isMobile){
						window.onresize = function(){
							set_video_attr('video_item')
							}
						}
				</script>
			
			</div>
		</div>
    </div>



  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">第6章 - 循环神经网络 - 书籍</h1>
            
              <p class="note note-info">
                
                  
                    本文最后更新于：13 天前
                  
                
              </p>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>alec：</p>
<ul>
<li>在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构</li>
<li>循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．</li>
</ul>
</blockquote>
<p>循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络．在循环神经网络中，神经元不但可以接受其他神经元的信息，也可以接受自身的信息，形成具有环路的网络结构．和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构．目前，循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上．</p>
<p>本章内容基于《神经网络与深度学习》第6章：循环神经网络的相关内容进行设计。在阅读本章之前，建议先了解如图6.1所示的关键知识点，以便更好地理解和掌握相应的理论和实践知识。</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220155549904.png" srcset="/img/loading.gif" lazyload alt="image-20221220110618357"></p>
<p>本章内容主要包含两部分：</p>
<ul>
<li>模型解读：介绍经典循环神经网络原理，为了更好地理解长程依赖问题，我们设计一个简单的数字求和任务来验证简单循环网络的记忆能力。长程依赖问题具体可分为梯度爆炸和梯度消失两种情况。对于梯度爆炸，我们复现简单循环网络的梯度爆炸现象并尝试解决。对于梯度消失，一种有效的方式是改进模型，我们也动手实现一个长短期记忆网络，并观察是否可以缓解长程依赖问题。</li>
<li>案例实践：基于双向长短期记忆网络实现文本分类任务．并了解如何进行补齐序列数据，如何将文本数据转为向量表示，如何对补齐位置进行掩蔽等实践知识。</li>
</ul>
<blockquote>
<p>alec：</p>
<ul>
<li>循环神经网络非常擅于处理序列数据，通过使用带自反馈的神经元，能够处理任意长度的序列数据</li>
<li>循环神经网络从左到右扫描该序列，并不断调用一个相同的组合函数$f(\cdot)$来处理时序信息．这个函数也称为循环神经网络单元（RNN Cell）</li>
<li>在每个时刻$t$，循环神经网络接受输入信息$\boldsymbol{x}<em>t \in \mathbb{R}^{M}$，并与前一时刻的隐状态$\boldsymbol{h}</em>{t-1} \in \mathbb{R}^D$一起进行计算，输出一个新的当前时刻的隐状态$\boldsymbol{h}_t$</li>
<li>循环神经网络的参数可以通过梯度下降法来学习</li>
<li>循环神经网络被认为是图灵完备的，一个完全连接的循环神经网络可以近似解决所有的可计算问题</li>
<li>长程依赖问题：虽然理论上循环神经网络可以建立长时间间隔的状态之间的依赖关系，但是由于具体的实现方式和参数学习方式会导致梯度爆炸或梯度消失问题，实际上，通常循环神经网络只能学习到短期的依赖关系，很难建模这种长距离的依赖关系，称为长程依赖问题（Long-Term Dependencies Problem）</li>
</ul>
</blockquote>
<blockquote>
<p>alec：</p>
<p>输入序列$[\boldsymbol{x}_0, \boldsymbol{x}_1, \boldsymbol{x}_2, …]$，其中x0-xn指的是从时间0，到时刻n，每个时刻对应的输入数据。</p>
<p>每个时刻t，xt是一个M维的向量，和前一时刻的D维状态一起运算，得到当前时刻对应的新的D维的状态</p>
</blockquote>
<p>循环神经网络非常擅于处理序列数据，通过使用带自反馈的神经元，能够处理任意长度的序列数据．给定输入序列$[\boldsymbol{x}_0, \boldsymbol{x}_1, \boldsymbol{x}_2, …]$，循环神经网络从左到右扫描该序列，并不断调用一个相同的组合函数$f(\cdot)$来处理时序信息．这个函数也称为循环神经网络单元（RNN Cell）. 在每个时刻$t$，循环神经网络接受输入信息$\boldsymbol{x}<em>t \in \mathbb{R}^{M}$，并与前一时刻的隐状态$\boldsymbol{h}</em>{t-1} \in \mathbb{R}^D$一起进行计算，输出一个新的当前时刻的隐状态$\boldsymbol{h}_t$.<br>$$<br>\boldsymbol{h}<em>t &#x3D; f(\boldsymbol{h}</em>{t-1}, \boldsymbol{x}_t),<br>$$</p>
<p>其中$\boldsymbol{h}_{0} &#x3D; 0$，$f(\cdot)$是一个非线性函数. </p>
<p>循环神经网络的参数可以通过梯度下降法来学习。和前馈神经网络类似，我们可以使用随时间反向传播（BackPropagation Through Time，BPTT）算法高效地手工计算梯度，也可以使用自动微分的方法，通过计算图自动计算梯度。</p>
<p>循环神经网络被认为是图灵完备的，一个完全连接的循环神经网络可以近似解决所有的可计算问题。然而，虽然理论上循环神经网络可以建立长时间间隔的状态之间的依赖关系，但是由于具体的实现方式和参数学习方式会导致梯度爆炸或梯度消失问题，实际上，通常循环神经网络只能学习到短期的依赖关系，很难建模这种长距离的依赖关系，称为长程依赖问题（Long-Term Dependencies Problem）。</p>
<hr>
<h2 id="√-6-1-循环神经网络的记忆能力实验"><a href="#√-6-1-循环神经网络的记忆能力实验" class="headerlink" title="[√] 6.1 - 循环神经网络的记忆能力实验"></a>[√] 6.1 - 循环神经网络的记忆能力实验</h2><hr>
<p>循环神经网络的一种简单实现是简单循环网络（Simple Recurrent Network，SRN）．</p>
<p>令向量$\boldsymbol{x}_t \in \mathbb{R}^M$表示在时刻$t$时网络的输入，$\boldsymbol{h_t} \in \mathbb{R}^D$ 表示隐藏层状态（即隐藏层神经元活性值），则$\boldsymbol{h}_t$不仅和当前时刻的输入$\boldsymbol{x}<em>t$相关，也和上一个时刻的隐藏层状态$\boldsymbol{h}</em>{t-1}$相关. 简单循环网络在时刻$t$的更新公式为</p>
<p>$$<br>\boldsymbol{h}_t &#x3D; f(\boldsymbol{W}\boldsymbol{x}<em>t + \boldsymbol{U}\boldsymbol{h}</em>{t-1} + b),<br>$$</p>
<p>其中$\boldsymbol{h}_{t}$为隐状态向量，$\boldsymbol{U} \in \mathbb{R}^{D\times D}$为<strong>状态-状态</strong>权重矩阵，$\boldsymbol{W} \in \mathbb{R}^{D\times M}$为<strong>状态-输入</strong>权重矩阵，$\boldsymbol{b}\in \mathbb{R}^{D}$为偏置向量。</p>
<blockquote>
<p>alec：</p>
<p>参数U用于上一时刻D维的状态映射到当前时刻新的D维的状态</p>
<p>参数W用于将M维的输入向量映射为新的D维的状态</p>
<p>比如下图中的X1，M&#x3D;3，时刻1输入了一个3维的向量，然后和H0一起计算，得到新的D&#x3D;3维的状态，然后循环计算</p>
</blockquote>
<p><strong>图6.2</strong> 展示了一个按时间展开的循环神经网络。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/a813c79080c84187ace2267f0c40352c61f69c8f5c7a4fa3a1f57eb24ed9fa27" srcset="/img/loading.gif" lazyload width=50%></center>
<center>图6.2 循环神经网络结构</center></br>

<p>简单循环网络在参数学习时存在长程依赖问题，很难建模长时间间隔（Long Range）的状态之间的依赖关系。为了测试简单循环网络的记忆能力，本节构建一个数字求和任务进行实验。</p>
<p>数字求和任务的输入是一串数字，前两个位置的数字为0-9，其余数字随机生成（主要为0），预测目标是输入序列中前两个数字的加和。图6.3展示了长度为10的数字序列．</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/40d2e25dfa5a44a386c6f98ff93e2c2a2a44bab0c1764c0ab22aec45c18e61f5" srcset="/img/loading.gif" lazyload width=50%></center>
<center>图6.3 数字求和任务示例</center></br>

<p>如果序列长度越长，准确率越高，则说明网络的记忆能力越好．因此，我们可以构建不同长度的数据集，通过验证简单循环网络在不同长度的数据集上的表现，从而测试简单循环网络的长程依赖能力.</p>
<h4 id="√-6-1-1-数据集构建"><a href="#√-6-1-1-数据集构建" class="headerlink" title="[√] 6.1.1 数据集构建"></a>[√] 6.1.1 数据集构建</h4><hr>
<p>我们首先构建不同长度的数字预测数据集DigitSum.</p>
<h6 id="√-6-1-1-1-数据集的构建函数"><a href="#√-6-1-1-1-数据集的构建函数" class="headerlink" title="[√] 6.1.1.1 数据集的构建函数"></a>[√] 6.1.1.1 数据集的构建函数</h6><hr>
<p>由于在本任务中，输入序列的前两位数字为 0 − 9，其组合数是固定的，所以可以穷举所有的前两位数字组合，并在后面默认用0填充到固定长度. 但考虑到数据的多样性，这里对生成的数字序列中的零位置进行随机采样，并将其随机替换成0-9的数字以增加样本的数量．</p>
<p>我们可以通过设置k的数值来指定一条样本随机生成的数字序列数量.当生成某个指定长度的数据集时，会同时生成训练集、验证集和测试集。当k&#x3D;3时，生成训练集。当k&#x3D;1时，生成验证集和测试集. 代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><span class="hljs-comment"># 固定随机种子</span><br>random.seed(<span class="hljs-number">0</span>)<br>np.random.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_data</span>(<span class="hljs-params">length, k, save_path</span>):<br>    <span class="hljs-keyword">if</span> length &lt; <span class="hljs-number">3</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The length of data should be greater than 2.&quot;</span>)<br>    <span class="hljs-keyword">if</span> k == <span class="hljs-number">0</span>:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;k should be greater than 0.&quot;</span>)<br>    <span class="hljs-comment"># 生成100条长度为length的数字序列，除前两个字符外，序列其余数字暂用0填充</span><br>    base_examples = []<br>    <span class="hljs-keyword">for</span> n1 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):<br>        <span class="hljs-keyword">for</span> n2 <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>):<br>            seq = [n1, n2] + [<span class="hljs-number">0</span>] * (length - <span class="hljs-number">2</span>)<br>            label = n1 + n2<br>            base_examples.append((seq, label))<br>    <br>    examples = []<br>    <span class="hljs-comment"># 数据增强：对base_examples中的每条数据，默认生成k条数据，放入examples</span><br>    <span class="hljs-comment"># 对于每个数据，生成k条带干扰数值的数据</span><br>    <span class="hljs-keyword">for</span> base_example <span class="hljs-keyword">in</span> base_examples:<br>        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            <span class="hljs-comment"># 随机生成替换的元素位置和元素</span><br>            idx = np.random.randint(<span class="hljs-number">2</span>, length)<br>            val = np.random.randint(<span class="hljs-number">0</span>, <span class="hljs-number">10</span>)<br>            <span class="hljs-comment"># 对序列中的对应零元素进行替换</span><br>            seq = base_example[<span class="hljs-number">0</span>].copy()<br>            label = base_example[<span class="hljs-number">1</span>]<br>            seq[idx] = val<br>            examples.append((seq, label))<br><br>    <span class="hljs-comment"># 保存增强后的数据</span><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(save_path, <span class="hljs-string">&quot;w&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples:<br>            <span class="hljs-comment"># 将数据转为字符串类型，方便保存</span><br>            seq = [<span class="hljs-built_in">str</span>(e) <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> example[<span class="hljs-number">0</span>]]<br>            label = <span class="hljs-built_in">str</span>(example[<span class="hljs-number">1</span>])<br>            line = <span class="hljs-string">&quot; &quot;</span>.join(seq) + <span class="hljs-string">&quot;\t&quot;</span> + label + <span class="hljs-string">&quot;\n&quot;</span><br>            f.write(line)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;generate data to: <span class="hljs-subst">&#123;save_path&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 定义生成的数字序列长度</span><br>lengths = [<span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    <span class="hljs-comment"># 生成长度为length的训练数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/train.txt&quot;</span><br>    k = <span class="hljs-number">3</span><br>    generate_data(length, k, save_path)<br>    <span class="hljs-comment"># 生成长度为length的验证数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/dev.txt&quot;</span><br>    k = <span class="hljs-number">1</span><br>    generate_data(length, k, save_path)<br>    <span class="hljs-comment"># 生成长度为length的测试数据</span><br>    save_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>/test.txt&quot;</span><br>    k = <span class="hljs-number">1</span><br>    generate_data(length, k, save_path)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">generate data to: ./datasets/<span class="hljs-number">5</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">5</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">5</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">10</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">15</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">20</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">25</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">30</span>/test.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/train.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/dev.txt.<br>generate data to: ./datasets/<span class="hljs-number">35</span>/test.txt.<br></code></pre></td></tr></table></figure>

<h6 id="√-6-1-1-2-加载数据并进行数据划分"><a href="#√-6-1-1-2-加载数据并进行数据划分" class="headerlink" title="[√] 6.1.1.2 加载数据并进行数据划分"></a>[√] 6.1.1.2 加载数据并进行数据划分</h6><hr>
<p>为方便使用，本实验提前生成了长度分别为5、10、 15、20、25、30和35的7份数据，存放于“.&#x2F;datasets”目录下，读者可以直接加载使用。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-comment"># 加载数据</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_data</span>(<span class="hljs-params">data_path</span>):<br>    <span class="hljs-comment"># 加载训练集</span><br>    train_examples = []<br>    train_path = os.path.join(data_path, <span class="hljs-string">&quot;train.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(train_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            train_examples.append((seq, label))<br><br>    <span class="hljs-comment"># 加载验证集</span><br>    dev_examples = []<br>    dev_path = os.path.join(data_path, <span class="hljs-string">&quot;dev.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(dev_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            dev_examples.append((seq, label))<br><br>    <span class="hljs-comment"># 加载测试集</span><br>    test_examples = []<br>    test_path = os.path.join(data_path, <span class="hljs-string">&quot;test.txt&quot;</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(test_path, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> f.readlines():<br>            <span class="hljs-comment"># 解析一行数据，将其处理为数字序列seq和标签label</span><br>            items = line.strip().split(<span class="hljs-string">&quot;\t&quot;</span>)<br>            seq = [<span class="hljs-built_in">int</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> items[<span class="hljs-number">0</span>].split(<span class="hljs-string">&quot; &quot;</span>)]<br>            label = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br>            test_examples.append((seq, label))<br><br>    <span class="hljs-keyword">return</span> train_examples, dev_examples, test_examples<br><br><span class="hljs-comment"># 设定加载的数据集的长度</span><br>length = <span class="hljs-number">5</span><br><span class="hljs-comment"># 该长度的数据集的存放目录</span><br>data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br><span class="hljs-comment"># 加载该数据集</span><br>train_examples, dev_examples, test_examples = load_data(data_path)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;dev example:&quot;</span>, dev_examples[:<span class="hljs-number">4</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练集数量：&quot;</span>, <span class="hljs-built_in">len</span>(train_examples))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;验证集数量：&quot;</span>, <span class="hljs-built_in">len</span>(dev_examples))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;测试集数量：&quot;</span>, <span class="hljs-built_in">len</span>(test_examples))<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">dev example: [([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">6</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], <span class="hljs-number">0</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">8</span>], <span class="hljs-number">1</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">5</span>, <span class="hljs-number">0</span>], <span class="hljs-number">2</span>), ([<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>], <span class="hljs-number">3</span>)]<br>训练集数量： <span class="hljs-number">300</span><br>验证集数量： <span class="hljs-number">100</span><br>测试集数量： <span class="hljs-number">100</span><br></code></pre></td></tr></table></figure>

<h6 id="√-6-1-1-3-构造Dataset类"><a href="#√-6-1-1-3-构造Dataset类" class="headerlink" title="[√] 6.1.1.3 构造Dataset类"></a>[√] 6.1.1.3 构造Dataset类</h6><hr>
<p>为了方便使用梯度下降法进行优化，我们构造了DigitSum数据集的Dataset类，函数<code>__getitem__</code>负责根据索引读取数据，并将数据转换为张量。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> paddle.io <span class="hljs-keyword">import</span> Dataset<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DigitSumDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br>    <br>    <span class="hljs-comment"># 获取指定索引的数据并稍作处理</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        example = self.data[idx]<br>        seq = paddle.to_tensor(example[<span class="hljs-number">0</span>], dtype=<span class="hljs-string">&quot;int64&quot;</span>)<br>        label = paddle.to_tensor(example[<span class="hljs-number">1</span>], dtype=<span class="hljs-string">&quot;int64&quot;</span>)<br>        <span class="hljs-keyword">return</span> seq, label<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br>        <br></code></pre></td></tr></table></figure>

<h4 id="√-6-1-2-模型构建"><a href="#√-6-1-2-模型构建" class="headerlink" title="[√] 6.1.2 模型构建"></a>[√] 6.1.2 模型构建</h4><hr>
<p>使用SRN模型进行数字加和任务的模型结构为如图6.4所示.</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/4bdc5e83a4e24feba0c3300535454b5a2030e48a992b4c3184f86c032484a928" srcset="/img/loading.gif" lazyload width=50%></center>
<center>图6.4 基于SRN模型的数字预测</center>

<p>整个模型由以下几个部分组成：  </p>
<p>（1） 嵌入层：将输入的数字序列进行向量化，即将每个数字映射为向量；  </p>
<p>（2） SRN 层：接收向量序列，更新循环单元，将最后时刻的隐状态作为整个序列的表示；  </p>
<p>（3） 输出层：一个线性层，输出分类的结果.  </p>
<blockquote>
<p>alec：</p>
<p>网络具有记忆能力，即最后时刻的输出，等于刚开始的时候输入的两个元素的和</p>
</blockquote>
<h6 id="√-6-1-2-1-嵌入层"><a href="#√-6-1-2-1-嵌入层" class="headerlink" title="[√] 6.1.2.1 嵌入层"></a>[√] 6.1.2.1 嵌入层</h6><hr>
<p>本任务输入的样本是数字序列，为了更好地表示数字，需要将数字映射为一个嵌入（Embedding）向量。嵌入向量中的每个维度均能用来刻画该数字本身的某种特性。由于向量能够表达该数字更多的信息，利用向量进行数字求和任务，可以使得模型具有更强的拟合能力。</p>
<p>首先，我们构建一个嵌入矩阵（Embedding Matrix）$\boldsymbol{E}\in \mathbb{R}^{10\times M}$，其中第$i$行对应数字$i$的嵌入向量，每个嵌入向量的维度是$M$。如图6.5所示。<br>给定一个组数字序列$\boldsymbol{S} \in \mathbb{R}^{B\times L}$，其中$B$为批大小，$L$为序列长度，可以通过查表将其映射为嵌入表示$\boldsymbol{X}\in \mathbb{R}^{B\times L \times M}$。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/22d9a00278914221a4074618567af6492323e5b2ed0c47849c8669540ece7dfb" srcset="/img/loading.gif" lazyload width=50%></center>
<center>图6.5 嵌入矩阵</center>

<blockquote>
<p>alec：</p>
<p>如上图所示举例，一个词向量的长度是5，一个批次的数据包含3个向量。</p>
</blockquote>
<blockquote>
<p><strong>提醒</strong>：为了和代码的实现保持一致性，这里使用形状为$(样本数量\times 序列长度\times 特征维度)$的张量来表示一组样本。</p>
</blockquote>
<p>或者也可以将每个数字表示为10维的one-hot向量，使用矩阵运算得到嵌入表示：</p>
<p>$$<br>\boldsymbol{X} &#x3D; \boldsymbol{S}^{‘} \boldsymbol{E}，<br>$$</p>
<p>其中$\boldsymbol{S}’ \in \mathbb{R}^{B\times L\times 10}$是序列$\boldsymbol{S}$对应的one-hot表示。</p>
<blockquote>
<p><strong>思考</strong>：如果不使用嵌入层，直接将数字作为SRN层输入有什么问题？</p>
</blockquote>
<p>基于索引方式的嵌入层的实现如下：</p>
<p>嵌入层对输入序列中的每个元素设置一个权重参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-comment"># 嵌入层的实现代码</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Embedding</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_embeddings, embedding_dim, para_attr=paddle.ParamAttr(<span class="hljs-params">initializer=nn.initializer.XavierUniform(<span class="hljs-params"></span>)</span>)</span>):<br>        <span class="hljs-built_in">super</span>(Embedding, self).__init__()<br>        <span class="hljs-comment"># 定义嵌入矩阵</span><br>        self.W = paddle.create_parameter(shape=[num_embeddings, embedding_dim], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=para_attr)<br>    <br>    <span class="hljs-comment"># alec：得到随机初始化的参数矩阵，并返回第inputs行的参数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 根据索引获取对应词向量</span><br>        embs = self.W[inputs]<br>        <span class="hljs-keyword">return</span> embs<br><br><span class="hljs-comment"># 得到10行5列的嵌入矩阵，并返回第0、1、2、3行的参数</span><br>emb_layer = Embedding(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>)<br>inputs = paddle.to_tensor([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])<br>emb_layer(inputs)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">4</span>, <span class="hljs-number">5</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[-<span class="hljs-number">0.42956319</span>,  <span class="hljs-number">0.24104618</span>, -<span class="hljs-number">0.37770554</span>,  <span class="hljs-number">0.20341983</span>, -<span class="hljs-number">0.22462121</span>],<br>        [ <span class="hljs-number">0.12806311</span>,  <span class="hljs-number">0.11553246</span>, -<span class="hljs-number">0.12461890</span>,  <span class="hljs-number">0.43855545</span>, -<span class="hljs-number">0.04116940</span>],<br>        [-<span class="hljs-number">0.11202095</span>,  <span class="hljs-number">0.13205586</span>,  <span class="hljs-number">0.58078343</span>, -<span class="hljs-number">0.49379382</span>,  <span class="hljs-number">0.06259152</span>],<br>        [-<span class="hljs-number">0.51902902</span>,  <span class="hljs-number">0.04430389</span>, -<span class="hljs-number">0.37035075</span>, -<span class="hljs-number">0.21242915</span>,  <span class="hljs-number">0.46721438</span>]])<br></code></pre></td></tr></table></figure>

<h6 id="√-6-1-2-2-SRN层"><a href="#√-6-1-2-2-SRN层" class="headerlink" title="[√] 6.1.2.2 SRN层"></a>[√] 6.1.2.2 SRN层</h6><hr>
<blockquote>
<p>alec：</p>
<p>SRN的非线性激活函数是使用的tanh函数</p>
</blockquote>
<p>数字序列$\boldsymbol{S} \in \mathbb{R}^{B\times L}$经过嵌入层映射后，转换为$\boldsymbol{X}\in \mathbb{R}^{B\times L\times M}$，其中$B$为批大小，$L$为序列长度，$M$为嵌入维度。</p>
<p>在时刻$t$，SRN将当前的输入$\boldsymbol{X}<em>t \in \mathbb{R}^{B \times M}$与隐状态$\boldsymbol{H}</em>{t-1}  \in \mathbb{R}^{B \times D}$进行线性变换和组合，并通过一个非线性激活函数$f(\cdot)$得到新的隐状态，SRN的状态更新函数为:</p>
<p>$$<br>\boldsymbol{H}_t &#x3D; \text{Tanh}(\boldsymbol{X}<em>t\boldsymbol{W} + \boldsymbol{H}</em>{t-1}\boldsymbol{U} + \boldsymbol{b}),<br>$$</p>
<p>其中$\boldsymbol{W} \in \mathbb{R}^{M \times D}, \boldsymbol{U} \in \mathbb{R}^{D \times D}, \boldsymbol{b} \in \mathbb{R}^{1 \times D}$是可学习参数，$D$表示隐状态向量的维度。</p>
<p>简单循环网络的代码实现如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> paddle.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># SRN模型</span><br><span class="hljs-comment"># 隐状态是指的中间状态的维度？</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SRN</span>(nn.Layer):<br>    <span class="hljs-comment"># input_size是输入数据的通道维度，hidden_size是中间状态的通道维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size,  hidden_size, W_attr=<span class="hljs-literal">None</span>, U_attr=<span class="hljs-literal">None</span>, b_attr=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(SRN, self).__init__()<br>        <span class="hljs-comment"># 嵌入向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># 隐状态的维度</span><br>        self.hidden_size = hidden_size<br>        <span class="hljs-comment"># 定义模型参数W，其shape为 input_size x hidden_size</span><br>        self.W = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=W_attr)<br>        <span class="hljs-comment"># 定义模型参数U，其shape为hidden_size x hidden_size</span><br>        self.U = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>,attr=U_attr)<br>        <span class="hljs-comment"># 定义模型参数b，其shape为 1 x hidden_size</span><br>        self.b = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=b_attr)<br><br>    <span class="hljs-comment"># 初始化向量</span><br>    <span class="hljs-comment"># 向量有batch_size个，每个长度为hidden_size</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, batch_size</span>):<br>        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">return</span> hidden_state<br><br>    <span class="hljs-comment"># 定义前向计算</span><br>    <span class="hljs-comment"># input_size指的是输入数据的维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, hidden_state=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># inputs: 输入数据, 其shape为batch_size x seq_len x input_size</span><br>        batch_size, seq_len, input_size = inputs.shape<br><br>        <span class="hljs-comment"># 初始化起始状态的隐向量, 其shape为 batch_size x hidden_size</span><br>        <span class="hljs-comment"># 隐状态的向量用于接收中间的状态</span><br>        <span class="hljs-keyword">if</span> hidden_state <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            hidden_state = self.init_state(batch_size)<br><br>        <span class="hljs-comment"># 循环执行RNN计算</span><br>        <span class="hljs-comment"># 序列中的每个元素，一步步的来计算</span><br>        <span class="hljs-comment"># 循环累计隐状态</span><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>            <span class="hljs-comment"># batch_size是输入数据的批数量，input_size是输入数据的维度</span><br>            <span class="hljs-comment"># 获取当前时刻的输入数据step_input, 其shape为 batch_size x input_size</span><br>            step_input = inputs[:, step, :]<br>            <span class="hljs-comment"># 获取当前时刻的隐状态向量hidden_state, 其shape为 batch_size x hidden_size</span><br>            <span class="hljs-comment"># 对于batch_size x seq_len x input_size的输入数据，其中seq_len是指的时间长度，因此对于当前时刻，</span><br>            <span class="hljs-comment"># 维度就只有对于batch_size × input_size，然后将当前时刻的输入数据转为了batch_size × hidden_state</span><br>            <span class="hljs-comment"># 权重w用于和当前时刻的输入向量相乘</span><br>            <span class="hljs-comment"># 权重U用于和当前的隐状态相乘</span><br>            hidden_state = F.tanh(paddle.matmul(step_input, self.W) + paddle.matmul(hidden_state, self.U) + self.b)<br>        <span class="hljs-keyword">return</span> hidden_state<br></code></pre></td></tr></table></figure>

<blockquote>
<p>提醒： 这里只保留了简单循环网络的最后一个时刻的输出向量。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 初始化参数并运行</span><br><span class="hljs-comment"># 输入的数据通道维度为2，隐状态的维度为2</span><br>W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.2</span>]]))<br>U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>,<span class="hljs-number">0.0</span>]]))<br>b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br><br>srn = SRN(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)<br><span class="hljs-comment"># 输入数据的维度为batch_size x seq_len x input_size，1批1个、序列长度为2，通道维度为2</span><br>inputs = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>],[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>]]], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>hidden_state = srn(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hidden_state&quot;</span>, hidden_state)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">hidden_state Tensor(shape=[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[<span class="hljs-number">0.31773996</span>, <span class="hljs-number">0.47749740</span>]])<br></code></pre></td></tr></table></figure>

<p>飞桨框架已经内置了SRN的API <code>paddle.nn.SimpleRNN</code>，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：</p>
<p>$$<br>\boldsymbol{H}_t &#x3D; \text{Tanh}(\boldsymbol{W}\boldsymbol{X}_t + \boldsymbol{b}<em>x +  \boldsymbol{U}\boldsymbol{H}</em>{t-1} + \boldsymbol{b}_h),<br>$$</p>
<p>其中$\boldsymbol{W} \in \mathbb{R}^{M \times D}, \boldsymbol{U} \in \mathbb{R}^{D \times D}, \boldsymbol{b}_x \in \mathbb{R}^{1 \times D}, \boldsymbol{b}_h \in \mathbb{R}^{1 \times D}$是可学习参数，$M$表示嵌入向量的维度，$D$表示隐状态向量的维度。</p>
<p>另外，内置SRN API在执行完前向计算后，会返回两个参数：序列向量和最后时刻的隐状态向量。在飞桨实现时，考虑到了双向和多层SRN的因素，返回的向量附带了这些信息。</p>
<p>其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的隐状态向量shape为[num_layers * num_directions, batch_size, hidden_size]。</p>
<p>这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size、</span><br><span class="hljs-comment"># 每批8个数据，时间长度是20，每个时刻的特征维度是32</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br><span class="hljs-comment"># 隐层特征数量也是32个</span><br>hidden_size = <span class="hljs-number">32</span><br>paddle_srn = nn.SimpleRNN(input_size, hidden_size)<br>self_srn = SRN(input_size, hidden_size)<br><br>self_hidden_state = self_srn(inputs)<br>paddle_outputs, paddle_hidden_state = paddle_srn(inputs)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self_srn hidden_state: &quot;</span>, self_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_srn outpus:&quot;</span>, paddle_outputs.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_srn hidden_state:&quot;</span>, paddle_hidden_state.shape)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">self_srn hidden_state:  [<span class="hljs-number">8</span>, <span class="hljs-number">32</span>] <span class="hljs-comment"># 当前时刻的状态，每批8个，隐藏状态特征数是32</span><br>paddle_srn outpus: [<span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span>]<br>paddle_srn hidden_state: [<span class="hljs-number">1</span>, <span class="hljs-number">8</span>, <span class="hljs-number">32</span>]<br></code></pre></td></tr></table></figure>

<p>可以看到，自己实现的SRN由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化SRN时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].</p>
<p>接下来，我们可以将自己实现的SRN与Paddle内置的SRN在输出值的精度上进行对比，这里首先根据Paddle内置的SRN实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_x$设置为0），然后提取该模型对应的参数，使用该参数去初始化自己实现的SRN，从而保证两者在参数初始化时是一致的。</p>
<p>在进行实验时，首先定义输入数据<code>inputs</code>，然后将该数据分别传入Paddle内置的SRN与自己实现的SRN模型中，最后通过对比两者的隐状态输出向量。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br><span class="hljs-comment"># 每批2个数据，每个时刻的每个数据维度是10维，时间长度为5，隐层特征的数据维度是10维</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>bx_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([hidden_size, ])))<br>paddle_srn = nn.SimpleRNN(input_size, hidden_size, bias_ih_attr=bx_attr)<br><br><span class="hljs-comment"># 获取paddle_srn中的参数，并设置相应的paramAttr,用于初始化SRN</span><br>W_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_ih_l0.T))<br>U_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.weight_hh_l0.T))<br>b_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle_srn.bias_hh_l0))<br>self_srn = SRN(input_size, hidden_size, W_attr=W_attr, U_attr=U_attr, b_attr=b_attr)<br><br><span class="hljs-comment"># 进行前向计算，获取隐状态向量，并打印展示</span><br>self_hidden_state = self_srn(inputs)<br>paddle_outputs, paddle_hidden_state = paddle_srn(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle SRN:\n&quot;</span>, paddle_hidden_state.numpy().squeeze(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self SRN:\n&quot;</span>, self_hidden_state.numpy())<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">paddle SRN:<br> [[ <span class="hljs-number">0.3246606</span>  -<span class="hljs-number">0.05465741</span> -<span class="hljs-number">0.3090897</span>  -<span class="hljs-number">0.51604617</span> -<span class="hljs-number">0.11149617</span>  <span class="hljs-number">0.4267313</span><br>   <span class="hljs-number">0.47200006</span> -<span class="hljs-number">0.06585315</span>  <span class="hljs-number">0.85319966</span>  <span class="hljs-number">0.18898569</span>]<br> [-<span class="hljs-number">0.4299355</span>  -<span class="hljs-number">0.6067489</span>  -<span class="hljs-number">0.59150505</span>  <span class="hljs-number">0.30245274</span> -<span class="hljs-number">0.03939498</span>  <span class="hljs-number">0.61462754</span><br>   <span class="hljs-number">0.4030218</span>   <span class="hljs-number">0.49883503</span>  <span class="hljs-number">0.02484456</span> -<span class="hljs-number">0.38516262</span>]]<br>self SRN:<br> [[ <span class="hljs-number">0.32466057</span> -<span class="hljs-number">0.05465744</span> -<span class="hljs-number">0.3090897</span>  -<span class="hljs-number">0.51604617</span> -<span class="hljs-number">0.11149605</span>  <span class="hljs-number">0.4267313</span><br>   <span class="hljs-number">0.47200006</span> -<span class="hljs-number">0.06585318</span>  <span class="hljs-number">0.85319966</span>  <span class="hljs-number">0.18898569</span>]<br> [-<span class="hljs-number">0.42993543</span> -<span class="hljs-number">0.6067488</span>  -<span class="hljs-number">0.59150493</span>  <span class="hljs-number">0.3024528</span>  -<span class="hljs-number">0.03939501</span>  <span class="hljs-number">0.61462754</span><br>   <span class="hljs-number">0.40302184</span>  <span class="hljs-number">0.49883503</span>  <span class="hljs-number">0.02484456</span> -<span class="hljs-number">0.38516262</span>]]<br></code></pre></td></tr></table></figure>

<p>可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 实例化模型</span><br>self_srn = SRN(input_size, hidden_size)<br>paddle_srn = nn.SimpleRNN(input_size, hidden_size)<br><br><span class="hljs-comment"># 计算自己实现的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    out = self_srn(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;self_srn speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br><span class="hljs-comment"># 计算Paddle内置的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    out = paddle_srn(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;paddle_srn speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br></code></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">self_srn</span> speed: <span class="hljs-number">0</span>.<span class="hljs-number">0010721974902682834</span> s<br><span class="hljs-attribute">paddle_srn</span> speed: <span class="hljs-number">0</span>.<span class="hljs-number">0004733721415201823</span> s<br></code></pre></td></tr></table></figure>

<p>可以看到，由于Paddle内部相关算子由C++实现，Paddle框架实现的SRN的运行效率显著高于自己实现的SRN效率。</p>
<h6 id="√-6-1-2-3-线性层"><a href="#√-6-1-2-3-线性层" class="headerlink" title="[√] 6.1.2.3 线性层"></a>[√] 6.1.2.3 线性层</h6><hr>
<p>线性层会将最后一个时刻的隐状态向量$\boldsymbol{H}_L \in \mathbb{R}^{B \times D}$进行线性变换，输出分类的对数几率（Logits）为：<br>$$<br>\boldsymbol{Y} &#x3D; \boldsymbol{H}_L \boldsymbol{W}_o + \boldsymbol{b}_o，<br>$$</p>
<p>其中$\boldsymbol{W}_o \in \mathbb{R}^{D \times 19}$，$\boldsymbol{b}_o \in \mathbb{R}^{19}$为可学习的权重矩阵和偏置。</p>
<blockquote>
<p>提醒：在分类问题的实践中，我们通常只需要模型输出分类的对数几率（Logits），而不用输出每个类的概率。这需要损失函数可以直接接收对数几率来损失计算。</p>
</blockquote>
<p>线性层直接使用<code>paddle.nn.Linear</code>算子。</p>
<h6 id="√-6-1-2-4-模型汇总"><a href="#√-6-1-2-4-模型汇总" class="headerlink" title="[√] 6.1.2.4 模型汇总"></a>[√] 6.1.2.4 模型汇总</h6><hr>
<p>在定义了每一层的算子之后，我们定义一个数字求和模型Model_RNN4SeqClass，该模型会将嵌入层、SRN层和线性层进行组合，以实现数字求和的功能.</p>
<p>具体来讲，Model_RNN4SeqClass会接收一个SRN层实例，用于处理数字序列数据，同时在<code>__init__</code>函数中定义一个<code>Embedding</code>嵌入层，其会将输入的数字作为索引，输出对应的向量，最后会使用<code>paddle.nn.Linear</code>定义一个线性层。</p>
<blockquote>
<p>提醒：为了方便进行对比实验，我们将SRN层的实例化放在\code{Model_RNN4SeqClass}类外面。通常情况下，模型内部算子的实例化是放在模型里面。</p>
</blockquote>
<p>在<code>forward</code>函数中，调用上文实现的嵌入层、SRN层和线性层处理数字序列，同时返回最后一个位置的隐状态向量。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 基于RNN实现数字预测的模型</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model_RNN4SeqClass</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, num_digits, input_size, hidden_size, num_classes</span>):<br>        <span class="hljs-built_in">super</span>(Model_RNN4SeqClass, self).__init__()<br>        <span class="hljs-comment"># 传入实例化的RNN层，例如SRN</span><br>        self.rnn_model = model<br>        <span class="hljs-comment"># 词典大小</span><br>        self.num_digits = num_digits<br>        <span class="hljs-comment"># 嵌入向量的维度</span><br>        self.input_size = input_size<br>        <span class="hljs-comment"># 定义Embedding层</span><br>        self.embedding = Embedding(num_digits, input_size)<br>        <span class="hljs-comment"># 定义线性层</span><br>        self.linear = nn.Linear(hidden_size, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs</span>):<br>        <span class="hljs-comment"># 将数字序列映射为相应向量</span><br>        inputs_emb = self.embedding(inputs)<br>        <span class="hljs-comment"># 调用RNN模型</span><br>        hidden_state = self.rnn_model(inputs_emb)<br>        <span class="hljs-comment"># 使用最后一个时刻的状态进行数字预测</span><br>        logits = self.linear(hidden_state)<br>        <span class="hljs-keyword">return</span> logits<br><br><span class="hljs-comment"># 实例化一个input_size为4， hidden_size为5的SRN</span><br>srn = SRN(<span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br><span class="hljs-comment"># 基于srn实例化一个数字预测模型实例</span><br>model = Model_RNN4SeqClass(srn, <span class="hljs-number">10</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">19</span>)<br><span class="hljs-comment"># 生成一个shape为 2 x 3 的批次数据</span><br>inputs = paddle.to_tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]])<br><span class="hljs-comment"># 进行模型前向预测</span><br>logits = model(inputs)<br><span class="hljs-built_in">print</span>(logits)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">Tensor(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">19</span>], dtype=float32, place=CUDAPlace(<span class="hljs-number">0</span>), stop_gradient=<span class="hljs-literal">False</span>,<br>       [[ <span class="hljs-number">0.36087763</span>, -<span class="hljs-number">0.03377634</span>, -<span class="hljs-number">0.04800312</span>,  <span class="hljs-number">0.49252868</span>, -<span class="hljs-number">0.45962709</span>,<br>         -<span class="hljs-number">0.44703209</span>,  <span class="hljs-number">0.64295375</span>,  <span class="hljs-number">0.53624588</span>, -<span class="hljs-number">0.19376591</span>,  <span class="hljs-number">0.11085325</span>,<br>         -<span class="hljs-number">0.31243768</span>,  <span class="hljs-number">0.29747075</span>, -<span class="hljs-number">0.31725749</span>, -<span class="hljs-number">0.41438878</span>,  <span class="hljs-number">0.00990404</span>,<br>          <span class="hljs-number">0.45916951</span>, -<span class="hljs-number">0.31540897</span>,  <span class="hljs-number">0.57389849</span>, -<span class="hljs-number">0.03416194</span>],<br>        [ <span class="hljs-number">0.01424524</span>, -<span class="hljs-number">0.12685573</span>, -<span class="hljs-number">0.07969519</span>,  <span class="hljs-number">0.56528699</span>, -<span class="hljs-number">0.65557188</span>,<br>         -<span class="hljs-number">0.57581109</span>,  <span class="hljs-number">0.84303617</span>,  <span class="hljs-number">0.07659776</span>,  <span class="hljs-number">0.01592400</span>, -<span class="hljs-number">0.38144892</span>,<br>         -<span class="hljs-number">0.24371660</span>,  <span class="hljs-number">0.38759732</span>, -<span class="hljs-number">0.46055052</span>, -<span class="hljs-number">0.87889659</span>, -<span class="hljs-number">0.16003403</span>,<br>          <span class="hljs-number">0.67612255</span>, -<span class="hljs-number">0.36139122</span>,  <span class="hljs-number">0.40609291</span>, -<span class="hljs-number">0.26660436</span>]])<br></code></pre></td></tr></table></figure>



<h4 id="√-6-1-3-模型训练"><a href="#√-6-1-3-模型训练" class="headerlink" title="[√] 6.1.3 模型训练"></a>[√] 6.1.3 模型训练</h4><hr>
<h6 id="√-6-1-3-1-训练指定长度的数字预测模型"><a href="#√-6-1-3-1-训练指定长度的数字预测模型" class="headerlink" title="[√] 6.1.3.1 训练指定长度的数字预测模型"></a>[√] 6.1.3.1 训练指定长度的数字预测模型</h6><hr>
<p>基于RunnerV3类进行训练，只需要指定length便可以加载相应的数据。设置超参数，使用Adam优化器，学习率为 0.001，实例化模型，使用第4.5.4节定义的Accuracy计算准确率。使用Runner进行训练，训练回合数设为500。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> Accuracy, RunnerV3<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">500</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">8</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><span class="hljs-comment"># 通过指定length进行不同长度数据的实验</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">length</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training SRN with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    <span class="hljs-comment"># 固定随机种子</span><br>    np.random.seed(<span class="hljs-number">0</span>)<br>    random.seed(<span class="hljs-number">0</span>)<br>    paddle.seed(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)<br>    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br>    <span class="hljs-comment"># 实例化模型</span><br>    base_model = SRN(input_size, hidden_size)<br>    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br>    <span class="hljs-comment"># 指定优化器</span><br>    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters()) <br>    <span class="hljs-comment"># 定义评价指标</span><br>    metric = Accuracy()<br>    <span class="hljs-comment"># 定义损失函数</span><br>    loss_fn = nn.CrossEntropyLoss()<br><br>    <span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>    runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br>    <span class="hljs-comment"># 进行模型训练</span><br>    model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_srn_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">100</span>, save_path=model_save_path)<br><br>    <span class="hljs-keyword">return</span> runner<br><br></code></pre></td></tr></table></figure>





<h6 id="√-6-1-3-2-多组训练"><a href="#√-6-1-3-2-多组训练" class="headerlink" title="[√] 6.1.3.2 多组训练"></a>[√] 6.1.3.2 多组训练</h6><hr>
<p>接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的<code>runner</code>保存至<code>runners</code>字典中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">srn_runners = &#123;&#125;<br><br>lengths = [<span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = train(length)<br>    srn_runners[length] = runner<br><br></code></pre></td></tr></table></figure>







<h6 id="√-6-1-3-3-损失曲线展示"><a href="#√-6-1-3-3-损失曲线展示" class="headerlink" title="[√] 6.1.3.3 损失曲线展示"></a>[√] 6.1.3.3 损失曲线展示</h6><hr>
<p>定义<code>plot_training_loss</code>函数，分别画出各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，实现代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_training_loss</span>(<span class="hljs-params">runner, fig_name, sample_step</span>):<br><br>    plt.figure()<br>    train_items = runner.train_step_losses[::sample_step]<br>    train_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    train_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> train_items]<br>    plt.plot(train_steps, train_losses, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>, label=<span class="hljs-string">&quot;Train loss&quot;</span>)<br>    <br>    dev_steps=[x[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>    dev_losses = [x[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> runner.dev_losses]<br>    plt.plot(dev_steps, dev_losses, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, linestyle=<span class="hljs-string">&#x27;--&#x27;</span>, label=<span class="hljs-string">&quot;Dev loss&quot;</span>)<br><br>    <span class="hljs-comment">#绘制坐标轴和图例</span><br>    plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>    plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>    plt.savefig(fig_name)<br>    plt.show()<br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 画出训练过程中的损失图</span><br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = srn_runners[length]<br>    fig_name = <span class="hljs-string">f&quot;./images/6.6_<span class="hljs-subst">&#123;length&#125;</span>.pdf&quot;</span><br>    plot_training_loss(runner, fig_name, sample_step=<span class="hljs-number">100</span>)<br><br></code></pre></td></tr></table></figure>

<p>图6.6展示了在6个数据集上的损失变化情况，数据集的长度分别为10、15、20、25、30和35. 从输出结果看，随着数据序列长度的增加，虽然训练集损失逐渐逼近于0，但是验证集损失整体趋向越来越大，这表明当序列变长时，SRN模型保持序列长期依赖能力在逐渐变弱，越来越无法学习到有用的知识.</p>
<blockquote>
<p>alec：</p>
<p>随着序列的变长，SRN模型越来越不能学习到有用的知识。</p>
</blockquote>
<h4 id="√-6-1-4-模型评价"><a href="#√-6-1-4-模型评价" class="headerlink" title="[√] 6.1.4 模型评价"></a>[√] 6.1.4 模型评价</h4><hr>
<p>在模型评价时，加载不同长度的效果最好的模型，然后使用测试集对该模型进行评价，观察模型在测试集上预测的准确度. 同时记录一下不同长度模型在训练过程中，在验证集上最好的效果。代码实现如下。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">srn_dev_scores = []<br>srn_test_scores = []<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    runner = srn_runners[length]<br>    <span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>    model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_srn_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.load_model(model_path)<br>    <br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    test_set = DigitSumDataset(test_examples)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br><br>    <span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>    score, _ = runner.evaluate(test_loader)<br>    srn_test_scores.append(score)<br>    srn_dev_scores.append(<span class="hljs-built_in">max</span>(runner.dev_scores))<br><br><span class="hljs-keyword">for</span> length, dev_score, test_score <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(lengths, srn_dev_scores, srn_test_scores):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, dev_score: <span class="hljs-subst">&#123;dev_score&#125;</span>, test_score: <span class="hljs-subst">&#123;test_score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>接下来，将SRN在不同长度的验证集和测试集数据上的表现，绘制成图片进行观察。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>plt.plot(lengths, srn_dev_scores, <span class="hljs-string">&#x27;-o&#x27;</span>, color=<span class="hljs-string">&#x27;#8E004D&#x27;</span>,  label=<span class="hljs-string">&quot;Dev Accuracy&quot;</span>)<br>plt.plot(lengths, srn_test_scores,<span class="hljs-string">&#x27;-o&#x27;</span>, color=<span class="hljs-string">&#x27;#E20079&#x27;</span>, label=<span class="hljs-string">&quot;Test Accuracy&quot;</span>)<br><br><span class="hljs-comment">#绘制坐标轴和图例</span><br>plt.ylabel(<span class="hljs-string">&quot;loss&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br>plt.legend(loc=<span class="hljs-string">&#x27;upper right&#x27;</span>, fontsize=<span class="hljs-string">&#x27;x-large&#x27;</span>)<br><br>fig_name = <span class="hljs-string">&quot;./images/6.7.pdf&quot;</span><br>plt.savefig(fig_name)<br>plt.show()<br></code></pre></td></tr></table></figure>

<p>图6.7 展示了SRN模型在不同长度数据训练出来的最好模型在验证集和测试集上的表现。可以看到，随着序列长度的增加，验证集和测试集的准确度整体趋势是降低的，这同样说明SRN模型保持长期依赖的能力在不断降低.</p>
<p><img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221220110618357.png" srcset="/img/loading.gif" lazyload alt="image-20221220155549904"></p>
<h2 id="√-6-2-梯度爆炸实验"><a href="#√-6-2-梯度爆炸实验" class="headerlink" title="[√] 6.2 梯度爆炸实验"></a>[√] 6.2 梯度爆炸实验</h2><hr>
<p>造成简单循环网络较难建模长程依赖问题的原因有两个：梯度爆炸和梯度消失。一般来讲，循环网络的梯度爆炸问题比较容易解决，一般通过权重衰减或梯度截断可以较好地来避免；对于梯度消失问题，更加有效的方式是改变模型，比如通过长短期记忆网络LSTM来进行缓解。</p>
<p>本节将首先进行复现简单循环网络中的梯度爆炸问题，然后尝试使用梯度截断的方式进行解决。这里采用长度为20的数据集进行实验，训练过程中将进行输出$W$,$U$,$b$的梯度向量的范数，以此来衡量梯度的变化情况。</p>
<blockquote>
<p>alec：</p>
<ul>
<li>梯度爆炸问题解决：<ul>
<li>通过权重衰减或者梯度截断解决</li>
</ul>
</li>
<li>梯度消失问题解决：<ul>
<li>有效的方式是改变模型，比如使用LSTM网络</li>
</ul>
</li>
</ul>
</blockquote>
<h4 id="√-6-2-1-梯度打印函数"><a href="#√-6-2-1-梯度打印函数" class="headerlink" title="[√] 6.2.1 梯度打印函数"></a>[√] 6.2.1 梯度打印函数</h4><hr>
<p>使用<code>custom_print_log</code>实现了在训练过程中打印梯度的功能，<code>custom_print_log</code>需要接收runner的实例，并通过<code>model.named_parameters()</code>获取该模型中的参数名和参数值. 这里我们分别定义<code>W_list</code>, <code>U_list</code>和<code>b_list</code>，用于分别存储训练过程中参数$W, U 和 b$的梯度范数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">W_list = []<br>U_list = []<br>b_list = []<br><span class="hljs-comment"># 计算梯度范数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">custom_print_log</span>(<span class="hljs-params">runner</span>):<br>    model = runner.model<br>    W_grad_l2, U_grad_l2, b_grad_l2 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters(): <br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.W&quot;</span>:  <br>            W_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.U&quot;</span>: <br>            U_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">if</span> name == <span class="hljs-string">&quot;rnn_model.b&quot;</span>: <br>            b_grad_l2 = paddle.norm(param.grad, p=<span class="hljs-number">2</span>).numpy()[<span class="hljs-number">0</span>]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[Training] W_grad_l2: <span class="hljs-subst">&#123;W_grad_l2:<span class="hljs-number">.5</span>f&#125;</span>, U_grad_l2: <span class="hljs-subst">&#123;U_grad_l2:<span class="hljs-number">.5</span>f&#125;</span>, b_grad_l2: <span class="hljs-subst">&#123;b_grad_l2:<span class="hljs-number">.5</span>f&#125;</span> &quot;</span>) <br>    W_list.append(W_grad_l2)<br>    U_list.append(U_grad_l2)<br>    b_list.append(b_grad_l2)<br><br>    <br></code></pre></td></tr></table></figure>



<h4 id="√-6-2-2-复现梯度爆炸现象"><a href="#√-6-2-2-复现梯度爆炸现象" class="headerlink" title="[√] 6.2.2 复现梯度爆炸现象"></a>[√] 6.2.2 复现梯度爆炸现象</h4><hr>
<p>为了更好地复现梯度爆炸问题，使用SGD优化器将批大小和学习率调大，学习率为0.2，同时在计算交叉熵损失时，将reduction设置为sum，表示将损失进行累加。 代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">50</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.2</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">64</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><br><span class="hljs-comment"># 可以设置不同的length进行不同长度数据的预测实验</span><br>length = <span class="hljs-number">20</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training SRN with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 加载长度为length的数据</span><br>data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>train_examples, dev_examples, test_examples = load_data(data_path)<br>train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples),DigitSumDataset(test_examples)<br>train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br><span class="hljs-comment"># 实例化模型</span><br>base_model = SRN(input_size, hidden_size)<br>model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br><span class="hljs-comment"># 指定优化器</span><br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters()) <br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&quot;sum&quot;</span>)<br><br><span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br><span class="hljs-comment"># 进行模型训练</span><br>model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">1</span>, <br>             save_path=model_save_path, custom_print_log=custom_print_log)<br><br><br><br></code></pre></td></tr></table></figure>

<p>接下来，可以获取训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_grad</span>(<span class="hljs-params">W_list, U_list, b_list, save_path, keep_steps=<span class="hljs-number">40</span></span>):<br><br>    <span class="hljs-comment"># 开始绘制图片</span><br>    plt.figure()<br>    <span class="hljs-comment"># 默认保留前40步的结果</span><br>    steps = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(keep_steps))<br>    plt.plot(steps, W_list[:keep_steps], <span class="hljs-string">&quot;r-&quot;</span>, color=<span class="hljs-string">&quot;#8E004D&quot;</span>, label=<span class="hljs-string">&quot;W_grad_l2&quot;</span>)<br>    plt.plot(steps, U_list[:keep_steps], <span class="hljs-string">&quot;-.&quot;</span>, color=<span class="hljs-string">&quot;#E20079&quot;</span>, label=<span class="hljs-string">&quot;U_grad_l2&quot;</span>)<br>    plt.plot(steps, b_list[:keep_steps], <span class="hljs-string">&quot;--&quot;</span>, color=<span class="hljs-string">&quot;#3D3D3F&quot;</span>, label=<span class="hljs-string">&quot;b_grad_l2&quot;</span>)<br>    <br>    plt.xlabel(<span class="hljs-string">&quot;step&quot;</span>)<br>    plt.ylabel(<span class="hljs-string">&quot;L2 Norm&quot;</span>)<br>    plt.legend(loc=<span class="hljs-string">&quot;upper right&quot;</span>)<br>    plt.savefig(save_path)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;image has been saved to: &quot;</span>, save_path)<br><br>save_path =  <span class="hljs-string">f&quot;./images/6.8.pdf&quot;</span><br>plot_grad(W_list, U_list, b_list, save_path)<br></code></pre></td></tr></table></figure>

<p>图6.8 展示了在训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，可以看到经过学习率等方式的调整，梯度范数急剧变大，而后梯度范数几乎为0. 这是因为$\text{Tanh}$为$\text{Sigmoid}$型函数，其饱和区的导数接近于0，由于梯度的急剧变化，参数数值变的较大或较小，容易落入梯度饱和区，导致梯度为0，模型很难继续训练.</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/8942af416fdc461cb0115956598b32a4d83f147aee0d4ec8a9ebff43e65e85bf" srcset="/img/loading.gif" lazyload width=50%></center>
<center>图6.8 梯度变化图</center>

<p>接下来，使用该模型在测试集上进行测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.load_model(model_path)<br><br><span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>score, _ = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, Score: <span class="hljs-subst">&#123;score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<h4 id="√-6-2-3-使用梯度截断解决梯度爆炸问题"><a href="#√-6-2-3-使用梯度截断解决梯度爆炸问题" class="headerlink" title="[√] 6.2.3 使用梯度截断解决梯度爆炸问题"></a>[√] 6.2.3 使用梯度截断解决梯度爆炸问题</h4><hr>
<p>梯度截断是一种可以有效解决梯度爆炸问题的启发式方法，当梯度的模大于一定阈值时，就将它截断成为一个较小的数。一般有两种截断方式：按值截断和按模截断．本实验使用按模截断的方式解决梯度爆炸问题。</p>
<p>按模截断是按照梯度向量$\boldsymbol{g}$的模进行截断，保证梯度向量的模值不大于阈值$b$，裁剪后的梯度为:</p>
<p>$$<br>\boldsymbol{g} &#x3D; \left{\begin{matrix} \boldsymbol{g},  &amp;  ||\boldsymbol{g}||\leq b \ \frac{b}{||\boldsymbol{g}||} * \boldsymbol{g},   &amp;  ||\boldsymbol{g}||\gt b \end{matrix} \right..<br>$$</p>
<p>当梯度向量$\boldsymbol{g}$的模不大于阈值$b$时，$\boldsymbol{g}$数值不变，否则对$\boldsymbol{g}$进行数值缩放。</p>
<blockquote>
<p>在飞桨中，可以使用<a target="_blank" rel="noopener" href="https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/ClipGradByNorm_cn.html#clipgradbynorm">paddle.nn.ClipGradByNorm</a>进行按模截断. 在代码实现时，将ClipGradByNorm传入优化器，优化器在反向迭代过程中，每次梯度更新时默认可以对所有梯度裁剪。</p>
</blockquote>
<p>在引入梯度截断之后，将重新观察模型的训练情况。这里我们重新实例化一下：模型和优化器，然后组装runner，进行训练。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 清空梯度列表</span><br>W_list.clear()<br>U_list.clear()<br>b_list.clear()<br><span class="hljs-comment"># 实例化模型</span><br>base_model = SRN(input_size, hidden_size)<br>model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br><br><span class="hljs-comment"># 定义clip，并实例化优化器</span><br><span class="hljs-comment"># alec：按模进行梯度截断，将梯度截断的工具传给优化器使用，在优化的过程中发现梯度超过了阈值就进行梯度截断</span><br>clip = nn.ClipGradByNorm(clip_norm=<span class="hljs-number">5.0</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(clip))<br>optimizer = paddle.optimizer.SGD(learning_rate=lr, parameters=model.parameters(), grad_clip=clip)<br><span class="hljs-comment"># 定义评价指标</span><br>metric = Accuracy()<br><span class="hljs-comment"># 定义损失函数</span><br>loss_fn = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&quot;sum&quot;</span>)<br><br><span class="hljs-comment"># 实例化Runner</span><br>runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br><span class="hljs-comment"># 训练模型</span><br>model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_fix_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">1</span>, save_path=model_save_path, custom_print_log=custom_print_log)<br><br></code></pre></td></tr></table></figure>

<p>在引入梯度截断后，获取训练过程中关于$\boldsymbol{W}$，$\boldsymbol{U}$和$\boldsymbol{b}$参数梯度的L2范数，并将其绘制为图片以便展示，相应代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">save_path =  <span class="hljs-string">f&quot;./images/6.9.pdf&quot;</span><br>plot_grad(W_list, U_list, b_list, save_path, keep_steps=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure>

<p><strong>图6.9</strong> 展示了引入按模截断的策略之后，模型训练时参数梯度的变化情况。可以看到，随着迭代步骤的进行，梯度始终保持在一个有值的状态，表明按模截断能够很好地解决梯度爆炸的问题.</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/c80fb991231c417dad312c6c1396fdbd0db944566c2c46f29c4772387bcee278" srcset="/img/loading.gif" lazyload width=40%></center>
<center>图6.9 增加梯度截断策略后，SRN参数梯度L2范数变化趋势</center>

<blockquote>
<p>alec：</p>
<p>使用了梯度截断策略之后，模型的梯度没有爆炸</p>
</blockquote>
<p>接下来，使用梯度截断策略的模型在测试集上进行测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Evaluate SRN with data length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br><br><span class="hljs-comment"># 加载训练过程中效果最好的模型</span><br>model_path = os.path.join(save_dir, <span class="hljs-string">f&quot;srn_fix_explosion_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>runner.load_model(model_path)<br><br><span class="hljs-comment"># 使用测试集评价模型，获取测试集上的预测准确率</span><br>score, _ = runner.evaluate(test_loader)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;[SRN] length:<span class="hljs-subst">&#123;length&#125;</span>, Score: <span class="hljs-subst">&#123;score: <span class="hljs-number">.5</span>f&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>

<p>由于为复现梯度爆炸现象，改变了学习率，优化器等，因此准确率相对比较低。但由于采用梯度截断策略后，在后续训练过程中，模型参数能够被更新优化，因此准确率有一定的提升。</p>
<h2 id="6-3-LSTM的记忆能力实验"><a href="#6-3-LSTM的记忆能力实验" class="headerlink" title="[] 6.3 LSTM的记忆能力实验"></a>[] 6.3 LSTM的记忆能力实验</h2><hr>
<p>长短期记忆网络（Long Short-Term Memory Network，LSTM）是一种可以有效缓解长程依赖问题的循环神经网络．LSTM 的特点是引入了一个新的内部状态（Internal State）$c \in \mathbb{R}^D$ 和门控机制（Gating Mechanism）．不同时刻的内部状态以近似线性的方式进行传递，从而缓解梯度消失或梯度爆炸问题．同时门控机制进行信息筛选，可以有效地增加记忆能力．例如，输入门可以让网络忽略无关紧要的输入信息，遗忘门可以使得网络保留有用的历史信息．在上一节的数字求和任务中，如果模型能够记住前两个非零数字，同时忽略掉一些不重要的干扰信息，那么即时序列很长，模型也有效地进行预测.</p>
<p>LSTM 模型在第 $t$ 步时，循环单元的内部结构如图6.10所示．</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/6ddebc52f6494af49d1c77c1548ec4086c9a466869fd4389b3a02df3333e271a" srcset="/img/loading.gif" lazyload width=700></center>
<br><center>图6.10 LSTM网络的循环单元结构</center></br>

<blockquote>
<p><strong>提醒</strong>：为了和代码的实现保存一致性，这里使用形状为 (样本数量 × 序列长度 × 特征维度) 的张量来表示一组样本.</p>
</blockquote>
<p>假设一组输入序列为$\boldsymbol{X}\in \mathbb{R}^{B\times L\times M}$，其中$B$为批大小，$L$为序列长度，$M$为输入特征维度，LSTM从从左到右依次扫描序列，并通过循环单元计算更新每一时刻的状态内部状态$\boldsymbol{C}<em>{t}  \in \mathbb{R}^{B \times D}$和输出状态$\boldsymbol{H}</em>{t}  \in \mathbb{R}^{B \times D}$。</p>
<p>具体计算分为三步：</p>
<p><strong>（1）计算三个“门”</strong></p>
<p>在时刻$t$，LSTM的循环单元将当前时刻的输入$\boldsymbol{X}<em>t \in \mathbb{R}^{B \times M}$与上一时刻的输出状态$\boldsymbol{H}</em>{t-1}  \in \mathbb{R}^{B \times D}$，计算一组输入门$\boldsymbol{I}_t$、遗忘门$\boldsymbol{F}_t$和输出门$\boldsymbol{O}_t$，其计算公式为</p>
<p>$$<br>\boldsymbol{I}_{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>i+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_i+\boldsymbol{b}<em>i) \in \mathbb{R}^{B \times D},\<br>\boldsymbol{F}</em>{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>f+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_f+\boldsymbol{b}<em>f) \in \mathbb{R}^{B \times D},\<br>\boldsymbol{O}</em>{t}&#x3D;\sigma(\boldsymbol{X}_t\boldsymbol{W}<em>o+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_o+\boldsymbol{b}_o) \in \mathbb{R}^{B \times D},<br>$$</p>
<p>其中$\boldsymbol{W}<em>* \in \mathbb{R}^{M \times D},\boldsymbol{U}</em>* \in \mathbb{R}^{D \times D},\boldsymbol{b}_* \in \mathbb{R}^{D}$为可学习的参数，$\sigma$表示Logistic函数，将“门”的取值控制在$(0,1)$区间。这里的“门”都是$B$个样本组成的矩阵，每一行为一个样本的“门”向量。</p>
<p><strong>（2）计算内部状态</strong></p>
<p>首先计算候选内部状态：</p>
<p>$$<br>\tilde{\boldsymbol{C}}_{t}&#x3D;\tanh(\boldsymbol{X}_t\boldsymbol{W}<em>c+\boldsymbol{H}</em>{t-1}\boldsymbol{U}_c+\boldsymbol{b}_c) \in \mathbb{R}^{B \times D},<br>$$</p>
<p>其中$\boldsymbol{W}_c \in \mathbb{R}^{M \times D}, \boldsymbol{U}_c \in \mathbb{R}^{D \times D},\boldsymbol{b}_c \in \mathbb{R}^{D}$为可学习的参数。</p>
<p>使用遗忘门和输入门，计算时刻$t$的内部状态：<br>$$<br>\boldsymbol{C}<em>{t} &#x3D; \boldsymbol{F}<em>t \odot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}</em>{t} \odot \boldsymbol{\tilde{C}}_{t},<br>$$<br>其中$\odot$为逐元素积。</p>
<p><strong>3）计算输出状态</strong><br>当前LSTM单元状态（候选状态）的计算公式为:<br>LSTM单元状态向量$\boldsymbol{C}<em>{t}$和$\boldsymbol{H}<em>t$的计算公式为<br>$$<br>\boldsymbol{C}</em>{t} &#x3D; \boldsymbol F_t \odot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}<em>{t} \odot \boldsymbol{\tilde{C}}</em>{t}，\<br>\boldsymbol{H}<em>{t} &#x3D; \boldsymbol{O}</em>{t} \odot \text{tanh}(\boldsymbol{C}_{t}).<br>$$</p>
<p>LSTM循环单元结构的输入是$t-1$时刻内部状态向量$\boldsymbol{C}<em>{t-1} \in \mathbb{R}^{B \times D}$和隐状态向量$\boldsymbol{H}</em>{t-1} \in \mathbb{R}^{B \times D}$，输出是当前时刻$t$的状态向量$\boldsymbol{C}<em>{t} \in \mathbb{R}^{B \times D}$和隐状态向量$\boldsymbol{H}</em>{t} \in \mathbb{R}^{B \times D}$。通过LSTM循环单元，整个网络可以建立较长距离的时序依赖关系。</p>
<p>通过学习这些门的设置，LSTM可以选择性地忽略或者强化当前的记忆或是输入信息，帮助网络更好地学习长句子的语义信息。</p>
<p>在本节中，我们使用LSTM模型重新进行数字求和实验，验证LSTM模型的长程依赖能力。</p>
<h4 id="√-6-3-1-模型构建"><a href="#√-6-3-1-模型构建" class="headerlink" title="[√] 6.3.1 模型构建"></a>[√] 6.3.1 模型构建</h4><hr>
<p>在本实验中，我们将使用第6.1.2.4节中定义Model_RNN4SeqClass模型，并构建 LSTM 算子．只需要实例化 LSTM 算，并传入Model_RNN4SeqClass模型，就可以用 LSTM 进行数字求和实验。</p>
<h6 id="√-6-3-1-1-LSTM层"><a href="#√-6-3-1-1-LSTM层" class="headerlink" title="[√] 6.3.1.1 LSTM层"></a>[√] 6.3.1.1 LSTM层</h6><hr>
<p>LSTM层的代码与SRN层结构相似，只是在SRN层的基础上增加了内部状态、输入门、遗忘门和输出门的定义和计算。这里LSTM层的输出也依然为序列的最后一个位置的隐状态向量。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-comment"># 声明LSTM和相关参数</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LSTM</span>(nn.Layer):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_size, hidden_size, Wi_attr=<span class="hljs-literal">None</span>, Wf_attr=<span class="hljs-literal">None</span>, Wo_attr=<span class="hljs-literal">None</span>, Wc_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 Ui_attr=<span class="hljs-literal">None</span>, Uf_attr=<span class="hljs-literal">None</span>, Uo_attr=<span class="hljs-literal">None</span>, Uc_attr=<span class="hljs-literal">None</span>, bi_attr=<span class="hljs-literal">None</span>, bf_attr=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 bo_attr=<span class="hljs-literal">None</span>, bc_attr=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(LSTM, self).__init__()<br>        self.input_size = input_size<br>        self.hidden_size = hidden_size<br><br>        <span class="hljs-comment"># 初始化模型参数</span><br>        self.W_i = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wi_attr)<br>        self.W_f = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wf_attr)<br>        self.W_o = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wo_attr)<br>        self.W_c = paddle.create_parameter(shape=[input_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Wc_attr)<br>        self.U_i = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Ui_attr)<br>        self.U_f = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uf_attr)<br>        self.U_o = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uo_attr)<br>        self.U_c = paddle.create_parameter(shape=[hidden_size, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=Uc_attr)<br>        self.b_i = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bi_attr)<br>        self.b_f = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bf_attr)<br>        self.b_o = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bo_attr)<br>        self.b_c = paddle.create_parameter(shape=[<span class="hljs-number">1</span>, hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>, attr=bc_attr)<br>    <br>    <span class="hljs-comment"># 初始化状态向量和隐状态向量</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_state</span>(<span class="hljs-params">self, batch_size</span>):<br>        hidden_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        cell_state = paddle.zeros(shape=[batch_size, self.hidden_size], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>        <span class="hljs-keyword">return</span> hidden_state, cell_state<br><br>    <span class="hljs-comment"># 定义前向计算</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, inputs, states=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-comment"># inputs: 输入数据，其shape为batch_size x seq_len x input_size</span><br>        batch_size, seq_len, input_size = inputs.shape <br>        <br>        <span class="hljs-comment"># 初始化起始的单元状态和隐状态向量，其shape为batch_size x hidden_size</span><br>        <span class="hljs-keyword">if</span> states <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            states = self.init_state(batch_size)<br>        hidden_state, cell_state = states<br><br>        <span class="hljs-comment"># 执行LSTM计算，包括：输入门、遗忘门和输出门、候选内部状态、内部状态和隐状态向量</span><br>        <span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(seq_len):<br>            <span class="hljs-comment"># 获取当前时刻的输入数据step_input: 其shape为batch_size x input_size</span><br>            step_input = inputs[:, step, :]<br>            <span class="hljs-comment"># 计算输入门, 遗忘门和输出门, 其shape为：batch_size x hidden_size</span><br>            I_gate = F.sigmoid(paddle.matmul(step_input, self.W_i) + paddle.matmul(hidden_state, self.U_i) + self.b_i)<br>            F_gate = F.sigmoid(paddle.matmul(step_input, self.W_f) + paddle.matmul(hidden_state, self.U_f) + self.b_f)<br>            O_gate = F.sigmoid(paddle.matmul(step_input, self.W_o) + paddle.matmul(hidden_state, self.U_o) + self.b_o)<br>            <span class="hljs-comment"># 计算候选状态向量, 其shape为：batch_size x hidden_size</span><br>            C_tilde = F.tanh(paddle.matmul(step_input, self.W_c) + paddle.matmul(hidden_state, self.U_c) + self.b_c)<br>            <span class="hljs-comment"># 计算单元状态向量, 其shape为：batch_size x hidden_size</span><br>            <span class="hljs-comment"># alec：单元状态向量是由候选状态向量和上一次的单元状态向量得到的</span><br>            cell_state = F_gate * cell_state + I_gate * C_tilde<br>            <span class="hljs-comment"># 计算隐状态向量，其shape为：batch_size x hidden_size</span><br>            <span class="hljs-comment"># 隐状态向量是由候选状态向量非线性变换得到的</span><br>            hidden_state = O_gate * F.tanh(cell_state)<br><br>        <span class="hljs-keyword">return</span> hidden_state<br>        <br><br><br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.2</span>]]))<br>Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.1</span>], [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.0</span>]]))<br>bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br>bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.1</span>]]))<br><br>lstm = LSTM(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,<br>                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,<br>                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)<br><br>inputs = paddle.to_tensor([[[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]]], dtype=<span class="hljs-string">&quot;float32&quot;</span>)<br>hidden_state = lstm(inputs)<br><span class="hljs-built_in">print</span>(hidden_state)<br></code></pre></td></tr></table></figure>

<p>飞桨框架已经内置了LSTM的API <code>paddle.nn.LSTM</code>，其与自己实现的SRN不同点在于其实现时采用了两个偏置，同时矩阵相乘时参数在输入数据前面，如下公式所示：。</p>
<p>$$<br>\boldsymbol{I}<em>{t}&#x3D;\sigma(\boldsymbol{W}</em>{ii}\boldsymbol{X}<em>t + \boldsymbol{b}</em>{ii} + \boldsymbol{U}<em>{hi}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}<em>{hi}) \<br>\boldsymbol{F}</em>{t}&#x3D;\sigma(\boldsymbol{W}<em>{if}\boldsymbol{X}<em>t + \boldsymbol{b}</em>{if}+ \boldsymbol{U}</em>{hf}\boldsymbol{H}<em>{t-1}+\boldsymbol{b}</em>{hf}) \<br>\boldsymbol{O}<em>{t}&#x3D;\sigma(\boldsymbol{W}</em>{io}\boldsymbol{X}<em>t+ \boldsymbol{b}</em>{io} +\boldsymbol{U}<em>{ho}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}_{ho}),<br>$$</p>
<p>$$<br>\tilde{\boldsymbol{C}}<em>{t}&#x3D;\tanh(\boldsymbol{W}</em>{ic}\boldsymbol{X}<em>t+\boldsymbol{b}</em>{ic}+\boldsymbol{U}<em>{hc}\boldsymbol{H}</em>{t-1}+\boldsymbol{b}_{hc}) ,<br>$$</p>
<p>$$<br>\boldsymbol{C}<em>{t} &#x3D; \boldsymbol F_t \cdot \boldsymbol{C}</em>{t-1} + \boldsymbol{I}<em>{t} \cdot \boldsymbol{\tilde{C}}</em>{t}，\<br>\boldsymbol{H}<em>{t} &#x3D; \boldsymbol{O}</em>{t} \cdot \text{tanh}(\boldsymbol{C}_{t}).<br>$$</p>
<p>其中$\boldsymbol{W}<em>* \in \mathbb{R}^{M \times D}, \boldsymbol{U}</em>* \in \mathbb{R}^{D \times D}, \boldsymbol{b}<em>{i*} \in \mathbb{R}^{1 \times D}, \boldsymbol{b}</em>{h*} \in \mathbb{R}^{1 \times D}$是可学习参数。</p>
<p>另外，在Paddle内置LSTM实现时，对于参数$\boldsymbol{W}<em>{ii}, \boldsymbol{W}</em>{if}, \boldsymbol{W}<em>{io}, \boldsymbol{W}</em>{ic}$ ，并不是分别申请这些矩阵，而是申请了一个大的矩阵$\boldsymbol{W}<em>{ih}$，将这个大的矩阵分割为4份，便可以得到$\boldsymbol{W}</em>{ii}, \boldsymbol{W}<em>{if},\boldsymbol{W}</em>{ic},\boldsymbol{W}<em>{io}$。 同理，将会得到$\boldsymbol{W}</em>{hh}$, $\boldsymbol{b}<em>{ih}$和$\boldsymbol{b}</em>{hh}$.</p>
<p>最后，Paddle内置LSTM API将会返回参数序列向量outputs和最后时刻的状态向量，其中序列向量outputs是指最后一层SRN的输出向量，其shape为[batch_size, seq_len, num_directions * hidden_size]；最后时刻的状态向量是个元组，其包含了两个向量，分别是隐状态向量和单元状态向量，其shape均为[num_layers * num_directions, batch_size, hidden_size]。</p>
<p>这里我们可以将自己实现的SRN和Paddle框架内置的SRN返回的结果进行打印展示，实现代码如下。</p>
<blockquote>
<p>alec：</p>
<p>候选状态向量是核心组成，遗忘门控×前一个单元状态向量 + 输入门控×候选状态向量 &#x3D; 单元状态向量</p>
<p>输出门控 × 激活函数（单元状态向量） &#x3D; 隐状态向量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>hidden_size = <span class="hljs-number">32</span><br>paddle_lstm = nn.LSTM(input_size, hidden_size)<br>self_lstm = LSTM(input_size, hidden_size)<br><br>self_hidden_state = self_lstm(inputs)<br>paddle_outputs, (paddle_hidden_state, paddle_cell_state) = paddle_lstm(inputs)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self_lstm hidden_state: &quot;</span>, self_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm outpus:&quot;</span>, paddle_outputs.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm hidden_state:&quot;</span>, paddle_hidden_state.shape)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle_lstm cell_state:&quot;</span>, paddle_cell_state.shape)<br><br></code></pre></td></tr></table></figure>

<p>可以看到，自己实现的LSTM由于没有考虑多层因素，因此没有层次这个维度，因此其输出shape为[8, 32]。同时由于在以上代码使用Paddle内置API实例化LSTM时，默认定义的是1层的单向SRN，因此其shape为[1, 8, 32]，同时隐状态向量为[8,20, 32].</p>
<p>接下来，我们可以将自己实现的LSTM与Paddle内置的LSTM在输出值的精度上进行对比，这里首先根据Paddle内置的LSTM实例化模型（为了进行对比，在实例化时只保留一个偏置，将偏置$b_{ih}$设置为0），然后提取该模型对应的参数，进行参数分割后，使用相应参数去初始化自己实现的LSTM，从而保证两者在参数初始化时是一致的。</p>
<p>在进行实验时，首先定义输入数据<code>inputs</code>，然后将该数据分别传入Paddle内置的LSTM与自己实现的LSTM模型中，最后通过对比两者的隐状态输出向量。代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> paddle<br>paddle.seed(<span class="hljs-number">0</span>)<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size, hidden_size = <span class="hljs-number">2</span>, <span class="hljs-number">5</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>bih_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(paddle.zeros([<span class="hljs-number">4</span>*hidden_size, ])))<br>paddle_lstm = nn.LSTM(input_size, hidden_size, bias_ih_attr=bih_attr)<br><br><span class="hljs-comment"># 获取paddle_lstm中的参数，并设置相应的paramAttr,用于初始化lstm</span><br><span class="hljs-built_in">print</span>(paddle_lstm.weight_ih_l0.T.shape)<br>chunked_W = paddle.split(paddle_lstm.weight_ih_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br>chunked_U = paddle.split(paddle_lstm.weight_hh_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br>chunked_b = paddle.split(paddle_lstm.bias_hh_l0.T, num_or_sections=<span class="hljs-number">4</span>, axis=-<span class="hljs-number">1</span>)<br><br>Wi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">0</span>]))<br>Wf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">1</span>]))<br>Wc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">2</span>]))<br>Wo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_W[<span class="hljs-number">3</span>]))<br>Ui_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">0</span>]))<br>Uf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">1</span>]))<br>Uc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">2</span>]))<br>Uo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_U[<span class="hljs-number">3</span>]))<br>bi_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">0</span>]))<br>bf_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">1</span>]))<br>bc_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">2</span>]))<br>bo_attr = paddle.ParamAttr(initializer=nn.initializer.Assign(chunked_b[<span class="hljs-number">3</span>]))<br>self_lstm = LSTM(input_size, hidden_size, Wi_attr=Wi_attr, Wf_attr=Wf_attr, Wo_attr=Wo_attr, Wc_attr=Wc_attr,<br>                 Ui_attr=Ui_attr, Uf_attr=Uf_attr, Uo_attr=Uo_attr, Uc_attr=Uc_attr,<br>                 bi_attr=bi_attr, bf_attr=bf_attr, bo_attr=bo_attr, bc_attr=bc_attr)<br><br><span class="hljs-comment"># 进行前向计算，获取隐状态向量，并打印展示</span><br>self_hidden_state = self_lstm(inputs)<br>paddle_outputs, (paddle_hidden_state, _) = paddle_lstm(inputs)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;paddle SRN:\n&quot;</span>, paddle_hidden_state.numpy().squeeze(<span class="hljs-number">0</span>))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self SRN:\n&quot;</span>, self_hidden_state.numpy())<br></code></pre></td></tr></table></figure>

<p>可以看到，两者的输出基本是一致的。另外，还可以进行对比两者在运算速度方面的差异。代码实现如下：</p>
<blockquote>
<p>alec：</p>
<p>paddle_lstm &#x3D; nn.LSTM(input_size, hidden_size)</p>
<p><code>nn.name()</code>的方式，也是在实例化一个类</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> time<br><br><span class="hljs-comment"># 这里创建一个随机数组作为测试数据，数据shape为batch_size x seq_len x input_size</span><br>batch_size, seq_len, input_size = <span class="hljs-number">8</span>, <span class="hljs-number">20</span>, <span class="hljs-number">32</span><br>inputs = paddle.randn(shape=[batch_size, seq_len, input_size])<br><br><span class="hljs-comment"># 设置模型的hidden_size</span><br>hidden_size = <span class="hljs-number">32</span><br>self_lstm = LSTM(input_size, hidden_size)<br>paddle_lstm = nn.LSTM(input_size, hidden_size)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(paddle_lstm))<br><br><span class="hljs-comment"># 计算自己实现的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    hidden_state = self_lstm(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;self_lstm speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br><br><span class="hljs-comment"># 计算Paddle内置的SRN运算速度</span><br>model_time = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    strat_time = time.time()<br>    outputs, (hidden_state, cell_state) = paddle_lstm(inputs)<br>    <span class="hljs-comment"># 预热10次运算，不计入最终速度统计</span><br>    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">10</span>:<br>        <span class="hljs-keyword">continue</span><br>    end_time = time.time()<br>    model_time += (end_time - strat_time)<br>avg_model_time = model_time / <span class="hljs-number">90</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;paddle_lstm speed:&#x27;</span>, avg_model_time, <span class="hljs-string">&#x27;s&#x27;</span>)<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">self_lstm speed: <span class="hljs-number">0.02104825178782145</span> s<br>paddle_lstm speed: <span class="hljs-number">0.001053280300564236</span> s<br></code></pre></td></tr></table></figure>

<p>可以看到，由于Paddle框架的LSTM底层采用了C++实现并进行优化，Paddle框架内置的LSTM运行效率远远高于自己实现的LSTM。</p>
<h6 id="√-6-3-1-2-模型汇总"><a href="#√-6-3-1-2-模型汇总" class="headerlink" title="[√] 6.3.1.2 模型汇总"></a>[√] 6.3.1.2 模型汇总</h6><hr>
<p>在本节实验中，我们将使用6.1.2.4的Model_RNN4SeqClass作为预测模型，不同在于在实例化时将传入实例化的LSTM层。</p>
<blockquote>
<p>动手联系6.2 在我们手动实现的LSTM算子中，是逐步计算每个时刻的隐状态。请思考如何实现更加高效的LSTM算子。</p>
</blockquote>
<h4 id="√-6-3-2-模型训练"><a href="#√-6-3-2-模型训练" class="headerlink" title="[√] 6.3.2 模型训练"></a>[√] 6.3.2 模型训练</h4><hr>
<h6 id="√-6-3-2-1-训练指定长度的数字预测模型"><a href="#√-6-3-2-1-训练指定长度的数字预测模型" class="headerlink" title="[√] 6.3.2.1 训练指定长度的数字预测模型"></a>[√] 6.3.2.1 训练指定长度的数字预测模型</h6><hr>
<p>本节将基于RunnerV3类进行训练，首先定义模型训练的超参数，并保证和简单循环网络的超参数一致. 然后定义一个<code>train</code>函数，其可以通过指定长度的数据集，并进行训练. 在<code>train</code>函数中，首先加载长度为<code>length</code>的数据，然后实例化各项组件并创建对应的Runner，然后训练该Runner。同时在本节将使用4.5.4节定义的准确度（Accuracy）作为评估指标，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> paddle<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> nndl <span class="hljs-keyword">import</span> RunnerV3<br><br><span class="hljs-comment"># 训练轮次</span><br>num_epochs = <span class="hljs-number">500</span><br><span class="hljs-comment"># 学习率</span><br>lr = <span class="hljs-number">0.001</span><br><span class="hljs-comment"># 输入数字的类别数</span><br>num_digits = <span class="hljs-number">10</span><br><span class="hljs-comment"># 将数字映射为向量的维度</span><br>input_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 隐状态向量的维度</span><br>hidden_size = <span class="hljs-number">32</span><br><span class="hljs-comment"># 预测数字的类别数</span><br>num_classes = <span class="hljs-number">19</span><br><span class="hljs-comment"># 批大小 </span><br>batch_size = <span class="hljs-number">8</span><br><span class="hljs-comment"># 模型保存目录</span><br>save_dir = <span class="hljs-string">&quot;./checkpoints&quot;</span><br><br><span class="hljs-comment"># 可以设置不同的length进行不同长度数据的预测实验</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">length</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;\n====&gt; Training LSTM with data of length <span class="hljs-subst">&#123;length&#125;</span>.&quot;</span>)<br>    np.random.seed(<span class="hljs-number">0</span>)<br>    random.seed(<span class="hljs-number">0</span>)<br>    paddle.seed(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 加载长度为length的数据</span><br>    data_path = <span class="hljs-string">f&quot;./datasets/<span class="hljs-subst">&#123;length&#125;</span>&quot;</span><br>    train_examples, dev_examples, test_examples = load_data(data_path)<br>    train_set, dev_set, test_set = DigitSumDataset(train_examples), DigitSumDataset(dev_examples), DigitSumDataset(test_examples)<br>    train_loader = paddle.io.DataLoader(train_set, batch_size=batch_size)<br>    dev_loader = paddle.io.DataLoader(dev_set, batch_size=batch_size)<br>    test_loader = paddle.io.DataLoader(test_set, batch_size=batch_size)<br>    <span class="hljs-comment"># 实例化模型</span><br>    base_model = LSTM(input_size, hidden_size)<br>    model = Model_RNN4SeqClass(base_model, num_digits, input_size, hidden_size, num_classes) <br>    <span class="hljs-comment"># 指定优化器</span><br>    optimizer = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())<br>    <span class="hljs-comment"># 定义评价指标</span><br>    metric = Accuracy()<br>    <span class="hljs-comment"># 定义损失函数</span><br>    loss_fn = paddle.nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># 基于以上组件，实例化Runner</span><br>    runner = RunnerV3(model, optimizer, loss_fn, metric)<br><br>    <span class="hljs-comment"># 进行模型训练</span><br>    model_save_path = os.path.join(save_dir, <span class="hljs-string">f&quot;best_lstm_model_<span class="hljs-subst">&#123;length&#125;</span>.pdparams&quot;</span>)<br>    runner.train(train_loader, dev_loader, num_epochs=num_epochs, eval_steps=<span class="hljs-number">100</span>, log_steps=<span class="hljs-number">100</span>, save_path=model_save_path)<br><br>    <span class="hljs-keyword">return</span> runner<br></code></pre></td></tr></table></figure>

<h6 id="√-6-3-2-2-多组训练"><a href="#√-6-3-2-2-多组训练" class="headerlink" title="[√] 6.3.2.2 多组训练"></a>[√] 6.3.2.2 多组训练</h6><hr>
<p>接下来，分别进行数据长度为10, 15, 20, 25, 30, 35的数字预测模型训练实验，训练后的<code>runner</code>保存至<code>runners</code>字典中。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lstm_runners = &#123;&#125;<br><br>lengths = [<span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30</span>, <span class="hljs-number">35</span>]<br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = train(length)<br>    lstm_runners[length] = runner<br></code></pre></td></tr></table></figure>

<p>[] 6.3.2.3 损失曲线展示</p>
<hr>
<p>分别画出基于LSTM的各个长度的数字预测模型训练过程中，在训练集和验证集上的损失曲线，代码实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 画出训练过程中的损失图</span><br><span class="hljs-keyword">for</span> length <span class="hljs-keyword">in</span> lengths:<br>    runner = lstm_runners[length]<br>    fig_name = <span class="hljs-string">f&quot;./images/6.11_<span class="hljs-subst">&#123;length&#125;</span>.pdf&quot;</span><br>    plot_training_loss(runner, fig_name, sample_step=<span class="hljs-number">100</span>)<br></code></pre></td></tr></table></figure>

<p>图6.11展示了LSTM模型在不同长度数据集上进行训练后的损失变化，同SRN模型一样，随着序列长度的增加，训练集上的损失逐渐不稳定，验证集上的损失整体趋向于变大，这说明当序列长度增加时，保持长期依赖的能力同样在逐渐变弱. 同图6.5相比，LSTM模型在序列长度增加时，收敛情况比SRN模型更好。</p>
<center><img src="https://ai-studio-static-online.cdn.bcebos.com/f319b75922c74c0c822cb0ab6383a22ca2a8d473ffa9472589e4db90cb70a062" srcset="/img/loading.gif" lazyload width=100%></center>
<center>图6.11 LSTM在不同长度数据集训练损失变化图</center></br>










                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/" class="category-chain-item">深度学习技术栈</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="category-chain-item">深度学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E8%B7%B5%E5%AD%A6%E4%B9%A0/" class="category-chain-item">实践学习</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%8A%80%E6%9C%AF%E6%A0%88/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%AE%9E%E8%B7%B5%E5%AD%A6%E4%B9%A0/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%EF%BC%9A%E6%A1%88%E4%BE%8B%E4%B8%8E%E5%AE%9E%E8%B7%B5-%E9%A3%9E%E6%A1%A8-%E9%82%B1%E9%94%A1%E9%B9%8F/" class="category-chain-item">神经网络与深度学习：案例与实践 - 飞桨 - 邱锡鹏</a>
  
  

  

  

  

      </span>
    
  
</span>

    </div>
  
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>第6章 - 循环神经网络 - 书籍</div>
      <div>https://alec-97.github.io/posts/1133786115/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Shuai Zhao</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2022年12月19日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/1254558254/" title="第5章 - 卷积神经网络 - 书籍">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">第5章 - 卷积神经网络 - 书籍</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/1748706165/" title="6 - 循环神经网络 - 实践：基于双向LSTM模型完成文本分类任务">
                        <span class="hidden-mobile">6 - 循环神经网络 - 实践：基于双向LSTM模型完成文本分类任务</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'alec-97/alec-97.github.io');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'Comment');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
      <div class="col-lg-7 mx-auto nopadding-x-md">
        <div class="container custom mx-auto">
           <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css"> <script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/meting@1.2/dist/Meting.min.js"></script> <div id="player" class="aplayer aplayer-withlist aplayer-fixed" data-id="7729098320" data-server="netease" data-type="playlist" data-lrctype="-1" data-preload="auto" data-autoplay="true" data-order="random" data-fixed="true" data-listfolded="true" data-theme="#2D8CF0"></div> 
        </div>
      </div>
    
  </main>

  <footer>
    <div class="footer-inner" style="font-size: 0.85rem">
  <div class="alec_diy_footer">
  <!-- color:#d9dbdc -->
    
      <div class="footer-content">
         <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Fluid</span></a> <i class="iconfont icon-love"></i> <a href="https://https://alec-97.github.io/" target="_blank" rel="nofollow noopener"><span style="color: #d9dbdc;">Alec</span></a>
<div style="font-size: 0.85rem"> <span id="timeDate">载入天数...</span> <span id="times">载入时分秒...</span> <script src="/vvd_js/duration.js"></script> </div>

      </div>
    

    
      <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        总访问量 
        <span id="busuanzi_value_site_pv"></span>
         次
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        总访客数 
        <span id="busuanzi_value_site_uv"></span>
         人
      </span>
    
    
  
</div>

    

    
      <div class="footer-content">
        <a target="_blank" rel="noopener" href="https://developer.hitokoto.cn/" id="hitokoto_text"><span style="color: #d9dbdc;"  id="hitokoto"></span></a> <script src="https://v1.hitokoto.cn/?encode=js&select=%23hitokoto" defer></script> 
      </div>
    

    

    

  </div>  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="/alec_diy/mouse_click/firework.js"></script>
<script src="/alec_diy/live2d-widget/autoload.js"></script>
<script src="//cdn.jsdelivr.net/npm/jquery/dist/jquery.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>




</body>
</html>
