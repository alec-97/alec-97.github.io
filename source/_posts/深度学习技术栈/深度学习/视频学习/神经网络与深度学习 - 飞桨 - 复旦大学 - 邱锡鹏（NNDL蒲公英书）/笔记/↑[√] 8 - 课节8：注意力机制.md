---
title: 8 - 注意力机制
categories:
  - 深度学习技术栈
  - 深度学习
  - 视频学习
  - 神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）
  - 笔记
abbrlink: 1405476561
date: 2022-12-29 20:04:56
---

## [√] 8.0 - 注意力机制与外部记忆

---

> alec：
>
> - 注意力机制和外部记忆是两个部件，部件的意思是能够和其它的网络架构相融合。
> - transformer是RNN中序列到序列的模型的一个实现



#### [√] 内容

---

![image-20221229201847723](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115298.png)

> alec：
>
> - 注意力机制能够筛选重要的信息，并且大幅的降低计算量
> - 自注意力模型是基于注意力机制的一个非常流行的模型
> - transformer是自注意力模型的一个具体的实现

> alec：
>
> - 记忆增强网络是给神经网络增加外部记忆，使得可以增强网络的记忆能力
> - 外部记忆的实现有两种方式：
>     - 基于机构化的外部记忆
>     - 基于神经动力学的联想记忆

> alec：
>
> - 注意力机制和记忆增强网络是相辅相成的
> - NN想要从外部记忆中选择和当前相关的记忆，就需要注意力机制
> - 很多的场景中，外部的信息也可以看做外部的记忆

#### [√] 网络能力

---

![image-20221229202205910](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115299.png)

> alec：
>
> - FNN的缺点是网络密集连接，使得在少量数据上非常难学，很容易过拟合
> - CNN相对FNN，更优
> - RNN通过引入内部的记忆，使得网络能够处理时序相关的数据
> - 扩展到图网络，进一步提升网络能力
>
> ---
>
> 增加网络能力的另一种思路：
> - 注意力机制
> - 外部记忆

#### [√] 例子：阅读理解

---

![image-20221229203812011](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115300.png)

![image-20221229203938882](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115301.png)

> alec：
>
> - 上图这个段落很长，全部编码不现实，严重的长程依赖问题。因此这个时候就需要找一些问题相关的句子，这里就的机制就类似于信息筛选/注意力机制。

## [√] 8.1 - 注意力机制

---

#### [√] 大脑中的信息超载问题

---

![image-20221229204511063](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115302.png)

#### [√] 注意力示例

---

![image-20221229204542398](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115303.png)

#### [√] 注意力实验

---

![image-20221229204826433](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115304.png)

#### [√] 两种注意力

---

![image-20221229205254176](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115305.png)

> alec：
>
> - 看报纸的注意力是自下而上的，不是主动的，而是被动的，这些注意力自带显著的特征，不需要主动的聚焦，就能通过神经系统得到关注
>     - 比如max pooling，会自动的筛选出最大的去注意
> - 注意力实验中的注意力是带着问题的注意力，是自上而下的，会主动的去关注一些信息

![image-20221229205437945](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115306.png)

#### [√] 如何实现？

---

> alec：
>
> - 如何实现`自上而下`的注意力机制呢？

![image-20221229205521632](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115307.png)

> alec：
>
> - 上述例子给了我们启示：我们应该以一个较高的权重关注我们关心的内容，但是不关心的内容，也不能完全不关注，而是以一个较低的权重来关注。

## [√] 8.2 - 人工神经网络中的注意力机制

---

#### [√] 问题

---

![image-20221229211251718](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115308.png)

> alec：
>
> - 上图右侧，如何在x_n中找到一个和q相关的向量呢？
>     - 方法是拿着q一一的去和x_n作比较。然后把相关的x_n筛选出来。
>     - 这就是注意力机制的数学描述
>     - 基本有两种注意力方式：
>         - 硬注意力：只把最相关的找出来
>         - 软注意力：用每个任务向量给每个x打分，打出每个x和任务的相关度是多少

#### [√] 软性注意力机制(soft attention mechanism)

---

![image-20221229211852541](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115309.png)

> alec：
>
> - 先对所有的输入进行概率打分，然后计算信息的加权平均得到注意力

#### [√] 注意力打分函数s(x_n, q)

---

> alec：
>
> - 打分函数计算x_n和q之间的相关度
> - 如何设计打分函数跟任务相关

![image-20221229212628237](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115310.png)

> alec：
>
> - 上述的模型在效果差不多的情况下，考虑计算效率。实际中可能使用`缩放点积模型`可能会比较多

#### [√] 注意力机制的变体

---

###### [√] 硬性注意力 && 键值对注意力

---



![image-20221230104804214](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115311.png)

> alec：
>
> - 软性注意力是连续的，连续才可导才能够学习；硬性注意力机制是0-1，没有梯度无法学习。硬性注意力机制通常和强化学习来结合。通常主流的注意力不采用硬性注意力机制。
> - 软性注意力中，x出现在两个地方：
>     - 第一个地方：用q和x计算相似度α
>     - 第二个地方：注意力机制计算出的注意力α，用在对x进行加权汇总的时候，用α进行加权汇总。
> - 键值对注意力：
>     - 把输入信息分为键和值
>     - 先使用q和key运算，然后通过softmax，得到相似度α
>     - 然后使用相似度α和val进行加权汇总，得到最终选出来的注意力

###### [√] 多头注意力 && 结构化注意力

---

![image-20221230105724789](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115312.png)

> alec：
>
> - 一个注意力可以看做用一个`查询`去输入信息中选一组信息，多头注意力可以看成用多个查询去输入信息中选择多组信息

###### [√] 指针网络

---

![image-20221230110935283](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115313.png)

## [√] 8.3 - 注意力机制的应用

---

#### [√] 文本分类

---

![image-20221230112313387](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115314.png)

![image-20221230112435511](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115315.png)

> alec：
>
> - 用不同的任务查询向量，关注点就不一样

![image-20221230112728435](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115316.png)

#### [√] 机器翻译

---

![image-20221230113106433](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115317.png)

![image-20221230113352039](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115318.png)

![image-20221230113518088](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115319.png)

> alec：
>
> - 注意力机制对机器翻译的帮助是非常大的

#### [√] Image Caption（看图说话）

---

![image-20221230113805460](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115320.png)

![image-20221230113836296](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115321.png)

## [√] 8.4 - 自注意力模型

---

> alec：
>
> - 自注意力模型是一种应用十分广泛的注意力机制的模型

#### [√] 变长序列的建模

---

![image-20221230114421849](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115322.png)

> alec：
>
> - 由于长程依赖问题，双向RNN只能建模局部的依赖关系
> - 如何建模比较长的非局部的依赖关系呢？

#### [√] 自注意力模型

---

![image-20221230114821001](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115323.png)

#### [√] 自注意力示例

---

![image-20221230120206753](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115324.png)

> alec：
>
> - 自注意力：每个词自身作为查询去计算注意力，自己attention自己

#### [√] 自注意力模型的矩阵表示

---

![image-20221230120957464](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115325.png)

#### [√] QKV模式(Query-Key-Value）

---

![image-20221230121516272](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115326.png)

> alec：
>
> - 输入信息自身分成三份变成查询Q、K、V。查询Q和K计算得到权重矩阵α，然后α和V计算得到注意力
>
> - 相比之下，QKV模式更多的优点是，引入了三个参数矩阵W，这样QKV模式就变得可学习了，更加灵活
> - QKV模式现在已经变成了非常常用的自注意力模式，默认的自注意力基本上就是使用这种模式了

#### [√] 自注意力模型

---

![image-20221230121803966](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115327.png)

#### [√] 多头(multi-head)自注意力模型

---

> alec：
>
> - 在QKV的基础上，通过多头注意力机制，进一步使得自注意力机制的功能更加的强大

![image-20221230122118438](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115328.png)

## [√] 8.5 - Transformer

---

> alec：
>
> - transformer是自注意力机制一个成功的模型

#### [√] Transformer Encoder

---

![image-20221230141350929](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115329.png)

> alec：
>
> - 自注意力的权重只和内容相关，交换位置、权重不变。但是在序列中，位置信息也是非常重要的。

#### [√] Transformer

---

![image-20221230141547805](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115330.png)

> alec：
>
> - 相比于序列一步一步的建立i和j之间的关系，transformer全连接的连接i和j，效率更高。

#### [√] 复杂度分析

---



![image-20221230142212628](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115331.png)

#### [√] Transformer

---



![image-20221230142611023](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115332.png)

> alec：
>
> - 用transformer之前，通常会先进行预训练。transformer在小数据集上非常容易过拟合。

## [√] 8.6 - 外部记忆

---

![image-20221230143426355](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115333.png)

#### [√] 记忆网络

---



![image-20221230143900925](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115334.png)

## [√] 8.7 - 结构化的外部记忆

---

#### [√] 结构化的外部记忆

---

![image-20221230144229237](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115335.png)

#### [√] 记忆网络

---

![image-20221230152320762](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115336.png)

## [√] 8.8 - 基于神经动力学的联想记忆

---

#### [√] 联想记忆

---

![image-20221230184110727](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115337.png)

#### [√] 如何实现联想记忆

---

![image-20221230184235875](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115338.png)

#### [√] 神经网络如何学习？

---

![image-20221230184351413](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115339.png)

> alec：
>
> - 赫布法则：两个神经元AB，AB之间间接的兴奋等导致二者连接加强了，那么时间久了之后AB就能直接导致另一方兴奋了。即无意的刺激多了，神经元的联系就加强了。这种无意的刺激就变成了有意的刺激。

#### [√] Hopfield网络

---

![image-20221230185033392](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115340.png)

> alec：
>
> - Hopfield网络是赫布法则对应的人工神经网络的实现。
> - 这个网络也可以看做是一个全连接的网络

#### [√] Hopfield网络的更新过程

---

![image-20221230185448024](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115341.png)

#### [√] 能量函数

---

![image-20221230185730725](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115342.png)

#### [√] 检索过程（联想记忆）

---

![image-20221230190103538](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115343.png)

#### [√] 存储过程（学习过程）

---

![image-20221230190334919](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115344.png)

#### [√] 使用联想记忆增加网络容量

---

![image-20221230190441448](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115345.png)

## [√] 8.9 - 总结

---

#### [√] 通用近似定理

---

![image-20221230190659744](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115346.png)

#### [√] 总结

---

![image-20221230190742881](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212311115347.png)

