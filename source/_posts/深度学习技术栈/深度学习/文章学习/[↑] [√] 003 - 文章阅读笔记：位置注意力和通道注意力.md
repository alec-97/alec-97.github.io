---
title: 003 - 文章阅读笔记：位置注意力和通道注意力
index_img: >-
  https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202301271615223.png
tags:
  - 注意力机制
  - 深度学习
categories:
  - 深度学习技术栈
  - 深度学习
  - 文章学习
abbrlink: 2714358425
date: 2023-01-20 23:47:55
---

> 参考：位置注意力和通道注意力机制 - CSDN - shu_0233 - 深度学习

## [√] Position Attention Module

---

捕获特征图的任意两个位置之间的空间依赖，对于某个特定的特征，被所有位置上的特征加权和更新。权重为相应的两个位置之间的特征相似性。**因此，任何两个现有相似特征的位置可以相互贡献提升，而不管它们之间的距离.。**

![img](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202301271615223.png)

> alec：
>
> - 位置注意力计算的是特征图的任意两个位置之间的空间依赖
> - 位置注意力的计算过程和非局部注意力非常像

- 特征图A（C*H*W）首先分别通过3个卷积层（BN和Relu）得到三个特征图{B,C,D},shape为（C*H*W），然后reshape为C*N，其中N=H*W，为像素的数量。
- 矩阵C和B的转置相乘，在通过softmax得到spatial attention map S（N*N）
- 矩阵D和S的转置相乘，reshape result到(CxHxW)再乘以尺度系数 α 再reshape为原来形状（C*H*W），最后与A相加得到最后的输出E

![image-20230120235744112](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202301271615224.png)

## [√] Channel Attention Module

---

> alec：
>
> - 通道注意力CA是计算的两个通道之间的关系

![img](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202301271615225.png)

- 在Channel Attention Module中，分别对A做reshape（C*N）和reshape与transpose(N*C)
- 将得到的两个特征图相乘再通过softmax得到channel attention map X(C×C)
- X与A做乘积再乘以尺度系数β再reshape为原来形状（C*H*W），最后与A相加得到最后的输出E。



![image-20230121000114485](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202301271615226.png)









