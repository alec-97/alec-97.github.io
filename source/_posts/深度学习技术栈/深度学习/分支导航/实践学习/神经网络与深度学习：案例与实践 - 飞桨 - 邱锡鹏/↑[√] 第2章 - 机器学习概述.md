---
title: ç¬¬2ç«  - æœºå™¨å­¦ä¹ æ¦‚è¿°
date: '2022å¹´12æœˆ18æ—¥18:17:59'
categories:
  - æ·±åº¦å­¦ä¹ æŠ€æœ¯æ ˆ
  - æ·±åº¦å­¦ä¹ 
  - åˆ†æ”¯å¯¼èˆª
  - å®è·µå­¦ä¹ 
  - ç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ï¼šæ¡ˆä¾‹ä¸å®è·µ - é£æ¡¨ - é‚±é”¡é¹
abbrlink: 2499540575
---



## [âˆš] ç¬¬äºŒç«  æœºå™¨å­¦ä¹ æ¦‚è¿°

**æœºå™¨å­¦ä¹ **ï¼ˆMachine Learningï¼ŒMLï¼‰å°±æ˜¯è®©è®¡ç®—æœºä»æ•°æ®ä¸­è¿›è¡Œè‡ªåŠ¨å­¦ä¹ ï¼Œå¾—åˆ°æŸç§çŸ¥è¯†ï¼ˆæˆ–è§„å¾‹ï¼‰ã€‚ä½œä¸ºä¸€é—¨å­¦ç§‘ï¼Œæœºå™¨å­¦ä¹ é€šå¸¸æŒ‡ä¸€ç±»é—®é¢˜ä»¥åŠè§£å†³è¿™ç±»é—®é¢˜çš„æ–¹æ³•ï¼Œå³å¦‚ä½•ä»è§‚æµ‹æ•°æ®ï¼ˆæ ·æœ¬ï¼‰ä¸­å¯»æ‰¾è§„å¾‹ï¼Œå¹¶åˆ©ç”¨å­¦ä¹ åˆ°çš„è§„å¾‹ï¼ˆæ¨¡å‹ï¼‰å¯¹æœªçŸ¥æˆ–æ— æ³•è§‚æµ‹çš„æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚

åœ¨å­¦ä¹ æœ¬ç« å†…å®¹å‰ï¼Œå»ºè®®æ‚¨å…ˆé˜…è¯»ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ç¬¬ 2 ç« ï¼šæœºå™¨å­¦ä¹ æ¦‚è¿°çš„ç›¸å…³å†…å®¹ï¼Œå…³é”®çŸ¥è¯†ç‚¹å¦‚**å›¾2.1**æ‰€ç¤ºï¼Œä»¥ä¾¿æ›´å¥½çš„ç†è§£å’ŒæŒæ¡ç›¸åº”çš„ç†è®ºçŸ¥è¯†ï¼ŒåŠå…¶åœ¨å®è·µä¸­çš„åº”ç”¨æ–¹æ³•ã€‚

![image-20221211210937929](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211220940535.png)

æœ¬ç« å†…å®¹åŸºäºã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ç¬¬ 2 ç« ï¼šæœºå™¨å­¦ä¹ æ¦‚è¿° ç›¸å…³å†…å®¹è¿›è¡Œè®¾è®¡ï¼Œä¸»è¦åŒ…å«ä¸¤éƒ¨åˆ†ï¼š

- **æ¨¡å‹è§£è¯»**ï¼šä»‹ç»æœºå™¨å­¦ä¹ å®è·µäº”è¦ç´ ï¼ˆæ•°æ®ã€æ¨¡å‹ã€å­¦ä¹ å‡†åˆ™ã€ä¼˜åŒ–ç®—æ³•ã€è¯„ä¼°æŒ‡æ ‡ï¼‰çš„åŸç†å‰–æå’Œç›¸åº”çš„ä»£ç å®ç°ã€‚é€šè¿‡ç†è®ºå’Œä»£ç çš„ç»“åˆï¼ŒåŠ æ·±å¯¹æœºå™¨å­¦ä¹ çš„ç†è§£ï¼›
- **æ¡ˆä¾‹å®è·µ**ï¼šåŸºäºæœºå™¨å­¦ä¹ çº¿æ€§å›å½’æ–¹æ³•ï¼Œé€šè¿‡æ•°æ®å¤„ç†ã€æ¨¡å‹æ„å»ºã€è®­ç»ƒé…ç½®ã€ç»„è£…è®­ç»ƒæ¡†æ¶Runnerã€æ¨¡å‹è®­ç»ƒå’Œæ¨¡å‹é¢„æµ‹ç­‰è¿‡ç¨‹å®Œæˆæ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹ä»»åŠ¡ã€‚

### [âˆš] 2.1 - æœºå™¨å­¦ä¹ å®è·µäº”è¦ç´ 

è¦é€šè¿‡æœºå™¨å­¦ä¹ æ¥è§£å†³ä¸€ä¸ªç‰¹å®šçš„ä»»åŠ¡æ—¶ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡5ä¸ªæ–¹é¢çš„è¦ç´ ï¼š

1. æ•°æ®é›†ï¼šæ”¶é›†ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ç”¨æ¥è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ï¼Œå¯åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼›
2. æ¨¡å‹ï¼šå®ç°è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ï¼Œé€šå¸¸ä¸ºå¯å­¦ä¹ çš„å‡½æ•°ï¼›
3. å­¦ä¹ å‡†åˆ™ï¼šæ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡ï¼Œé€šå¸¸ä¸ºæŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹çš„åŠ æƒç»„åˆï¼›
4. ä¼˜åŒ–ç®—æ³•ï¼šæ ¹æ®å­¦ä¹ å‡†åˆ™ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„å‚æ•°ï¼›
5. è¯„ä»·æŒ‡æ ‡ï¼šç”¨æ¥è¯„ä»·å­¦ä¹ åˆ°çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼

**å›¾2.2**ç»™å‡ºå®ç°ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸»è¦ç¯èŠ‚å’Œè¦ç´ ã€‚ä»æµç¨‹è§’åº¦çœ‹ï¼Œå®ç°æœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè®­ç»ƒé˜¶æ®µå’Œè¯„ä»·é˜¶æ®µã€‚è®­ç»ƒé˜¶æ®µéœ€è¦ç”¨åˆ°è®­ç»ƒé›†ã€éªŒè¯é›†ã€å¾…å­¦ä¹ çš„æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ï¼Œè¾“å‡ºå­¦ä¹ åˆ°çš„æ¨¡å‹ï¼›è¯„ä»·é˜¶æ®µä¹Ÿç§°ä¸ºæµ‹è¯•é˜¶æ®µï¼Œéœ€è¦ç”¨åˆ°æµ‹è¯•é›†ã€å­¦ä¹ åˆ°çš„æ¨¡å‹ã€è¯„ä»·æŒ‡æ ‡ä½“ç³»ï¼Œå¾—åˆ°æ¨¡å‹çš„æ€§èƒ½è¯„ä»·ã€‚**å›¾2.2**ç»™å‡ºå®ç°ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¸»è¦ç¯èŠ‚å’Œè¦ç´ ã€‚ä»æµç¨‹è§’åº¦çœ‹ï¼Œå®ç°æœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼šè®­ç»ƒé˜¶æ®µå’Œè¯„ä»·é˜¶æ®µã€‚è®­ç»ƒé˜¶æ®µéœ€è¦ç”¨åˆ°è®­ç»ƒé›†ã€éªŒè¯é›†ã€å¾…å­¦ä¹ çš„æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ï¼Œè¾“å‡ºå­¦ä¹ åˆ°çš„æ¨¡å‹ï¼›è¯„ä»·é˜¶æ®µä¹Ÿç§°ä¸ºæµ‹è¯•é˜¶æ®µï¼Œéœ€è¦ç”¨åˆ°æµ‹è¯•é›†ã€å­¦ä¹ åˆ°çš„æ¨¡å‹ã€è¯„ä»·æŒ‡æ ‡ä½“ç³»ï¼Œå¾—åˆ°æ¨¡å‹çš„æ€§èƒ½è¯„ä»·ã€‚

![image-20221211213007230](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211210937929.png)

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ†åˆ«å¯¹è¿™äº”ä¸ªè¦ç´ è¿›è¡Œç®€å•çš„ä»‹ç»ã€‚

------

ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ç¬¬ 2.2 èŠ‚è¯¦ç»†ä»‹ç»äº†æœºå™¨å­¦ä¹ çš„ä¸‰ä¸ªåŸºæœ¬è¦ç´ ï¼šâ€œæ¨¡å‹â€ã€â€œå­¦ä¹ å‡†åˆ™â€å’Œâ€œä¼˜åŒ–ç®—æ³•â€ï¼åœ¨æœºå™¨å­¦ä¹ å®è·µä¸­ï¼Œâ€œæ•°æ®â€å’Œâ€œè¯„ä»·æŒ‡æ ‡â€ä¹Ÿéå¸¸é‡è¦ï¼å› æ­¤ï¼Œæœ¬ä¹¦å°†æœºå™¨å­¦ä¹ åœ¨å®è·µä¸­çš„ä¸»è¦å…ƒç´ å½’ç»“ä¸ºäº”è¦ç´ ï¼

#### [âˆš] 2.1.1 - æ•°æ®

åœ¨å®è·µä¸­ï¼Œæ•°æ®çš„è´¨é‡ä¼šå¾ˆå¤§ç¨‹åº¦ä¸Šå½±å“æ¨¡å‹æœ€ç»ˆçš„æ€§èƒ½ï¼Œé€šå¸¸æ•°æ®é¢„å¤„ç†æ˜¯å®Œæˆæœºå™¨å­¦ä¹ å®è·µçš„ç¬¬ä¸€æ­¥ï¼Œå™ªéŸ³è¶Šå°‘ã€è§„æ¨¡è¶Šå¤§ã€è¦†ç›–èŒƒå›´è¶Šå¹¿çš„æ•°æ®é›†å¾€å¾€èƒ½å¤Ÿè®­ç»ƒå‡ºæ€§èƒ½æ›´å¥½çš„æ¨¡å‹ã€‚æ•°æ®é¢„å¤„ç†å¯åˆ†ä¸ºä¸¤ä¸ªç¯èŠ‚ï¼šå…ˆå¯¹æ”¶é›†åˆ°çš„æ•°æ®è¿›è¡ŒåŸºæœ¬çš„é¢„å¤„ç†ï¼Œå¦‚åŸºæœ¬çš„ç»Ÿè®¡ã€ç‰¹å¾å½’ä¸€åŒ–å’Œå¼‚å¸¸å€¼å¤„ç†ç­‰ï¼›å†å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ï¼ˆå¼€å‘é›†ï¼‰å’Œæµ‹è¯•é›†ã€‚

- **è®­ç»ƒé›†**ï¼šç”¨äºæ¨¡å‹è®­ç»ƒæ—¶è°ƒæ•´æ¨¡å‹çš„å‚æ•°ï¼Œåœ¨è¿™ä»½æ•°æ®é›†ä¸Šçš„è¯¯å·®è¢«ç§°ä¸ºè®­ç»ƒè¯¯å·®ï¼›
- **éªŒè¯é›†ï¼ˆå¼€å‘é›†ï¼‰**ï¼šå¯¹äºå¤æ‚çš„æ¨¡å‹ï¼Œå¸¸å¸¸æœ‰ä¸€äº›è¶…å‚æ•°éœ€è¦è°ƒèŠ‚ï¼Œå› æ­¤éœ€è¦å°è¯•å¤šç§è¶…å‚æ•°çš„ç»„åˆæ¥åˆ†åˆ«è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼Œç„¶åå¯¹æ¯”å®ƒä»¬åœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ï¼Œé€‰æ‹©ä¸€ç»„ç›¸å¯¹æœ€å¥½çš„è¶…å‚æ•°ï¼Œæœ€åæ‰ä½¿ç”¨è¿™ç»„å‚æ•°ä¸‹è®­ç»ƒçš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æµ‹è¯•è¯¯å·®ã€‚
- **æµ‹è¯•é›†**ï¼šæ¨¡å‹åœ¨è¿™ä»½æ•°æ®é›†ä¸Šçš„è¯¯å·®è¢«ç§°ä¸ºæµ‹è¯•è¯¯å·®ã€‚è®­ç»ƒæ¨¡å‹çš„ç›®çš„æ˜¯ä¸ºäº†é€šè¿‡ä»è®­ç»ƒæ•°æ®ä¸­æ‰¾åˆ°è§„å¾‹æ¥é¢„æµ‹æœªçŸ¥æ•°æ®ï¼Œå› æ­¤æµ‹è¯•è¯¯å·®æ˜¯æ›´èƒ½åæ˜ å‡ºæ¨¡å‹è¡¨ç°çš„æŒ‡æ ‡ã€‚

æ•°æ®åˆ’åˆ†æ—¶è¦è€ƒè™‘åˆ°ä¸¤ä¸ªå› ç´ ï¼šæ›´å¤šçš„è®­ç»ƒæ•°æ®ä¼šé™ä½å‚æ•°ä¼°è®¡çš„æ–¹å·®ï¼Œä»è€Œå¾—åˆ°æ›´å¯ä¿¡çš„æ¨¡å‹ï¼›è€Œæ›´å¤šçš„æµ‹è¯•æ•°æ®ä¼šé™ä½æµ‹è¯•è¯¯å·®çš„æ–¹å·®ï¼Œä»è€Œå¾—åˆ°æ›´å¯ä¿¡çš„æµ‹è¯•è¯¯å·®ã€‚å¦‚æœç»™å®šçš„æ•°æ®é›†æ²¡æœ‰åšä»»ä½•åˆ’åˆ†ï¼Œæˆ‘ä»¬ä¸€èˆ¬å¯ä»¥å¤§è‡´æŒ‰ç…§7:3æˆ–è€…8:2çš„æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå†æ ¹æ®7:3æˆ–è€…8:2çš„æ¯”ä¾‹ä»è®­ç»ƒé›†ä¸­å†æ¬¡åˆ’åˆ†å‡ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚

éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œæµ‹è¯•é›†åªèƒ½ç”¨æ¥è¯„æµ‹æ¨¡å‹æœ€ç»ˆçš„æ€§èƒ½ï¼Œåœ¨æ•´ä¸ªæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¸èƒ½æœ‰æµ‹è¯•é›†çš„å‚ä¸ã€‚

---

#### [âˆš] 2.1.2 æ¨¡å‹

æœ‰äº†æ•°æ®åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ•°æ®æ¥è®­ç»ƒæ¨¡å‹ã€‚æˆ‘ä»¬å¸Œæœ›èƒ½è®©è®¡ç®—æœºä»ä¸€ä¸ªå‡½æ•°é›†åˆ $\mathcal{F} = \{f_1(\boldsymbol{x}), f_2(\boldsymbol{x}), \cdots \}$ä¸­
è‡ªåŠ¨å¯»æ‰¾ä¸€ä¸ªâ€œæœ€ä¼˜â€çš„å‡½æ•°$f^âˆ—(\boldsymbol{x})$ æ¥è¿‘ä¼¼æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾å‘é‡ $\boldsymbol{x}$ å’Œæ ‡ç­¾ $y$ ä¹‹é—´
çš„çœŸå®æ˜ å°„å…³ç³»ï¼Œå®é™…ä¸Šè¿™ä¸ªå‡½æ•°é›†åˆä¹Ÿè¢«ç§°ä¸º**å‡è®¾ç©ºé—´**ï¼Œåœ¨å®é™…é—®é¢˜ä¸­ï¼Œå‡è®¾ç©ºé—´$\mathcal{F}$é€šå¸¸ä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„å‡½æ•°æ—
$$
\mathcal{F}=\left\{f(\boldsymbol{x} ; \theta) \mid \theta \in \mathbb{R}^{D}\right\}, (2.1)
$$
å…¶ä¸­$f(\boldsymbol{x} ; \theta)$æ˜¯å‚æ•°ä¸º$\theta$çš„å‡½æ•°ï¼Œä¹Ÿç§°ä¸ºæ¨¡å‹ï¼Œğ· ä¸ºå‚æ•°çš„æ•°é‡ã€‚


å¸¸è§çš„å‡è®¾ç©ºé—´å¯ä»¥åˆ†ä¸ºçº¿æ€§å’Œéçº¿æ€§ä¸¤ç§ï¼Œå¯¹åº”çš„æ¨¡å‹ $f$ ä¹Ÿåˆ†åˆ«ç§°ä¸ºçº¿æ€§æ¨¡å‹å’Œéçº¿æ€§æ¨¡å‹ã€‚**çº¿æ€§æ¨¡å‹**çš„å‡è®¾ç©ºé—´ä¸ºä¸€ä¸ªå‚æ•°åŒ–çš„çº¿æ€§å‡½æ•°æ—ï¼Œå³ï¼š
$$
f(\boldsymbol{x} ; \theta)=\boldsymbol{w}^{\top} \boldsymbol{x}+b, (2.2)
$$
å…¶ä¸­å‚æ•°$\theta$ åŒ…å«äº†æƒé‡å‘é‡$\boldsymbol{w}$å’Œåç½®$b$ã€‚

çº¿æ€§æ¨¡å‹å¯ä»¥ç”±**éçº¿æ€§åŸºå‡½æ•°**$\phi(\boldsymbol{x})$å˜ä¸º**éçº¿æ€§æ¨¡å‹**ï¼Œä»è€Œå¢å¼ºæ¨¡å‹èƒ½åŠ›:

$$
f(\boldsymbol{x} ; \theta)=\boldsymbol{w}^{\top} \phi(\boldsymbol{x})+b, (2.3)
$$
å…¶ä¸­$\phi(\boldsymbol{x})=\left[\phi_{1}(\boldsymbol{x}), \phi_{2}(\boldsymbol{x}), \cdots, \phi_{K}(\boldsymbol{x})\right]^{\top}$ä¸ºğ¾ ä¸ªéçº¿æ€§åŸºå‡½æ•°ç»„æˆçš„å‘é‡ï¼Œå‚æ•° $\theta$ åŒ…å«äº†æƒé‡å‘é‡$\boldsymbol{w}$å’Œåç½®$b$ã€‚







---

#### [âˆš] 2.1.3 å­¦ä¹ å‡†åˆ™

ä¸ºäº†è¡¡é‡ä¸€ä¸ªæ¨¡å‹çš„å¥½åï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªæŸå¤±å‡½æ•°$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$ã€‚æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°å‡½æ•°ï¼Œç”¨æ¥é‡åŒ–æ¨¡å‹é¢„æµ‹æ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚å¸¸è§çš„æŸå¤±å‡½æ•°æœ‰ 0-1 æŸå¤±ã€å¹³æ–¹æŸå¤±å‡½æ•°ã€äº¤å‰ç†µæŸå¤±å‡½æ•°ç­‰ã€‚

æœºå™¨å­¦ä¹ çš„ç›®æ ‡å°±æ˜¯æ‰¾åˆ°æœ€ä¼˜çš„æ¨¡å‹$ğ‘“(ğ’™;\theta^âˆ—)$åœ¨çœŸå®æ•°æ®åˆ†å¸ƒä¸ŠæŸå¤±å‡½æ•°çš„æœŸæœ›æœ€å°ã€‚ç„¶è€Œåœ¨å®é™…ä¸­ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—çœŸå®æ•°æ®åˆ†å¸ƒï¼Œé€šå¸¸ä¼šç”¨åœ¨è®­ç»ƒé›†ä¸Šçš„å¹³å‡æŸå¤±æ›¿ä»£ã€‚

ä¸€ä¸ªæ¨¡å‹åœ¨è®­ç»ƒé›†$\mathcal{D}=\{(\boldsymbol{x}^{(n)},y^{(n)})\}_{n=1}^N$ä¸Šçš„å¹³å‡æŸå¤±ç§°ä¸º**ç»éªŒé£é™©**{Empirical Risk}ï¼Œå³:

$$
\mathcal{R}^{emp}_\mathcal{D}(\theta)=\frac{1}{N}\sum_{n=1}^{N}\mathcal{L}(y^{(n)},f({x}^{(n)};\theta))ã€‚ (2.4)
$$

$\mathcal{L}(\boldsymbol{y},f(\boldsymbol{x};\theta))$ä¸ºæŸå¤±å‡½æ•°ã€‚æŸå¤±å‡½æ•°æ˜¯ä¸€ä¸ªéè´Ÿå®æ•°å‡½æ•°ï¼Œç”¨æ¥é‡åŒ–æ¨¡å‹é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾ä¹‹é—´çš„å·®å¼‚ã€‚å¸¸è§çš„æŸå¤±å‡½æ•°æœ‰0-1æŸå¤±ã€å¹³æ–¹æŸå¤±å‡½æ•°ã€äº¤å‰ç†µæŸå¤±å‡½æ•°ç­‰ã€‚

åœ¨é€šå¸¸æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿å¾—**ç»éªŒé£é™©æœ€å°åŒ–**æ¥è·å¾—å…·æœ‰é¢„æµ‹èƒ½åŠ›çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œå½“æ¨¡å‹æ¯”è¾ƒå¤æ‚æˆ–è®­ç»ƒæ•°æ®é‡æ¯”è¾ƒå°‘æ—¶ï¼Œç»éªŒé£é™©æœ€å°åŒ–è·å¾—çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœæ¯”è¾ƒå·®ã€‚è€Œæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½æ‰æ˜¯æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„æŒ‡æ ‡ï¼å½“ä¸€ä¸ªæ¨¡å‹åœ¨è®­ç»ƒé›†é”™è¯¯ç‡å¾ˆä½ï¼Œè€Œåœ¨æµ‹è¯•é›†ä¸Šé”™è¯¯ç‡è¾ƒé«˜æ—¶ï¼Œé€šå¸¸æ„å‘³ç€å‘ç”Ÿäº†**è¿‡æ‹Ÿåˆ**ï¼ˆOverfittingï¼‰ç°è±¡ã€‚ä¸ºäº†ç¼“è§£æ¨¡å‹çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨ç»éªŒæŸå¤±ä¸ŠåŠ ä¸Šä¸€å®šçš„æ­£åˆ™åŒ–é¡¹æ¥é™åˆ¶æ¨¡å‹èƒ½åŠ›ã€‚

è¿‡æ‹Ÿåˆé€šå¸¸æ˜¯ç”±äºæ¨¡å‹å¤æ‚åº¦æ¯”è¾ƒé«˜å¼•èµ·çš„ã€‚åœ¨å®è·µä¸­ï¼Œæœ€å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹å¼æœ‰å¯¹æ¨¡å‹çš„å‚æ•°è¿›è¡Œçº¦æŸï¼Œæ¯”å¦‚$\ell_1$æˆ–è€…$\ell_2$èŒƒæ•°çº¦æŸã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ç»“æ„é£é™©ï¼ˆStructure Riskï¼‰ã€‚
$$
\mathcal{R}^{struct}_{\mathcal{D}}(\theta)=\mathcal{R}^{emp}_{\mathcal{D}}(\theta)+\lambda \ell_p(\theta), (2.5)
$$

å…¶ä¸­$\lambda$ä¸ºæ­£åˆ™åŒ–ç³»æ•°ï¼Œ$p=1$æˆ–$2$è¡¨ç¤º$\ell_1$æˆ–è€…$\ell_2$èŒƒæ•°ã€‚

---

#### [âˆš] 2.1.4 ä¼˜åŒ–ç®—æ³•

åœ¨æœ‰äº†ä¼˜åŒ–ç›®æ ‡ä¹‹åï¼Œæœºå™¨å­¦ä¹ é—®é¢˜å°±è½¬åŒ–ä¸ºä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å·²çŸ¥çš„ä¼˜åŒ–ç®—æ³•æ¥å­¦ä¹ æœ€ä¼˜çš„å‚æ•°ã€‚å½“ä¼˜åŒ–å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä»¤å‚æ•°çš„åå¯¼æ•°ç­‰äº0æ¥è®¡ç®—æœ€ä¼˜å‚æ•°çš„è§£æè§£ã€‚å½“ä¼˜åŒ–å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€é˜¶çš„ä¼˜åŒ–ç®—æ³•æ¥è¿›è¡Œä¼˜åŒ–ã€‚



ç›®å‰æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„ä¼˜åŒ–ç®—æ³•æ˜¯**æ¢¯åº¦ä¸‹é™æ³•**(Gradient Descent Method)ã€‚
å½“ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œå‚æ•°ä¼˜åŒ–æ—¶ï¼Œè¿˜å¯ä»¥åˆ©ç”¨éªŒè¯é›†æ¥**æ—©åœæ³•**(Early-Stop)æ¥ä¸­æ­¢æ¨¡å‹çš„ä¼˜åŒ–è¿‡ç¨‹ï¼Œé¿å…æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¿‡æ‹Ÿåˆã€‚æ—©åœæ³•ä¹Ÿæ˜¯ä¸€ç§å¸¸ç”¨å¹¶ä¸”ååˆ†æœ‰æ•ˆçš„æ­£åˆ™åŒ–æ–¹æ³•ã€‚

---

#### [âˆš] 2.1.5 è¯„ä¼°æŒ‡æ ‡

**è¯„ä¼°æŒ‡æ ‡**(Metric)ç”¨äºè¯„ä»·æ¨¡å‹æ•ˆæœï¼Œå³ç»™å®šä¸€ä¸ªæµ‹è¯•é›†ï¼Œç”¨æ¨¡å‹å¯¹æµ‹è¯•é›†ä¸­çš„æ¯ä¸ªæ ·æœ¬è¿›è¡Œé¢„æµ‹ï¼Œå¹¶æ ¹æ®é¢„æµ‹ç»“æœè®¡ç®—è¯„ä»·åˆ†æ•°ã€‚å›å½’ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡ä¸€èˆ¬æœ‰é¢„æµ‹å€¼ä¸çœŸå®å€¼çš„å‡æ–¹å·®ï¼Œåˆ†ç±»ä»»åŠ¡çš„è¯„ä¼°æŒ‡æ ‡ä¸€èˆ¬æœ‰å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1å€¼ç­‰ã€‚

å¯¹äºä¸€ä¸ªæœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä¸€èˆ¬ä¼šå…ˆç¡®å®šä»»åŠ¡ç±»å‹ï¼Œå†ç¡®å®šä»»åŠ¡çš„è¯„ä»·æŒ‡æ ‡ï¼Œå†æ ¹æ®è¯„ä»·æŒ‡æ ‡æ¥å»ºç«‹æ¨¡å‹ï¼Œé€‰æ‹©å­¦ä¹ å‡†åˆ™ã€‚ç”±äºè¯„ä»·æŒ‡æ ‡ä¸å¯å¾®ç­‰é—®é¢˜æœ‰æ—¶å€™å­¦ä¹ å‡†åˆ™å¹¶ä¸èƒ½å®Œå…¨å’Œè¯„ä»·æŒ‡æ ‡ä¸€è‡´ï¼Œæˆ‘ä»¬å¾€å¾€ä¼šé€‰æ‹©ä¸€å®šçš„æŸå¤±å‡½æ•°ä½¿å¾—ä¸¤è€…å°½å¯èƒ½ä¸€è‡´ã€‚



---

### [âˆš] 2.2 - å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹

**å›å½’ä»»åŠ¡**æ˜¯ä¸€ç±»å…¸å‹çš„ç›‘ç£æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œå¯¹è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å…³ç³»è¿›è¡Œå»ºæ¨¡åˆ†æï¼Œå…¶é¢„æµ‹å€¼é€šå¸¸ä¸ºä¸€ä¸ªè¿ç»­å€¼ï¼Œæ¯”å¦‚æˆ¿å±‹ä»·æ ¼é¢„æµ‹ã€ç”µæºç¥¨æˆ¿é¢„æµ‹ç­‰ã€‚**çº¿æ€§å›å½’**(Linear Regression)æ˜¯æŒ‡ä¸€ç±»åˆ©ç”¨çº¿æ€§å‡½æ•°æ¥å¯¹è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å…³ç³»è¿›è¡Œå»ºæ¨¡çš„å›å½’ä»»åŠ¡ï¼Œæ˜¯æœºå™¨å­¦ä¹ å’Œç»Ÿè®¡å­¦ä¸­æœ€åŸºç¡€å’Œæœ€å¹¿æ³›åº”ç”¨çš„æ¨¡å‹ã€‚

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬åŠ¨æ‰‹å®ç°ä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨æœ€å°äºŒä¹˜æ³•æ¥æ±‚è§£å‚æ•°ï¼Œä»¥å¯¹æœºå™¨å­¦ä¹ ä»»åŠ¡æœ‰æ›´ç›´è§‚çš„è®¤è¯†ã€‚

---

#### [âˆš] 2.2.1 - æ•°æ®é›†æ„å»º

é¦–å…ˆï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ªå°çš„å›å½’æ•°æ®é›†ã€‚å‡è®¾è¾“å…¥ç‰¹å¾å’Œè¾“å‡ºæ ‡ç­¾çš„ç»´åº¦éƒ½ä¸º 1ï¼Œéœ€è¦è¢«æ‹Ÿåˆçš„å‡½æ•°å®šä¹‰ä¸ºï¼š

```python
# çœŸå®å‡½æ•°çš„å‚æ•°ç¼ºçœå€¼ä¸º w=1.2ï¼Œb=0.5
def linear_func(x,w=1.2,b=0.5):
    y = w*x + b
    return y
```

ç„¶åï¼Œä½¿ç”¨`paddle.rand()`å‡½æ•°æ¥è¿›è¡Œéšæœºé‡‡æ ·è¾“å…¥ç‰¹å¾xx*x*ï¼Œå¹¶ä»£å…¥ä¸Šé¢å‡½æ•°å¾—åˆ°è¾“å‡ºæ ‡ç­¾ğ‘¦ğ‘¦*y*ã€‚ä¸ºäº†æ¨¡æ‹ŸçœŸå®ç¯å¢ƒä¸­æ ·æœ¬é€šå¸¸åŒ…å«å™ªå£°çš„é—®é¢˜ï¼Œæˆ‘ä»¬é‡‡æ ·è¿‡ç¨‹ä¸­åŠ å…¥é«˜æ–¯å™ªå£°å’Œå¼‚å¸¸ç‚¹ã€‚

ç”Ÿæˆæ ·æœ¬æ•°æ®çš„å‡½æ•°`create_toy_data`å®ç°å¦‚ä¸‹ï¼š

```python
import paddle

def create_toy_data(func, interval, sample_num, noise = 0.0, add_outlier = False, outlier_ratio = 0.001):
    """
    æ ¹æ®ç»™å®šçš„å‡½æ•°ï¼Œç”Ÿæˆæ ·æœ¬
    è¾“å…¥ï¼š
       - funcï¼šå‡½æ•°
       - intervalï¼š xçš„å–å€¼èŒƒå›´
       - sample_numï¼š æ ·æœ¬æ•°ç›®
       - noiseï¼š å™ªå£°å‡æ–¹å·®
       - add_outlierï¼šæ˜¯å¦ç”Ÿæˆå¼‚å¸¸å€¼
       - outlier_ratioï¼šå¼‚å¸¸å€¼å æ¯”
    è¾“å‡ºï¼š
       - X: ç‰¹å¾æ•°æ®ï¼Œshape=[n_samples,1]
       - y: æ ‡ç­¾æ•°æ®ï¼Œshape=[n_samples,1]
    """

    # å‡åŒ€é‡‡æ ·
    # ä½¿ç”¨paddle.randåœ¨ç”Ÿæˆsample_numä¸ªéšæœºæ•°
    X = paddle.rand(shape = [sample_num]) * (interval[1]-interval[0]) + interval[0]
    y = func(X)

    # ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒçš„æ ‡ç­¾å™ªå£°
    # ä½¿ç”¨paddle.normalç”Ÿæˆ0å‡å€¼ï¼Œnoiseæ ‡å‡†å·®çš„æ•°æ®
    epsilon = paddle.normal(0,noise,paddle.to_tensor(y.shape[0]))
    y = y + epsilon
    if add_outlier:     # ç”Ÿæˆé¢å¤–çš„å¼‚å¸¸ç‚¹
        outlier_num = int(len(y)*outlier_ratio)
        if outlier_num != 0:
            # ä½¿ç”¨paddle.randintç”Ÿæˆæœä»å‡åŒ€åˆ†å¸ƒçš„ã€èŒƒå›´åœ¨[0, len(y))çš„éšæœºTensor
            outlier_idx = paddle.randint(len(y),shape = [outlier_num])
            y[outlier_idx] = y[outlier_idx] * 5
    return X, y
```

åˆ©ç”¨ä¸Šé¢çš„ç”Ÿæˆæ ·æœ¬å‡½æ•°ï¼Œç”Ÿæˆ 150 ä¸ªå¸¦å™ªéŸ³çš„æ ·æœ¬ï¼Œå…¶ä¸­ 100 ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ50 ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå¹¶æ‰“å°å‡ºè®­ç»ƒæ•°æ®çš„å¯è§†åŒ–åˆ†å¸ƒã€‚

```python
from matplotlib import pyplot as plt # matplotlib æ˜¯ Python çš„ç»˜å›¾åº“

func = linear_func
interval = (-10,10)
train_num = 100 # è®­ç»ƒæ ·æœ¬æ•°ç›®
test_num = 50 # æµ‹è¯•æ ·æœ¬æ•°ç›®
noise = 2
X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise = noise, add_outlier = False)
X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise = noise, add_outlier = False)

X_train_large, y_train_large = create_toy_data(func=func, interval=interval, sample_num=5000, noise = noise, add_outlier = False)

# paddle.linspaceè¿”å›ä¸€ä¸ªTensorï¼ŒTensorçš„å€¼ä¸ºåœ¨åŒºé—´startå’Œstopä¸Šå‡åŒ€é—´éš”çš„numä¸ªå€¼ï¼Œè¾“å‡ºTensorçš„é•¿åº¦ä¸ºnum
X_underlying = paddle.linspace(interval[0],interval[1],train_num) 
y_underlying = linear_func(X_underlying)

# ç»˜åˆ¶æ•°æ®
plt.scatter(X_train, y_train, marker='*', facecolor="none", edgecolor='#e4007f', s=50, label="train data")
plt.scatter(X_test, y_test, facecolor="none", edgecolor='#f19ec2', s=50, label="test data")
plt.plot(X_underlying, y_underlying, c='#000000', label=r"underlying distribution")
plt.legend(fontsize='x-large') # ç»™å›¾åƒåŠ å›¾ä¾‹
plt.savefig('ml-vis.pdf') # ä¿å­˜å›¾åƒåˆ°PDFæ–‡ä»¶ä¸­
plt.show()
```

<img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212164136707.png" alt="image-20221211220940535" style="zoom: 50%;" />







---

#### [âˆš] 2.2.2 - æ¨¡å‹æ„å»º

åœ¨çº¿æ€§å›å½’ä¸­ï¼Œè‡ªå˜é‡ä¸ºæ ·æœ¬çš„ç‰¹å¾å‘é‡$\boldsymbol{x}\in \mathbb{R}^D$(æ¯ä¸€ç»´å¯¹åº”ä¸€ä¸ªè‡ªå˜é‡)ï¼Œå› å˜é‡æ˜¯è¿ç»­å€¼çš„æ ‡ç­¾$y\in R$ã€‚

çº¿æ€§æ¨¡å‹å®šä¹‰ä¸ºï¼š
$$
f(\boldsymbol{x};\boldsymbol{w},b)=\boldsymbol{w}^T\boldsymbol{x}+bã€‚ (2.6)
$$

å…¶ä¸­æƒé‡å‘é‡$\boldsymbol{w}\in \mathbb{R}^D$å’Œåç½®$b\in \mathbb{R}$éƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚

****

æ³¨æ„ï¼šã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹ä¸­ä¸ºäº†è¡¨ç¤ºçš„ç®€æ´æ€§ï¼Œä½¿ç”¨**å¢å¹¿æƒé‡å‘é‡**æ¥å®šä¹‰æ¨¡å‹ã€‚è€Œåœ¨æœ¬ä¹¦ä¸­ï¼Œä¸ºäº†å’Œä»£ç å®ç°ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬ä½¿ç”¨éå¢å¹¿å‘é‡çš„å½¢å¼æ¥å®šä¹‰æ¨¡å‹ã€‚

****

åœ¨å®è·µä¸­ï¼Œä¸ºäº†æé«˜é¢„æµ‹æ ·æœ¬çš„æ•ˆç‡ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šå°†$N$æ ·æœ¬å½’ä¸ºä¸€ç»„è¿›è¡Œæˆæ‰¹åœ°é¢„æµ‹ï¼Œè¿™æ ·å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨GPUè®¾å¤‡çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚

$$
\boldsymbol{y} =\boldsymbol{X} \boldsymbol{w} + b, (2.7)
$$

å…¶ä¸­$\boldsymbol{X}\in \mathbb{R}^{N\times D}$ä¸º$N$ä¸ªæ ·æœ¬çš„ç‰¹å¾çŸ©é˜µï¼Œ$\boldsymbol{y}\in \mathbb{R}^N$ä¸º$N$ä¸ªé¢„æµ‹å€¼ç»„æˆçš„åˆ—å‘é‡ã€‚

****

æ³¨æ„ï¼šåœ¨å®è·µä¸­ï¼Œæ ·æœ¬çš„çŸ©é˜µ$\boldsymbol{X}$æ˜¯ç”±$N$ä¸ª$\boldsymbol{x}$çš„**è¡Œå‘é‡**ç»„æˆã€‚è€ŒåŸæ•™æä¸­$\boldsymbol{x}$ä¸ºåˆ—å‘é‡ï¼Œå…¶ç‰¹å¾çŸ©é˜µä¸æœ¬ä¹¦ä¸­çš„ç‰¹å¾çŸ©é˜µåˆšå¥½ä¸ºè½¬ç½®å…³ç³»ã€‚

---

##### [âˆš] 2.2.2.1 - çº¿æ€§ç®—å­

å®ç°å…¬å¼(2.7)ä¸­çš„çº¿æ€§å‡½æ•°éå¸¸ç®€å•ï¼Œæˆ‘ä»¬ç›´æ¥åˆ©ç”¨å¦‚ä¸‹å¼ é‡è¿ç®—æ¥å®ç°ã€‚

```python
# X: tensor, shape=[N,D]
# y_pred: tensor, shape=[N]
# w: shape=[D,1]
# b: shape=[1]
y_pred = paddle.matmul(X,w)+b
```

ä½¿ç”¨é£æ¡¨æ„å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œä»£ç å¦‚ä¸‹ï¼š

**è¯´æ˜**

åœ¨é£æ¡¨æ¡†æ¶ä¸­ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨æ¨¡å‹çš„`forward()`æ–¹æ³•è¿›è¡Œå‰å‘æ‰§è¡Œï¼Œä¹Ÿå¯ä»¥è°ƒç”¨`__call__()`ï¼Œä»è€Œæ‰§è¡Œåœ¨ `forward()` å½“ä¸­å®šä¹‰çš„å‰å‘è®¡ç®—é€»è¾‘ã€‚

åœ¨é£æ¡¨æ¡†æ¶ä¸­ï¼Œæ¨¡å‹ä¸€èˆ¬ç»§æ‰¿[nn.Layer](https://www.paddlepaddle.org.cn/documentation/docs/en/api/paddle/nn/Layer_en.html)ï¼Œåœ¨æˆå‘˜å‡½æ•°`forward()`ä¸­æ‰§è¡Œæ¨¡å‹çš„å‰å‘è¿ç®—ã€‚ç”±äºæœ¬æ¡ˆä¾‹è¾ƒç®€å•ï¼Œæ‰€ä»¥æ²¡æœ‰ç»§æ‰¿`nn.Layer`ï¼Œä½†æ˜¯ä¿ç•™äº†åœ¨`forward()`å‡½æ•°ä¸­æ‰§è¡Œæ¨¡å‹çš„å‰å‘è¿ç®—çš„è¿‡ç¨‹ã€‚

```python
import paddle
from nndl.op import Op

paddle.seed(10) #è®¾ç½®éšæœºç§å­

# çº¿æ€§ç®—å­
class Linear(Op):
    def __init__(self, input_size):
        """
        è¾“å…¥ï¼š
           - input_size:æ¨¡å‹è¦å¤„ç†çš„æ•°æ®ç‰¹å¾å‘é‡é•¿åº¦
        """

        self.input_size = input_size

        # æ¨¡å‹å‚æ•°
        self.params = {}
        self.params['w'] = paddle.randn(shape=[self.input_size,1],dtype='float32') 
        self.params['b'] = paddle.zeros(shape=[1],dtype='float32')

    def __call__(self, X):
        return self.forward(X)

    # å‰å‘å‡½æ•°
    def forward(self, X):
        """
        è¾“å…¥ï¼š
           - X: tensor, shape=[N,D]
           æ³¨æ„è¿™é‡Œçš„XçŸ©é˜µæ˜¯ç”±Nä¸ªxå‘é‡çš„è½¬ç½®æ‹¼æ¥æˆçš„ï¼Œä¸åŸæ•™æè¡Œå‘é‡è¡¨ç¤ºæ–¹å¼ä¸ä¸€è‡´
        è¾“å‡ºï¼š
           - y_predï¼š tensor, shape=[N]
        """

        N,D = X.shape

        if self.input_size==0:
            return paddle.full(shape=[N,1], fill_value=self.params['b'])
        
        assert D==self.input_size # è¾“å…¥æ•°æ®ç»´åº¦åˆæ³•æ€§éªŒè¯

        # ä½¿ç”¨paddle.matmulè®¡ç®—ä¸¤ä¸ªtensorçš„ä¹˜ç§¯
        y_pred = paddle.matmul(X,self.params['w'])+self.params['b']
        
        return y_pred

# æ³¨æ„è¿™é‡Œæˆ‘ä»¬ä¸ºäº†å’Œåé¢ç« èŠ‚ç»Ÿä¸€ï¼Œè¿™é‡Œçš„XçŸ©é˜µæ˜¯ç”±Nä¸ªxå‘é‡çš„è½¬ç½®æ‹¼æ¥æˆçš„ï¼Œä¸åŸæ•™æè¡Œå‘é‡è¡¨ç¤ºæ–¹å¼ä¸ä¸€è‡´
input_size = 3
N = 2
X = paddle.randn(shape=[N, input_size],dtype='float32') # ç”Ÿæˆ2ä¸ªç»´åº¦ä¸º3çš„æ•°æ®
model = Linear(input_size)
y_pred = model(X)
print("y_pred:",y_pred) #è¾“å‡ºç»“æœçš„ä¸ªæ•°ä¹Ÿæ˜¯2ä¸ª
```

```
y_pred: Tensor(shape=[2, 1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[0.54838145],
        [2.03063798]])
```

#### [âˆš] 2.2.3 - æŸå¤±å‡½æ•°

å›å½’ä»»åŠ¡æ˜¯å¯¹**è¿ç»­å€¼**çš„é¢„æµ‹ï¼Œå¸Œæœ›æ¨¡å‹èƒ½æ ¹æ®æ•°æ®çš„ç‰¹å¾è¾“å‡ºä¸€ä¸ªè¿ç»­å€¼ä½œä¸ºé¢„æµ‹å€¼ã€‚å› æ­¤å›å½’ä»»åŠ¡ä¸­å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡æ˜¯**å‡æ–¹è¯¯å·®**ã€‚

ä»¤$\boldsymbol{y}\in \mathbb{R}^N$ï¼Œ$\hat{\boldsymbol{y}}\in \mathbb{R}^N$åˆ†åˆ«ä¸º$N$ä¸ªæ ·æœ¬çš„çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ï¼Œå‡æ–¹è¯¯å·®çš„å®šä¹‰ä¸ºï¼š

$$
\mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})=\frac{1}{2N}\|\boldsymbol{y}-\hat{\boldsymbol{y}}\|^2=\frac{1}{2N}\|\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}\|^2, (2.8)
$$
å…¶ä¸­$\boldsymbol{b}$ä¸º$N$ç»´å‘é‡ï¼Œæ‰€æœ‰å…ƒç´ å–å€¼éƒ½ä¸º$b$ã€‚

å‡æ–¹è¯¯å·®çš„ä»£ç å®ç°å¦‚ä¸‹:

> æ³¨æ„ï¼šä»£ç å®ç°ä¸­æ²¡æœ‰é™¤2ã€‚

```python
import paddle

def mean_squared_error(y_true, y_pred):
    """
    è¾“å…¥ï¼š
       - y_true: tensorï¼Œæ ·æœ¬çœŸå®æ ‡ç­¾
       - y_pred: tensor, æ ·æœ¬é¢„æµ‹æ ‡ç­¾
    è¾“å‡ºï¼š
       - error: floatï¼Œè¯¯å·®å€¼
    """

    assert y_true.shape[0] == y_pred.shape[0]
    
    # paddle.squareè®¡ç®—è¾“å…¥çš„å¹³æ–¹å€¼
    # paddle.meanæ²¿ axis è®¡ç®— x çš„å¹³å‡å€¼ï¼Œé»˜è®¤axisæ˜¯Noneï¼Œåˆ™å¯¹è¾“å…¥çš„å…¨éƒ¨å…ƒç´ è®¡ç®—å¹³å‡å€¼ã€‚
    error = paddle.mean(paddle.square(y_true - y_pred))

    return error


# æ„é€ ä¸€ä¸ªç®€å•çš„æ ·ä¾‹è¿›è¡Œæµ‹è¯•:[N,1], N=2
y_true= paddle.to_tensor([[-0.2],[4.9]],dtype='float32')
y_pred = paddle.to_tensor([[1.3],[2.5]],dtype='float32')

error = mean_squared_error(y_true=y_true, y_pred=y_pred).item()
print("error:",error)

```

```python
error: 4.005000114440918
```

---

#### [âˆš] 2.2.4 - æ¨¡å‹ä¼˜åŒ–

é‡‡ç”¨**ç»éªŒé£é™©æœ€å°åŒ–**ï¼Œçº¿æ€§å›å½’å¯ä»¥é€šè¿‡æœ€å°äºŒä¹˜æ³•æ±‚å‡ºå‚æ•°$\boldsymbol{w}$å’Œ$b$çš„è§£æè§£ã€‚è®¡ç®—å…¬å¼(2.8)ä¸­å‡æ–¹è¯¯å·®å¯¹å‚æ•°$b$çš„åå¯¼æ•°ï¼Œå¾—åˆ°
$$
\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial b} = \mathbf{1}^T (\boldsymbol{X}\boldsymbol{w}+\boldsymbol{b}-\boldsymbol{y}), (2.9)
$$

å…¶ä¸­$\mathbf{1}$ä¸º$N$ç»´çš„å…¨1å‘é‡ã€‚**è¿™é‡Œä¸ºäº†ç®€å•èµ·è§çœç•¥äº†å‡æ–¹è¯¯å·®çš„ç³»æ•°$\frac{1}{N}$ï¼Œå¹¶ä¸å½±å“æœ€åçš„ç»“æœ**ã€‚

ä»¤ä¸Šå¼ç­‰äº0ï¼Œå¾—åˆ°
$$
b^* =\bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w},(2.10)
$$

å…¶ä¸­$\bar{y} = \frac{1}{N}\mathbf{1}^T\boldsymbol{y}$ä¸ºæ‰€æœ‰æ ‡ç­¾çš„å¹³å‡å€¼ï¼Œ$\bar{\boldsymbol{x}} = \frac{1}{N}(\mathbf{1}^T \boldsymbol{X})^T$ ä¸ºæ‰€æœ‰ç‰¹å¾å‘é‡çš„å¹³å‡å€¼ã€‚å°†$b^*$ä»£å…¥å…¬å¼(2.8)ä¸­å‡æ–¹è¯¯å·®å¯¹å‚æ•°$\boldsymbol{w}$çš„åå¯¼æ•°ï¼Œå¾—åˆ°
$$
\frac{\partial \mathcal{L}(\boldsymbol{y},\hat{\boldsymbol{y}})}{\partial \boldsymbol{w}} = (\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T \Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\boldsymbol{w}-(\boldsymbol{y}-\bar{y})\Big).(2.11)
$$
ä»¤ä¸Šå¼ç­‰äº0ï¼Œå¾—åˆ°æœ€ä¼˜çš„å‚æ•°ä¸º
$$
\boldsymbol{w}^*=\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.12)
$$

$$
b^* =  \bar{y}-\bar{\boldsymbol{x}}^T \boldsymbol{w}^*.(2.13)
$$

è‹¥å¯¹å‚æ•°$\boldsymbol{w}$åŠ ä¸Š$\ell_2$æ­£åˆ™åŒ–ï¼Œåˆ™æœ€ä¼˜çš„$\boldsymbol{w}^*$å˜ä¸º
$$
\boldsymbol{w}^*=\Big((\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)+\lambda \boldsymbol{I}\Big)^{\mathrm{-}1}(\boldsymbol{X}-\bar{\boldsymbol{x}}^T)^T (\boldsymbol{y}-\bar{y}),(2.14)
$$

å…¶ä¸­$\lambda>0$ä¸ºé¢„å…ˆè®¾ç½®çš„æ­£åˆ™åŒ–ç³»æ•°ï¼Œ$\boldsymbol{I}\in \mathbb{R}^{D\times D}$ä¸ºå•ä½çŸ©é˜µã€‚

å°è¯•éªŒè¯å…¬å¼(2.14)ã€‚

==é€šè¿‡æœ€å°äºŒä¹˜æ³•ä¼˜åŒ–å‚æ•°==

```python
def optimizer_lsm(model, X, y, reg_lambda=0):
  '''
    è¾“å…¥ï¼š
       - model: æ¨¡å‹
       - X: tensor, ç‰¹å¾æ•°æ®ï¼Œshape=[N,D]
       - y: tensor,æ ‡ç­¾æ•°æ®ï¼Œshape=[N]
       - reg_lambda: float, æ­£åˆ™åŒ–ç³»æ•°ï¼Œé»˜è®¤ä¸º0
    è¾“å‡ºï¼š
       - model: ä¼˜åŒ–å¥½çš„æ¨¡å‹
   '''

  N, D = X.shape

  # å¯¹è¾“å…¥ç‰¹å¾æ•°æ®æ‰€æœ‰ç‰¹å¾å‘é‡æ±‚å¹³å‡
  x_bar_tran = paddle.mean(X,axis=0).T 
  
  # æ±‚æ ‡ç­¾çš„å‡å€¼,shape=[1]
  y_bar = paddle.mean(y)
  
  # paddle.subtracté€šè¿‡å¹¿æ’­çš„æ–¹å¼å®ç°çŸ©é˜µå‡å‘é‡
  x_sub = paddle.subtract(X,x_bar_tran)

  # ä½¿ç”¨paddle.allåˆ¤æ–­è¾“å…¥tensoræ˜¯å¦å…¨0
  if paddle.all(x_sub==0):
    model.params['b'] = y_bar
    model.params['w'] = paddle.zeros(shape=[D])
    return model
  
  # paddle.inverseæ±‚æ–¹é˜µçš„é€†
  tmp = paddle.inverse(paddle.matmul(x_sub.T,x_sub)+
          reg_lambda*paddle.eye(num_rows = (D)))

  w = paddle.matmul(paddle.matmul(tmp,x_sub.T),(y-y_bar))
  
  b = y_bar-paddle.matmul(x_bar_tran,w)
  
  model.params['b'] = b
  model.params['w'] = paddle.squeeze(w,axis=-1)

  return model

```





---

#### [âˆš] 2.2.5 - æ¨¡å‹è®­ç»ƒ

åœ¨å‡†å¤‡äº†æ•°æ®ã€æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œå‚æ•°å­¦ä¹ çš„å®ç°ä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æ¨¡å‹çš„è®­ç»ƒã€‚

åœ¨å›å½’ä»»åŠ¡ä¸­ï¼Œæ¨¡å‹çš„è¯„ä»·æŒ‡æ ‡å’ŒæŸå¤±å‡½æ•°ä¸€è‡´ï¼Œéƒ½ä¸ºå‡æ–¹è¯¯å·®ã€‚

é€šè¿‡ä¸Šæ–‡å®ç°çš„çº¿æ€§å›å½’ç±»æ¥æ‹Ÿåˆè®­ç»ƒæ•°æ®ï¼Œå¹¶è¾“å‡ºæ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„æŸå¤±ã€‚

```python
input_size = 1
model = Linear(input_size)
model = optimizer_lsm(model,X_train.reshape([-1,1]),y_train.reshape([-1,1]))
print("w_pred:",model.params['w'].item(), "b_pred: ", model.params['b'].item())

y_train_pred = model(X_train.reshape([-1,1])).squeeze()
train_error = mean_squared_error(y_true=y_train, y_pred=y_train_pred).item()
print("train error: ",train_error)
```

\# çœŸå®å‡½æ•°çš„å‚æ•°ç¼ºçœå€¼ä¸º w=1.2ï¼Œb=0.5

```python
w_pred: 1.200244426727295 b_pred:  0.41967445611953735
train error:  3.8776581287384033
```

```python
model_large = Linear(input_size)
model_large = optimizer_lsm(model_large,X_train_large.reshape([-1,1]),y_train_large.reshape([-1,1]))
print("w_pred large:",model_large.params['w'].item(), "b_pred large: ", model_large.params['b'].item())

y_train_pred_large = model_large(X_train_large.reshape([-1,1])).squeeze()
train_error_large = mean_squared_error(y_true=y_train_large, y_pred=y_train_pred_large).item()
print("train error large: ",train_error_large)
```

```python
w_pred large: 1.1942962408065796 b_pred large:  0.5079830884933472
train error large:  4.086254119873047
```

ä»è¾“å‡ºç»“æœçœ‹ï¼Œé¢„æµ‹ç»“æœä¸çœŸå®å€¼$\boldsymbol{w}=1.2$ï¼Œ$b=0.5$æœ‰ä¸€å®šçš„å·®è·ã€‚



---

#### [âˆš] 2.2.6 - æ¨¡å‹è¯„ä¼°

ä¸‹é¢ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹ä¸€ä¸‹æµ‹è¯•é›†çš„æ ‡ç­¾ï¼Œå¹¶è®¡ç®—åœ¨æµ‹è¯•é›†ä¸Šçš„æŸå¤±ã€‚

```python
y_test_pred = model(X_test.reshape([-1,1])).squeeze()
test_error = mean_squared_error(y_true=y_test, y_pred=y_test_pred).item()
print("test error: ",test_error)
```

```python
test error:  4.075982570648193
```

```python
y_test_pred_large = model_large(X_test.reshape([-1,1])).squeeze()
test_error_large = mean_squared_error(y_true=y_test, y_pred=y_test_pred_large).item()
print("test error large: ",test_error_large)
```

```python
test error large:  4.081584453582764
```

åŠ¨æ‰‹ç»ƒä¹ :

ä¸ºäº†åŠ æ·±å¯¹æœºå™¨å­¦ä¹ æ¨¡å‹çš„ç†è§£ï¼Œè¯·è‡ªå·±åŠ¨æ‰‹å®Œæˆä»¥ä¸‹å®éªŒï¼š

ï¼ˆ1ï¼‰ è°ƒæ•´è®­ç»ƒæ•°æ®çš„æ ·æœ¬æ•°é‡ï¼Œç”± 100 è°ƒæ•´åˆ° 5000ï¼Œè§‚å¯Ÿå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

ï¼ˆ2ï¼‰ è°ƒæ•´æ­£åˆ™åŒ–ç³»æ•°ï¼Œè§‚å¯Ÿå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚

ï¼ˆ1ï¼‰è®­ç»ƒé›†æ•°é‡5000ï¼Œæµ‹è¯•é›†æ•°é‡1000

```python
w_pred: 1.2003157138824463 b_pred:  0.5225465893745422
train error:  4.16726541519165
```

```python
test error:  3.9572794437408447
```

```python
test error large:  3.96787428855896
```

æ¨¡å‹çš„é”™è¯¯ç‡æœ‰æ‰€ä¸‹é™



---

### [âˆš] 2.3 - å¤šé¡¹å¼å›å½’

å¤šé¡¹å¼å›å½’æ˜¯å›å½’ä»»åŠ¡çš„ä¸€ç§å½¢å¼ï¼Œå…¶ä¸­è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´çš„å…³ç³»æ˜¯$M$æ¬¡å¤šé¡¹å¼çš„ä¸€ç§çº¿æ€§å›å½’å½¢å¼ï¼Œå³ï¼š
$$
f(\boldsymbol{x};\boldsymbol{w})=w_1x+w_2x^2+...+w_Mx^M+b=\boldsymbol{w}^T\phi(x)+b, ï¼ˆ2.10ï¼‰
$$
å…¶ä¸­$M$ä¸ºå¤šé¡¹å¼çš„é˜¶æ•°ï¼Œ$\boldsymbol{w}=[w_1,...,w_M]^T$ä¸ºå¤šé¡¹å¼çš„ç³»æ•°ï¼Œ$\phi(x)=[x,x^2,\cdots,x^M]^T$ä¸ºå¤šé¡¹å¼åŸºå‡½æ•°ï¼Œå°†åŸå§‹ç‰¹å¾$x$æ˜ å°„ä¸º$M$ç»´çš„å‘é‡ã€‚å½“$M=0$æ—¶ï¼Œ$f(\boldsymbol{x};\boldsymbol{w})=b$ã€‚

å…¬å¼ï¼ˆ2.10ï¼‰å±•ç¤ºçš„æ˜¯ç‰¹å¾ç»´åº¦ä¸º1çš„å¤šé¡¹å¼è¡¨è¾¾ï¼Œå½“ç‰¹å¾ç»´åº¦å¤§äº1æ—¶ï¼Œå­˜åœ¨ä¸åŒç‰¹å¾ä¹‹é—´äº¤äº’çš„æƒ…å†µï¼Œè¿™æ˜¯çº¿æ€§å›å½’æ— æ³•å®ç°ã€‚å…¬å¼ï¼ˆ2.11ï¼‰å±•ç¤ºçš„æ˜¯å½“ç‰¹å¾ç»´åº¦ä¸º2ï¼Œå¤šé¡¹å¼é˜¶æ•°ä¸º2æ—¶çš„å¤šé¡¹å¼å›å½’ï¼š

$$
f(\boldsymbol{x};\boldsymbol{w})=w_1x_1+w_2x_2+w_3x_1^2+w_4x_1x_2+w_5x_2^2+b, ï¼ˆ2.11ï¼‰
$$

å½“è‡ªå˜é‡å’Œå› å˜é‡ä¹‹é—´å¹¶ä¸æ˜¯çº¿æ€§å…³ç³»æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰éçº¿æ€§åŸºå‡½æ•°å¯¹ç‰¹å¾è¿›è¡Œå˜æ¢ï¼Œä»è€Œå¯ä»¥ä½¿å¾—çº¿æ€§å›å½’ç®—æ³•å®ç°éçº¿æ€§çš„æ›²çº¿æ‹Ÿåˆã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬åŸºäºç‰¹å¾ç»´åº¦ä¸º1çš„è‡ªå˜é‡ä»‹ç»å¤šé¡¹å¼å›å½’å®éªŒã€‚

---

#### [âˆš] 2.3.1 - æ•°æ®é›†æ„å»º

å‡è®¾æˆ‘ä»¬è¦æ‹Ÿåˆçš„éçº¿æ€§å‡½æ•°ä¸ºä¸€ä¸ªç¼©æ”¾åçš„sinå‡½æ•°ã€‚

```python
import math

# sinå‡½æ•°: sin(2 * pi * x)
def sin(x):
    y = paddle.sin(2 * math.pi * x)
    return y
```

è¿™é‡Œä»ç„¶ä½¿ç”¨å‰é¢å®šä¹‰çš„`create_toy_data`å‡½æ•°æ¥æ„å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼Œå…¶ä¸­è®­ç»ƒæ•°æ ·æœ¬ 15 ä¸ªï¼Œæµ‹è¯•æ ·æœ¬ 10 ä¸ªï¼Œé«˜æ–¯å™ªå£°æ ‡å‡†å·®ä¸º 0.1ï¼Œè‡ªå˜é‡èŒƒå›´ä¸º (0,1)ã€‚

```python
# ç”Ÿæˆæ•°æ®
func = sin
interval = (0,1)
train_num = 15
test_num = 10
noise = 0.1 #0.5 
X_train, y_train = create_toy_data(func=func, interval=interval, sample_num=train_num, noise = noise)
X_test, y_test = create_toy_data(func=func, interval=interval, sample_num=test_num, noise = noise)

X_underlying = paddle.linspace(interval[0],interval[1],num=100)
y_underlying = sin(X_underlying)

# ç»˜åˆ¶å›¾åƒ
plt.rcParams['figure.figsize'] = (8.0, 6.0)
plt.scatter(X_train, y_train, facecolor="none", edgecolor='#e4007f', s=50, label="train data")
#plt.scatter(X_test, y_test, facecolor="none", edgecolor="r", s=50, label="test data")
plt.plot(X_underlying, y_underlying, c='#000000', label=r"$\sin(2\pi x)$")
plt.legend(fontsize='x-large')
plt.savefig('ml-vis2.pdf')
plt.show()
```

<img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165459211.png" alt="image-20221212164136707" style="zoom:33%;" />





---

#### [âˆš] 2.3.2 - æ¨¡å‹æ„å»º

é€šè¿‡å¤šé¡¹å¼çš„å®šä¹‰å¯ä»¥çœ‹å‡ºï¼Œå¤šé¡¹å¼å›å½’å’Œçº¿æ€§å›å½’ä¸€æ ·ï¼ŒåŒæ ·å­¦ä¹ å‚æ•°$\boldsymbol{w}$ï¼Œåªä¸è¿‡éœ€è¦å¯¹è¾“å…¥ç‰¹å¾$\phi(x)$æ ¹æ®å¤šé¡¹å¼é˜¶æ•°è¿›è¡Œå˜æ¢ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥å¥—ç”¨æ±‚è§£çº¿æ€§å›å½’å‚æ•°çš„æ–¹æ³•æ¥æ±‚è§£å¤šé¡¹å¼å›å½’å‚æ•°ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬å®ç°**å¤šé¡¹å¼åŸºå‡½æ•°**`polynomial_basis_function`å¯¹åŸå§‹ç‰¹å¾$x$è¿›è¡Œè½¬æ¢ã€‚

```python
# å¤šé¡¹å¼è½¬æ¢
def polynomial_basis_function(x, degree = 2):
    """
    è¾“å…¥ï¼š
       - x: tensor, è¾“å…¥çš„æ•°æ®ï¼Œshape=[N,1]
       - degree: int, å¤šé¡¹å¼çš„é˜¶æ•°
       example Input: [[2], [3], [4]], degree=2
       example Output: [[2^1, 2^2], [3^1, 3^2], [4^1, 4^2]]
       æ³¨æ„ï¼šæœ¬æ¡ˆä¾‹ä¸­,åœ¨degree>=1æ—¶ä¸ç”Ÿæˆå…¨ä¸º1çš„ä¸€åˆ—æ•°æ®ï¼›degreeä¸º0æ—¶ç”Ÿæˆå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒï¼Œå…¨1çš„Tensor
    è¾“å‡ºï¼š
       - x_resultï¼š tensor
    """
    
    if degree==0:
        return paddle.ones(shape = x.shape,dtype='float32') 

    x_tmp = x
    x_result = x_tmp

    for i in range(2, degree+1):
        x_tmp = paddle.multiply(x_tmp,x) # é€å…ƒç´ ç›¸ä¹˜
        x_result = paddle.concat((x_result,x_tmp),axis=-1)

    return x_result

# ç®€å•æµ‹è¯•
data = [[2], [3], [4]]
X = paddle.to_tensor(data = data,dtype='float32')
degree = 3
transformed_X = polynomial_basis_function(X,degree=degree)
print("è½¬æ¢å‰ï¼š",X)
print("é˜¶æ•°ä¸º",degree,"è½¬æ¢åï¼š",transformed_X)
```

```python
è½¬æ¢å‰ï¼š Tensor(shape=[3, 1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[2.],
        [3.],
        [4.]])
é˜¶æ•°ä¸º 3 è½¬æ¢åï¼š Tensor(shape=[3, 3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [[2. , 4. , 8. ],
        [3. , 9. , 27.],
        [4. , 16., 64.]])
```

---

#### [âˆš] 2.3.3 - æ¨¡å‹è®­ç»ƒ

å¯¹äºå¤šé¡¹å¼å›å½’ï¼Œæˆ‘ä»¬å¯ä»¥åŒæ ·ä½¿ç”¨å‰é¢çº¿æ€§å›å½’ä¸­å®šä¹‰çš„`LinearRegression`ç®—å­ã€è®­ç»ƒå‡½æ•°`train`ã€å‡æ–¹è¯¯å·®å‡½æ•°`mean_squared_error`ã€‚æ‹Ÿåˆè®­ç»ƒæ•°æ®çš„ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•°ï¼ŒåŒçº¿æ€§å›å½’ä¸€æ ·ï¼Œä¹Ÿå¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—ç›´æ¥æ±‚å‡º$\boldsymbol{w}$çš„å€¼ã€‚

æˆ‘ä»¬è®¾å®šä¸åŒçš„å¤šé¡¹å¼é˜¶ï¼Œ$M$çš„å–å€¼åˆ†åˆ«ä¸º0ã€1ã€3ã€8ï¼Œä¹‹å‰æ„é€ çš„è®­ç»ƒé›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè§‚å¯Ÿæ ·æœ¬æ•°æ®å¯¹$\sin$æ›²çº¿çš„æ‹Ÿåˆç»“æœã€‚

```python
plt.rcParams['figure.figsize'] = (12.0, 8.0)

for i, degree in enumerate([0, 1, 3, 8]): # []ä¸­ä¸ºå¤šé¡¹å¼çš„é˜¶æ•°
    model = Linear(degree)
    X_train_transformed = polynomial_basis_function(X_train.reshape([-1,1]), degree)
    X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-1,1]), degree)
    
    model = optimizer_lsm(model,X_train_transformed,y_train.reshape([-1,1])) #æ‹Ÿåˆå¾—åˆ°å‚æ•°

    y_underlying_pred = model(X_underlying_transformed).squeeze()

    print(model.params)
    
    # ç»˜åˆ¶å›¾åƒ
    plt.subplot(2, 2, i + 1)
    plt.scatter(X_train, y_train, facecolor="none", edgecolor='#e4007f', s=50, label="train data")
    plt.plot(X_underlying, y_underlying, c='#000000', label=r"$\sin(2\pi x)$")
    plt.plot(X_underlying, y_underlying_pred, c='#f19ec2', label="predicted function")
    plt.ylim(-2, 1.5)
    plt.annotate("M={}".format(degree), xy=(0.95, -1.4))

#plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)
plt.legend(loc='lower left', fontsize='x-large')
plt.savefig('ml-vis3.pdf')
plt.show()
```

```python
{'w': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [0.]), 'b': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [0.06668118])}
{'w': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [-1.34165502]), 'b': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [0.74084926])}
{'w': Tensor(shape=[3], dtype=float32, place=CPUPlace, stop_gradient=True,
       [ 11.50885582, -33.70772171,  22.45395279]), 'b': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [-0.17524421])}
{'w': Tensor(shape=[8], dtype=float32, place=CPUPlace, stop_gradient=True,
       [ 6.39199781  ,  38.72201538 , -377.10943604,  912.26202393,
        -1013.89459229,  374.96646118,  198.60220337, -150.03273010]), 'b': Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True,
       [3.29901624])}
```

![image-20221212165145316](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165722946.png)

è§‚å¯Ÿå¯è§†åŒ–ç»“æœï¼Œçº¢è‰²çš„æ›²çº¿è¡¨ç¤ºä¸åŒé˜¶å¤šé¡¹å¼åˆ†å¸ƒæ‹Ÿåˆæ•°æ®çš„ç»“æœï¼š
* å½“ $M=0$ æˆ– $M=1$ æ—¶ï¼Œæ‹Ÿåˆæ›²çº¿è¾ƒç®€å•ï¼Œæ¨¡å‹æ¬ æ‹Ÿåˆï¼›
* å½“ $M=8$ æ—¶ï¼Œæ‹Ÿåˆæ›²çº¿è¾ƒå¤æ‚ï¼Œæ¨¡å‹è¿‡æ‹Ÿåˆï¼›
* å½“ $M=3$ æ—¶ï¼Œæ¨¡å‹æ‹Ÿåˆæœ€ä¸ºåˆç†ã€‚



---

#### [âˆš] 2.3.4 - æ¨¡å‹è¯„ä¼°

ä¸‹é¢é€šè¿‡å‡æ–¹è¯¯å·®æ¥è¡¡é‡è®­ç»ƒè¯¯å·®ã€æµ‹è¯•è¯¯å·®ä»¥åŠåœ¨æ²¡æœ‰å™ªéŸ³çš„åŠ å…¥ä¸‹`sin`å‡½æ•°å€¼ä¸å¤šé¡¹å¼å›å½’å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œæ›´åŠ çœŸå®åœ°åæ˜ æ‹Ÿåˆç»“æœã€‚å¤šé¡¹å¼åˆ†å¸ƒé˜¶æ•°ä»0åˆ°8è¿›è¡Œéå†ã€‚

```python
# è®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®
training_errors = []
test_errors = []
distribution_errors = []

# éå†å¤šé¡¹å¼é˜¶æ•°
for i in range(9):
    model = Linear(i)

    X_train_transformed = polynomial_basis_function(X_train.reshape([-1,1]), i) 
    X_test_transformed = polynomial_basis_function(X_test.reshape([-1,1]), i) 
    X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-1,1]), i)
    
    optimizer_lsm(model,X_train_transformed,y_train.reshape([-1,1]))
    
    y_train_pred = model(X_train_transformed).squeeze()
    y_test_pred = model(X_test_transformed).squeeze()
    y_underlying_pred = model(X_underlying_transformed).squeeze()

    train_mse = mean_squared_error(y_true=y_train, y_pred=y_train_pred).item()
    training_errors.append(train_mse)

    test_mse = mean_squared_error(y_true=y_test, y_pred=y_test_pred).item()
    test_errors.append(test_mse)

    #distribution_mse = mean_squared_error(y_true=y_underlying, y_pred=y_underlying_pred).item()
    #distribution_errors.append(distribution_mse)

print ("train errors: \n",training_errors)
print ("test errors: \n",test_errors)
#print ("distribution errors: \n", distribution_errors)

# ç»˜åˆ¶å›¾ç‰‡
plt.rcParams['figure.figsize'] = (8.0, 6.0)
plt.plot(training_errors, '-.', mfc="none", mec='#e4007f', ms=10, c='#e4007f', label="Training")
plt.plot(test_errors, '--', mfc="none", mec='#f19ec2', ms=10, c='#f19ec2', label="Test")
#plt.plot(distribution_errors, '-', mfc="none", mec="#3D3D3F", ms=10, c="#3D3D3F", label="Distribution")
plt.legend(fontsize='x-large')
plt.xlabel("degree")
plt.ylabel("MSE")
plt.savefig('ml-mse-error.pdf')
plt.show()
```

```python
train errors: 
 [0.4672910273075104, 0.2713072597980499, 0.25924286246299744, 0.007656396366655827, 0.007648141589015722, 0.004982923157513142, 0.055855292826890945, 0.01697038859128952, 13.178251266479492]
test errors: 
 [0.48431509733200073, 0.20496585965156555, 0.22155332565307617, 0.02112610451877117, 0.021505268290638924, 0.009555519558489323, 0.07138563692569733, 0.014405457302927971, 8.89521312713623]
```

![image-20221212165459211](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221211213007230.png)

è§‚å¯Ÿå¯è§†åŒ–ç»“æœï¼š
* å½“é˜¶æ•°è¾ƒä½çš„æ—¶å€™ï¼Œæ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›æœ‰é™ï¼Œè®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®éƒ½å¾ˆé«˜ï¼Œä»£è¡¨æ¨¡å‹æ¬ æ‹Ÿåˆï¼›
* å½“é˜¶æ•°è¾ƒé«˜çš„æ—¶å€™ï¼Œæ¨¡å‹è¡¨ç¤ºèƒ½åŠ›å¼ºï¼Œä½†å°†è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°ä¹Ÿä½œä¸ºç‰¹å¾è¿›è¡Œå­¦ä¹ ï¼Œä¸€èˆ¬æƒ…å†µä¸‹è®­ç»ƒè¯¯å·®ç»§ç»­é™ä½è€Œæµ‹è¯•è¯¯å·®æ˜¾è‘—å‡é«˜ï¼Œä»£è¡¨æ¨¡å‹è¿‡æ‹Ÿåˆã€‚

> æ­¤å¤„å¤šé¡¹å¼é˜¶æ•°å¤§äºç­‰äº5æ—¶ï¼Œè®­ç»ƒè¯¯å·®å¹¶æ²¡æœ‰ä¸‹é™ï¼Œå°¤å…¶æ˜¯åœ¨å¤šé¡¹å¼é˜¶æ•°ä¸º7æ—¶ï¼Œè®­ç»ƒè¯¯å·®å˜å¾—éå¸¸å¤§ï¼Œè¯·æ€è€ƒåŸå› ï¼Ÿæç¤ºï¼šè¯·ä»å¹‚å‡½æ•°ç‰¹æ€§è§’åº¦æ€è€ƒã€‚

å¯¹äºæ¨¡å‹è¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œå¯ä»¥å¼•å…¥æ­£åˆ™åŒ–æ–¹æ³•ï¼Œé€šè¿‡å‘è¯¯å·®å‡½æ•°ä¸­æ·»åŠ ä¸€ä¸ª**æƒ©ç½šé¡¹**æ¥é¿å…ç³»æ•°å€¾å‘äºè¾ƒå¤§çš„å–å€¼ã€‚ä¸‹é¢åŠ å…¥$\mathcal{l_{2}}$æ­£åˆ™åŒ–é¡¹ï¼ŒæŸ¥çœ‹æ‹Ÿåˆç»“æœã€‚

```python
degree = 8 # å¤šé¡¹å¼é˜¶æ•°
reg_lambda = 0.0001 # æ­£åˆ™åŒ–ç³»æ•°

X_train_transformed = polynomial_basis_function(X_train.reshape([-1,1]), degree)
X_test_transformed = polynomial_basis_function(X_test.reshape([-1,1]), degree)
X_underlying_transformed = polynomial_basis_function(X_underlying.reshape([-1,1]), degree)

model = Linear(degree) 

optimizer_lsm(model,X_train_transformed,y_train.reshape([-1,1]))

y_test_pred=model(X_test_transformed).squeeze()
y_underlying_pred=model(X_underlying_transformed).squeeze()

model_reg = Linear(degree) 

optimizer_lsm(model_reg,X_train_transformed,y_train.reshape([-1,1]),reg_lambda=reg_lambda)

y_test_pred_reg=model_reg(X_test_transformed).squeeze()
y_underlying_pred_reg=model_reg(X_underlying_transformed).squeeze()

mse = mean_squared_error(y_true = y_test, y_pred = y_test_pred).item()
print("mse:",mse)
mes_reg = mean_squared_error(y_true = y_test, y_pred = y_test_pred_reg).item()
print("mse_with_l2_reg:",mes_reg)

# ç»˜åˆ¶å›¾åƒ
plt.scatter(X_train, y_train, facecolor="none", edgecolor="#e4007f", s=50, label="train data")
plt.plot(X_underlying, y_underlying, c='#000000', label=r"$\sin(2\pi x)$")
plt.plot(X_underlying, y_underlying_pred, c='#e4007f', linestyle="--", label="$deg. = 8$")
plt.plot(X_underlying, y_underlying_pred_reg, c='#f19ec2', linestyle="-.", label="$deg. = 8, \ell_2 reg$")
plt.ylim(-1.5, 1.5)
plt.annotate("lambda={}".format(reg_lambda), xy=(0.82, -1.4))
plt.legend(fontsize='large')
plt.savefig('ml-vis4.pdf')
plt.show()
```

```python
mse: 8.89521312713623
mse_with_l2_reg: 0.009005023166537285
```

<img src="https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173347233.png" alt="image-20221212165722946" style="zoom: 50%;" />

è§‚å¯Ÿå¯è§†åŒ–ç»“æœï¼Œå…¶ä¸­é»„è‰²æ›²çº¿ä¸ºåŠ å…¥$\mathcal{l_{2}}$æ­£åˆ™åå¤šé¡¹å¼åˆ†å¸ƒæ‹Ÿåˆç»“æœï¼Œçº¢è‰²æ›²çº¿ä¸ºæœªåŠ å…¥$\mathcal{l_{2}}$æ­£åˆ™çš„æ‹Ÿåˆç»“æœï¼Œé»„è‰²æ›²çº¿çš„æ‹Ÿåˆæ•ˆæœæ˜æ˜¾å¥½äºçº¢è‰²æ›²çº¿ã€‚

---

### [âˆš] 2.4 - Runnerç±»ä»‹ç»

é€šè¿‡ä¸Šé¢çš„å®è·µï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šåº”ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•çš„æµç¨‹åŸºæœ¬ä¸ŠåŒ…æ‹¬ï¼šæ•°æ®é›†æ„å»ºã€æ¨¡å‹æ„å»ºã€æŸå¤±å‡½æ•°å®šä¹‰ã€ä¼˜åŒ–å™¨ã€æ¨¡å‹è®­ç»ƒã€æ¨¡å‹è¯„ä»·ã€æ¨¡å‹é¢„æµ‹ç­‰ç¯èŠ‚ã€‚

ä¸ºäº†æ›´æ–¹ä¾¿åœ°å°†ä¸Šè¿°ç¯èŠ‚è§„èŒƒåŒ–ï¼Œæˆ‘ä»¬å°†æœºå™¨å­¦ä¹ æ¨¡å‹çš„åŸºæœ¬è¦ç´ å°è£…æˆä¸€ä¸ª**Runner**ç±»ã€‚é™¤ä¸Šè¿°æåˆ°çš„è¦ç´ å¤–ï¼Œå†åŠ ä¸Šæ¨¡å‹ä¿å­˜ã€æ¨¡å‹åŠ è½½ç­‰åŠŸèƒ½ã€‚

**Runner**ç±»çš„æˆå‘˜å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š
* \_\_init\_\_å‡½æ•°ï¼šå®ä¾‹åŒ–**Runner**ç±»æ—¶é»˜è®¤è°ƒç”¨ï¼Œéœ€è¦ä¼ å…¥æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè¯„ä»·æŒ‡æ ‡ç­‰ï¼›
* trainå‡½æ•°ï¼šå®Œæˆæ¨¡å‹è®­ç»ƒï¼ŒæŒ‡å®šæ¨¡å‹è®­ç»ƒéœ€è¦çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼›
* evaluateå‡½æ•°ï¼šé€šè¿‡å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè¯„ä»·ï¼Œåœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸ŠæŸ¥çœ‹æ¨¡å‹è®­ç»ƒæ•ˆæœï¼›
* predictå‡½æ•°ï¼šé€‰å–ä¸€æ¡æ•°æ®å¯¹è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼›
* save\_modelå‡½æ•°ï¼šæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹å’Œè®­ç»ƒç»“æŸåéœ€è¦è¿›è¡Œä¿å­˜ï¼›
* load\_modelå‡½æ•°ï¼šè°ƒç”¨åŠ è½½ä¹‹å‰ä¿å­˜çš„æ¨¡å‹ã€‚

`Runner`ç±»çš„æ¡†æ¶å®šä¹‰å¦‚ä¸‹ï¼š

```python
class Runner(object):
    def __init__(self, model, optimizer, loss_fn, metric):
        self.model = model         # æ¨¡å‹
        self.optimizer = optimizer # ä¼˜åŒ–å™¨
        self.loss_fn = loss_fn     # æŸå¤±å‡½æ•°   
        self.metric = metric       # è¯„ä¼°æŒ‡æ ‡

    # æ¨¡å‹è®­ç»ƒ
    def train(self, train_dataset, dev_dataset=None, **kwargs):
        pass

    # æ¨¡å‹è¯„ä»·
    def evaluate(self, data_set, **kwargs):
        pass

    # æ¨¡å‹é¢„æµ‹
    def predict(self, x, **kwargs):
        pass

    # æ¨¡å‹ä¿å­˜
    def save_model(self, save_path):
        pass

    # æ¨¡å‹åŠ è½½
    def load_model(self, model_path):
        pass
```

**Runner**ç±»çš„æµç¨‹å¦‚**å›¾2.8**æ‰€ç¤ºï¼Œå¯ä»¥åˆ†ä¸º 4 ä¸ªé˜¶æ®µï¼š

1. åˆå§‹åŒ–é˜¶æ®µï¼šä¼ å…¥æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨å’Œè¯„ä»·æŒ‡æ ‡ã€‚
1. æ¨¡å‹è®­ç»ƒé˜¶æ®µï¼šåŸºäºè®­ç»ƒé›†è°ƒç”¨`train()`å‡½æ•°è®­ç»ƒæ¨¡å‹ï¼ŒåŸºäºéªŒè¯é›†é€šè¿‡`evaluate()`å‡½æ•°éªŒè¯æ¨¡å‹ã€‚é€šè¿‡`save_model()`å‡½æ•°ä¿å­˜æ¨¡å‹ã€‚
1. æ¨¡å‹è¯„ä»·é˜¶æ®µï¼šåŸºäºæµ‹è¯•é›†é€šè¿‡`evaluate()`å‡½æ•°å¾—åˆ°æŒ‡æ ‡æ€§èƒ½ã€‚
1. æ¨¡å‹é¢„æµ‹é˜¶æ®µï¼šç»™å®šæ ·æœ¬ï¼Œé€šè¿‡`predict()`å‡½æ•°å¾—åˆ°è¯¥æ ·æœ¬æ ‡ç­¾ã€‚

<center><img src="https://ai-studio-static-online.cdn.bcebos.com/fdb656daadb349a78560fa464b0de5fa5d63423fc2234adfac48e6ff020a6d60" width=700 ></img></center>

<center>å›¾2.8 Runnerç±»</center>



---

### [âˆš] 2.5 - åŸºäºçº¿æ€§å›å½’çš„æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çº¿æ€§å›å½’æ¥å¯¹é©¬è¨è¯¸å¡å·æ³¢å£«é¡¿éƒŠåŒºçš„æˆ¿å±‹è¿›è¡Œé¢„æµ‹ã€‚å®éªŒæµç¨‹ä¸»è¦åŒ…å«å¦‚ä¸‹5ä¸ªæ­¥éª¤ï¼š

- æ•°æ®å¤„ç†ï¼šåŒ…æ‹¬æ•°æ®æ¸…æ´—ï¼ˆç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼å¤„ç†ï¼‰ã€æ•°æ®é›†åˆ’åˆ†ï¼Œä»¥ä¾¿æ•°æ®å¯ä»¥è¢«æ¨¡å‹æ­£å¸¸è¯»å–ï¼Œå¹¶å…·æœ‰è‰¯å¥½çš„æ³›åŒ–æ€§;
- æ¨¡å‹æ„å»ºï¼šå®šä¹‰çº¿æ€§å›å½’æ¨¡å‹ç±»ï¼›
- è®­ç»ƒé…ç½®:è®­ç»ƒç›¸å…³çš„ä¸€äº›é…ç½®ï¼Œå¦‚ï¼šä¼˜åŒ–ç®—æ³•ã€è¯„ä»·æŒ‡æ ‡ç­‰ï¼›
- ç»„è£…è®­ç»ƒæ¡†æ¶Runner:`Runner`ç”¨äºç®¡ç†æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•è¿‡ç¨‹ï¼›
- æ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•:åˆ©ç”¨`Runner`è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ã€‚

---

#### [âˆš] 2.5.1 - æ•°æ®å¤„ç†

##### [âˆš] 2.5.1.1 - æ•°æ®é›†ä»‹ç»

æœ¬å®éªŒä½¿ç”¨æ³¢å£«é¡¿æˆ¿ä»·é¢„æµ‹æ•°æ®é›†ï¼Œå…±506æ¡æ ·æœ¬æ•°æ®ï¼Œæ¯æ¡æ ·æœ¬åŒ…å«äº†12ç§å¯èƒ½å½±å“æˆ¿ä»·çš„å› ç´ å’Œè¯¥ç±»æˆ¿å±‹ä»·æ ¼çš„ä¸­ä½æ•°ï¼Œå„å­—æ®µå«ä¹‰å¦‚**è¡¨2.1**æ‰€ç¤ºï¼š

![image-20221212173220360](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173220360.png)

é¢„è§ˆå‰5æ¡æ•°æ®ï¼š

```python
import pandas as pd # å¼€æºæ•°æ®åˆ†æå’Œæ“ä½œå·¥å…·

# åˆ©ç”¨pandasåŠ è½½æ³¢å£«é¡¿æˆ¿ä»·çš„æ•°æ®é›†
data=pd.read_csv("/home/aistudio/work/boston_house_prices.csv")
# é¢„è§ˆå‰5è¡Œæ•°æ®
data.head()
```

![image-20221212173347233](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173523825.png)

---

##### [âˆš] 2.5.1.2 - æ•°æ®æ¸…æ´—

å¯¹æ•°æ®é›†ä¸­çš„ç¼ºå¤±å€¼æˆ–å¼‚å¸¸å€¼ç­‰æƒ…å†µè¿›è¡Œåˆ†æå’Œå¤„ç†ï¼Œä¿è¯æ•°æ®å¯ä»¥è¢«æ¨¡å‹æ­£å¸¸è¯»å–ã€‚

- **ç¼ºå¤±å€¼åˆ†æ**

é€šè¿‡`isna()`æ–¹æ³•åˆ¤æ–­æ•°æ®ä¸­å„å…ƒç´ æ˜¯å¦ç¼ºå¤±ï¼Œç„¶åé€šè¿‡`sum()`æ–¹æ³•ç»Ÿè®¡æ¯ä¸ªå­—æ®µç¼ºå¤±æƒ…å†µï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
# æŸ¥çœ‹å„å­—æ®µç¼ºå¤±å€¼ç»Ÿè®¡æƒ…å†µ
data.isna().sum()
```

![image-20221212173523825](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212165145316.png)

å¯ä»¥çœ‹å‡ºä¸å­˜åœ¨ç¼ºå¤±æ•°æ®çš„æƒ…å†µ

* **å¼‚å¸¸å€¼å¤„ç†**

é€šè¿‡ç®±çº¿å›¾ç›´è§‚çš„æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒï¼Œå¹¶è§‚æµ‹æ•°æ®ä¸­çš„å¼‚å¸¸å€¼ã€‚ç®±çº¿å›¾ä¸€èˆ¬ç”±äº”ä¸ªç»Ÿè®¡å€¼ç»„æˆï¼šæœ€å¤§å€¼ã€ä¸Šå››åˆ†ä½ã€ä¸­ä½æ•°ã€ä¸‹å››åˆ†ä½å’Œæœ€å°å€¼ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œè§‚æµ‹åˆ°çš„æ•°æ®å¤§äºæœ€å¤§ä¼°è®¡å€¼æˆ–è€…å°äºæœ€å°ä¼°è®¡å€¼åˆ™åˆ¤æ–­ä¸ºå¼‚å¸¸å€¼ï¼Œå…¶ä¸­
$$
æœ€å¤§ä¼°è®¡å€¼ = ä¸Šå››åˆ†ä½ + 1.5 * (ä¸Šå››åˆ†ä½ - ä¸‹å››åˆ†ä½)\\
æœ€å°ä¼°è®¡å€¼=ä¸‹å››åˆ†ä½ - 1.5 * (ä¸Šå››åˆ†ä½ - ä¸‹å››åˆ†ä½)
$$

```python
import matplotlib.pyplot as plt # å¯è§†åŒ–å·¥å…·

# ç®±çº¿å›¾æŸ¥çœ‹å¼‚å¸¸å€¼åˆ†å¸ƒ
def boxplot(data, fig_name):
    # ç»˜åˆ¶æ¯ä¸ªå±æ€§çš„ç®±çº¿å›¾
    data_col = list(data.columns)
    
    # è¿ç»­ç”»å‡ ä¸ªå›¾ç‰‡
    plt.figure(figsize=(5, 5), dpi=300)
    # å­å›¾è°ƒæ•´
    plt.subplots_adjust(wspace=0.6)
    # æ¯ä¸ªç‰¹å¾ç”»ä¸€ä¸ªç®±çº¿å›¾
    for i, col_name in enumerate(data_col):
        plt.subplot(3, 5, i+1)
        # ç”»ç®±çº¿å›¾
        plt.boxplot(data[col_name], 
                    showmeans=True, 
                    meanprops={"markersize":1,"marker":"D","markeredgecolor":"#C54680"}, # å‡å€¼çš„å±æ€§
                    medianprops={"color":"#946279"}, # ä¸­ä½æ•°çº¿çš„å±æ€§
                    whiskerprops={"color":"#8E004D", "linewidth":0.4, 'linestyle':"--"},
                    flierprops={"markersize":0.4},
                    ) 
        # å›¾å
        plt.title(col_name, fontdict={"size":5}, pad=2)
        # yæ–¹å‘åˆ»åº¦
        plt.yticks(fontsize=4, rotation=90)
        plt.tick_params(pad=0.5)
        # xæ–¹å‘åˆ»åº¦
        plt.xticks([])
    plt.savefig(fig_name)
    plt.show()

boxplot(data, 'ml-vis5.pdf')
```

ç®±çº¿å›¾çš„å…·ä½“å«ä¹‰ï¼š

![image-20221212173925680](https://raw.githubusercontent.com/alec-97/alec-s-images-cloud/master/img2/image-20221212173925680.png)

ä»è¾“å‡ºç»“æœçœ‹ï¼Œæ•°æ®ä¸­å­˜åœ¨è¾ƒå¤šçš„å¼‚å¸¸å€¼ï¼ˆå›¾ä¸­ä¸Šä¸‹è¾¹ç¼˜ä»¥å¤–çš„ç©ºå¿ƒå°åœ†åœˆï¼‰ã€‚

ä½¿ç”¨å››åˆ†ä½å€¼ç­›é€‰å‡ºç®±çº¿å›¾ä¸­åˆ†å¸ƒçš„å¼‚å¸¸å€¼ï¼Œå¹¶å°†è¿™äº›æ•°æ®è§†ä¸ºå™ªå£°ï¼Œå…¶å°†è¢«ä¸´ç•Œå€¼å–ä»£ï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
# å››åˆ†ä½å¤„ç†å¼‚å¸¸å€¼
num_features=data.select_dtypes(exclude=['object','bool']).columns.tolist()

for feature in num_features:
    if feature =='CHAS':
        continue
    
    Q1  = data[feature].quantile(q=0.25) # ä¸‹å››åˆ†ä½
    Q3  = data[feature].quantile(q=0.75) # ä¸Šå››åˆ†ä½
    
    IQR = Q3-Q1 
    top = Q3+1.5*IQR # æœ€å¤§ä¼°è®¡å€¼
    bot = Q1-1.5*IQR # æœ€å°ä¼°è®¡å€¼
    values=data[feature].values
    values[values > top] = top # ä¸´ç•Œå€¼å–ä»£å™ªå£°
    values[values < bot] = bot # ä¸´ç•Œå€¼å–ä»£å™ªå£°
    data[feature] = values.astype(data[feature].dtypes)

# å†æ¬¡æŸ¥çœ‹ç®±çº¿å›¾ï¼Œå¼‚å¸¸å€¼å·²è¢«ä¸´ç•Œå€¼æ›¿æ¢ï¼ˆæ•°æ®é‡è¾ƒå¤šæˆ–æœ¬èº«å¼‚å¸¸å€¼è¾ƒå°‘æ—¶ï¼Œç®±çº¿å›¾å±•ç¤ºä¼šä¸å®¹æ˜“ä½“ç°å‡ºæ¥ï¼‰
boxplot(data, 'ml-vis6.pdf')
```

ä»è¾“å‡ºç»“æœçœ‹ï¼Œç»è¿‡å¼‚å¸¸å€¼å¤„ç†åï¼Œç®±çº¿å›¾ä¸­å¼‚å¸¸å€¼å¾—åˆ°äº†æ”¹å–„ã€‚

---

##### [âˆš] 2.5.1.3 - æ•°æ®é›†åˆ’åˆ†

ç”±äºæœ¬å®éªŒæ¯”è¾ƒç®€å•ï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸ºä¸¤ä»½ï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä¸åŒ…æ‹¬éªŒè¯é›†ã€‚

å…·ä½“ä»£ç å¦‚ä¸‹ï¼š

```python
import paddle

paddle.seed(10)

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
def train_test_split(X, y, train_percent=0.8):
    n = len(X)
    shuffled_indices = paddle.randperm(n) # è¿”å›ä¸€ä¸ªæ•°å€¼åœ¨0åˆ°n-1ã€éšæœºæ’åˆ—çš„1-D Tensor
    train_set_size = int(n*train_percent)
    train_indices = shuffled_indices[:train_set_size]
    test_indices = shuffled_indices[train_set_size:]

    X = X.values
    y = y.values

    X_train=X[train_indices]
    y_train = y[train_indices]
    
    X_test = X[test_indices]
    y_test = y[test_indices]

    return X_train, X_test, y_train, y_test 


X = data.drop(['MEDV'], axis=1)
y = data['MEDV']

X_train, X_test, y_train, y_test = train_test_split(X,y)# X_trainæ¯ä¸€è¡Œæ˜¯ä¸ªæ ·æœ¬ï¼Œshape[N,D]

```

---

##### [âˆš] 2.5.1.4 - ç‰¹å¾å·¥ç¨‹

ä¸ºäº†æ¶ˆé™¤çº²é‡å¯¹æ•°æ®ç‰¹å¾ä¹‹é—´å½±å“ï¼Œåœ¨æ¨¡å‹è®­ç»ƒå‰ï¼Œéœ€è¦å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œå°†æ•°æ®ç¼©æ”¾åˆ°[0, 1]åŒºé—´å†…ï¼Œä½¿å¾—ä¸åŒç‰¹å¾ä¹‹é—´å…·æœ‰å¯æ¯”æ€§ã€‚

ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import paddle

X_train = paddle.to_tensor(X_train,dtype='float32')
X_test = paddle.to_tensor(X_test,dtype='float32')
y_train = paddle.to_tensor(y_train,dtype='float32')
y_test = paddle.to_tensor(y_test,dtype='float32')

X_min = paddle.min(X_train,axis=0)
X_max = paddle.max(X_train,axis=0)

X_train = (X_train-X_min)/(X_max-X_min)

X_test  = (X_test-X_min)/(X_max-X_min)

# è®­ç»ƒé›†æ„é€ 
train_dataset=(X_train,y_train)
# æµ‹è¯•é›†æ„é€ 
test_dataset=(X_test,y_test)
```





---

#### [âˆš] 2.5.2 - æ¨¡å‹æ„å»º

å®ä¾‹åŒ–ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹ï¼Œç‰¹å¾ç»´åº¦ä¸º 12:

```python
from nndl.op import Linear

# æ¨¡å‹å®ä¾‹åŒ–
input_size = 12
model=Linear(input_size)
```



---

#### [âˆš] 2.5.3 - å®Œå–„Runnerç±»

æ¨¡å‹å®šä¹‰å¥½åï¼Œå›´ç»•æ¨¡å‹éœ€è¦é…ç½®æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ã€è¯„ä¼°ã€æµ‹è¯•ç­‰ä¿¡æ¯ï¼Œä»¥åŠæ¨¡å‹ç›¸å…³çš„ä¸€äº›å…¶ä»–ä¿¡æ¯ï¼ˆå¦‚æ¨¡å‹å­˜å‚¨è·¯å¾„ç­‰ï¼‰ã€‚

åœ¨æœ¬ç« ä¸­ä½¿ç”¨çš„**Runner**ç±»ä¸ºV1ç‰ˆæœ¬ã€‚å…¶ä¸­è®­ç»ƒè¿‡ç¨‹é€šè¿‡ç›´æ¥æ±‚è§£è§£æè§£çš„æ–¹å¼å¾—åˆ°æ¨¡å‹å‚æ•°ï¼Œæ²¡æœ‰æ¨¡å‹ä¼˜åŒ–åŠè®¡ç®—æŸå¤±å‡½æ•°è¿‡ç¨‹ï¼Œæ¨¡å‹è®­ç»ƒç»“æŸåä¿å­˜æ¨¡å‹å‚æ•°ã€‚

è®­ç»ƒé…ç½®ä¸­å®šä¹‰:
* è®­ç»ƒç¯å¢ƒï¼Œå¦‚GPUè¿˜æ˜¯CPUï¼Œæœ¬æ¡ˆä¾‹ä¸æ¶‰åŠï¼›
* ä¼˜åŒ–å™¨ï¼Œæœ¬æ¡ˆä¾‹ä¸æ¶‰åŠï¼›
* æŸå¤±å‡½æ•°ï¼Œæœ¬æ¡ˆä¾‹é€šè¿‡å¹³æ–¹æŸå¤±å‡½æ•°å¾—åˆ°æ¨¡å‹å‚æ•°çš„è§£æè§£ï¼›
* è¯„ä¼°æŒ‡æ ‡ï¼Œæœ¬æ¡ˆä¾‹åˆ©ç”¨MSEè¯„ä¼°æ¨¡å‹æ•ˆæœã€‚

åœ¨æµ‹è¯•é›†ä¸Šä½¿ç”¨MSEå¯¹æ¨¡å‹æ€§èƒ½è¿›è¡Œè¯„ä¼°ã€‚æœ¬æ¡ˆä¾‹åˆ©ç”¨é£æ¡¨æ¡†æ¶æä¾›çš„[MSELoss API](https://www.paddlepaddle.org.cn/documentation/docs/zh/api/paddle/nn/MSELoss_cn.html)å®ç°ã€‚

```python
import paddle.nn as nn
mse_loss = nn.MSELoss()
```

å…·ä½“å®ç°å¦‚ä¸‹ï¼š

```python
import paddle
import os
from nndl.opitimizer import optimizer_lsm

class Runner(object):
    def __init__(self, model, optimizer, loss_fn, metric):
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ä¸ºNone,ä¸å†å…³æ³¨

        # æ¨¡å‹
        self.model=model
        # è¯„ä¼°æŒ‡æ ‡
        self.metric = metric
        # ä¼˜åŒ–å™¨
        self.optimizer = optimizer
    
    def train(self,dataset,reg_lambda,model_dir):
        X,y = dataset
        self.optimizer(self.model,X,y,reg_lambda)

        # ä¿å­˜æ¨¡å‹
        self.save_model(model_dir)
    
    def evaluate(self, dataset, **kwargs):
        X,y = dataset

        y_pred = self.model(X)
        result = self.metric(y_pred, y)

        return result

    def predict(self, X, **kwargs):
        return self.model(X)
    
    def save_model(self, model_dir):
        if not os.path.exists(model_dir):
            os.makedirs(model_dir)
        
        params_saved_path = os.path.join(model_dir,'params.pdtensor')
        paddle.save(model.params,params_saved_path)

    def load_model(self, model_dir):
        params_saved_path = os.path.join(model_dir,'params.pdtensor')
        self.model.params=paddle.load(params_saved_path)

optimizer = optimizer_lsm

# å®ä¾‹åŒ–Runner
runner = Runner(model, optimizer=optimizer,loss_fn=None, metric=mse_loss)
```



---

#### [âˆš] 2.5.4 - æ¨¡å‹è®­ç»ƒ

åœ¨ç»„è£…å®Œæˆ`Runner`ä¹‹åï¼Œæˆ‘ä»¬å°†å¼€å§‹è¿›è¡Œæ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œæµ‹è¯•ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆå®ä¾‹åŒ–`Runner`ï¼Œç„¶åå¼€å§‹è¿›è¡Œè£…é…è®­ç»ƒç¯å¢ƒï¼Œæ¥ä¸‹æ¥å°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼Œç›¸å…³ä»£ç å¦‚ä¸‹ï¼š

```python
# æ¨¡å‹ä¿å­˜æ–‡ä»¶å¤¹
saved_dir = '/home/aistudio/work/models'

# å¯åŠ¨è®­ç»ƒ
runner.train(train_dataset,reg_lambda=0,model_dir=saved_dir)
```

æ‰“å°å‡ºè®­ç»ƒå¾—åˆ°çš„æƒé‡ï¼š

```python
columns_list = data.columns.to_list()
weights = runner.model.params['w'].tolist()
b = runner.model.params['b'].item()

for i in range(len(weights)):
    print(columns_list[i],"weight:",weights[i])

print("b:",b)

```

```python
CRIM weight: -6.7268967628479
ZN weight: 1.28081214427948
INDUS weight: -0.4696650803089142
CHAS weight: 2.235346794128418
NOX weight: -7.0105814933776855
RM weight: 9.76220417022705
AGE weight: -0.8556219339370728
DIS weight: -9.265738487243652
RAD weight: 7.973038673400879
TAX weight: -4.365403175354004
PTRATIO weight: -7.105883598327637
LSTAT weight: -13.165120124816895
b: 32.1007522583008
```

---

#### [âˆš] 2.5.5 - æ¨¡å‹æµ‹è¯•

åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼Œåœ¨æµ‹è¯•é›†ä¸Šå¾—åˆ°æ¨¡å‹çš„MSEæŒ‡æ ‡ã€‚

```python
# åŠ è½½æ¨¡å‹æƒé‡
runner.load_model(saved_dir)

mse = runner.evaluate(test_dataset)
print('MSE:', mse.item())
```

```
MSE: 12.345974922180176
```



---

#### [âˆš] 2.5.6 - æ¨¡å‹é¢„æµ‹

ä½¿ç”¨`Runner`ä¸­`load_model`å‡½æ•°åŠ è½½ä¿å­˜å¥½çš„æ¨¡å‹ï¼Œä½¿ç”¨`predict`è¿›è¡Œæ¨¡å‹é¢„æµ‹ï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
runner.load_model(saved_dir)
pred = runner.predict(X_test[:1])
print("çœŸå®æˆ¿ä»·ï¼š",y_test[:1].item())
print("é¢„æµ‹çš„æˆ¿ä»·ï¼š",pred.item())
```

```
çœŸå®æˆ¿ä»·ï¼š 33.099998474121094
é¢„æµ‹çš„æˆ¿ä»·ï¼š 33.04654312133789
```

---

