---
title: 4 - 前馈神经网络
categories:
  - 深度学习技术栈
  - 深度学习
  - 分支导航
  - 视频学习
  - 神经网络与深度学习 - 飞桨 - 复旦大学 - 邱锡鹏（NNDL蒲公英书）
  - 笔记
abbrlink: 2713595903
---

## [√] 4 - 课节4：前馈神经网络

### [√] 4.0 - 前馈神经网络概述

本节讲非线性的分类器，这个分类器主要就是神经网络。今天讲的第一种神经网络是前馈神经网络。

![image-20221215181003273](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122404.png)

---

### [√] 4.1 - 神经元

![image-20221215181534638](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122405.png)

![image-20221215181917766](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122406.png)

![image-20221215194456601](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122407.png)

偏置b的作用是：调节阈值，即达到什么程度就兴奋。

这个人工神经元可以看做是一个简单的线性模型。

这个神经元可以看做两部分，前半部分看做是收集信息，后半部分看做是一个非线性函数，用来将收集的信息映射到一个激活的状态上。

不同类型的神经元主要的区别其实就是在于激活函数怎么设计。

通常来讲a的取值范围是一个比z的取值范围更小的区域。

> 三种常用的激活函数

1. s型函数（sigmoid function）：比如sigmoid激活函数、logistic函数、tanh函数等
2. 斜坡函数（ramp function）：ReLU、leaky ReLU、ELU等
3. 复合函数：既带有s型函数的性质、又带有斜坡函数的性质

> S型函数

![image-20221215195938582](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122408.png)

比如logistic函数，值在0-1之间，模拟神经元的两种状态

tanh函数和logistic函数能够相互转换，因此这两种函数的能力基本上是等价的

tanh函数的能力比logistic函数的能力要好一些

logistic函数因为输出恒大于0，因此这个函数的输出作为输入的时候，会偏，因此对优化的性能不是很好。

比如在优化的时候，四个象限，logistic函数这种恒正的函数，只能在1、3象限优化，不能在第4象限优化，只能走之字形，因此效率会低。

![image-20221215195717382](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122409.png)

==解决这种非零中心化的方法==

1. 归一化到零中心化
2. 在函数的外面加一个可学习的参数 偏置b，缓解非零中心化带来的问题

![image-20221215195914705](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122411.png)

> 斜坡函数

代表函数是ReLU函数，也叫修正的线性单元

![image-20221215200026130](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122412.png)

这种函数非常简单，目前的神经网络中大量的使用这种激活函数，一般激活函数首选ReLU

![image-20221215200944694](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122413.png)

==ReLU函数的性质==

1. 计算简单
2. 生物学上的合理性，生物学家发现神经元兴奋的时候能够非常兴奋
3. 优化性质非常好，因此右边的导数是1，因此在优化的时候，不会太小，也不会太大，从而更加有效的来学习参数

==ReLU存在的问题==

死亡ReLU问题，因为左边为0，会导致梯度消失，神经元无法更新权重参数了。

==ReLU问题解决==

1. 通过归一化的方式来缓解死亡ReLU问题，使得数据的分散不要太集中。
2. 初始化参数的时候，避免全部都初始化为负值
3. 使用leaky ReLU，左边不要让其等于0，而是给一个很小的梯度
4. 同时也可以将γ这个参数变成可学习的，即带参数的ReLU

==ReLU函数的非零中心化问题==

> 复合激活函数

==swish函数==

![image-20221215213216501](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122414.png)

自门控函数，自己控制自己，`xσ(βx)`

这种函数能够通过变换β的值，实现在线性函数和ReLU函数之间变换，是一种非常灵活的函数

==高斯误差线性单元，GELU==

![image-20221215213639624](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122415.png)

p(X<x)的形状和s型函数是类似的，因此这个GELU函数的形状和swish函数的形状基本是一样的

目前在比较新的模型中基本都是用GELU作为激活函数，这种函数优化的性质相对来说要好一些

> 常见的激活函数和导数的总结

![image-20221215213848098](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122416.png)





---

### [√] 4.2 - 神经网络

logistic激活函数的输出在0-1之间，如果只是希望非线性就够了，那么就优先采用ReLU函数。

> logistic函数

![image-20221215214229668](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122417.png)

> ReLU函数

![image-20221215214249490](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122418.png)

> tanh函数

![image-20221215214312194](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122419.png)



不是所有的网络都是通过梯度下降的方式来更新参数，比如hopfield网络

![image-20221215214518942](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122420.png)

> 本课程会讲的3种网络

![image-20221215214610261](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122421.png)

记忆网络，有循环边，因此就会有状态的概念，即历史状态是什么

图网络一般是一组神经元，因此用方形的来表示。

这三种网络是分开讲的，但是在实际应用中，通常是不同的网络相互组合来用的。

神经网络主要是连接主义模型，区别于符号主义模型。符号主义中，知识或者信息是用符号来定义的，连接主义中是信息是存在连接上的

连接主义的模型是分布式并行处理网络，这种网络主要就是神经网络。

连接主义的三点：

- 由网络来共同表示信息，而不是像符号主义一样一个符号就表示一个信息
- 知识是定义在单元之间的连接上的，单元之间连接强度的改变可以来学习新的知识

神经网络就是一个典型的连接主义模型



---

### [√] 4.3 - 前馈神经网络

> 网络结构

![image-20221215220757911](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122422.png)

相邻层之间的神经元是全部两两连接（全连接）

单向传递

层内无连接

定义一个网络的层数的时候，不算输入层，因此上面的网络一共有3层

> 前馈网络

![image-20221215221101546](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122423.png)

> 信息传递过程

第l层的传递过程：

![image-20221215221524092](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122424.png)

![image-20221215224933824](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122425.png)

---

###### -> 通用近似定理

---

![image-20221215225311894](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122426.png)

---

###### -> 应用到机器学习

---

![image-20221215225634120](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122427.png)

---

###### -> 深层前馈神经网络

---

![image-20221215225737438](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122428.png)

前馈神经网络进行分类任务

---

###### -> 参数学习

---

![image-20221215230300503](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122429.png)

神经网络用作多分类任务，相当于最后一层设计C个神经元。



softmax这个激活函数和其它的激活函数的区别是，这个函数的输出内容不光和前一层的内容相关，还和同一层的其它神经元的内容相关

![image-20221215230415463](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122430.png)

---

###### -> 如何计算梯度

---

![image-20221215231932781](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122431.png)



---

### [√] 4.4 - 反向传播算法

---

###### -> 矩阵微积分

---

分母布局就是使用列向量表示

![image-20221215232449651](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122432.png)

---

###### -> 链式法则

---

![image-20221215232719986](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122433.png)

---

###### -> 计算梯度

---

![image-20221215233211582](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122434.png)

![image-20221215233508059](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122435.png)

上面的1、3项已经有了，核心是计算第二项。第二项定义为第L层的误差项。

![image-20221215233946968](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122436.png)

通过上面的推导能够看出，第L层的导数，能够通过第L+1层的导数以及第L+1的权重等推导出，因此这里得到反向传播的链式法则。

即前面的层能够通过后面的层推出，因此从最后一层开始，能够逐层的推导出前面层的导数（梯度）。然后通过比如梯度下降算法优化权重参数，就能够进行模型训练。

![image-20221215234352611](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122437.png)

由上图看出，损失函数对第L层的w和b的导数，能够通过第L+1层的导数和第L-1层的激活层求出。

---

###### -> 使用反向传播算法的随机梯度下降训练过程

---

![image-20221215234604104](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122438.png)

先前馈计算每一层的Z_l和A_l，其中A_l最后在梯度的反向传播计算的时候是需要用到的。

然后反向传播计算每一层参数的导数

最后用梯度下降算法更新参数

直到在验证集V上的错误率不再下降则停止训练

最后得到w和b

---

### [√] 4.5 - 计算图与自动微分

---

![image-20221217102433490](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122439.png)

---

###### -> 计算图与自动微分

---

![image-20221217103114468](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122440.png)

---

###### -> 计算图

---

![image-20221217103303107](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122441.png)

通过上面的这种链式的计算方法，框架能够自动计算梯度，因此就不用人工计算，非常方便。

---

###### -> 自动微分

---

![image-20221217103834792](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122442.png)

前向模式和反向模式的不同在于，前向模式由于是从前往后计算梯度链式法则中的每项，因此在计算的过程中需要保留中间项，如果链式非常长的话，那么需要保留的中间项非常多；因此一般使用反向模式。

---

###### -> PyTorch例子

---

![image-20221217103921199](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122443.png)

---

###### -> 反向传播算法（自动微分的反向模式）

---

![image-20221217104135791](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122444.png)

---

###### -> 静态计算图和动态计算图

---

![image-20221217104542264](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122445.png)

静态计算图的计算效率比较高

动态计算图更加的灵活

---

###### -> 如何实现

---

![image-20221217104633188](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122446.png)

![image-20221217104843147](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122447.png)

keras在tensorflow的基础上，又进行了一次封装

---

###### -> 深度学习的三个步骤

---

![image-20221217104950753](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122448.png)



---

### [√] 4.6 - 优化问题

---

---

###### -> 神经网络优化问题之非凸优化问题

---

![image-20221217105432953](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122449.png)

非凸函数，优化困难，比如存在局部最优问题，局部最小值如何再找到全局最小值是非常困难的。

另外，在高维中存在鞍点问题，使用梯度下降方法到了鞍点就走不动了

---

###### -> 神经网络优化问题之梯度消失问题

---

![image-20221217105910303](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122450.png)

当梯度的链式很长，每个因子都在0-1之间，那么最后梯度整体就非常小，非常接近于0，这就是梯度消失问题，会导致更新很慢、很难学。

因此让激活函数最后在1左右是最好的，不能太小，也不能太大。这也是为什么激活函数推荐使用ReLU函数。因为ReLU函数在正的范围梯度是1.

---

###### -> 优化问题

---

![image-20221217110122249](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202212212122451.png)





---





