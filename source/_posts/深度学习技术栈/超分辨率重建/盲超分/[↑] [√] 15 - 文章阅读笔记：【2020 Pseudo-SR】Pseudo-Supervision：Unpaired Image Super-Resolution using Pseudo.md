---
title: >-
  文章阅读笔记：【2020 Pseudo-SR】Pseudo-Supervision：Unpaired Image Super-Resolution
  using Pseud
index_img: >-
  https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261638826.png
tags:
  - 盲超分
  - 无监督
password: 972274
categories:
  - 深度学习技术栈
  - 超分辨率重建
  - 盲超分
abbrlink: 2574956691
date: 2023-02-26 14:47:15
---

> 原文链接：
>
> （1）【√】图像超分辨率 之 Unpaired Image Super-Resolution using Pseudo-Supervision 论文解读与感想 - 知乎 - 涑月听枫（link）
>
> 编辑于 2021-03-06 16:44
>
> ps：本文为依据个人日常阅读习惯，在原文的基础上记录阅读进度、记录个人想法和收获所写，关于原文一切内容的著作权全部归原作者所有。



#  [√] 文章信息

---

论文标题：Pseudo-Supervision：Unpaired Image Super-Resolution using Pseudo-Supervision

中文标题：使用伪监督进行未配对的图像超分

论文链接：https://arxiv.org/abs/2002.11397

论文代码：https://github.com/yoon28/pseudo-sr

论文发表：CVPR 2020



# [√] 文章1

---

> 总结：
>
> 【本文思想】
>
> - 本文是使用GAN网络来进行图像生成的。
> - 本文方法不需要成对的数据来训练，只需要不配对的LR图像数据集和HR图像数据集。
> - 本文是受风格转换中的CycleGAN启发的。
> - 一个GAN用来生成和真实的自然LR相近的数据
> - 一个GAN用来生成和真实HR下采样之后的图像相近的数据
> - 
>
> 【本文贡献】
>
> 【网络结构】
>
> ![img](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647186.png)
>
> （图3：本文所提出方法的数据流向图。SR网络能够以一种成对的数据方法进行训练，即使训练数据X和Y不是成对的。整个网络是端到端可训练的。）
>
> 【可以用于自己论文的话】
>
> - “GAN网络的一个特殊优势在于，它可以直接将输入输入进行端到端的训练，而不需要对其二者的关系进行预先假定，这对于拟合未知下采样核简直再适合不过。因此接下来，就是如何组合运用这个网络结构。”
> - “通过这种将无配对LR图像和HR图像结合起来互相转换的方法，训练出的模型并不是拟合了某一固定的上采样模式，因此在真实场景应用时，依然可以保持优秀的性能。”
>
> 【可以用于自己论文的idea】
>
> - 本文的网络结构和训练方式是一个很好的参考，非常的适用于自己的模型。自己的模型可以参考借鉴这个模型。然后在这个模型的基础上创新。本文训练方式总结来说：就是左边的两个GAN网络让G_XY↓网络拥有去除自然噪声的能力。然后再通过右边的第三个GAN网络让其能够将G_XY↓(x_自然)生成高分辨率的Y空间的数据的能力。
>
> 
>
> 【问题记录】
>
> 【零碎点】
>
> - 相比于本年其它文章，本文最大的特点就是实现了真正的未配对数据训练。



## [√] 引言

---

本文是某不太知名公司（Navier Inc.）发布于2020人工智能顶会（CVPR）的一篇图像超分辨率文章，相比较于本年发布的其他图像SR文章，本文最大的特点就是真正的实现了unpaird data训练。

![main image](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647187.PNG)

（图3：本文所提出方法的数据流向图。SR网络能够以一种成对的数据方法进行训练，即使训练数据X和Y不是成对的。整个网络是端到端可训练的。）



## [√] 一、文章切入点

---

本文提出当前的训练方式大多是通过使用HR图像来人为下采样构建LR图像，获得成对的训练图像进行网络训练。由这种方法获得的成对训练图像训练得的网络已经被很多研究吐槽过，尽管在生成的成对图像上效果很好，但是在实际应用时（或者是换成其他下采样方法获得的LR图像）却烂的一塌糊涂。造成这种情况，归根结底还是因为我们使用的训练图像仅仅是某种（或者某几种的组合）下采样模式，然而实际上的下采样模型是根本未知的。**为了模拟这种未知的下采样模式，理所当然的就要应用万能的神经网络。**

GAN网络的一个特殊优势在于，它可以**直接将输入输入进行端到端的训练，而不需要对其二者的关系进行预先假定，**这对于拟合未知下采样核简直再适合不过。因此接下来，就是如何组合运用这个网络结构。



## [√] 二、文章创新点

---

本文提出一种全新的SR框架，这种框架不需要成对的LR图像和HR图像，只需要两类图像（就是不配对的LR图像数据集和HR图像数据集）就可进行网络的训练。*接触过图像风格转换（Image-to-Image Translation）的孩子乍一听，很容易就会将其和图像风格转换中一个经典模型CycleGAN联系起来，没错，本文就是以此为启发设计的，并且模型中也确实用到了cyclegan的部分设置。*通过这种将无配对LR图像和HR图像结合起来互相转换的方法，训练出的模型并不是拟合了某一固定的上采样模式，因此在真实场景应用时，依然可以保持优秀的性能。



## [√] 三、模型具体实现方法和原理

---

通过本文开头的图片可以看出，这个框架是**六个网络联合优化**。

乍一看有点复杂，但是在理清楚他们各自的作用和互相之间的关系后，就会发现它们环环相扣，非常巧妙：

- 首先模型有两个输入，一个是真实LR图像，一个是真实HR图像，我们可以将其表示为 x(∈X) 和 y(∈Y) ，之后便是对二者进行分别变换。
- 先看HR图像y，y经过**一个人为的**下采样模块后，成为了一个人工LR图像，我们可以将其表示为y↓(∈Y↓) ，这个下采样模块是为了锚定一个固定的下采样空间，便于后面在这个空间进行上采样的变换。
- 之后，我们会通过一个转换网络 G(Y↓,X) 将 y↓ 转换为 X 空间内的样式，我们称之为 G(Y↓,X)(y↓) ，为了判断 G(Y↓,X) 转换的是否正确，我们就要用到一个判别器 D(X) ，输入G(Y↓,X)(y↓) 和 x 对其进行判别，到这里我们知道了**G(Y↓,X) 和 D(X) 组成一个生成对抗网络。**
- 接下来再往下走，是 G(X,Y↓) 网络，这个网络的作用是将 X 空间的图像转换为 Y↓ 空间，我们将通过它的 x 称之为 G(X,Y↓)(x) ，那么怎么来确保这个网络按照我们的设想来运行呢？很显然我们需要另一个判别器，我们将其称之为 D(Y↓) 。通过将 G(Y↓,X)(y↓) 和 y↓ 输入到判别器中，就可以使得这个生成器按照我们预先的想法工作。**则**G(X,Y↓) **和** D(Y↓) **又组成一个生成对抗网络。**

- 到这里我们已经分析了两个生成网络和两个鉴别网络，不知道大家有没有发现，**这四个网络组成一个闭环，刚好就是我们开头所说的CycleGAN的结构！**

- 事实上，在训练过程中，作者也的确将其按照CycleGAN来训练的， y↓ 通过一个完整的循环后的结果称之为 y°↓ （这个我个人觉得这个y圈↓是真的形象，y↓从Y空间到X空间又到Y空间，正好转了一圈），而这两个量，就可以求经典的cycle一致性损失：

![image-20230226162817377](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647188.png)

此外作者为了减少目标空间，还引入了一种最近提出的损失——集合整体损失：

![image-20230226162836034](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647189.png)

（这种损失个人认为可以看做一种数据增强，类似于一个数据算了8次，事实上原文也说了，这个损失的添加使得整个网络的训练时间增加了1.5倍）通过将以上提到的5个损失联合优化，就可以对四个网络进行训练了。

---

> 其实到这里，整个网络的主体部分已经介绍完了，然而还有两个网络，我们再来依次说一下。首先是上采样网络 � ，这个网络将 �↓ 空间中的LR图像映射为 � 空间中的图像，说的直观一些，这个 � 实际上就是一个最开始人为下采样的一个逆映射，你也可以将其看成是一个生成器，那么下面的最后一个网络 �(�↑) 就很好理解了，它就是这个 � 的鉴别器，鉴别输入 � 的图像是来自 �(�,�↓)(�) 还是来自 ◦�(�,�↓)◦�(�↓�)(�↓) 。之后求了一个 � 的重建损失：
>
> ![image-20230226163100351](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647190.png)

![image-20230226163136502](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647191.png)

来保证 U 的输出空间为 Y 。

这里需要说一下的是，作者将鉴别器的损失：

![image-20230226163211017](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647192.png)

> 并到了前面的修正网络进行 �(�↓,�) 和 �(�,�↓) 的参数更新，而对于 � 的重建损失，不仅对其自身进行更新，也会对前面的修正网络进行更新。
>
> ![image-20230226163244807](https://cdn.jsdelivr.net/gh/Alec-97/alec-s-images-cloud/img/202302261647193.png)

## [√] 四、个人感想

---

- 本文提出的框架非常的灵活：首先，从前面的分割线就可以看出来，这个框架完全可以拆成两个独立的模块互相独立训练，并且最后再将其组合起来。
- 并且对于每个模块的优化，可以单独进行。无论是最新的风格转换模型，还是LR—HR模型，都可以吸收进本文框架，进一步提高性能。此外，也可以直接将已训练好的上采样模型直接应用上来，以减少工作量。不过这样做的一个注意点是，要事先知道上采样模型是用什么下采样方法获得的配对数据训练的，否则还是要乖乖的从头训练。
- 其次，本框架真正实现了无配对+盲的训练方法，从与配对训练方法来比较，可以看出本文方法结果已经非常接近配对结果的上边界。





## [√] 评论

---

解释的很好，谢谢

问一下，为什么说重建损失不仅只更新U还会更新前面的去噪网络？因为3.1节损失函数在倒数第二个部分表明了生成器判别器的总损失，我觉得重建损失应该是只用于更新U超分网络的。Full objective. Our full objective for the two generators and three discriminators is as follows:XXXXX

请问有代码吗

很抱歉，作者暂时没有发布代码，我给作者发过两次邮件，也没有要到





